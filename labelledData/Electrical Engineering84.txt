('tla', 34);('rl', 21);('fast controller', 16);('reinforcement learning', 16);('slow controller', 15);('response time', 13);('continuous', 12);('slow policy', 12);('additionally', 11);('layered architecture', 11);('adaptive', 11);('distributed', 11);('action repetition', 11);('td3', 11);('fast action', 10);('fast policy', 9);('arxiv', 9);('tla-o', 8);('tla-c', 8);('temporl', 8);('fast network', 6);('mountaincarcontinuous-v0', 6);('learning', 6);('invertedpendulum-v2', 6);('usa', 5);('task horizon', 5);('ai', 5);('nal action', 5);('slow network', 5);('pendulum-v1', 5);('massachusetts amherst amherst', 4);('ma', 4);('open loop control', 4);('openai', 4);('figure', 4);('macro actions', 4);('fig', 4);('reinforcement', 4);('mdp', 4);('pr', 4);('processing time', 4);('biedenkapp', 4);('auc', 4);('mujoco', 4);('deep reinforcement learning', 4);('r. s. sutton', 4);('continuous control tasks', 3);('complex environments', 3);('temporally layered architecture', 3);('real-time setting', 3);('st', 3);('real-time environment', 3);('unit time', 3);('mnih', 3);('goal state', 3);('different response times', 3);('residual', 3);('slow agent', 3);('standard deviation', 3);('optimal policy', 3);('average return', 3);('average', 3);('action repetition percentage', 3);('temporal', 3);('icml', 3);('aaai', 3);('nips', 3);('b. ravindran', 3);('d. silver', 3);('d. wierstra', 3);('information', 2);('jolla', 2);('ca', 2);('temporal abstraction', 2);('environments demands', 2);('closed-loop', 2);('closed-loop control', 2);('processing inputs', 2);('closed', 2);('loop control', 2);('time step', 2);('fast actions', 2);('optimal value', 2);('different environments', 2);('braylan', 2);('brains design', 2);('recent work', 2);('nakahira', 2);('different response frequencies', 2);('different temporal context', 2);('s\x02a', 2);('time t.', 2);('actor-critic methods', 2);('silver', 2);('q-function', 2);('sutton', 2);('learning rate', 2);('fujimoto', 2);('agent needs', 2);('ramstedt', 2);('pal', 2);('state space', 2);('response speed', 2);('mcgovern', 2);('deep', 2);('neural network', 2);('important states', 2);('multiple controllers', 2);('hansen', 2);('action sequences', 2);('sharma', 2);('atari', 2);('continuous domain', 2);('lazy-mdps', 2);('sub-optimal base policy', 2);('frequency network', 2);('mdps', 2);('katsikopoulos', 2);('reaction time', 2);('firoiu', 2);('real-time control tasks', 2);('chen', 2);('options framework', 2);('options', 2);('successor representation', 2);('invertedpendulum', 2);('reward penalty', 2);('openai gym', 2);('neural networks', 2);('pytorch', 2);('pendulum', 2);('repetitions', 2);('adam', 2);('default timestep', 2);('learning curves', 2);('stable learning', 2);('future work', 2);('compute savings', 2);('ar', 2);('response times', 2);('average action frequency', 2);('real-time environments', 2);('fast time-step', 2);('brains ability', 2);('multiple timescales', 2);('multiple motor plans', 2);('future research', 2);('technical', 2);('continuous control', 2);('g. ostrovski', 2);('a. barreto', 2);('b. tenenbaum', 2);('a. gupta', 2);('s. levine', 2);('a. g. barto', 2);('frontiers', 2);('experimental biology', 2);('robot control', 2);('international conference', 2);('robotics', 2);('corr', 2);('n. m. o. heess', 2);('t. erez', 2);('tassa', 2);('d. precup', 2);('m. a. riedmiller', 2);('neurips', 2);('s. sharma', 2);('a. srinivas', 2);('temporally layered architecture for adaptive', 1);('distributed and continuous control devdhar patel', 1);('devdharpatel @ cs.umass.edujoshua', 1);('russell', 1);('computer', 1);('jgrussell @ cs.umass.edu', 1);('francesca walsh', 1);('fnwalsh @ umass.edutauhidur', 1);('rahman', 1);('california san diego', 1);('trahman @ ucsd.eduterrance', 1);('sejnowski salk', 1);('biological studies', 1);('terry @ salk.edu', 1);('hava siegelmann', 1);('hava @ cs.umass.edu', 1);('abstract', 1);('different time-scale', 1);('human brain', 1);('executes actions', 1);('different timescales', 1);('control design', 1);('biological systems', 1);('increases survivability', 1);('uncertain environments', 1);('persistent exploration', 1);('adaptive control', 1);('explainable temporal behavior', 1);('compute efciency', 1);('different algorithms', 1);('partially', 1);('open loop-control', 1);('strong baselines', 1);('keywords reinforcement learning', 1);('control\x01real-time\x01continuous', 1);('introduction deep', 1);('remarkable success', 1);('control tasks', 1);('real time games', 1);('constant frequency', 1);('average human response time', 1);('response frequency increases', 1);('energy consumption', 1);('fast', 1);('jerky behaviour', 1);('response time increases', 1);('agent.arxiv:2301.00723v1 [ cs.ne ]', 1);('dec', 1);('fast controller acts', 1);('long actions', 1);('multiple time-steps', 1);('response time forces', 1);('continuous-time environments', 1);('might lead', 1);('poor performance', 1);('optimizing', 1);('different actions', 1);('action sequence', 1);('important hyper-parameter', 1);('different situations', 1);('unfamiliar complex environments', 1);('speed/accuracy trade-off', 1);('heitz', 1);('multiple independent systems', 1);('multiple layers', 1);('biological neural network', 1);('control muscle groups', 1);('complex behaviors', 1);('central nervous system', 1);('situation demands', 1);('inspired', 1);('biological design', 1);('reinforcement learning architecture', 1);('different networks', 1);('adaptive behavior', 1);('exponential increase', 1);('constant response frequency', 1);('response frequency', 1);('abstract hierarchical temporal knowledge', 1);('different time-frames', 1);('fog', 1);('2.we introduce', 1);('different advantage', 1);('3.we experiment', 1);('open loop setting', 1);('comparable performance', 1);('previous work', 1);('control 4.we', 1);('show robust performance', 1);('different delay', 1);('background', 1);('standard reinforcement learning setting', 1);('markov decision process', 1);('sis', 1);('ais', 1);('s\x02a\x02s', 1);('transition function', 1);('ris', 1);('reward function', 1);('initial state distribution', 1);('discount factor', 1);('policy \x19', 1);('at=ajst=s', 1);('optimal policy \x19\x03', 1);('1x t=0 trtj\x19 #', 1);('=e [', 1);('gtj\x19', 1);('time t', 1);('policy gradient algorithm', 1);('=e [ r \x19', 1);('ja=\x19 s ]', 1);('gtjst=s', 1);('at=a', 1);('action-value function', 1);('action aat statesand', 1);('policy \x19thereafter', 1);('temporal difference learning', 1);('barto', 1);('+ \x02', 1);('rt+', 1);('max aq', 1);('st+1', 1);('q-learning', 1);('differentiable function approximator', 1);('q\x12', 1);('deterministic policy gradient', 1);('q-functions', 1);('pessimistic value', 1);('delayed reinforcement learning', 1);('compensatory policies', 1);('spinal cord reexes', 1);('biological agents', 1);('sandrini', 1);('such sensory processing delays', 1);('detrimental damage', 1);('processing states', 1);('action frequency', 1);('input state', 1);('state st+1', 1);('real-time markov reward process', 1);('rtmrp', 1);('x=s\x02a', 1);('environment transition function', 1);('current state', 1);('at=a0jst\x001=s', 1);('at\x001=a', 1);('fast actor', 1);('action twill persist', 1);('shorter period', 1);('timestep', 1);('agents timestep', 1);('fast changes', 1);('constant response time', 1);('control problem', 1);('agents performance', 1);('reinforcement learning task', 1);('environmental changes', 1);('agent slices', 1);('action-value propagation', 1);('slow convergence', 1);('optimal performance', 1);('faster', 1);('energy expenditure', 1);('size', 1);('reinforcement learning algorithms', 1);('experience replay memory', 1);('small memory size', 1);('response speed increases', 1);('ip side', 1);('memory size', 1);('efcient memory use', 1);('size/network complexity', 1);('agent uses', 1);('neural network size', 1);('policys complexity', 1);('small neural network', 1);('simple policy', 1);('complex policies', 1);('policy complexity increases', 1);('distribution', 1);('reward', 1);('certain state', 1);('failure state', 1);('temporal density', 1);('reward/state transitions', 1);('total state transitions increase', 1);('especially', 1);('sparse rewards', 1);('positive reward', 1);('zero reward', 1);('zero-reward state-action transitions', 1);('mountain-car problem', 1);('moore', 1);('related', 1);('frame-skipping', 1);('partial open-loop control', 1);('intermediate states', 1);('mixed-loop control setting', 1);('possible action sequences', 1);('length lis exponential', 1);('area focuses', 1);('possible number', 1);('tan', 1);('mccallum', 1);('ballard', 1);('exponential number', 1);('possible actions', 1);('lawrence', 1);('kalyanakrishnan', 1);('srinivas', 1);('additional action-repetition policy', 1);('time steps', 1);('action decision points', 1);('experiments section', 1);('randlv', 1);('dqn', 1);('additional exploration', 1);('recently', 1);('yu', 1);('closed-loop temporal abstraction method', 1);('state-action value', 1);('decision networks', 1);('layered rl recently', 1);('jacq', 1);('base policy', 1);('agent chooses', 1);('similarly', 1);('continuous environments', 1);('residual policy', 1);('johannink', 1);('residual rl', 1);('temporal residual learning', 1);('residual learning', 1);('delayed-aware', 1);('brooks', 1);('leondes', 1);('engelbrecht', 1);('observation delays', 1);('action delays', 1);('travnik', 1);('human reaction time', 1);('predictive model', 1);('delay-aware versions', 1);('soft-actor-critic algorithm', 1);('haarnoja', 1);('similar setup', 1);('precup', 1);('common framework', 1);('3-tuples hi', 1);('whereiis', 1);('initiation states', 1);('option policy', 1);('option termination', 1);('machado', 1);('connectedness graph', 1);('chaganty', 1);('similar vein', 1);('dabney', 1);('methods', 1);('different timesteps', 1);('different temporal information', 1);('fast agent', 1);('invertedpendulum-v2 figure', 1);('region shows', 1);('action combination function c', 1);('a\x02a', 1);('rthat', 1);('cas', 1);('maximum action', 1);('slow action', 1);('af', 1);('t=afjas t=as', 1);('st=s', 1);('combining', 1);('slow network behaviour promotes', 1);('reward states', 1);('identify emergency states', 1);('jerky behavior', 1);('long smooth actions', 1);('zero vector', 1);('above condition', 1);('action dimension', 1);('different joint', 1);('1.fast action penalty', 1);('non-zero value', 1);('unseen state transitions', 1);('drastic changes', 1);('maximize return', 1);('fast policy stabilizes training', 1);('negative reward proportional', 1);('fast policy output', 1);('multiple dimensions', 1);('average magnitude', 1);('2.fast state augmentation', 1);('pa', 1);('output action', 1);('often', 1);('combination function', 1);('possible action', 1);('fast policy inputs', 1);('environment auc avg', 1);('return td3 temporl tla-o tla-c td3 temporl tla-o tla-c pendulum', 1);('mountaincarcontinuous', 1);('average return results', 1);('loop forms', 1);('average action frequency w.r.t', 1);('average actions', 1);('open-loop', 1);('binary gate output g2f0', 1);('at=gt\x01as t+', 1);('\x01af t', 1);('whereaf tis', 1);('fast controller computation', 1);('naive approach', 1);('\x01rt\x012 ifrt\x140 gt\x01rt+', 1);('experiments', 1);('brockman', 1);('paszke', 1);('turn-based environments open-loop', 1);('hard problem', 1);('previous works', 1);('discrete environments', 1);('step sizes', 1);('continuous environment', 1);('adversarial example', 1);('continuous control environments', 1);('classic control problems', 1);('environment', 1);('td3 temporl tla-o tla-c td3 temporl tla-o tla-c pendulum', 1);('critic networks', 1);('kingma', 1);('ba', 1);('two-layer neural networks', 1);('random policy', 1);('noise ofn', 1);('ddpg', 1);('lillicrap', 1);('training convergence speed', 1);('average area', 1);('difcult task', 1);('policy converges', 1);('surprisingly', 1);('sub-optimal policy', 1);('zero action', 1);('tla-os', 1);('eq', 1);('5.1.1 action', 1);('action repetition percentage demonstrates', 1);('long action', 1);('good measure', 1);('average number', 1);('different policies', 1);('open-loop algorithm', 1);('complete information', 1);('adaptive compute capability', 1);('5.1.2 threshold', 1);('return', 1);('different controllers', 1);('examples', 1);('environment tla td3 auc return ar auc return ar inverteddoublependulum-v2', 1);('hopper-v2', 1);('walker2d-v2', 1);('similar plots', 1);('particular case', 1);('maximum possible reward', 1);('interestingly', 1);('frequency range', 1);('temporal generalization gap', 1);('performance deteriorates', 1);('maximize rewards', 1);('different system', 1);('biological reexes', 1);('control policy', 1);('layer focuses', 1);('different reaction time', 1);('previous action', 1);('fair comparison', 1);('original time-step', 1);('slow layer time-step', 1);('\x02max action', 1);('individual environment', 1);('equal average return', 1);('complex continuous environment', 1);('closed-loop variant', 1);('supplementary material', 1);('discussion', 1);('utilize environmental information', 1);('future planning', 1);('environments energy constraints', 1);('task demands', 1);('balance reexes', 1);('motor movements', 1);('efcient responses', 1);('tlas', 1);('detect emergency situations', 1);('unknown environments', 1);('long periods', 1);('process environmental information', 1);('human decision-making systems', 1);('van der meer', 1);('process sensory information', 1);('large array', 1);('intelligent design', 1);('biological designs', 1);('neural architectures', 1);('adaptive systems', 1);('affordance competition hypothesis cisek', 1);('specic theory', 1);('motor planning', 1);('future designs', 1);('new architectures', 1);('mammalian brain creates', 1);('motor plan', 1);('theory predicts', 1);('action plans', 1);('action costs', 1);('environmental information', 1);('model predicts', 1);('animal chooses', 1);('appropriate action', 1);('animals goal', 1);('uid movement plan', 1);('new prey', 1);('brain creates', 1);('comparison competition', 1);('motor plans cost/benet ratio', 1);('environmental demands', 1);('human gait switches', 1);('kram', 1);('phenomenon results', 1);('relative locomotion speed', 1);('body sizes', 1);('iriarte-daz', 1);('important step', 1);('shorter task horizon', 1);('temporal details', 1);('only activate', 1);('urgent action', 1);('urgent situations', 1);('knowledge distillation', 1);('aid learning', 1);('in-depth exploration', 1);('early-exit architectures', 1);('scardapane', 1);('early-exit scenario', 1);('real-time control setting', 1);('network needs', 1);('different times', 1);('own processing', 1);('actuation delay', 1);('action needs', 1);('real-time experiments', 1);('optimal action', 1);('conclusion', 1);('adaptive response time', 1);('smooth control', 1);('fast controller monitors', 1);('alternative setting', 1);('efcient control', 1);('closed-loop approach', 1);('partially-open loop approach', 1);('real time setting', 1);('actuation delays', 1);('approach outperforms', 1);('current approaches', 1);('work demonstrates', 1);('adaptive approach', 1);('similar benets', 1);('important direction', 1);('intelligent control', 1);('acknowledgments', 1);('advanced', 1);('projects', 1);('darpa', 1);('agreement', 1);('hr00112190041', 1);('references a. biedenkapp', 1);('r. rajan', 1);('f. hutter', 1);('m. t. lindauer', 1);('a. braylan', 1);('m. hollenbeck', 1);('e. meyerson', 1);('r. miikkulainen', 1);('frame', 1);('powerful parameter', 1);('competency', 1);('video', 1);('g. brockman', 1);('cheung', 1);('l. pettersson', 1);('j. schneider', 1);('j. schulman', 1);('j. tang', 1);('w. zaremba', 1);('d. m. brooks', 1);('c. t. leondes', 1);('markov decision processes', 1);('oper', 1);('res', 1);('k. m. buckland', 1);('p. d. lawrence', 1);('transition', 1);('a. t. chaganty', 1);('p. gaur', 1);('small world', 1);('aamas', 1);('b. chen', 1);('m. xu', 1);('l. li', 1);('d. zhao', 1);('delay-aware', 1);('neurocomputing', 1);('p. cisek', 1);('cortical', 1);('action selection', 1);('affordance competition hypothesis', 1);('philosophical transactions', 1);('royal society b', 1);('biological', 1);('w. dabney', 1);('temporally-extended', 1);('\x0f-greedy exploration', 1);('t. ju', 1);('human speed', 1);('action delay', 1);('s. fujimoto', 1);('h.', 1);('hoof', 1);('d. meger', 1);('addressing', 1);('function approximation error', 1);('t. haarnoja', 1);('a. zhou', 1);('k. hartikainen', 1);('g. tucker', 1);('s. ha', 1);('j. tan', 1);('kumar', 1);('h. zhu', 1);('p. abbeel', 1);('soft', 1);('actor-critic algorithms', 1);('e. a. hansen', 1);('s. zilberstein', 1);('r. p. heitz', 1);('speed-accuracy tradeoff', 1);('j. iriarte-daz', 1);('differential', 1);('locomotor performance', 1);('large terrestrial mammals', 1);('pt', 1);('a. jacq', 1);('j. ferret', 1);('o. pietquin', 1);('m. geist', 1);('towards', 1);('interpretable reinforcement learning', 1);('t. johannink', 1);('s. bahl', 1);('a. nair', 1);('j. luo', 1);('a. kumar', 1);('m. loskyll', 1);('a. ojea', 1);('e. solowjow', 1);('automation', 1);('icra', 1);('s. kalyanakrishnan', 1);('s. aravindan', 1);('bagdawat', 1);('bhatt', 1);('h. goka', 1);('k. krishna', 1);('piratla', 1);('k. v', 1);('s. e. engelbrecht', 1);('markov', 1);('decision processes', 1);('ieee trans', 1);('autom', 1);('d. p. kingma', 1);('j. ba', 1);('stochastic optimization', 1);('r. kram', 1);('a. domingo', 1);('d. p. ferris', 1);('effect', 1);('walk-run transition speed', 1);('t. p. lillicrap', 1);('j. j', 1);('hunt', 1);('a. pritzel', 1);('m. c. machado', 1);('a. mccallum', 1);('d. h. ballard', 1);('selective perception', 1);('a. mcgovern', 1);('a. h. fagg', 1);('roles', 1);('k. kavukcuoglu', 1);('a. rusu', 1);('j. veness', 1);('m. g. bellemare', 1);('a. graves', 1);('a. fidjeland', 1);('s. petersen', 1);('c. beattie', 1);('a. sadik', 1);('i. antonoglou', 1);('h. king', 1);('d. kumaran', 1);('s. legg', 1);('d. hassabis', 1);('human-level', 1);('nature', 1);('a. w. moore', 1);('efcient', 1);('cambridge', 1);('q. liu', 1);('t. j. sejnowski', 1);('j. c. doyle', 1);('diversity-enabled', 1);('sweet spots', 1);('speedaccuracy trade-offs', 1);('sensorimotor control', 1);('proceedings', 1);('national academy', 1);('accessed:2022-08-12', 1);('a. paszke', 1);('s. gross', 1);('f. massa', 1);('a. lerer', 1);('j. bradbury', 1);('g. chanan', 1);('t. killeen', 1);('z. lin', 1);('n. gimelshein', 1);('l. antiga', 1);('a. desmaison', 1);('a. kopf', 1);('e. yang', 1);('z. devito', 1);('m. raison', 1);('a. tejani', 1);('s. chilamkurthy', 1);('b. steiner', 1);('l. fang', 1);('j. bai', 1);('s. chintala', 1);('imperative style', 1);('learning library', 1);('h. wallach', 1);('h. larochelle', 1);('a. beygelzimer', 1);('f.', 1);("d 'alch-buc", 1);('e. fox', 1);('r. garnett', 1);('advances', 1);('neural information processing systems', 1);('curran associates', 1);('url', 1);('//papers.neurips.cc/ paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf', 1);('s. ramstedt', 1);('c. pal', 1);('real-time', 1);('j. randlv', 1);('g. sandrini', 1);('m. serrao', 1);('p. rossi', 1);('a. romaniello', 1);('g. cruccu', 1);('j. c. willer', 1);('limb exion reex', 1);('s. scardapane', 1);('m. scarpiniti', 1);('e. baccarelli', 1);('a. uncini', 1);('cognitive computation', 1);('g. lever', 1);('t. degris', 1);('deterministic', 1);('policy gradient algorithms', 1);('t. silver', 1);('k. r. allen', 1);('l. p. kaelbling', 1);('policy learning', 1);('dynamic', 1);('mit', 1);('m. tan', 1);('cost-sensitive', 1);('adaptive classication', 1);('e. todorov', 1);('physics engine', 1);('ieee/rsj', 1);('intelligent robots', 1);('systems', 1);('ieee', 1);('2012. doi', 1);('b. travnik', 1);('k. mathewson', 1);('p. m. pilarski', 1);('reactive', 1);('asynchronous environments', 1);('m. van der meer', 1);('z. kurth-nelson', 1);('a. d. redish', 1);('decision-making systems', 1);('neuroscientist', 1);('h. yu', 1);('w. xu', 1);('h. zhang', 1);('taac', 1);('temporally', 1);('abstract actor-critic', 1);