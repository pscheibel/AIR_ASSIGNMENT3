('cnn', 9);('experimental results', 9);('cifar10', 8);('figure', 7);('central server', 4);('speci', 4);('mnist', 4);('deep hierarchy quantization compression', 3);('machine learning', 3);('private data', 3);('federal learning', 3);('federated', 3);('mask', 3);('equation', 3);('communication cost', 3);('terngrad', 3);('public datasets', 2);('smart devices', 2);('global model', 2);('compared', 2);('whole process', 2);('local models', 2);('communication bandwidth', 2);('ineach round communication', 2);('compression algorithm', 2);('communication costs', 2);('original static', 2);('based', 2);('gradient updates', 2);('aji hea', 2);('original model', 2);('model', 2);('wi', 2);('lossy compression', 2);('static', 2);('high proportion clients', 2);('rounds communication', 2);('explore performance', 2);('learning involves communication', 2);('linux', 2);('server simulate', 2);('main idea', 2);('typical gradient quantization algorithm', 2);('mnistdataset', 2);('accuracy comparison communication', 2);('experimental results model', 2);('stages training', 2);('stage training', 2);('baseline algorithm', 2);('model performance', 2);('mcmahan e moore ramage hampson', 2);('arcas communicatione\x0ecientlearning', 2);('deep networks', 2);('samplingwan jianga gang liua xiaofeng chenaand yipeng zhoubacollege computer', 1);('software engineering shenzhen universitybdepartment computing macquarie universityabstractunlike', 1);('learning stores data', 1);('training aggregates models server solves data security problem mayarise', 1);('training process transmission model parameters impose signi cant load network bandwidth', 1);('vast majority model parameters redundant model parametertransmission paper explore data distribution law', 1);('partial model parameters basis', 1);('deep hierarchical quantization compression algorithm whichfurther compresses model reduces network load', 1);('data transmission throughthe hierarchical quantization model parameters', 1);('strategyfor selection clients accelerate convergence model', 1);('experimental', 1);('results ondi erent', 1);('e ectiveness algorithm1', 1);('introductionwith', 1);('rapid growth number', 1);('intelligent devices worldwide progress scienceand technology', 1);('power storage capacity devices', 1);('unprecedented scale speed timedeep learning', 1);('bene ts', 1);('ability hardware devices', 1);('data hasmade', 1);('great breakthroughs elds image recognition speech recognition', 1);('rapid development smart devices', 1);('way devicesextract information data', 1);('continuous growth', 1);('data scale', 1);('extent breakthrough', 1);('large datasets', 1);('deep learningmodel', 1);('great potential', 1);('data smart devices users', 1);('wemust', 1);('data privacy', 1);('theserver training', 1);('huge data security problems', 1);('dont upload thesedata server', 1);('trainingin order', 1);('data security problems', 1);('google', 1);('federal optimization', 1);('allowsmultiple parties', 1);('training models', 1);('data verycompatible local data storage processing', 1);('iot', 1);('data local device', 1);('federal learningiteration', 1);('clients download', 1);('global model fromthe server', 1);('nodes train model', 1);('local data update themodel parameters server', 1);('server aggregate updates client improvethe model performance', 1);('federal learning ensures thesecurity user privacy data data', 1);('local device', 1);('privacy data training', 1);('author gliuszueducn1arxiv221214760v1 cslg', 1);('dec', 1);('user', 1);('users device train local', 1);('server aggregationparticipants upload privacy data', 1);('main process', 1);('federal learning isshown', 1);('participant downloads thelatest aggregation model', 1);('learning data', 1);('participants upload', 1);('models thecentral server aggregate models', 1);('participant form', 1);('new globalmodel steps', 1);('certain convergence criteria', 1);('whole trainingprocess', 1);('private data participants', 1);('local communication thecentral server model update', 1);('protects data privacy users', 1);('processof communication malicious users', 1);('users data information', 1);('privacy data', 1);('additionalmechanisms homomorphic encryption', 1);('di erential privacy 6the training process', 1);('multiple machine learning models', 1);('withthe aim', 1);('loss function', 1);('common machine learning models', 1);('large number training samples consist data', 1);('corresponding labels lossfunction sample jcan', 1);('fxjyjw wherexjyjandwrepresent vector inputsample sample', 1);('corresponding label', 1);('current weight vector', 1);('convenience usefjw', 1);('fxjyjw dataset', 1);('diof', 1);('clienti loss function', 1);('pj2difiwjdij1stochastic', 1);('gradient descent minibatchsgd', 1);('client usewit\x001', 1);('model client iin t\x001 round update process modelwit thetround client', 1);('wit\x001\x00', 1);('learning rate training process model', 1);('client server aggregates client model generate', 1);('new global model weight2of client needs', 1);('aggregation aggregation process', 1);('asfollowswt nal aggregation modelwt', 1);('x0ik', 1);('1jdijdiwit 3an', 1);('important challenge', 1);('learning communication cost model', 1);('becauseduring', 1);('full model update duringeach training iteration update size', 1);('model communication cost nonnegligible', 1);('modern architectures millions parameters', 1);('federal learning devices', 1);('actual scene', 1);('mobile phones smart', 1);('wearable devices etc', 1);('resources network resources devices', 1);('communication cost high joint learning', 1);('ine ective', 1);('infeasible order', 1);('communication e\x0eciency', 1);('present work reducingcommunication costs', 1);('amount data', 1);('number communications theclient server', 1);('amount data round communication quantify themodel parameters', 1);('numerical distribution model parameterson basis', 1);('reduces amount communicationdata gradient transmission process compression model gradient gradientquantization lossy compression method', 1);('loss model accuracyso', 1);('work reducethe number communication rounds', 1);('proportion clients communication', 1);('time paper', 1);('method accuracy loss', 1);('selection high proportion clients', 1);('early communication', 1);('accelerates convergence model contributions asfollows1', 1);('communication compression algorithm', 1);('analyze distributionof model parameters', 1);('hierarchy', 1);('quantization compression algorithm whichfurther reduces communication cost2 use', 1);('strategy exponential', 1);('adverse impact', 1);('level quantizationcompression model', 1);('accelerates convergence speed model3', 1);('experiments', 1);('di erent', 1);('relate work21 gradient sparsi', 1);('constant threshold', 1);('selection threshold', 1);('easy threshold changes time trainingtherefore', 1);('dryden', 1);('positive negative gradients', 1);('heuristic gradient sparse algorithm algorithm', 1);('gradient communication process', 1);('uploads gradients', 1);('threshold gradients', 1);('gradients', 1);('reduces communication cost training order alleviate impact', 1);('eld model performance', 1);('lin han', 1);('deep gradientcompression algorithm', 1);('deep', 1);('gradient compression uses momentum correction', 1);('impact gradient sparsi cation model performance', 1);('experimental results thedeep gradient compression algorithm', 1);('image classi cation', 1);('imagenet', 1);('13rnn language model', 1);('penn treebank', 1);('speech recognition', 1);('librispeech corpus', 1);('thatthe lossless compression', 1);('gradient quantizationquantizing', 1);('low precision value', 1);('seideet', 1);('sgd', 1);('size gradient transmission data', 1);('traditional voice applications', 1);('alistarh', 1);('method calledqsgd', 1);('tradeo accuracy gradient accuracy', 1);('qsgd wen', 1);('threelevel gradient', 1);('convergence ofquantitative training', 1);('terngrad detects', 1);('cnn qsgd', 1);('detects training lossof', 1);('rnn', 1);('entire model', 1);('gradients algorithm proposedby', 1);('zhou', 1);('uses 1bit weight 2bit gradient3', 1);('workin', 1);('section introduce speci c process', 1);('gradient sparsi', 1);('federal learning client needs upload model server training', 1);('beforeuploading', 1);('threshold sparsi cation model lter gradients', 1);('local model server', 1);('onedimensionalvectorization expansion model', 1);('preset sparsi cation factor kto', 1);('select top k gradient', 1);('whole model gradient model uploadedand', 1);('absolute value kth gradient value threshold thr', 1);('part ofthe gradient', 1);('model discard', 1);('local client forgradient accumulation', 1);('communication use', 1);('matrix sparsi cationoperation speci c value', 1);('absolute value gradientmask \x1a1abswi\x15thr0abswithr4we', 1);('sparsi cation processas', 1);('threshold gradient basedon position information', 1);('sparse modelfigure', 1);('gradient sparsi cation process example threshold gradient', 1);('hierarchy quantization compressionthe', 1);('original model sparsi', 1);('communication cost model transmission quantify sparsi edmodel', 1);('traditional quantization strategy quantization', 1);('learning uses', 1);('small number bits quantize model parameters', 1);('reduces communication cost result loss model performance goal is4to', 1);('balance model performance model compression rate', 1);('distribution pattern gradients', 1);('approach order toensure', 1);('model gradients deviate', 1);('original gradients duringquantization strategy', 1);('traditional lowbit quantization terms asingle quantization', 1);('previous model sparsi cation operation communication cost', 1);('multibit quantization', 1);('lowbit quantizationcompression algorithms speci c process quanti cation', 1);('ndthe gradient', 1);('absolute value gradient vectorization', 1);('theupper limit gradient', 1);('gradient range thrandintervals thegradient interval', 1);('bits rst bit', 1);('interval number num', 1);('quantize gradient inthe', 1);('corresponding interval range combination interval number', 1);('tothe gradient', 1);('quanti cation process', 1);('denotes thegradient', 1);('thr andsignrepresents gradient signwi 8thrsign\x02location\x02\x00thrnumwi\x150sign\x02absthrlocation\x02\x00thrnumwi05the server aggregates model gradients', 1);('clients form', 1);('new globalmodel distributes clients', 1);('round training model convergesalgorithm', 1);('deep hierarchy quantization compression algorithmdhqcinput trained', 1);('ww0w1w sparsity', 1);('factor k', 1);('currently', 1);('selectedclientsc 12n', 1);('hierarchical', 1);('quantization compression', 1);('hqc output', 1);('uploaded1fori 0i\x14mi do2thr kof', 1);('w3mask withr4gwi wijmask', 1);('wijmask', 1);('i6 forvingwido7max maxabsvmax 8endfor9 forvingwido10v', 1);('hqc', 1);('v11endfor12endfor13compressedmodel fw', 1);('pmi0encode', 1);('wimask i14sendcompressedmodel fw', 1);('gradient sparsi cation multibit quantization compression strategy', 1);('hierarchy gradient compression', 1);('dhqc algorithm', 1);('algorithm ow denotes', 1);('hadamard product', 1);('speci c role', 1);('maski', 1);('matrixis mask gradient', 1);('sparsi cation', 1);('encode', 1);('means encode gradient33', 1);('dynamic samplingmask', 1);('sparsi cation multibit quantization strategies compress communication cost evenfurther lossy compression', 1);('impact model performance compensate for5the accuracy loss', 1);('strategy federation learning', 1);('rate toselect clients', 1);('model training process aggregates', 1);('new model', 1);('proportionof client models distributes', 1);('new model clients', 1);('butis conducive', 1);('fast model convergence order accelerate convergence model weadopt', 1);('rate number communications increases allowas', 1);('client models', 1);('model aggregation', 1);('initial trainingand', 1);('clients accelerate convergence themodel', 1);('large communication cost pretrainingperiod number', 1);('clients decreases', 1);('choosingdi', 1);('erent decay rates', 1);('actual situation control communication cost', 1);('communication cost static', 1);('control model samplingrate exponential function', 1);('mexp', 1);('t6mrepresents total number clients', 1);('rrepresents', 1);('proportion round communicationclients', 1);('training trepresents communication round', 1);('attenuation rate increase communication times number client samples decreases graduallyafter', 1);('number clients selecteach time', 1);('line expectations', 1);('thenumber clients', 1);('clients model', 1);('normallya result mnist b result cifar10figure', 1);('comparison', 1);('rate number communications mnist datasetand cifar10 dataset4', 1);('experimentsin', 1);('benchmark datasets', 1);('experimental comparison', 1);('classical communication compression algorithms image classi cation tasks', 1);('theperformance', 1);('accuracy model theoreticalcompression ratio model addition', 1);('attention convergence speed themodel training stage', 1);('experimental setup', 1);('experimental results641', 1);('experimental setupto', 1);('algorithm paper section conducts comparative experiments', 1);('classical datasets image classi cation domain betterrepresent performance algorithm', 1);('algorithm withother e\x0ecient communication algorithms federation learning', 1);('computational resource constraintswe use federation learning setup single', 1);('real scenarios ignoringcommunication noise latency network', 1);('experimental con guration411', 1);('algorithmsthe algorithms', 1);('experiments followsbaseline', 1);('learning algorithmdgc', 1);('typical gradient sparse compression algorithm', 1);('part gradient', 1);('values u', 1);('u u', 1);('maximum value thegradientdhqc algorithm412', 1);('datasetswe', 1);('representative benchmark datasets', 1);('image classi cationmnist', 1);('gray handwritten image samples total of10 classes dimension image', 1);('easy extractthis datasets', 1);('small networkscifar10', 1);('color images', 1);('objects frogs aircraft', 1);('benchmark datasets413', 1);('modelin', 1);('performance algorithms', 1);('model theexperiments', 1);('new network model', 1);('convolution network', 1);('josttobias', 1);('previous convolution networks', 1);('full connection layer', 1);('network replacedthe', 1);('step convolution layer', 1);('full connection layer convolutionkernel', 1);('whole network', 1);('convolution layers', 1);('cnn42 experiments resultsfederated', 1);('central servers', 1);('howeverdue', 1);('resources ignore communication noise delay thenetwork use', 1);('learning settings', 1);('real scenefor image classi cation task use', 1);('model client model', 1);('cifar10and mnist', 1);('model order', 1);('e ectiveness', 1);('firstly', 1);('deep hierarchyquantization compression', 1);('strategy switch static', 1);('dynamic samplingstrategy', 1);('datasets model', 1);('unchanged order distinguish compression', 1);('compression algorithms', 1);('name thealgorithm', 1);('dhqcand', 1);('convergence rate compression ratio accuracy', 1);('performance ofour experiment421', 1);('static samplingwe', 1);('deep hierarchical quantization compression', 1);('dhqcusing', 1);('clients experiments', 1);('model simulate', 1);('federal learning realenvironment', 1);('percentage clients', 1);('training communicationwhen', 1);('data data', 1);('assumethat part data', 1);('private client', 1);('copy ensure overall datadistribution satis es', 1);('independent homogeneous distribution', 1);('communication e\x0ecient algorithms setting', 1);('maximum number communications', 1);('4a result cifar10 b result mnistfigure', 1);('cifar10 dataset bmnist datasetthe', 1);('di erent communicatione\x0ecient joint learning methods', 1);('dataset b', 1);('number iterations yaxis', 1);('accuracyof model observe communication compression methods', 1);('fast convergencewhich', 1);('fedavg', 1);('notbetter baseline', 1);('experimental results canmake inference model', 1);('sensitive performance loss', 1);('stage training model', 1);('roundsof training model', 1);('model gradients model', 1);('decreasein direction', 1);('stages training compression algorithms achievesimilar performance baseline algorithm', 1);('analyze results di erent compressionalgorithms detailfedavgthe', 1);('typical federal learning algorithm', 1);('baseline algorithm experiment', 1);('experimental results di erent datasetsdemonstrate', 1);('baseline algorithm performs', 1);('early stage trainingwhich attribute', 1);('strong sensitivity model parameters lossy compression earlystage', 1);('complete model update facilitates model generate', 1);('capability ofthe model', 1);('various communication compression algorithms', 1);('similar accuracy interms model performance baseline algorithm signi cant advantage', 1);('baseline algorithm transmits', 1);('complete model update thecommunication process', 1);('communication costterngradthe', 1);('main idea reducethe communication cost', 1);('values f\x00u0uguis', 1);('maximum valueof gradient', 1);('theoretically terngrad', 1);('amount communication', 1);('nodes parameter server', 1);('log23\x192018 times fact', 1);('bits encode f101g', 1);('times reasonfor', 1);('poor performance', 1);('stages training e ect lossy compression model parameters', 1);('multiple iterations trainingterngrad', 1);('similar accuracy levels baseline algorithm', 1);('stages training8and reduces transmission cost', 1);('federal learning communication quantization', 1);('themodel compression rate', 1);('satisfactorydgcdgc algorithms', 1);('important gradient updates', 1);('absolute magnitude gradient evaluation metric', 1);('dgc', 1);('transmits gradient updates', 1);('gradient changes lie inthe topk', 1);('experimental results conclude', 1);('reasonable judge importance thegradient', 1);('magnitude gradient', 1);('performance model', 1);('phase supports conclusion', 1);('transmits part model gradientthe', 1);('gradient gradient lead loss model information lead', 1);('poor performance model', 1);('training period', 1);('experimental results di erent datasetsshow sparsi cation quantization model parameters', 1);('adverse e ects themodel convergence', 1);('model parameters lossy compression ofthe model parametersdhqcour algorithm', 1);('communication cost communication', 1);('sparse multi bit quantization strategies', 1);('similar toother lossy compression algorithms', 1);('algorithm performs', 1);('training proceeds performance model', 1);('similar accuracyof baseline algorithm', 1);('late stage training', 1);('method isconvergentthe performance di erent algorithms model', 1);('previous section analyzethe', 1);('experimental phenomena', 1);('clear advantage model performance eld communication optimizationwe', 1);('metric compression rate', 1);('speci cdata compression ratio model performance compression algorithma model performance di erent algorithms b compression ratio di erent algorithmsfigure', 1);('performance di erent communication e\x0ecient algorithms di erentimage classi cation tasks xaxis', 1);('di erent algorithms yaxis', 1);('model performance b', 1);('compression ratios', 1);('di erentalgorithms', 1);('complete communication xaxis', 1);('di erent compressionalgorithms yaxis', 1);('compression ratioas', 1);('model performance algorithm', 1);('dataset reducedby', 1);('baseline outperforms compression algorithms', 1);('experimental results algorithms', 1);('quantization compression algorithms increase cost thecommunication process', 1);('sparsi cation operation model quantizingit', 1);('compression ratio', 1);('faroutperforms compression algorithms', 1);('image classi cationtasks', 1);('conclusions1 algorithm outperforms communication e\x0ecient algorithms terms theoretical compression rate metrics', 1);('signi cant', 1);('of9communication costs2', 1);('experimental data', 1);('figs', 1);('satisfactorybalance compression rate model performance43', 1);('dynatic samplingwe', 1);('previous experiments model information bene cial theconvergence model', 1);('federal learning training', 1);('transfermore model information', 1);('stage training bene cial convergence', 1);('speed model', 1);('conclusion order', 1);('impactof lossy compression model performance', 1);('stage speed convergence ofthe model', 1);('client selection strategy', 1);('callywe use', 1);('information high proportionof clients', 1);('early stage training facilitates model generate generalizedmodel', 1);('stage ensure validity fairness experiments', 1);('method conduct experiments', 1);('mnistdataset cifar10', 1);('dataset depth hierarchical quantization compression algorithm usingdi erent', 1);('dhqc', 1);('ddhqc', 1);('dynamic samplinga result cifar10 b result mnistfigure', 1);('cifar10 dataset bmnist datasetfigure', 1);('hierarchy quantization compression di erent', 1);('image classi cation datasets xaxis', 1);('numberof communications yaxis', 1);('accuracy model test', 1);('themaximum number communications', 1);('rounds themnist dataset', 1);('thatconvergence speed', 1);('dataset show algorithm', 1);('certain improvement convergence speed thereis signi cant improvement', 1);('datasetmodel accuracy algorithm', 1);('dataset outperforms', 1);('traditional static', 1);('approachcommunication cost', 1);('high percentage clientsto', 1);('training proceeds thenumber clients', 1);('round communication', 1);('preset minimum value process control communication cost decay factor exceedthe communication cost static', 1);('discussionin', 1);('classical communication compression algorithmson image classi cation tasks', 1);('algorithms terms compression multiplicity accuracywhich', 1);('new ideas directions', 1);('communication costs federation communication process sake analysis ignore complexities', 1);('real networks undeniablethat', 1);('federation learning', 1);('real scene requiresfurther simulation experiments', 1);('modern deep neural models', 1);('large number parameters suchas', 1);('vgg16', 1);('total number parameters exceeds', 1);('lack computationalresources', 1);('simulation experiments', 1);('work toexplore simulation experiments largescale', 1);('neural networks computational resources6', 1);('conclusionto', 1);('communicatione\x0ecient federation learning paper', 1);('deep hierarchical', 1);('compression method', 1);('communication costs order', 1);('impact lossy compression model performance use', 1);('client selectionthe', 1);('improvement prediction accuracy ofthe model', 1);('communication costin communication process results experiments', 1);('image classi cationtask show', 1);('competitive resultsacknowledgementthis work', 1);('shenzhen', 1);('fundamental research', 1);('program20200814105901001references1 h', 1);('mcmahan e moore ramage', 1);('arcas federated', 1);('lin han h mao wang', 1);('j dally deep', 1);('gradient compression', 1);('reducing', 1);('thecommunication bandwidth', 1);('e bagdasaryan veit hua estrin v shmatikov', 1);('k bonawitz v ivanov', 1);('kreuter marcedone k seth practical', 1);('acm sigsac', 1);('hardy', 1);('henecka h iveylaw r nock g patrini g smith', 1);('thorne privatefederated', 1);('entity resolution', 1);('marti n abadi chu ian j goodfellow hugh brendan mcmahan mironov k talwar', 1);('li deep', 1);('learning di erential privacy', 1);('acm', 1);('e r ziegel r myers classical', 1);('modern regression applications', 1);('technometrics', 1);('n strom', 1);('dnn training', 1);('commodity gpu cloud', 1);('interspeech2015', 1);('n dryden moon jacobs', 1);('v essen communication', 1);('quantization dataparallel training', 1);('neural networks', 1);('machine learning hpcenvironments mlhpc', 1);('aji k hea', 1);('sparse', 1);('gradient descent', 1);('krizhevsky g hinton learning', 1);('multiple layers', 1);('tiny images', 1);('handbookof systemic autoimmune diseases', 1);('jia wei r socher', 1);('j li', 1);('kai', 1);('f f', 1);('li imagenet', 1);('largescale hierarchicalimage database pages', 1);('maamouri bies buckwalter', 1);('mekki', 1);('penn arabic treebank building alargescale', 1);('arabic corpus', 1);('v panayotov g chen povey khudanpur librispeech', 1);('asr corpus', 1);('publicdomain audio books', 1);('icassp', 1);('ieee', 1);('international conference', 1);('acoustics speechand', 1);('processing icassp', 1);('seide h fu j droppo g li yu', 1);('1bit stochastic gradient descent applicationto dataparallel', 1);('training speech dnns conference', 1);('speechcommunication', 1);('dan j li r tomioka vojnovic qsgd randomized', 1);('quantization communicationoptimal stochastic gradient descent', 1);('wen', 1);('xu', 1);('yan', 1);('wu wang chen h li terngrad ternary', 1);('gradients toreduce communication', 1);('deep learning', 1);('zhou wu z ni x zhou h wen zou dorefanet training', 1);('low bitwidthconvolutional neural networks', 1);('low bitwidth gradients', 1);('lecun', 1);('bottou gradientbased', 1);('document recognition', 1);('proceedingsof ieee', 1);('jt springenberg dosovitskiy brox riedmiller striving', 1);('simplicity allconvolutional net eprint arxiv', 1);