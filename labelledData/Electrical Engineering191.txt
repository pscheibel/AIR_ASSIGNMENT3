('proc', 17);('svs', 15);('attention mechanism', 15);('temporal structure', 10);('musical score', 9);('voice synthesis', 8);('tts', 8);('p-trans', 8);('icassp', 8);('fig', 7);('mos', 7);('k. tokuda', 7);('auxiliary note', 6);('t-trans', 6);('k. oura', 6);('nankaku', 6);('zhang', 6);('musical note position-aware attention mechanism', 5);('penalty matrix', 5);('transition probability', 5);('attention loss', 5);('base', 5);('prop', 5);('k. hashimoto', 5);('interspeech', 5);('seq2seq model', 4);('singing', 4);('notrans', 4);('attention weights', 3);('systems [', 3);('pitch normalization', 3);('attention mechanism [', 3);('eq', 3);('note boundaries', 3);('sec', 3);('np+nf', 3);('noatt', 3);('hono', 3);('voice synthesis system', 3);('s. takaki', 3);('wang', 3);('wu', 3);('r. j. weiss', 3);('j. shen', 3);('liu', 3);('neural', 3);('x. tan', 3);('h. zen', 3);('j. luan', 3);('furthermore', 2);('experimental', 2);('musical scores', 2);('dnn', 2);('phoneme duration', 2);('time-lag models', 2);('ar', 2);('note position', 2);('note timing', 2);('musical', 2);('attention weight', 2);('decoder time-step', 2);('input score', 2);('nx', 2);('> tanh', 2);('transition probabilities', 2);('current note', 2);('pre-net', 2);('note duration', 2);('pitch', 2);('note pitch', 2);('frame-level note pitch sequence', 2);('% condence intervals', 2);('prop x x x', 2);('world', 2);('frame-level auxiliary note', 2);('experiment', 2);('ve systems', 2);('eqs', 2);('subjective evaluation results', 2);('appropriate alignment', 2);('np', 2);('prop fig', 2);('examples', 2);('m. blaauw', 2);('j. bonada', 2);('fast', 2);('z.-h. ling', 2);('l.-r. dai', 2);('bengio', 2);('speech synthesis', 2);('n. jaitly', 2);('z. yang', 2);('z. chen', 2);('towards', 2);('sequence-to-sequence acoustic', 2);('ieee', 2);('s. zhao', 2);('t. qin', 2);('t.-y', 2);('jia', 2);('i. elias', 2);('robust', 2);('j. wu', 2);('j. chen', 2);('ieice transactions', 2);('information', 2);('systems', 2);('singing voice synthesis based on', 1);('musical note position-aware attention mechanism yukiya hono', 1);('kei hashimoto', 1);('yoshihiko nankaku', 1);('keiichi tokuda nagoya', 1);('nagoya', 1);('japan abstract', 1);('novel sequence-to-sequence', 1);('duration information', 1);('additional modules', 1);('seq2seq models', 1);('insufcient robustness', 1);('index terms singing', 1);('sequence-to-sequence model', 1);('introduction statistical', 1);('machine learning techniques [', 1);('sequence transform', 1);('acoustic fea- ture sequence', 1);('voice needs', 1);('important issues', 1);('typical deep neural network', 1);('] model', 1);('vocal timing deviation', 1);('acoustic model', 1);('pipeline systems', 1);('alignment errors', 1);('sufcient ability', 1);('inspired', 1);('non-ar generation models', 1);('modern text-to-speech', 1);('jsps kakenhi grant number jp22h03614', 1);('casio science promotion foundation', 1);('foundation of public interest of tatematsu.voice', 1);('voices varies', 1);('corresponding note duration', 1);('encoder-decoder models', 1);('explicit length regulator', 1);('ap- proach', 1);('external duration information', 1);('conventional pipeline systems', 1);('studies [', 1);('] use', 1);('critical issue', 1);('novel seq2seq model', 1);('attention mechanism calculates', 1);('in- troduce', 1);('additional techniques', 1);('robust alignment', 1);('at- tention loss', 1);('voice align- ments', 1);('proper vocal tim-', 1);('extra supplementary temporal', 1);('related work seq2seq', 1);('key tech- nique', 1);('dif- ferent lengths', 1);('lo- cality properties', 1);('hybrid location-', 1);('sensitive attention', 1);('at- tention mechanisms [', 1);('algorithm [', 1);('mono- tonic attention mechanisms [', 1);('noticeable differences', 1);('typi- cal seq2seq tasks', 1);('alignment path', 1);('authors [', 1);('global duration control attention', 1);('global transition', 1);('insuf- cient ability', 1);('duration control', 1);('time-variant transition probabilities', 1);('attention probability', 1);('account note position informa- tion', 1);('note position embeddings', 1);('appropriate mod-', 1);('attention mechanism.arxiv:2212.13703v1 [ eess.as ]', 1);('dec', 1);('conv layersbidirectional lstm score featurenote pitchattention', 1);('position-aware attention2 unidirectional lstm', 1);('encoded', 1);('position projection+t', 1);('positionauxiliary', 1);('featureprojection2 layer pre-netprojectionweighted sumpredicted acoustic featurepredicted acoustic feature5 conv layer post-net musical scoreaddition concatenationfig', 1);('overview', 1);('proposed model', 1);('encoder-decoder model', 1);('generate frame-level acoustic', 1);('phoneme-level score', 1);('singing-specic requirements', 1);('high controllability', 1);('note position-aware attention mechanism', 1);('] calculates', 1);('state x= [ x1', 1);('xn ]', 1);('context vector ctcan', 1);('byct=pn n=1 t', 1);('attention weight rep-', 1);('t-th decoder step', 1);('output vector otcan', 1);('context vector ctby', 1);('encoder states', 1);('sat- isfy', 1);('current attention weight t', 1);('previous alignment', 1);('\x01 \x01yt', 1);('output probability', 1);('attention mechanism notices', 1);('n-th phoneme', 1);('thet-th decoder step', 1);('n+ 1-th phoneme', 1);('t+ 1-th decoder steps', 1);('dependent time-variant transition probability', 1);('transition agent [', 1);('note duration andtempo', 1);('musical note positional', 1);('output probability yt', 1);('musical note position-aware', 1);('additional term', 1);('= exp', 1);('note position representation [ p', 1);('n ]', 1);('positional representations', 1);('n=8 > < >', 1);('t < s n', 1);('< t', 1);('n-th mu- sical note', 1);('past alignments', 1);('location-sensitive attention [', 1);('] -like for- mula', 1);('=\x1b\x00 v', 1);('n +t', 1);('location-sensitive term', 1);('uses con- volutional', 1);('previous cumulative align- ments', 1);('auxiliary', 1);('con- text', 1);('musical note context', 1);('attention query qt', 1);('score fea- ture sequence', 1);('frame-level sequence', 1);('ac- cordance', 1);('original note timing', 1);('frame position', 1);('dense layer', 1);('decoder input', 1);('current frame position', 1);('note context', 1);('corresponding musical note', 1);('attention query', 1);('guided', 1);('mu- sical score', 1);('attention loss [', 1);('penalty matrixpauz oosanz opauz oosaz onnote boundariespseudo-determinedmora boundariesfig', 1);('penalty', 1);('matrix examples', 1);('ba- sis', 1);('mora boundaries', 1);('note durations de-', 1);('di- agonal elements', 1);('specically', 1);('mora boundary', 1);('alignment estimation', 1);('multiple morae', 1);('start positions', 1);('con- sider vocal timing deviation', 1);('alignment matrix', 1);('a2rn\x02tbe', 1);('total number', 1);('latt', 1);('ntkg ak1', 1);('element- wise product', 1);('nal loss function', 1);('lof', 1);('l=lfeat', 1);('lfeat', 1);('tdtx', 1);('de- coder', 1);('post-net', 1);('dis', 1);('gaussian', 1);('distribu- tion', 1);('vector \x16and', 1);('covariance matrix \x06', 1);('identity matrix', 1);('adjustment parameter', 1);('previous work [', 1);('log fundamen- tal frequency', 1);('f0', 1);('f0de-', 1);('f0sequence', 1);('frame-by- frame', 1);('phone-level input note pitch se- quence', 1);('experiments', 1);('japanese childrens songs', 1);('fe- male singer', 1);('sixty', 1);('voice signals', 1);('systemspt', 1);('aux', 1);('note feat.lattmos', 1);('nf x', 1);('np x', 1);('np+nf x x', 1);('49-th mel-cepstral coefcients', 1);('continu- ous logf0value', 1);('25-dimensional analysis aperiodicity measures', 1);('1- dimensional vibrato component', 1);('binary ag', 1);('mel-cepstral', 1);('dif- ference', 1);('original log', 1);('f0and', 1);('f0were', 1);('vibrato component', 1);('model architectures', 1);('post-', 1);('linear projection layer', 1);('extra linear projection layer', 1);('frame po- sition', 1);('sys- tems', 1);('reduction factor', 1);('hyperparameter \x15in', 1);('pe-', 1);('riodnet [', 1);('non-ar neural vocoder', 1);('parallel structure', 1);('reconstruct waveforms', 1);('subjective', 1);('opinion score', 1);('na- tive', 1);('japanese-speaking', 1);('test songs', 1);('overall naturalness', 1);('vo- cal timing', 1);('ex- periment', 1);('lyrics errors', 1);('monotonic alignment', 1);('1audio samples', 1);('url', 1);('sp.nitech.ac.jp/ hono/demos/icassp2023/', 1);('nf', 1);('attention', 1);('mechanismtransition probability', 1);('systems phoneme-', 1);('dependenttime- variantmos', 1);('notrans x', 1);('p-trans x x', 1);('t-trans x x', 1);('3.87\x060.12 4.2.2', 1);('attention mech- anisms', 1);('tables', 1);('phoneme boundaries', 1);('synthe- sis stage', 1);('five-state', 1);('semi-markov mod- els', 1);('hsmms', 1);('alignment [', 1);('time-variant regardless', 1);('phoneme-dependent transition probability', 1);('output probabilities yt', 1);('transition probabilities ut', 1);('right column', 1);('output probabilities', 1);('figs', 1);('t- trans', 1);('undergoes transitions', 1);('consonant skip-', 1);('model tem- poral structures', 1);('subjective evaluation', 1);('figures', 1);('transition probabil- ity', 1);('temporal structures', 1);('conclusion', 1);('note positions', 1);('generate acoustic', 1);('additional temporal', 1);('experi-', 1);('mental results', 1);('hsmm-based', 1);('duration controllabil- ity', 1);('robust alignment estimation', 1);('acknowledgments', 1);('mr. shumma murata', 1);('references', 1);('a. mase', 1);('t. yamada', 1);('s. muto', 1);('recent', 1);('hmm-based', 1);('voice synthesis systemsinsy', 1);('isca ssw7', 1);('sinsy', 1);('deep neural', 1);('ieee/acm transactions', 1);('audio', 1);('speech', 1);('lan-', 1);('processing', 1);('neural parametric', 1);('syn- thesizer', 1);('generative adversarial net-', 1);('k. nakamura', 1);('voice synthe- sis system', 1);('convolutional neural networks', 1);('yi', 1);('ai', 1);('voice syn- thesis', 1);('deep autoregressive neural networks', 1);('j. sotelo', 1);('s. mehri', 1);('k. kumar', 1);('j. f. santos', 1);('k. kastner', 1);('a. courville', 1);('char2wav', 1);('end-to-end', 1);('r. skerry-ryan', 1);('d. stanton', 1);('xiao', 1);('s. bengio', 1);('tacotron', 1);('end-to-end speech synthesis', 1);('in-', 1);('r. pang', 1);('m. schuster', 1);('r. skerrv-ryan', 1);('wavenet', 1);('mel spectrogram predictions', 1);('j.-x', 1);('forward', 1);('international conference', 1);('signal processing', 1);('n. li', 1);('s. liu', 1);('m. liu', 1);('speech syn- thesis', 1);('transformer network', 1);('aaai', 1);('articial intelligence', 1);('ren', 1);('c. hu', 1);('z. zhao', 1);('fastspeech', 1);('high-quality end-to-end text', 1);('arxiv preprint arxiv:2006.04558', 1);('m. chrzanowski', 1);('non-attentive tacotron', 1);('controllable neu- ral tts synthesis', 1);('arxiv preprint arxiv:2010.04301', 1);('parallel tacotron', 1);('non-autoregressive', 1);('lable tts', 1);('j. lee', 1);('h.-s. choi', 1);('c.-b', 1);('jeon', 1);('j. koo', 1);('k. lee', 1);('adversari-', 1);('end-to-end korean', 1);('sequence-to-sequence', 1);('feed-forward transformer', 1);('p. lu', 1);('l. zhou', 1);('xiaoicesing', 1);('hifisinger', 1);('high-delity neural', 1);('arxiv preprint arxiv:2009.01776', 1);('adversarially', 1);('multi-singer sequence-to-sequence', 1);('inter-', 1);('gu', 1);('x. yin', 1);('rao', 1);('wan', 1);('b. tang', 1);('z. ma', 1);('bytesing', 1);('voice syn- thesis system', 1);('encoder-decoder acous- tic models', 1);('wavernn vocoders', 1);('iscslp', 1);('j. shi', 1);('s. guo', 1);('n. huo', 1);('q. jin', 1);('sequence-to-', 1);('perceptual entropy loss', 1);('o. angelini', 1);('a. moinet', 1);('k. yanagisawa', 1);('t. drugman', 1);('t. wang', 1);('r. fu', 1);('j. yi', 1);('j. tao', 1);('z. wen', 1);('singing- tacotron', 1);('global', 1);('duration control attention', 1);('dynamic l- ter', 1);('arxiv preprint arxiv:2202.07907', 1);('d. bahdanau', 1);('k. cho', 1);('machine trans- lation', 1);('iclr', 1);('m.', 1);('deng', 1);('l.', 1);('stepwise monotonic attention', 1);('neu- ral', 1);('yasuda', 1);('x. wang', 1);('j. yamagishi', 1);('initial', 1);('encoder-decoder end-to-end tts', 1);('mono- tonic', 1);('hard alignments', 1);('isca ssw10', 1);('h. tachibana', 1);('k. uenoyama', 1);('s. aihara', 1);('efciently', 1);('able text-to-speech system', 1);('deep convolutional net-', 1);('m. morise', 1);('f. yokomori', 1);('k. ozawa', 1);('high-quality speech synthesis system', 1);('real-time ap- plications', 1);('periodnet', 1);('raw waveform generative model', 1);('ape- riodic components', 1);('ieee access', 1);('t. masuko', 1);('t. kobayasih', 1);('t. kita-', 1);('speech synthesis system', 1);('e90-d', 1);('k. sumiya', 1);('t. yoshimura', 1);('sequence- to-sequence speech synthesis', 1);('semi-markov model', 1);('arxiv preprint arxiv:2108.13985', 1);