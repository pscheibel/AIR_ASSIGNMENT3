('pareto', 21);('lagrangian', 9);('rl', 8);('morl', 6);('qlearning', 5);('calascibetta', 5);('lyapunov', 5);('fig', 5);('decision time', 5);('taming lagrangian chaos multiobjective reinforcement learning', 4);('fig5', 3);('cumulative reward', 3);('heuristic policies', 3);('fig6left', 3);('reinforcement learning', 2);('possible actions', 2);('namely', 2);('furthermore', 2);('sec4', 2);('heuristic analysis', 2);('uctrl', 2);('cw', 2);('acw', 2);('fig1', 2);('fig3', 2);('notice', 2);('xj1r\x15tj', 2);('lagrangianpair', 2);('policy passive', 2);('qlearningalgorithm', 2);('problem nontrivial', 2);('paretofrontier', 2);('sec', 2);('qmatrix', 2);('black line', 2);('fig7', 2);('cc', 2);('rev mod phys', 2);('biferale', 2);('intelligence', 2);('proceedings', 2);('epj', 1);('lagrangian chaos multiobjective reinforcementlearningchiara calascibetta1a luca biferale1 francesco borra2 antonio celani3and massimo cencini41department physics infn', 1);('rome tor vergata via', 1);('ricerca scienti', 1);('rome italy2laboratory physics', 1);('normale sup\x13', 1);('rue lhomond paris', 1);('france3quantitative', 1);('abdus salam', 1);('centre theoretical physics ictp trieste', 1);('italy4istituto', 1);('sistemi complessi cnr via', 1);('taurini', 1);('rome', 1);('italy infn tor vergatathe', 1);('date receipt acceptance', 1);('active particles 2d', 1);('complex ows multiobjectivegoals', 1);('dispersion rate energy consumption pair approach theproblem', 1);('multi objective reinforcement learning morl', 1);('scalarization techniquestogether', 1);('weshow morl', 1);('able nd', 1);('tradeo solutions', 1);('frontier abenchmark show', 1);('heuristic strategies', 1);('solutions considerthe situation agents', 1);('update control variables', 1);('adiscrete decision time show range decision times', 1);('lyapunovtime', 1);('nds strategies signi cantlyimprove heuristics', 1);('large decision times', 1);('knowledge ofthe ow whereas', 1);('alla priori heuristic strategies', 1);('introductionin', 1);('engineering geophysical applications roboticinstruments', 1);('egwhere eet', 1);('information ocean multirobots', 1);('hazardous substances', 1);('typical application tokeep eet control eg', 1);('geometric formation andor', 1);('pointtopoint path', 1);('typical owsthe', 1);('relative distance', 1);('passive drifters', 1);('close di usive way', 1);('large scaleswhen nonlinear e ects', 1);('animalbehaviour', 1);('direction ofresearch', 1);('biomimetic strategies 8910however unclear', 1);('heuristic hardwiredrules', 1);('control swarm presenceof', 1);('realisticapplications agents need', 1);('biological limitations', 1);('advantage ow', 1);('thegoal result search', 1);('active complex policies control', 1);('complex environments', 1);('chaotic turbulent ows problem', 1);('strong sensitivity ofthe system perturbation making meaningaemail calascibettaroma2infnit', 1);('corresponding authorof optimal control fragile notion direction fewattempts control single', 1);('reinforcement learning rl', 1);('algorithms havebeen', 1);('zermelos', 1);('optimal navigationproblem', 1);('moreoverrl', 1);('glider thermal currents', 1);('harnesswind airborne energy', 1);('recently', 1);('adversarial gamesbetween', 1);('proposedto study chaseandescape sh strategies', 1);('reynoldsnumber', 1);('18in paper', 1);('agents particle', 1);('ow act', 1);('soas contrast growth separation thesame time minimize', 1);('possible cost forcontrol', 1);('problem twoparticles', 1);('distances ow di erentiableso', 1);('control separation', 1);('realismwe model problem', 1);('limitations detectionpartial observability', 1);('partial maneuverability', 1);('eachagent sample local properties', 1);('decision times', 1);('interval orderto update actions', 1);('objects swimarxiv221209612v1 physicsfludyn', 1);('dec', 1);('taming lagrangian chaos multiobjective reinforcement learningeither', 1);('direction separation perpendicular', 1);('variable speed', 1);('ableto ful ll objective minimize energy cost alsoinclude action', 1);('couple ofparticles', 1);('result multiagent 2lagrangian pair multitask minimize chaoticdispersion energy consumption problem', 1);('typical longterm optimization problem withcon', 1);('multiobjective reinforcement learning morl', 1);('classical singletask', 1);('reward scalar whereasin', 1);('reward vector element eachobjective approach', 1);('scalarization ie byde', 1);('new scalar total reward', 1);('sum alongall element', 1);('original reward vector', 1);('forthis', 1);('reason exists', 1);('tradeo solutions formingthe', 1);('solutionon frontier', 1);('pareto e\x0ecient', 1);('ie single objectivecan', 1);('approach show nd', 1);('ofpareto optimal policies', 1);('minimize chaoticdispersion', 1);('cost benchmark strategies', 1);('heuristic baselinesin', 1);('particular show', 1);('strategies ableto', 1);('nontrivial information', 1);('owthe paper', 1);('sec2', 1);('describe general setup problem model thelagrangian pair act sense environment details', 1);('uid ow', 1);('insec3', 1);('introduce concepts', 1);('detailson choice reward function learningprotocol', 1);('present concepts', 1);('paretodominance pareto', 1);('themain results', 1);('role decision time', 1);('sec52', 1);('active', 1);('pairs owin', 1);('typical ows', 1);('causes exponential growth separation \x0ext kx2t\x00x1tkbetween pairs', 1);('uncontrolled tracer particles', 1);('ie hln\x0extiln\x0ex0 \x15t', 1);('lagrangian lyapunov', 1);('exponent goal hereis', 1);('particles control', 1);('chaotic dispersion atthe time', 1);('particular considerparticles oneway', 1);('approximation anautonomous', 1);('mechanism speed', 1);('vt', 1);('inthe direction pt', 1);('particles obey followingequations motionx ux tt', 1);('tuctrl t', 1);('tp t1where', 1);('agents index ux tt thevelocity', 1);('tis control contribution particle velocitywe', 1);('agents interact environment', 1);('time unitsat decision time agents measure owproperties proxy environmental state sensetheir', 1);('mutual separation basis information', 1);('action ie', 1);('intensity direction way controluctrl t', 1);('constant time functioniep t p tj andv t', 1);('tj fortj\x14ttj withtjj', 1);('thejthdecision time', 1);('clearly', 1);('important role', 1);('dimensionless combination ofthe', 1);('parameters \x15', 1);('control toosporadic velocity eld', 1);('separate considerablythe agents hand \x150the controlproblem', 1);('directions p t assumethat agents', 1);('direction nk transversal onen nk', 1);('x2\x00x1\x0ex', 1);('t proportional agent distance', 1);('decisiontime iev t f t\x0ext 2and', 1);('f t f', 1);('o f t', 1);('energy choice rst objective', 1);('system hand useda', 1);('typical threshold distance abovebelow agents willalways', 1);('able controlnotcontrol', 1);('study problem', 1);('dynamics ie', 1);('decision time tj agent canpick', 1);('actions a2ff tj 0f tj fwithp', 1);('tobe passive swim', 1);('longitudinal perpendicular directions', 1);('na\x7f \x10ve policy strategy agents', 1);('navigate towardseach ie p1nkand p2\x00nk', 1);('likewise', 1);('passive policy strategy', 1);('f1f2', 1);('0in principle', 1);('available thecouple agents', 1);('space actions', 1);('symmetries eg', 1);('con guration inwhich p1nkand', 1);('equivalent tothe', 1);('con guration rst agent passive andp2\x00nk way', 1);('actions couplereduces', 1);('aof', 1);('fig122 sensing', 1);('relative position distance decision time agents', 1);('cues uid envicalascibetta', 1);('left', 1);('available agent', 1);('passive swim longitudinal direction\x06nkor transversal', 1);('right scheme', 1);('actions couple agents', 1);('removingsymmetrical action pairs', 1);('longitudinal actions', 1);('transversal directions', 1);('agents rotate withrespect clockwise', 1);('acwronmental', 1);('concerning', 1);('observability environment', 1);('agents arough estimates', 1);('relative longitudinal transversegradients', 1);('as\x1bkux2t\x00ux1t\x0ex\x01nk 3\x1bux2t\x00ux1t\x0ex\x01n 4which', 1);('information abouttheir local velocities', 1);('statespace wesuppose agents', 1);('able measure velocity di erence separation gradients approximationwith', 1);('values of\x1b\x1bkto', 1);('longitudinal gradients direction rotates transversegradients', 1);('summary states valueof discretization', 1);('constant cis', 1);('the16 states', 1);('case23 space control policiesgiven', 1);('possible set of1516deterministic policies \x19sa making bruteforce optimization search', 1);('resort toreinforcement', 1);('learning', 1);('sec3', 1);('intuitive way trace', 1);('policies space', 1);('baseline policies restrictingperceptions longitudinal components', 1);('direction arefig', 1);('de', 1);('discretization longitudinal', 1);('local rate expansioncontraction', 1);('rate ofclockwise', 1);('rotation imposedby ow', 1);('direction gure', 1);('thestates discretization ca discretization constantimportant', 1);('actions theright panel', 1);('policies ourreference heuristics benchmark', 1);('ourmorl implementation4', 1);('taming lagrangian chaos multiobjective reinforcement learning24 model', 1);('owas uid environment', 1);('isotropic incompressible timedependent owas', 1);('ref', 1);('particular velocity eld', 1);('nedin terms stream function uxt r xt \x00x', 1);('superposition fewfourier modes xt', 1);('xk2kakteik\x01xcc', 1);('5kf2\x19l0\x062\x19l2\x19l02\x19lg wherelis scale periodicity ow', 1);('akt arkt', 1);('iaiktare random timedependent amplitudes', 1);('ornsteinuhlenbeck', 1);('process 28a kt \x001 fa kt \x102\x1b2k f\x1112\x11 kt6with ri', 1);('fsets ow correlation time\x11 kt zeromean', 1);('gaussian', 1);('variables correlationh\x11 kt\x11 k0t0i\x0e \x0ekk0\x0et\x00t0 and\x1b2k u2rms2kkk2we', 1);('f 1l 1urms', 1);('choice themaximum', 1);('meanexponential rate divergence', 1);('uncontrolledtracers particles \x15143', 1);('reinforcement', 1);('states actions aim tosolve optimization problem', 1);('rate separation growth energyconsumption', 1);('multiobjective optimization moo', 1);('optimality ofsuch solutions', 1);('solution dominates', 1);('equal onall others instance', 1);('aandbdominatec', 1);('incomparable becauseeach', 1);('solutions form', 1);('black circles', 1);('learning rl', 1);('scalar reward', 1);('representinga single longterm objective', 1);('moo', 1);('algorithms eg', 1);('qlearning29', 1);('new total singleobjective optimization problem', 1);('weightedsum subobjective functions', 1);('solvingthe scalar optimization problem', 1);('weights inthe sum', 1);('optimal solutions themoo', 1);('following', 1);('ne di erent rewardfunction', 1);('subproblemsthe rst', 1);('agents judge performancein', 1);('separation rater\x15tj \x001tmaxln\x10\x0extj\x0extj\x00 \x11 7fig', 1);('concepts pareto', 1);('dominance aand', 1);('particle separation power consumptionwhich penalizes actions', 1);('consecutive decisions', 1);('increase distance', 1);('tmaxis', 1);('time horizon', 1);('terminal statefor learning episodes', 1);('tmax', 1);('whole episoder\x15tmax', 1);('episodes havehr\x15i\x001tmaxd\x0extmax\x0ex0e\x00\x15c 9thus optimization problem', 1);('policy minimizes', 1);('system \x15cthe', 1);('reward function informs agent aboutthe energy costretj \x001tmax\x15', 1);('natj', 1);('counts number agents whichhave', 1);('actions swim', 1);('normalization term \x15', 1);('rewards ofthe order magnitude average estimater\x15 \x15tmaxfor multi objective optimization need', 1);('scalarization parameter rtottj r\x15tj retj 11and', 1);('singleobjective problems', 1);('reward rtot', 1);('goal isto nd policy', 1);('cumulative total rewardrtottmax', 1);('r\x15 re12calascibetta', 1);('5from expression', 1);('clear limits', 1);('limit minimize particledistance', 1);('energy consumption goalthat', 1);('case simpler cost', 1);('howdoes', 1);('regimestake place question', 1);('frontier multitaskproblem', 1);('appendix', 1);('implementeddue learning stochasticity fact performances di erent policies', 1);('independent learning sections ie', 1);('di erent initialization seeds foreach scalarization parameter learning section lasts50000 episodes', 1);('policies havebeen', 1);('di erent realizations owthen', 1);('policies ones thatmaximize average validation', 1);('resultshere', 1);('present results rst section presenta', 1);('analysis policy', 1);('decisiontime value', 1);('uncontrolledlyapunov exponent \x15\x1914 corresponds case \x151', 1);('control dynamicsbut interval decision times su\x0ecientlyhigh', 1);('sectionwe analyze speci', 1);('heuristicanalytical prediction', 1);('limit numericalstudy', 1);('heuristic policies function', 1);('particular show concavity', 1);('control nontrivial x swimmingrate tof', 1);('similar results41', 1);('detailed', 1);('approach multi objectiveoptimization reinforcement learning', 1);('learning processes withthe protocol', 1);('thescalarization parameter', 1);('rewards see12', 1);('typical learning process asingle value', 1);('learning phase', 1);('algorithm explores di erent random policiesafter', 1);('episodes learning parameters \x0fand', 1);('theinset fig', 1);('displays evolution', 1);('components ofthe reward', 1);('learning process', 1);('learnedpolicy value', 1);('example', 1);('learning process total reward', 1);('rtotvsthe', 1);('solid line show', 1);('average 1500episodes', 1);('area shows uctuations inthe', 1);('individual episodes inset show evolution thetwo contributions reward', 1);('r\x15red', 1);('datarefer', 1);('learning trial', 1);('ofthe total', 1);('minimize separationhr\x15i9 vs', 1);('energy hreidptmax j1retjefor di erent values identify optimalthe solutions', 1);('others sense ofpareto dominance', 1);('policies reachrewards outperform', 1);('optimal solutions inset', 1);('quantitative ideaof improvement respect na\x7f \x10ve passive baseline', 1);('improvement ofthe', 1);('policies function', 1);('thanthe baselines', 1);('na\x7f \x10ve baselineto good strategy', 1);('agents separation', 1);('isa nontrivial optimal strategy', 1);('agents thatoutperform na\x7f \x10ve', 1);('improvement 12in maximization total reward discoveredstrategy velocity eld', 1);('agent withrespect', 1);('convenientto counterrotate respect rotation', 1);('velocity eld', 1);('due nitedecision time', 1);('e ective swim', 1);('otherin direction identi', 1);('ow hand highvalues', 1);('high cost', 1);('thetwo agents', 1);('switch o engine valueslead transition region', 1);('new navigation strategies', 1);('taming lagrangian chaos multiobjective reinforcement learningfig', 1);('setting decision time', 1);('black triangles performances', 1);('heuristic policies lightblue circles policies', 1);('multiobjective problem', 1);('particular lledlarge circles', 1);('blue line', 1);('heuristic ones', 1);('inset relative', 1);('improvement hrtotias function', 1);('fromrl respect na\x7f \x10ve', 1);('red squares passive', 1);('blue pentagon respectivelyfig', 1);('lefthrtotivs', 1);('di erent policies', 1);('straight lines', 1);('theheuristic performances', 1);('extreme cases na\x7f \x10ve baseline passive policy', 1);('increases na\x7f \x10ve policy', 1);('passive policy', 1);('independent light', 1);('heuristic performances', 1);('right pictorial', 1);('representation 4policies', 1);('andcw rotation actions', 1);('countermove respect', 1);('ow rotationticular', 1);('policies regioncalascibetta', 1);('show performance', 1);('comparison 81heuristic ones', 1);('value hrtotias afunction', 1);('lines asshown', 1);('maximization total', 1);('cumulative rewardhrtoti', 1);('fig6rightad', 1);('show tabular representation policies', 1);('optimal ie lie onthe', 1);('frontier appertain region', 1);('ow rotation', 1);('important increases', 1);('convenient navigate ow', 1);('agents separation eg policy', 1);('fig6c', 1);('stateswe end section', 1);('possible symmetries', 1);('strategies fact counterrotatingaction respect', 1);('ow rotation thatemerges', 1);('counterrotation clockwise anticlockwise', 1);('thiscould', 1);('lead ambiguity choice swimmingstrategies', 1);('equivalent strategies', 1);('convergence issues', 1);('ableto identify optimal solutions42', 1);('heuristic', 1);('role interval decisiontimes', 1);('heuristic hardwiredstrategies', 1);('policies space the4 longitudinal states rst', 1);('qualitative e ect', 1);('decision time neededto', 1);('show plot', 1);('di erentvalues', 1);('small', 1);('frequent measurements thesystem', 1);('major adjustments control variables lead high qualityperformances', 1);('time step dt', 1);('frontierhr\x15ihrei policies', 1);('equivalent meaningthat', 1);('paretodominates', 1);('others terms', 1);('sense environment', 1);('need search optimal policies', 1);('thedecision time values', 1);('concave frontiers thestrategies', 1);('di erent roles starts largewith respect', 1);('ine ective', 1);('separation growth', 1);('anenergy cost instance', 1);('policiesspace alower', 1);('thedashed', 1);('linear dependence', 1);('small show empty symbolsthe', 1);('problem study', 1);('heuristic policiesit', 1);('increases frontier', 1);('linear behavior', 1);('thewhole duration episode linear regimeof separation ie agents', 1);('erentiable velocity eld', 1);('thelyapunov exponent', 1);('system \x15c actions', 1);('direction introduce aclear contraction factor', 1);('small wecan estimate', 1);('exponent theheuristic policies follows\x15c\x15tmax\x002ft2\x00ft1tmax 13where', 1);('total episode duration astmaxt0t1t2', 1);('average time inwhich', 1);('t0and', 1);('at2is theaverage time agent passive', 1);('na\x7f \x10ve baselinet2tmax', 1);('t1t00', 1);('estimate \x15na\x7f \x10ve\x15\x002fif', 1);('thismeans', 1);('\x1514 andf', 1);('na\x7f \x10ve baselineshould', 1);('close perfect control ie keepthe distance', 1);('constant hand', 1);('thesame decomposition total time hreican', 1);('ashrei\x00\x15tmax2t2t1 14which implies linear dependence hr\x15iandhreihr\x15i\x00\x15\x00f\x15hrei 15which', 1);('frontier dtin', 1);('relation applies ie', 1);('total reward ashrtoti\x00\x15hrei\x10 \x00f\x15\x11', 1);('taming lagrangian chaos multiobjective reinforcement learningit', 1);('clear policies lie', 1);('equivalent f\x15 task isto minimizehreiremember thathreiis', 1);('systemfor f\x15 goal maximize hrei maximal ie', 1);('agents passive', 1);('cf\x15all policies', 1);('linearity frontier', 1);('cfr', 1);('eq 10which linear number', 1);('di', 1);('erent nonlinear choices', 1);('linearity willnot invalidate decomposition', 1);('nition ofrewards', 1);('nition whichis', 1);('highlight non', 1);('trivial role', 1);('thediscrete decision time', 1);('due discreteness agents need', 1);('intelligent way di erent policies notequivalent', 1);('terms performances5', 1);('discussions', 1);('multiagent multitask problemset minimize time dispersion rate ofa', 1);('lagrangian chaos', 1);('astochastic ow energy consumption', 1);('theactive control system', 1);('observation capabilities', 1);('onthe longitudinal transversal velocity gradients', 1);('possible choices ofaction agent pair', 1);('space deterministicpolicies', 1);('large counts 1516navigation strategiesfurthermore agents', 1);('velocity intensity', 1);('able toovercome chaoticity properties system solvethis problem', 1);('approach basedon combination', 1);('algorithm andthe scalarization technique', 1);('show systematic investigation problem', 1);('discrete decision times', 1);('interval decisiontimes control variables performances', 1);('easy guess priori thelimit', 1);('continuous control', 1);('problem reduces toa linear', 1);('frontier policies equivalentie', 1);('concave strategies', 1);('di erentroles', 1);('high decision time 1\x15 with\x15the', 1);('lagrangian lyapunovexponent', 1);('ine ective tocontrol pair separationwe stress', 1);('techniques implementedis modelfree', 1);('local instantaneous information', 1);('ow notrequire', 1);('individual optimization initializationand', 1);('di erent fromana\x7f \x10ve baseline general di erent fromheuristic references', 1);('longitudinal actions onlyit', 1);('present approach tothe case smart tracers', 1);('able control separationwithin scales velocity eld di erentiableie inertial range turbulent owsacknowledgmentsthis work', 1);('european research council', 1);('erc', 1);('unions horizon', 1);('research innovation programme', 1);('grant agreement no882340author', 1);('contribution statementall authors', 1);('thenumerical simulations data analysis authors', 1);('paper revision andinput authorsdata availability statementdata', 1);('applicable article datasetswere', 1);('current studyappendix', 1);('optimization problem', 1);('actionvalue function', 1);('qsa', 1);('agents state sand takeactiona algorithm', 1);('converge optimal policy', 1);('iterative trialanderror protocol decision time tj agents pair measuresits statestjand', 1);('an\x0fgreedystrategy atjstj arg max afqstjagwith probability 1\x00\x0foratjis', 1);('dynamical system evolve time', 1);('control directions andvelocity intensity', 1);('qstjatj', 1);('rtottj1 maxaqstj1a\x00qstjatj17where', 1);('learning rate', 1);('updates', 1);('upto end episode ttmax reward', 1);('learning protocol', 1);('withanother pair', 1);('initial distance', 1);('local optimum', 1);('theequationq\x03stja rtottj1 maxaq\x03stj1a andde', 1);('policyas arg maxafq\x03sagcalascibetta', 1);('9in order', 1);('convergence algorithm', 1);('functions thetime', 1);('stateaction pair explorationparameters decreases time', 1);('nsa number decision times inwhich couple sa', 1);('xansajaj', 1);('nsa 19\x0f', 1);('ns 20with', 1);('numerical values constants havebeen', 1);('preliminary tests theinitialization matrix', 1);('qwe', 1);('largeoptimistic value stateaction pairsreferences1', 1);('p lermusiaux subramani j lin', 1);('kulkarni gupta dutt lolla h jr', 1);('hajj alic mirabito jana', 1);('intelligent autonomous ocean', 1);('j mar res', 1);('elor bruckstein tworobot', 1);('source seekingwith point measurements', 1);('theor comput sci', 1);('wu couzin', 1);('zhang bioinspired', 1);('explicit gradient estimation', 1);('ifac proceedings volumes', 1);('ifac', 1);('distributed estimation', 1);('networkedsystems4fstaxis algorithm bioinspired emergent gradienttaxis', 1);('alife', 1);('fifteenth internationalconference synthesis simulation', 1);('systems alife', 1);('arti', 1);('bechinger r di leonardo h l\x7f', 1);('owen c', 1);('reichhardtg volpe g volpe active', 1);('crisanti falcioni vulpiani g paladin lagrangian', 1);('chaos transport', 1);('di usion uidsriv', 1);('nuovo cim', 1);('cencini', 1);('cecconi vulpiani chaos fromsimple', 1);('complex systems', 1);('series advances instatistical mechanics world', 1);('scienti', 1);('ginelli', 1);('physics vicsek model', 1);('eurphys j spec top', 1);('marchetti j', 1);('joanny ramaswamy', 1);('liverpool j prost rao r aditi simha hydrodynamics', 1);('soft active matter', 1);('ballerini n cabibbo r candelier cavagnae cisbani giardina v lecomte orlandi g parisia procaccini viale v zdravkovic interactionruling', 1);('collective behavior', 1);('topologicalrather metric distance', 1);('evidence', 1);('eld studyproc', 1);('natl acad sci', 1);('n khurana n ouellette stability', 1);('model ocksin turbulentlike ow', 1);('j phys', 1);('bonaccorso buzzicotti p clark dileoni k gustavsson zermelos', 1);('optimalpointtopoint', 1);('turbulent ows', 1);('chaos', 1);('buzzicotti', 1);('bonaccorso p clarkdi leoni k gustavsson optimal', 1);('control pointtopoint navigation turbulent time', 1);('dependent ows usingreinforcement learning', 1);('aixia', 1);('advances arti', 1);('cham', 1);('springerinternational publishing14 j k alageshan k verma j bec r pandit machine', 1);('learning strategies', 1);('microswimmersin turbulent ows', 1);('phys rev e', 1);('apr', 1);('g reddy celani j sejnowski vergassolalearning', 1);('soar turbulent environments', 1);('proc natlacad sci', 1);('g reddy j wongng celani j sejnowski', 1);('vergassola glider', 1);('reinforcement learningin eld', 1);('nature', 1);('n orzan', 1);('leone mazzolini j oyero celanioptimizing', 1);('energy reinforcement learning arxiv preprint arxiv220314271', 1);('borra', 1);('biferale cencini celani reinforcement', 1);('learning pursuit evasion microswimmers atlow reynolds number', 1);('phys rev fluid', 1);('cac coello handling', 1);('evolutionary multiobjective optimization survey', 1);('congress evolutionary computation cec00cat no00th8512', 1);('coello veldhuizen g lamont evolutionaryalgorithms solving multiobjective problems secondedition springer', 1);('liu x xu hu multiobjective', 1);('comprehensive overview', 1);('ieee trans systman cybern syst', 1);('p vamplew r dazeley berry r issabekov', 1);('dekker empirical', 1);('evaluation methods multiobjective reinforcement learning algorithms', 1);('mach learn', 1);('natarajan p tadepalli dynamic', 1);('preferences inmulticriteria reinforcement learning', 1);('ofthe 22nd', 1);('international conference', 1);('machine learning icml', 1);('york ny usa', 1);('computing machinery24 castelletti g corani e rizzoli r soncini sessaand e weber reinforcement', 1);('learning operationalmanagement water system pages', 1);('pergamonpress', 1);('p vamplew j yearwood r dazeley berry onthe', 1);('limitations scalarisation multiobjective reinforcement learning pareto fronts', 1);('ai', 1);('advancesin arti', 1);('springer berlinheidelberg', 1);('e zitzler', 1);('thiele laumanns', 1);('fonseca', 1);('fonseca performance', 1);('assessment multiobjective optimizers analysis review', 1);('ieee trans evolcomput', 1);('j bec multifractal', 1);('concentrations inertial particles insmooth random ows', 1);('j fluid mech', 1);('c w', 1);('gardiner handbook stochastic methods', 1);('chemistry', 1);('springercomplexity springer', 1);('taming lagrangian chaos multiobjective reinforcement learning29 r sutton g barto reinforcement', 1);('anintroduction mit', 1);