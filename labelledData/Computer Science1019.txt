('fig', 20);('jiang', 16);('sec', 13);('theorem', 13);('rl', 10);('mdp', 9);('metalearning', 7);('eq', 6);('task structure', 6);('pomrl', 5);('aggregating', 5);('oracle', 5);('pmlr', 5);('baxter', 4);('khodak', 4);('pt', 4);('appendix', 4);('losst1 t3 t6 t15', 4);('error', 4);('finn', 3);('horizon online', 3);('average regret', 3);('po', 3);('pomrlandadapomrl', 3);('task similarity', 3);('selectionprocedure', 3);('alg', 3);('pomrland', 3);('recall', 3);('international conference', 3);('machine learning', 3);('neural information processing systems', 3);('lemma', 3);('losst1t2t3 t4 t5', 3);('denevi', 2);('model accuracy', 2);('reinforcement learning', 2);('discount factor', 2);('arumugam', 2);('kearns', 2);('xu', 2);('model estimators', 2);('aruba', 2);('mdp mwith', 2);('ptfor', 2);('iid transition samples batch', 2);('importantly', 2);('online metareinforcement learninginputset', 2);('po1to', 2);('sa', 2);('andevalfortaskttdofortthbatch ofmsamplesdoptm', 2);('squares minimizer', 2);('q3', 2);('adaptation', 2);('large initialization', 2);('dependent variances', 2);('poand', 2);('pomrlalg', 2);('100independent runs', 2);('dirichlet', 2);('knowledge', 2);('new unseen tasks dier', 2);('ccmtsa', 2);('cis', 2);('machine', 2);('springer', 2);('pineau', 2);('zahavy', 2);('mse', 2);('concentration', 2);('vptangbracketright', 2);('bm 20t', 2);('individual task', 2);('state space', 2);('sm', 2);('pomrl noregret learningtoplan increasinghorizonskhimya khetarpalkhimyakhetarpalmailmcgillcaschool computer sciencemcgill', 1);('milaclaire vernadevernadedeepmindcomdeepmind londonbrendan odonoghue', 1);('londonsatinder singh', 1);('londontom zahavy', 1);('londonabstractwe', 1);('study problem planning model uncertainty online', 1);('setting agent', 1);('task agent use experience task andacrosstasks', 1);('transition model distribution tasks', 1);('analgorithm metalearn', 1);('tasks utilize plan taskand upperbound regret planning loss', 1);('suggests average regretover tasks decreases number tasks increases tasks', 1);('inthe', 1);('classical singletask setting', 1);('models accuracy number samples', 1);('task generalizethis', 1);('metarl study dependence planning horizons number oftasks', 1);('based', 1);('theoretical ndings', 1);('increasingdiscount factors validate signicance empirically1', 1);('introductionmetalearning caruana', 1);('thrun pratt', 1);('powerful paradigm leverage past experience', 1);('sample complexity learning', 1);('online', 1);('considers sequential setting agent', 1);('accumulatesknowledge uses past experience', 1);('good priors', 1);('structure approaches', 1);('sample complexity', 1);('schmidhuber huber1991 thrun pratt', 1);('balcan', 1);('agent uses', 1);('model environment planactions', 1);('rewards key component agents decision making isthe horizon', 1);('general evaluation horizon', 1);('task theequal', 1);('contributionwork', 1);('deepmind1arxiv221214530v1', 1);('dec', 1);('2022gr wing', 1);('e ak ea w asksimilarity amples', 1);('figure', 1);('horizons metareinforcement learning', 1);('sequenceof tasksptwhose optimal parameters', 1);('agent builds transitionmodel task plans inaccurate models', 1);('previous tasks agentmetalearns initialization model', 1);('pot', 1);('scales tasksimilarity inverselywith number tasks knowledge accumulates uncertainty diminishes agent plan withlonger horizonslearner', 1);('shorter guidance horizon', 1);('setting thesize evaluation horizon order 1eval1 discount factor eval01 agentmay usenegationslashevalfor planning instance', 1);('classic result', 1);('blackwell optimality blackwell1962', 1);('states exists discount factor', 1);('corresponding optimal policy policy alsooptimal', 1);('agent plans optimal foranyevalin', 1);('arcade learning environment bellemare', 1);('discount factor eval', 1);('mnih', 1);('discountfactor acts regularizer', 1);('amit', 1);('petrik scherrer', 1);('van seijen', 1);('franoislavetet', 1);('reduces planner', 1);('mdps arumugam', 1);('choice planning horizon plays signicant role computation', 1);('kocsis szepesvri', 1);('complexity policy class', 1);('inaddition', 1);('discount factors', 1);('signicant improvements performance', 1);('flennerhag', 1);('luketina', 1);('model optimal guidance planning horizon', 1);('accuracy model', 1);('data scarce guidance discount factor evalshould', 1);('reason straightforward model', 1);('inaccuratethen errors', 1);('trajectory shorter eective planning horizon willaccumulate', 1);('true eval', 1);('whilethat', 1);('batch singletask setting question eective planning horizon', 1);('openin online', 1);('setting agent accumulates knowledge', 1);('tasks limitedinteractions', 1);('taskin work', 1);('weleverage', 1);('structural task similarity', 1);('convergence tasks areseen', 1);('central question work iscan metalearn model', 1);('tasks adapt eective planning horizon accordinglywe', 1);('average regretupperbound analysis aruba', 1);('2019to generalize planning loss bounds metarl setting highlevel intuitive', 1);('main contributions follows2we formalize planning', 1);('metarl setting average planning loss minimizationproblem', 1);('itunder structural tasksimilarity assumption', 1);('novel highprobability', 1);('regretupperbound planning loss algorithm', 1);('tasksimilarity parameter onthey', 1);('knowledge rstformal', 1);('arubastyle', 1);('analysis show metarl ecient', 1);('rlour', 1);('theoretical result highlights', 1);('new dependence planning horizon size withintask datamandon number tasks observation', 1);('heuristics toadapt planning horizon', 1);('overall samplesize2', 1);('preliminariesreinforcementlearning', 1);('markov decision processes mdps mangbracketleftsarp', 1);('evalangbracketrightwheresis nite', 1);('ais', 1);('ssandaa', 1);('state ss', 1);('available action aa probability vector', 1);('psadenes', 1);('atransition model state space probability distribution', 1);('feasible models', 1);('dpswhere sthe', 1);('probability simplex dimension', 1);('s1', 1);('1the diameter ofdp policy afunctionsaand characterizes agents behaviorwe', 1);('reward setting ier0rmaxand', 1);('loss generality', 1);('rmax', 1);('task policy letvmrsbe valuefunction', 1);('esummationtextt0trsts0s', 1);('goal agent nd optimal policy', 1);('marg', 1);('positive measure', 1);('ambiguity givenstate action spaces reward function', 1);('sar', 1);('optimal policies fordiscount factor', 1);('pstmwheremangbracketleftsarpangbracketright', 1);('bigo', 1);('oando', 1);('universal constants polylogarithmic terms', 1);('tsaand0the', 1);('true model world', 1);('unknown mustbe', 1);('optimization problem constructa modelangbracketleftrpangbracketrightfrom data nd', 1);('mfor', 1);('mdp mangbracketleftsarpangbracketright', 1);('ce', 1);('inaccurate models setting', 1);('dene planning loss gapin', 1);('mdp mwhen', 1);('evaland optimal policy approximate model', 1);('mlmm', 1);('eval bardblvm evalm evalvmm evalbardblthus theoptimaleectiveplanninghorizon 11is', 1);('discount factor minimizesthe planning loss ie min 0evallmm evaltheorem', 1);('mbe mdp', 1);('rewards evaluationdiscount factor eval', 1);('mbe', 1);('true reward function theapproximate transition model', 1);('0samples stateaction pair withprobability', 1);('1vextendsinglevextendsinglevextendsinglevextendsinglevextendsinglevextendsinglevm evalm evalvmm evalvextendsinglevextendsinglevextendsinglevextendsinglevextendsinglevextendsingleeval1eval12rmax12parenleftbiggradicalbigg2mlog2saparenrightbigg1where', 1);('ppsthis', 1);('model estimator ie empirical average', 1);('transitions givenby generator model pair sa', 1);('upperbound planning loss function theguidance discount factor', 1);('result decomposes loss', 1);('constant bias which3', 1);('ground truth', 1);('transition model planninghorizon planning loss function discount factor', 1);('priorknowledge learner 5102050samples', 1);('task estimate model', 1);('corresponding thecurves sub gure', 1);('inspecting', 1);('sub gures observe', 1);('values mlead', 1);('eective discount factor', 1);('value macross tasks egm', 1);('eective discount occurs thelearner', 1);('weight ground truth', 1);('decreases tends toeval variance uncertainty term increases decreases as1m', 1);('asmthat', 1);('factor vanishes lowsample regime optimal eective planninghorizon tradeo termsillustration eects', 1);('simple 10state 2action random', 1);('leftmostplot uses', 1);('model estimator reproduces results', 1);('wethen', 1);('poas fig', 1);('factor 01pm', 1);('1summationtextixim observe increasingthe weight 01on good', 1);('metalearning regret', 1);('online metarl problem agent presentedwith sequence tasks', 1);('m1m2mt', 1);('ttmtangbracketleftsaptr evalangbracketright', 1);('mdpsonly', 1);('dier transition matrix dynamics model', 1);('sequentiallyestimate model', 1);('task tfrom batch mtransitions', 1);('stateaction pair1its goal minimize average planning loss', 1);('form task', 1);('aslm1tm1teval 1ttsummationdisplayt1lmtmteval 1ttsummationdisplayt1bardblvmtevalmtevalvmtmtevalbardbl 2note reference', 1);('mt', 1);('discount factor tasksone', 1);('objective stochastic', 1);('dynamic regret task tt learner competes againstthe optimal policy currenttrue model', 1);('policy inhindsight', 1);('classical denitions regretnote', 1);('dynamic regret dierent', 1);('aruba khodak', 1);('online setting data', 1);('arbitrary stream', 1);('eachtask comparator', 1);('minimum withintask loss hindsight model howeverwe', 1);('access simulator', 1);('dene regret respect', 1);('parameterone key consequence dierence regret bounds', 1);('new results below1so total msasamples43', 1);('online metareinforcement learningwe', 1);('formalize planning', 1);('metarl setting', 1);('present approach', 1);('main result highprobability', 1);('average planning loss assumedstructure', 1);('structural assumption across tasksin', 1);('practical scenarios key reason employ', 1);('learner leverage tasksimilarity task variance structure', 1);('bounded', 1);('core assumptionin analysis', 1);('recent meta learning', 1);('cesabianchi', 1);('algorithms work', 1);('structural assumption ttptpcenteredat', 1);('posasand', 1);('sabardblptsaposabardbl maxsasaas 3this', 1);('implies maxttprimebardblptsaptprimesabardbl2 metadistribution', 1);('pis', 1);('asmall subset simplex', 1);('results highprobability assumption insteadof', 1);('sure statement experiments', 1);('gaussian dirichlet', 1);('priors thesimplex', 1);('thata multitask environment', 1);('wheniewhen eective diameter models issmaller', 1);('entire feasible space32', 1);('approachforsimplicityweshallassumethroughoutthattherewardsareknown2andfocusonlearningandplanningwithan', 1);('approximate dynamics model', 1);('task ttwe access simulator transitions', 1);('miid samples', 1);('xtisai1msmptsacategorical', 1);('distributionfor sa compute empirical estimator sprimesptsasprime summationtextmi11xtisasprimemwith naturallysummationtextsprimeptsasprime', 1);('leastsquares loss outerloop step optimize regularization optimallybalance bias variance', 1);('dynamics model', 1);('squares adapt', 1);('standard technique', 1);('cella', 1);('model estimation problem round', 1);('current model', 1);('square loss', 1);('regularizer htto', 1);('below3and parameter t0foreach sasa solveptsa arg minpsasparenleftbigglscripttpsax1mthtt bardbl1mmsummationdisplayi11xtisapsabardbl22tbardblpsahtbardbl22parenrightbigg4where use 1xtisato', 1);('state vector', 1);('rs importantly', 1);('aect bias variance', 1);('estimatorsince dene withintask loss solution equation', 1);('form convexcombination empirical average', 1);('pttht', 1);('parameter2we note', 1);('reward straightforward', 1);('main result3in principle loss', 1);('regularizer htbut specify', 1);('induces goodperformance5outerloop', 1);('task 1tt learner hasalready', 1);('dierent tasks dene htas anaverage', 1);('means aom', 1);('rate sett compute', 1);('mean squared error mse ptsa', 1);('mse ptsa2t21', 1);('1t 1t21m', 1);('complete pseudo code note', 1);('tasksimilarity parameter', 1);('number tasks priori tasks', 1);('onlinethe learner performs metarl', 1);('withintask estimation dynamics model', 1);('ptvia', 1);('abatch ofmsamples task outer loop step metaupdate regularizer', 1);('pot1alongside', 1);('rate t1 task use', 1);('horizon evalwe defer details step', 1);('nontrivial partial consequence theoreticalanalysis', 1);('learner performs', 1);('imperfect model', 1);('particular policy iteration combination policy evaluation improvement valueiteration', 1);('optimal policy', 1);('mdp mtalgorithm', 1);('uniform tasksimilarity saa matrix size', 1);('ptplanning ptmoutputptupdate pot1t11211tm1metaupdate aom eq', 1);('average regret bound', 1);('onlinemetalearningour', 1);('main theoretical result', 1);('task structure ie true0theorem', 1);('average planning loss equation', 1);('pomrl leval1eval12s12oradicalbig1tparenleftbiggradicalbig2mparenrightbigg2m', 1);('12mradicalbigm2m 16with probability', 1);('where21is measure tasksimilarity max sasathe proof result', 1);('new concentration', 1);('model estimator', 1);('term rhs corresponds uncertainty dynamicsfirst', 1);('term dominates', 1);('oradicalbigmas22m', 1);('o1mbut', 1);('order term', 1);('bias mis', 1);('small for6small', 1);('1m rst term dominates rhs', 1);('tooparenleftbig1mtparenrightbig highlights interplay structural assumption parameter amountof', 1);('datamavailable round regimes', 1);('various similarity levels', 1);('show dependence regret', 1);('mandtfor xedin', 1);('appendix fig f3implications', 1);('degree tasksimilarity ievalues', 1);('suggests degree ofimprovement', 1);('meta learning scales task similarity', 1);('thusfor', 1);('meta learning', 1);('algorithm', 1);('structure problem space formalize', 1);('large techniques', 1);('scale accordinglywhen', 1);('rate t1for allt algorithmboils', 1);('potfor', 1);('stream batches', 1);('potaggregatesunsurprisingly', 1);('reects wehave estimate', 1);('model space size withmtsamplesleval1eval12s12oparenleftbiggradicalbiggmtparenrightbigg7when', 1);('assumption relevant boundremains', 1);('degrades reect need estimate', 1);('tmodels', 1);('usual estimation error task rst term', 1);('order in1m', 1);('pothat', 1);('relevant hereleval1eval12s12oparenleftbig1mparenleftbig1', 1);('radicalbigg1 1mparenrightbig1mparenrightbig8connections', 1);('comparable ofaruba', 1);('parallel highprobability average regret bounds', 1);('o1m', 1);('upper bounds average withintask regret', 1);('ubmremark', 1);('role', 1);('new databatch model estimation knowledge', 1);('exact intuitive updaterule theory', 1);('pomrlequipped', 1);('andplugin tin practice', 1);('empiricalvariance estimator tools', 1);('bernstein', 1);('scope workas', 1);('order term ino1t', 1);('problem study4', 1);('practical considerations adaption ontheflyin', 1);('pomrlthat', 1);('meta learns task similarity parameter calladapomrl', 1);('15tasks details experiment setup', 1);('pomrlrecall pomrlis', 1);('learning regularizer assumes knowledgeof', 1);('task similarity ie observe', 1);('round ttpomrlis', 1);('able toplan', 1);('learns adapts regularizer', 1);('tasks convergence rate nalperformance corroborates theory7123456789101112131415task', 1);('t000510152025303540planning loss099eval', 1);('initial set tasks', 1);('butimproves taskscan', 1);('metalearn tasksimilarity parameter practicethe parameter', 1);('c details', 1);('adapomrl uses', 1);('welfordsalgorithm', 1);('compute online estimate variance', 1);('plugsin estimate whereverpomrlwas', 1);('true value perspective adapomrl', 1);('pomrlis', 1);('oracle ie', 1);('howeverin', 1);('practical scenarios learner informationa priori', 1);('observe metalearningthe', 1);('adapomrl adapt incomingtasks', 1);('onthey adapomrl', 1);('ata costie performance gap comparison', 1);('pomrlbut', 1);('eventuallyconverges albeit', 1);('rate intuitive', 1);('similar theoreticalguarantee applies', 1);('remark', 1);('1this online estimation', 1);('practitioner aectthe results nite number tasks beginning', 1);('initial tasks', 1);('result inan', 1);('bias hand setting 1too', 1);('large ie', 1);('weight theprior increase variance', 1);('particular cases', 1);('true smalla', 1);('slow convergence observe empirical', 1);('extreme case', 1);('slow adapomrl asit', 1);('tasks discovers optimal behavior', 1);('aggregate batchesalgorithm', 1);('uniform initialize 1as matrix size', 1);('ptplanning ptmevaloutputptupdate pot1t1 welfords', 1);('online algorithmparenleftbig otpot1potparenrightbigmetaupdate', 1);('aomeq', 1);('tasksimilarity parameterupdatet11t1211tm1metaupdate', 1);('rate plug maxsatasks', 1);('certain states actions', 1);('uniform notion tasksimilarity', 1);('practical settings transition distribution mightremains part state space', 1);('dierent tasks', 1);('thesescenarios', 1);('hard analyse general local changes model parameters', 1);('implychanges optimal value function', 1);('modify optimal policy', 1);('tight metadistribution nonuniform noise levels', 1);('ourconcentration result', 1);('valid sapair', 1);('uniform localsa', 1);('parameter andrespectivelyaresamatrices stateaction', 1);('rate t85', 1);('experimentswe', 1);('study empirical behavior planning online', 1);('q1does', 1);('initialization dynamics model facilitate', 1);('planningaccuracy choice eval', 1);('q2does', 1);('initialization dynamicsmodel', 1);('q3how', 1);('tasks ie', 1);('source', 1);('experiment x', 1);('posassee', 1);('new tasktt sample', 1);('ptfrom dirichlet', 1);('sec32we', 1);('samatrices', 1);('directional variance', 1);('dirichletdistributions', 1);('priors nonuniform variance levels simplex', 1);('closeto simplex boundary', 1);('aligned', 1);('theory use max', 1);('aforementioned single scalar value', 1);('eachpt characterizesa random chain', 1);('10states6anda 2actions', 1);('stateactionpair transition function', 1);('psasprimeis', 1);('k 5states', 1);('skremaining', 1);('01and normalize', 1);('metareinforcement', 1);('accuracy eval', 1);('q1we', 1);('aforementioned problem setting total', 1);('tasks focus', 1);('loss gains', 1);('model accuracy x eval', 1);('selectionprocedureand', 1);('show planning loss', 1);('baselines 1oracle', 1);('knowledgeknows', 1);('uses estimator', 1);('exact regularizerpoand optimal', 1);('rate t1211tm1 2without', 1);('ptpt', 1);('metalearns theregularizer', 1);('metalearns onlythe regularizer', 1);('tasksimilarity online oracle', 1);('strong baseline providesa', 1);('inaccurate model play role empirical', 1);('baselinesthe number samples', 1);('competitive heuristics', 1);('selectionprocedure sec', 1);('wealso run', 1);('ignores metarl structure justplans', 1);('appendix f2inspecting fig', 1);('approach adapomrl green results', 1);('loss tasks', 1);('stable andapproaches optimal value', 1);('knowledge baseline', 1);('blue contrary', 1);('red agent struggles', 1);('new tasks', 1);('round performance notimprove adapomrl', 1);('improves tasks', 1);('whilst adaptation', 1);('tasksimilarityonthey primary', 1);('performance gap adapomrl andpomrl', 1);('practical algorithm theoreticalguarantees', 1);('strong baseline corresponds bothknown task structure regularizer52', 1);('q2we', 1);('run adapomrl fort 15with001as', 1);('losses range values ofguidancefactors', 1);('4b observe4the variance distribution', 1);('coecient parameters 1s', 1);('variancemore details choices', 1);('appendix f1 dirichlet', 1);('small variance', 1);('highprobabilityversion structural assumption', 1);('max ii5our priors multivariate', 1);('distribution dimension', 1);('sso', 1);('theoretical rate', 1);('sto', 1);('app', 1);('f implementation details6we', 1);('additional experiments', 1);('size state space', 1);('appendix fig f59123456789101112131415task t000510152025303540planning loss099eval', 1);('knowledgewithout metalearningpomrla00', 1);('t000204060810planning losseval099', 1);('loss', 1);('optimal guidance discount factoroptimal gamma cfigure', 1);('online metalearning pertask', 1);('loss algorithmspomrlandadapomrl', 1);('baselines methods', 1);('xedeval 099badapomrl planning loss decreases tasks', 1);('markers', 1);('thethat minimizes', 1);('respective tasks', 1);('standard error cadapomrl', 1);('optimal guidance discount factor', 1);('axis depicts eective planning horizon ieone minimizes', 1);('optimal', 1);('aka eective planning horizon', 1);('axis shows minimum planning loss', 1);('agent thatroundt', 1);('100independent runs error', 1);('1standard deviationin', 1);('4b agent', 1);('tasks intermediate value discount optimal ie onethat minimizes', 1);('structureacross tasks agent', 1);('tasks eective planning horizon 07shifts alarger value', 1);('evaluation eval 099as incorporate knowledge', 1);('task distribution ie', 1);('initialization ofthe dynamics model note adaptive', 1);('rate tputs', 1);('amounts weight', 1);('weight model initializationthat', 1);('theory pertask planning loss decreases', 1);('tgrows', 1);('values meaning', 1);('inaddition appendix fig f4', 1);('depicts eective planning horizon', 1);('andwithout meta learning baselines53', 1);('q3we', 1);('scenarios learner', 1);('strong task structure ie0011smfor', 1);('low data', 1);('task iem', 1);('good amounts tasksimilaritywe letvary', 1);('convergence 0025is theintermediate regime needs', 1);('case dont expectmuch', 1);('small inset gures', 1);('taskdistribution simplex cases adapomrl estimatesonline report planning losses fora range ofs', 1);('inspecting fig', 1);('observe presence', 1);('5aall methods', 1);('task structure decreases forintermediate regime', 1);('consistent performance', 1);('infig 5c adapomrl andpomrlcan', 1);('comparison baselinesnext report', 1);('loss plot adapomrl', 1);('figures', 1);('5d5e 5f', 1);('intermediate value tasksimilarity', 1);('gains albeit ata', 1);('speed convergence contrast', 1);('large value 0047indicates', 1);('minimal gains', 1);('5f learner struggles', 1);('goodinitialization model dynamics', 1);('loss curves', 1);('ushaped', 1);('intermediate optimal guidance value', 1);('adapomrl worseoverall', 1);('initial run', 1);('meaning signicant improvement method10strong', 1);('structure123456789101112131415task t000510152025303540planning loss099eval', 1);('knowledgewithout metalearningpomrlamedium structure123456789101112131415task t000510152025303540planning loss099eval', 1);('knowledgewithout metalearningpomrlbloosely structured123456789101112131415task t000510152025303540planning loss099eval', 1);('knowledgewithout metalearningpomrlc\x13\x11\x13\x13\x11\x14\x13\x11\x15\x13\x11\x16\x13\x11\x17\x13\x11\x18\x13\x11\x19\x13\x11\x1a\x13\x11\x1b\x13\x11', 1);('\x14\x11\x13\x18\x13\x11\x13\x13\x11\x18\x14\x11\x13\x14\x11\x18\x15\x11\x13\x15\x11\x18\x16\x11\x13\x16\x11\x18\x17\x11\x133odqqlqj\x03rvv7 \x147 \x157 \x167 \x187 \x197', 1);('\x14\x177 \x14\x18d\x13\x11\x13\x13\x11\x14\x13\x11\x15\x13\x11\x16\x13\x11\x17\x13\x11\x18\x13\x11\x19\x13\x11\x1a\x13\x11\x1b\x13\x11 \x14\x11\x13\x18\x13\x11\x13\x13\x11\x18\x14\x11\x13\x14\x11\x18\x15\x11\x13\x15\x11\x18\x16\x11\x13\x16\x11\x18\x17\x11\x133odqqlqj\x03rvv7 \x147 \x157 \x167 \x187 \x197', 1);('\x14\x177 \x14\x18 e\x13\x11\x13\x13\x11\x14\x13\x11\x15\x13\x11\x16\x13\x11\x17\x13\x11\x18\x13\x11\x19\x13\x11\x1a\x13\x11\x1b\x13\x11 \x14\x11\x13\x18\x13\x11\x13\x13\x11\x18\x14\x11\x13\x14\x11\x18\x15\x11\x13\x15\x11\x18\x16\x11\x13\x16\x11\x18\x17\x11\x133odqqlqj\x03rvv7 \x147 \x157 \x167 \x187 \x197', 1);('\x14\x177 \x14\x18 ffigure', 1);('adapomrl robust', 1);('amount ofdatam', 1);('5available round tt', 1);('small value reects fact tasks', 1);('toeach share good', 1);('structure whereas', 1);('taskssimplex plots', 1);('metadistribution dimension', 1);('good model initialization', 1);('learner struggles copewith', 1);('1standard deviation uncertaintyacross 100independent runsdoes hurt performance', 1);('adapomrl apriori knowledge ofthe number tasks', 1);('taskstructure ie adaptation onthey6', 1);('horizon', 1);('heuristics design adaptive schedule', 1);('schedule', 1);('dong', 1);('phasesttt0 tune discount factor 11t15t adapt schedule problem timeis', 1);('tasks t0 dene phase size', 1);('ttand', 1);('corresponding tast0m', 1);('ttsalparenleftbig1tmtmt1bracehtipupleftbracehtipdownrightbracehtipdownleftbracehtipuprightecient', 1);('sample sizeparenrightbig 11t15twherelis', 1);('maximum trajectory length size', 1);('ttt1', 1);('ecient samplesize', 1);('current tasks samples samples', 1);('usedto construct estimator', 1);('pomrl7the', 1);('average planning loss', 1);('bias practicewe observe', 1);('order terms1162', 1);('schedule min10', 1);('rhs function formumapsto11eval11cmtsa12where rst term', 1);('simplify scale', 1);('termsct 1t1m2m', 1);('constant equation', 1);('uwith', 1);('cdoes', 1);('analytical value', 1);('uis', 1);('true shapeof loss wrt', 1);('growth schedule', 1);('choices online generalthe existence strict minimum', 1);('uin01is', 1);('values ofccmtsa function', 1);('monotonic minimum', 1);('explicitranges proposition', 1);('appendix eproposition', 1);('existence strict minimum 01is', 1);('follows0ifc11ifc 121c1cotherwise ie 12c 1we use values', 1);('typically', 1);('small multiplicative term', 1);('large andthe', 1);('informative concentration', 1);('small potentiallyclose', 1);('equal zero heuristic', 1);('additional 0such theguidance discount factor min10 where0should', 1);('practitioner toallow shorthorizon planning', 1);('empirically', 1);('reasonable random', 1);('setting corresponds empirical minima', 1);('4b123456789101112131415number tasks', 1);('t100planning lossbest fixeddynamic besteval099fmttt', 1);('min10 a123456789101112131415number tasks', 1);('t100planning lossbest fixeddynamic bestfmttt', 1);('min10 b123456789101112131415number tasks', 1);('t100planning lossmin100035002500450050', 1);('onlinemeta learning shows allbaselines outperform', 1);('constant discount', 1);('plot average planning loss progression tasks shows', 1);('competitive performancewith', 1);('guidance beats', 1);('competitive thedynamicbest baseline', 1);('outperforms evalandis', 1);('depict 1standard error 600independent runs63', 1);('empirical validationnext', 1);('schedules adaptation discount factors', 1);('15tasks 10state 2action random', 1);('distribution tasks 001in', 1);('plot planning loss', 1);('pomrlwith', 1);('value discount tasks', 1);('hindsight and12dynamic', 1);('optimal round', 1);('itis', 1);('important note', 1);('outperformwe observe', 1);('6a evalresults', 1);('plan farahead', 1);('model uncertainty', 1);('upon', 1);('6b observe', 1);('similar performance', 1);('signicance range', 1);('good performance', 1);('6c shows theeect dierent values 0in', 1);('range results', 1);('possible toadapt planning horizon function problems structure', 1);('tasksimilarity samplesizes', 1);('adapting', 1);('open problem', 1);('scope work7', 1);('discussion', 1);('workwe', 1);('inaccurate models online', 1);('regret upperbound planning loss', 1);('entire search space', 1);('algorithmically', 1);('agent use itsexperience task andacross tasks', 1);('transition model distribution overtasks', 1);('good initialization transition model acrosstasks', 1);('limitation work', 1);('access simulator whichallows', 1);('fullyonline setting', 1);('idea direction', 1);('current average', 1);('regularizer onlinemetagradient step', 1);('aruba maml', 1);('similar concentrationresults', 1);('gradient updates', 1);('nonstationary', 1);('metadistribution consideredthat', 1);('taskdistribution stationary', 1);('realworld scenarios', 1);('distribution eg weather', 1);('promising future direction', 1);('nonstationaryenvironments optimal initialization varies time', 1);('guarantees', 1);('tabular case afruitful direction', 1);('similar guarantees', 1);('amenable transition models function approximationwhile work', 1);('learning nontrivial extendthese bounds', 1);('theoryacknowledgmentsthe authors', 1);('nan jiang', 1);('valuable insights discount factors pointersto reproduce planning loss results', 1);('task setting authors', 1);('sebastian flennerhag', 1);('van roy', 1);('insightful discussions', 1);('zheng wen', 1);('helpful feedbackreferencesron', 1);('amit ron meir kamil ciosek discount', 1);('factor regularizer reinforcement learning', 1);('ininternational', 1);('conference machine learning pp', 1);('arumugam david abel kavosh asadi nakul gopalan christopher grimm jun ki lee lucas lehnertand michael', 1);('littman mitigating', 1);('reinforcement learning arxivpreprint arxiv181201129 2018mariaflorina', 1);('balcan mikhail khodak ameet talwalkar', 1);('provable guarantees', 1);('model inductive bias learning journal articial intelligence research 121491982000marc', 1);('g bellemare yavar naddaf joel veness michael bowling', 1);('arcade learning environment', 1);('anevaluation', 1);('platform general agents journal', 1);('articial intelligence', 1);('blackwell discrete', 1);('annals mathematical statistics', 1);('caruana multitask', 1);('cella alessandro lazaric massimiliano pontil metalearning', 1);('stochastic linear banditsininternational conference', 1);('cesabianchi pierre laforgue andrea paudice massimiliano pontil multitask', 1);('online mirrordescent arxiv preprint arxiv210602393 2021giulia', 1);('denevi carlo ciliberto dimitris stamos massimiliano pontil learning', 1);('denevi dimitris stamos carlo ciliberto massimiliano pontil onlinewithinonline', 1);('dong benjamin van roy zhengyuan zhou simple', 1);('complex environment', 1);('ecient', 1);('reinforcement learning agent state arxiv preprint arxiv210205261 2021william', 1);('fedus carles gelada yoshua bengio marc g bellemare hugo larochelle hyperbolicdiscounting', 1);('multiple horizons arxiv preprint arxiv190206865 2019chelsea', 1);('finn pieter abbeel sergey levine modelagnostic', 1);('adaptation deepnetworks', 1);('finn aravind rajeswaran sham kakade sergey levine online', 1);('internationalconference machine learning', 1);('flennerhag yannick schroecker tom zahavy hado', 1);('hasselt david silver satinder singhbootstrapped', 1);('arxiv preprint arxiv210904504 2021sebastian', 1);('flennerhag tom zahavy brendan odonoghue hado', 1);('hasselt andrs gyrgy satindersingh optimistic', 1);('neuralinformation processing systems', 1);('franoislavet guillaume rabusseau joelle pineau damien ernst raphael fonteneau onovertting', 1);('asymptotic bias batch reinforcement learning partial observability journal ofarticial', 1);('intelligence', 1);('jiang alex kulesza satinder singh richard lewis', 1);('dependence eective planning horizonon model accuracy', 1);('proceedings', 1);('autonomous agents', 1);('systems', 1);('international foundation', 1);('autonomous agents multiagentsystems', 1);('kearns yishay mansour andrew ng', 1);('algorithm nearoptimal planningin', 1);('large markov decision processes', 1);('khodak mariaflorina balcan ameet talwalkar adaptive', 1);('metalearningmethods arxiv preprint arxiv190602717 2019levente', 1);('kocsis csaba szepesvri bandit', 1);('montecarlo planning', 1);('european conference onmachine learning pp', 1);('luketina sebastian flennerhag yannick schroecker david abel tom zahavy satinder singhmetagradients', 1);('nonstationary environments', 1);('iclr', 1);('agent learning openendedness', 1);('mnih koray kavukcuoglu david silver andrei rusu joel veness marc g bellemare alexgraves martin riedmiller andreas k fidjeland georg ostrovski', 1);('humanlevel', 1);('control throughdeep reinforcement learning', 1);('nature', 1);('oh matteo hessel wojciech czarnecki zhongwen xu hado', 1);('hasselt satinder singh', 1);('silver discovering', 1);('reinforcement learning algorithms arxiv preprint arxiv200708794 202014marek', 1);('petrik bruno scherrer biasing', 1);('discount factoradvances neural information processing systems', 1);('machine learning reproducibility checklist arxiv 2019juergen', 1);('schmidhuber rudolf huber learning', 1);('generate articial fovea trajectories target detectioninternational journal', 1);('neural systems', 1);('tao van vu', 1);('random matrices universality local spectral statistics nonhermitian matricesthe', 1);('annals probability', 1);('thrun lorien pratt learning', 1);('introduction', 1);('learning', 1);('van seijen hado van hasselt shimon whiteson marco wiering', 1);('theoretical empiricalanalysis', 1);('ieee symposium adaptive', 1);('ieee', 1);('xu hado', 1);('hasselt david silver metagradient', 1);('reinforcement learning arxiv preprintarxiv180509801 2018tom', 1);('zahavy zhongwen xu vivek veeriah matteo hessel junhyuk oh hado p', 1);('hasselt david silverand satinder singh', 1);('actorcritic algorithm', 1);('advances', 1);('additional related workdiscount factor adaptation', 1);('realworld applications', 1);('largerenvironment agent capacity context computational memory complexity egthe internet', 1);('inevitably', 1);('crucial adapt planning horizon time', 1);('expensive suboptimal hasbeen', 1);('context planning inaccurate models reinforcement learning', 1);('jianget', 1);('inspiration section', 1);('problem innitehorizon averageregret setting', 1);('true horizon', 1);('discount value proportional time agents life results insignicant gains focus contributions dierent', 1);('connection ndings', 1);('interesting avenue', 1);('metarl', 1);('tremendous success online discovery ofdierent aspects', 1);('complete objectivefunctions', 1);('oh', 1);('recent years', 1);('fedus', 1);('traditional approach', 1);('formal understanding whythis', 1);('tasks analysis motivatedby', 1);('aforementioned empirical success adapting discount factor signicantprogress', 1);('metagradients techniques', 1);('rl xu', 1);('empirical analysis lot room indepth insightsabout source', 1);('closedform', 1);('squareswe note', 1);('pshould', 1);('psasprimelscriptph', 1);('2mmsummationdisplayi1xip 2phlscriptph', 1);('summationtextixim1 h 9h 1summationtextiximwhere1 10derivation', 1);('mixing rate', 1);('tto chooset', 1);('nal estimatorexptparenleftbigptptparenrightbig2exptparenleftbigtht 1tptptparenrightbig2exptparenleftbigthtpt 1tptptparenrightbig22thtpt2 1t2exptparenleftbigptptparenrightbig2where cross term 2thtpt1texptebracketleftbigptptbracketrightbig 0sinceept', 1);('classicbiasvariance decomposition estimator', 1);('choice htplays role', 1);('xitis', 1);('instance forthe choicehtpo structural assumption', 1);('getexptparenleftbigptptparenrightbig222 121m16and minimize', 1);('withintask estimators variance', 1);('poin', 1);('intuitively', 1);('potpoand', 1);('retrieve result show', 1);('proof ofour', 1);('main theorem', 1);('simple expression minimize metamse estimatoreptpoparenleftbigptptparenrightbig22teptpoexptparenleftbightptparenrightbig2 1t2eptpoexptparenleftbigptptparenrightbig22teptpo1t1t1summationdisplayj1pjpopopt2 1t21m2t21 1t 1t21mwhere', 1);('variance of1t1summationtextt1j1pjthe', 1);('p0t', 1);('ptis', 1);('popoby', 1);('structural assumption', 1);('minimizing', 1);('upperbound intleads tot1211tm1t whent', 1);('uncertainty priorimplies weight estimator', 1);('fast rate optimal valuewhen', 1);('exact optimal', 1);('probability 1because use theconcentration', 1);('potsee', 1);('online estimationonline estimation', 1);('task learner', 1);('stateaction pair taskt', 1);('learner compute', 1);('iept1osprimesa summationtextmi1xit1mat', 1);('subsequent taskspt2osprimesa summationtext2mi1xit122m12parenleftbigsummationtextmi1ximsummationtext2mim1ximparenrightbig12parenleftbigpt1osprimesa summationtext2mim1ximparenrightbigsimilarlypt3osprimesa summationtext3mi1xit133msummationtext2mi1xit12summationtext3mi2m1xit33m13parenleftbig2summationtext2mi1xit122msummationtext3mi2m1xit3mparenrightbigptosprimesa 1tparenleftbigt1pt1osprimesa summationtexttmit1m1ximparenrightbigthereforeptosprimesa parenleftbig11tparenrightbigpt1osprimesa parenleftbig1tparenrightbigsummationtexttmit1m1xim11online', 1);('estimation variance similarly', 1);('online estimate variance2ot 2ot1xmtpt1oxmtpto2ot1t1217since method', 1);('unstable employ', 1);('welfords', 1);('online algorithm varianceestimated', 1);('term losslemma', 1);('mssvptevalptevalsvptptevals', 1);('parenleftbigvptevalptevalsvptevalptsparenrightbigbracehtipupleftbracehtipdownrightbracehtipdownleft bracehtipuprightatparenleftbigvptevalptsvptptevalsparenrightbigbracehtipupleftbracehtipdownrightbracehtipdownleft bracehtipuprightbtwe', 1);('term corresponds bias', 1);('rewards 0rmaxsaandevalvptvptevalvpteval1eval1rmax 13we', 1);('c eval1eval1rmaxand notice thatsummationtexttattcso bounds rst part theaverage lossto', 1);('bt', 1);('rst use', 1);('equation', 1);('upper boundvptevalptsvptptevals2 maxssrvptevalptsvptptevals', 1);('setting estimate', 1);('rsorrqptsa rsa', 1);('vptangbracketrightandqptsa rsa', 1);('vptangbracketrightangbracketleftptsa vptangbracketrightvextendsinglevextendsinglevextendsingle16notice', 1);('value functions', 1);('mdps', 1);('nontrivial leveragethe result', 1);('reader proof', 1);('therein intermediate stepsnow', 1);('tasks havesummationtexttbtt1ttsummationdisplayt121maxssaarvextendsinglevextendsinglevextendsingleangbracketleftptsa', 1);('vptangbracketrightangbracketleftptsa vptangbracketrightvextendsinglevextendsinglevextendsingle211ttsummationdisplayt1maxssaarvextendsinglevextendsinglevextendsingleangbracketleftptsa ptsa vptangbracketrightvextendsinglevextendsinglevextendsingle2rmax11ttsummationdisplayt1summationdisplaysprimesmaxssaarvextendsinglevextendsinglevextendsingleptsasprimeptsasprimevextendsinglevextendsinglevextendsinglevpt2rmax12sttsummationdisplayt1maxssprimesaavextendsinglevextendsinglevextendsingleptsasprimeptsasprimevextendsinglevextendsinglevextendsingle18where', 1);('value function', 1);('rmax1and', 1);('sbysmaxsprimes notethat', 1);('step diers', 1);('boil average worstcase estimationerror transition model', 1);('direct involves', 1);('controllingthe deviations scalar random variables', 1);('rsa', 1);('qptsa', 1);('policy space', 1);('randresults', 1);('factor logrunder square root', 1);('asimilar result', 1);('alternative choice', 1);('informative case thisfactor', 1);('sterm', 1);('direct dependence onthe size policy class function play role', 1);('atthe price', 1);('extra looseness', 1);('heuristic gammaschedule section', 1);('easy straightforward adapt concentration', 1);('boundrsa angbracketleftptsa', 1);('vptangbracketrightqptsaas jiang', 1);('aseq equation', 1);('extra logrd2', 1);('model estimatorto', 1);('clutter notation section drop sasprimeeverywhere', 1);('appendix babove', 1);('pand p0are', 1);('probability 1maxsasprimeptpt12m 1radicalbigglog6t logts2a2 log26t2mtradicalbigglog3ts2at 22m22m 1radicalbigg log3ts2a2m17for anyttsasprimeandr dene', 1);('pttpo', 1);('pofor', 1);('eacht havevextendsinglevextendsinglevextendsingleptptvextendsinglevextendsinglevextendsingleptptptpttpotpobracehtipupleftbracehtipdownrightbracehtipdownleftbracehtipuprighta 1tptmptbracehtipupleftbracehtipdownrightbracehtipdownleftbracehtipuprightbtpoptbracehtipupleftbracehtipdownrightbracehtipdownleftbracehtipupright2by', 1);('term asubstituting', 1);('ptt1msummationtextmixi', 1);('1tptoatpto1t1t1summationdisplayj1pj1t1t1summationdisplayj1pjpo12m 1pto1t1t1summationdisplayj1pjbracehtipupleftbracehtipdownrightbracehtipdownleftbracehtipuprighta11t1t1summationdisplayj1pjpobracehtipupleftbracehtipdownrightbracehtipdownleftbracehtipuprighta2wheretis', 1);('initial value12m1and', 1);('epp1t1summationtextt1j1pjpoand', 1);('variance of19this estimator', 1);('2t1by structure assumption', 1);('a2using hoedings', 1);('random variables probability', 1);('t19we', 1);('a1a1vextendsinglevextendsinglevextendsinglevextendsinglevextendsinglevextendsingle1t1summationdisplayjpjmpjvextendsinglevextendsinglevextendsinglevextendsinglevextendsinglevextendsinglewe', 1);('note rst term', 1);('a1', 1);('mtsummationtextt1j1zj', 1);('wherezjpjmpj thateach increment', 1);('high probability jzjcjwp16 wherecjradicalbigmlog6t2moreover dierences', 1);('zjzj1are', 1);('high probabilityzjzj1pjpj1pjpj12 2cjdj 2parenleftbigg log6t2mparenrightbiggthen', 1);('tao vu', 1);('prop', 1);('epsilon10pparenleftbigvextendsinglevextendsinglevextendsinglemtt1vextendsinglevextendsinglevextendsingleepsilon1t1radicaltpradicalvertexradicalvertexradicalbtt1summationdisplayj1d2jparenrightbig2 exp2epsilon12 t1summationdisplayj16t2choosingepsilon1radicalbig12log12t getpvextendsinglevextendsinglevextendsingle1t1t1summationdisplayj1pjmpjvextendsinglevextendsinglevextendsingleradicaltpradicalvertexradicalvertexradicalbt log6t2m2log6tt6t6t3twith union', 1);('13a1radicalbigglog6t logts2a2 log26t2mt20because radicalbigm22mby', 1);('getmaxsasprimetpotpo12m 1radicalbigglog6t logts2a2log26t2mtradicalbigglog3s2at', 1);('t21bounding term bterm', 1);('concentration average', 1);('ptm1msummationtextixi', 1);('hoedings', 1);('inequality union', 1);('14maxsasprimeptmptradicalbigg log4ts2a2m20to', 1);('term b', 1);('1tfor alltt1t21 1tm21 1tm 12m22m 1we', 1);('probability 13maxsasprimeb2m22m 1radicalbigg', 1);('boundsto conclude', 1);('bounds terms equation', 1);('equation 21equation 22and union', 1);('probability 1maxsasprimeptpt12m 1radicalbigglog6t logts2a2 log26t2mtradicalbigglog3s2at 22m22m 1radicalbigg log3ts2a2m23discussionthe', 1);('main terms', 1);('oradicalbig1mtoradicalbig1to1mand oradicalbig1m', 1);('2andm rst remark mis', 1);('oradicalbig1mdominatesdue', 1);('factor2m2m11 coecient rst', 1);('fast 12me', 1);('proposition', 1);('1we study function', 1);('udened', 1);('generalshape function dierentiate respect dudc', 1);('value parameter', 1);('cif01c', 1);('01and minimumis', 1);('2c 10c', 1);('umay', 1);('ifc122 minimum exists', 1);('c21f experiments', 1);('details ablations additional resultsf1', 1);('implementation detailswe', 1);('distribution tasks tasks ttptpare', 1);('figure f1', 1);('task distribution', 1);('pois', 1);('variance distribution', 1);('wecompute variance distribution ii1i01 ii0and0summationtextsii', 1);('f1 dirichlet', 1);('distribution', 1);('fors 3states', 1);('dir', 1);('inour tasksimilarity measure', 1);('ablationswe', 1);('naive baseline', 1);('ignores metarl structureand plans', 1);('task observe', 1);('fig f2', 1);('atparwith method', 1);('pomrlwhich', 1);('intuitive tasks', 1);('task structure decreases note', 1);('onesingle task problematic suers', 1);('due new task onaverage error', 1);('havethe guarantees', 1);('pomrlandadapomrl strong structure123456789101112131415task t000510152025303540planning loss099eval', 1);('knowledgewithout metalearningaggregating1pomrlamedium structure123456789101112131415task t000510152025303540planning loss099eval', 1);('knowledgewithout metalearningaggregating1pomrlbloosely structured123456789101112131415task t000510152025303540planning loss099eval', 1);('knowledgewithout metalearningaggregating1pomrlcfigure f2 ablations ecacy pomrland', 1);('tasksimilarity depicts theeect tasksimilarity parameter', 1);('amount data', 1);('5available round', 1);('werun', 1);('ignores metarl structure acts ifit', 1);('task presence', 1);('strong structure', 1);('structure alongsidea good model initialization', 1);('tasks transitions mightseem work', 1);('naive method', 1);('task similarity decreases learner struggles', 1);('loss doesntimprove', 1);('1standard deviation uncertainty', 1);('100independent runs22000', 1);('losst1 t2 t3 t4 t5am', 1);('losst1t2t3t4t5t12t15t17t20t25t27t30', 1);('cm 5t 30figure', 1);('f3 eect average regret upper bound', 1);('value oftask similarity', 1);('number samples', 1);('task mand number tasks mtsmaller loss', 1);('small discount factor implies lot uncertainty', 1);('b mt task', 1);('eective discount factors lot', 1);('c mlessmuchtis', 1);('interesting case samples', 1);('howeverthe', 1);('number tasks', 1);('huge gains', 1);('additional experimentswe', 1);('properties adapomrl namelyeect ofm andton', 1);('loss fig f3varying', 1);('fig f4 varying', 1);('fig f500', 1);('losst1 t3 t6 t15aadapomrl00', 1);('knowledge00', 1);('metalearningfigure f4', 1);('online meta learning baselines', 1);('meta', 1);('updates includelearningpoas function tasks b', 1);('considers optimal truemean task distribution', 1);('apriori c', 1);('withoutmetalearning', 1);('estimates transition kernel round', 1);('twithout', 1);('15tasks andm 5samples', 1);('reproducibilitywe', 1);('reproducibility checklist', 1);('ensure research reproducible allalgorithms', 1);('clear description algorithm source code', 1);('thesesupplementary materials theoretical claims', 1);('statement result', 1);('clear explanationof assumptions', 1);('complete proofs claims gures', 1);('empirical results weinclude empirical details experiments', 1);('clear denition specic measure orstatistics', 1);('report results description results', 1);('standard error casesf5', 1);('computing', 1);('open source librariesall experiments', 1);('google colab', 1);('losst1t2t3 t4 t5amt', 1);('cm 5t 30s', 1);('losst1t2t3 t4 t5dmt', 1);('em 20t', 1);('losst1t3t6t16t17t18t19t20', 1);('fm 5t 15figure', 1);('f5 varying', 1);('size statespace number samples', 1);('task number oftaskst', 1);('taskaveraged regret upper bound', 1);('value task similarity note', 1);('statespace observe eect ie adg mt', 1);('small discount factor ie lot uncertainty inability plan', 1);('behformt task', 1);('eective discountfactors lot', 1);('cfi mlessmuchtis', 1);('interesting case assamples', 1);('signicantgains case', 1);('20independent runs anderror', 1);('1standard deviation24', 1);