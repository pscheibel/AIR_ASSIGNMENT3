('rl', 105);('neurips', 32);('august', 27);('latex class files vol', 25);('icml pmlr', 21);('dt', 18);('trl', 17);('figure', 15);('cvpr', 15);('rtg', 14);('mdp', 10);('tt', 9);('icml', 8);('reinforcement learning', 7);('bert', 7);('iclr', 6);('transformer', 6);('lstm', 6);('ltl', 6);('marl', 6);('iccv', 6);('corl pmlr', 6);('aaai', 6);('chen', 5);('vln', 5);('dqn', 5);('li', 5);('gata', 5);('pmlr', 5);('eccv springer', 5);('qlearning', 4);('ppo', 4);('mineclip', 4);('paster', 4);('d4rl', 4);('language models', 4);('bucker', 4);('tbgs', 4);('tuli', 4);('rul', 4);('atm', 4);('hu', 4);('inspired', 3);('nlp', 3);('gato', 3);('gtrxl', 3);('according', 3);('bc', 3);('pretraining', 3);('current state', 3);('neruips', 3);('corl', 3);('forthe', 3);('ffn', 3);('lstms', 3);('motivated', 3);('parisotto', 3);('dmlab30', 3);('vmpo', 3);('impala', 3);('monte carlo', 3);('rtgs', 3);('dp', 3);('ad', 3);('wang', 3);('reid', 3);('liu', 3);('atari', 3);('flexibit', 3);('hiveformer', 3);('ttp', 3);('hao', 3);('mlm', 3);('hamt', 3);('carla', 3);('extensive', 3);('villaor', 3);('updet', 3);('f song', 3);('transformers reinforcement learning', 3);('ral', 3);('articialintelligence', 3);('springer', 3);('proceedings', 3);('ieee', 3);('yu', 3);('multiagent reinforcement learning', 3);('natural language processing', 2);('robotic manipulation', 2);('games navigation autonomous', 2);('powerful transformer structure', 2);('capability transformer', 2);('transformer architecture', 2);('neural networks', 2);('recent years', 2);('critical observations', 2);('entire episode', 2);('trajectory', 2);('decision processes', 2);('brief overview', 2);('notice', 2);('development trajectory', 2);('dynamic environments', 2);('markov', 2);('learn', 2);('modeling', 2);('hindsight', 2);('language', 2);('modelbased', 2);('hierarchical', 2);('real environments', 2);('modelfree methods', 2);('methods eg', 2);('observablerl problems', 2);('subsection', 2);('insteadof', 2);('left', 2);('transformerxl', 2);('layer normalization', 2);('gru', 2);('fwp', 2);('coberl', 2);('deepmind', 2);('dreamer', 2);('rssm', 2);('transition model', 2);('sun', 2);('minedojo', 2);('continuous actions', 2);('promptdt', 2);('rlalgorithms', 2);('rvs', 2);('esper', 2);('rcsl', 2);('ofine dataset', 2);('rlmethods', 2);('shang', 2);('laskin', 2);('predicts actions', 2);('yamagata', 2);('mujoco', 2);('takagi', 2);('lee', 2);('poor performance', 2);('comparable performance', 2);('furuta', 2);('carroll', 2);('xu', 2);('boustati', 2);('anticausal aggregator', 2);('dasari', 2);('current observations', 2);('mosaic', 2);('experiments', 2);('guhur', 2);('qbert', 2);('albert', 2);('adhikari', 2);('paul', 2);('sdv', 2);('lidar', 2);('shao', 2);('splt', 2);('diaformer', 2);('meng', 2);('smac', 2);('madt', 2);('mat', 2);('google', 2);('efcient transformer models', 2);('tutorial', 2);('e parisotto', 2);('j rae r pascanu', 2);('gulcehre jayakumar jaderberg r', 2);('kaufman clark noury', 2);('stabilizing', 2);('icml pmlr2020', 2);('shridhar', 2);('p kaelbling', 2);('littman r cassandra planningand', 2);('observable stochastic domains', 2);('ofine reinforcement learning', 2);('h khorasgani h wang', 2);('gupta farahat', 2);('ofinedeep reinforcement learning maintenance decisionmakingarxiv preprint arxiv210915050', 2);('kapturowski g ostrovski j quan r munos', 2);('dabney recurrent', 2);('experience replay', 2);('v mnih p badia mirza graves lillicrap harleyd silver k kavukcuoglu asynchronous', 2);('methods deepreinforcement learning', 2);('learning', 2);('r k srivastava p shyam', 2);('mutz', 2);('ja', 2);('j schmidhuber training', 2);('upsidedown reinforcement learning arxiv preprint arxiv191202877', 2);('e mitchell r rafailov x', 2);('peng levine', 2);('finn ofine', 2);('metareinforcement learning advantage', 2);('iros ieee', 2);('x li', 2);('journal latex class files vol', 1);('transforming reinforcement learning', 1);('bytransformer development', 1);('trajectoryshengchao huli shen zhang yixin chen fellow ieee dacheng tao fellow ieeeabstract transformer', 1);('signicant success computer visionthanks', 1);('super expressive power researchers', 1);('ways deploy transformers reinforcement learning', 1);('potential representative', 1);('benchmarks paper', 1);('dissectrecent advances', 1);('rl trl', 1);('order explore development trajectory andfuture trend group', 1);('categories architecture enhancement trajectory optimization examinethe', 1);('main applications', 1);('architectureenhancement methods', 1);('rlframework', 1);('model agents environments', 1);('triad trajectory optimization methods', 1);('rlproblems', 1);('train joint stateaction model', 1);('entire trajectories behavior', 1);('able extract policies static datasets', 1);('giventhese', 1);('advancements extensions', 1);('proposals future direction', 1);('hopethat survey', 1);('future research', 1);('terms reinforcement', 1);('ofine', 1);('decision', 1);('ntroductionrecently', 1);('substantial progress', 1);('nlptasks', 1);('gpt', 1);('stateoftheart performance', 1);('wide rangeof', 1);('tasks eg question', 1);('success transformer architecture', 1);('applytransformers computer vision tasks', 1);('utilize transformer', 1);('pixels andvit', 1);('applies transformer sequences ofimage patches classify', 1);('full image achievedstateoftheart performance', 1);('multiple image recognitionbenchmarks', 1);('image sequence subimages', 1);('able extractrepresentative', 1);('selfattention mechanism', 1);('powerful multimodal visuallanguage models', 1);('dalle', 1);('11reinforcement learning', 1);('powerful controlstrategy', 1);('environment and\x0fthis work', 1);('science technology', 1);('innovation2030', 1);('brain science', 1);('brainlike', 1);('research key', 1);('project no2021zd0201405\x0fshengchao hu shanghai jiaotong', 1);('china emailcharleshusjtueducn\x0fli shen jd explore adacemy china emailmathshenligmailcom\x0fya zhang shanghai jiaotong', 1);('china emailyazhangsjtueducn\x0fyixin chen washington', 1);('st louis america emailchencsewustledu\x0fdacheng tao jd explore adacemy china', 1);('university ofsydney', 1);('australia email', 1);('april', 1);('2015an agent agent observes', 1);('current state environment', 1);('actions obtains reward thecurrent action state', 1);('moment thegoal agent maximize', 1);('cumulative reward itgets success', 1);('rlhas', 1);('thehighcapacity capabilities', 1);('neural networks makefunction approximation methods', 1);('accurate enableagents', 1);('withthe environment', 1);('data limitsthe', 1);('training data', 1);('encounter difcultywhen', 1);('expensive interact environment suchas robotic manipulation', 1);('healthcare15 autonomous', 1);('researchers interest datadriven learning method extract policies', 1);('ofine rl', 1);('full use theability', 1);('networks extract optimal policy alarge', 1);('ofine data', 1);('withthe distributional discrepancy ofine trainingdata target policydue sequential decision process', 1);('cnn lstm', 1);('memory information agentnetwork environments', 1);('methods sufferfrom instability issue', 1);('original transformer structure decision process 21arxiv221214164v1 cslg', 1);('dec', 1);('transformerenvagentfeature', 1);('bccanonical rlpretraininggenerali', 1);('gamesnavigationautonomousdrivingautomatic diagnosismaintenance decision', 1);('optimizationarchitecture enhancement', 1);('applications', 1);('overview transformers involvement', 1);('realistic application architecture enhancementblock demonstrates specic part transformer participation', 1);('methods trajectory optimizationblock shows usage transformer', 1);('corresponding research categories', 1);('application block introducesseveral', 1);('realistic applications', 1);('eg robotic manipulation', 1);('games onthus', 1);('motivates followon', 1);('powerful transformer architecture', 1);('rlproblem', 1);('conditional sequence', 1);('problem andleverage transformer architecture model sequential trajectory releases', 1);('capability transformer architecture andinspires', 1);('current worksenhance performance', 1);('rl trlfrom', 1);('various perspectives lack', 1);('order keeppace rate', 1);('new progress', 1);('helpfulfor researchers communityin paper focus', 1);('recent advances', 1);('thechallenges potential directions future improvementshown', 1);('order facilitate', 1);('future research ondifferent topics categorize', 1);('perspective ofthe methods application', 1);('architecture enhancement trajectory optimization andapplications architecture enhancement', 1);('thespecic part transformer participation divideit', 1);('representation environment representation', 1);('feature', 1);('introduces methodswhere transformer', 1);('representations multimodal input agent utilizesthese representations', 1);('decisions value', 1);('environment', 1);('representation mainlyillustrates leverage transformer architecture forthe dynamics reward', 1);('algorithmsfor trajectory optimization methods regardrl sequence', 1);('problem use decisiontransformer', 1);('policy behavior cloningframework', 1);('motivation target taskswe classify trajectory optimization methods conditional', 1);('conditional bc', 1);('adopts transformer architecture modelthe stateactionreward sequence', 1);('pioneeringworks conditions optimality', 1);('canonical rlattempts', 1);('advantages sequence modelingmethods', 1);('focuses pretrain decision transformers', 1);('performance downstreamrl tasks', 1);('datasetsand objectives', 1);('generalization', 1);('studies algorithmsthrough single agent', 1);('inmultiple environments', 1);('typical applicationsof', 1);('gamesvisionlanguage navigation autonomous', 1);('rlmethods towards', 1);('end paper introduce thetrl multiagent', 1);('insights prospectsthe rest paper', 1);('section 2introduces preliminaries survey includesa', 1);('standard transformer section', 1);('main parts ofthe paper summarize', 1);('methods andits applications', 1);('future directions andchallenges', 1);('summary concludethis work', 1);('recent preprint', 1);('due rapid development thiseld2', 1);('p reliminariesin', 1);('basic concept ofreinforcement learning transformer prejournal', 1);('agentenvironment1 agentenvironmenttabular q learning1 agentenvironmentrl drl trlfig', 1);('decisions currentmoment', 1);('drl', 1);('deep network estimate value policy functions', 1);('thetrl agents leverage ability transformer architecture', 1);('similar trend', 1);('rlliminary', 1);('knowledge survey21', 1);('reinforcement learningreinforcement', 1);('timestep maximize numerical reward signal stateactionpair', 1);('denitionof problem essence', 1);('learning throughinteraction environments agent interacts theenvironment', 1);('corresponding reward thenthe agent', 1);('onthe interaction consequence agent', 1);('supervision learning inmost settings', 1);('futurecumulative reward', 1);('subsequent rewards', 1);('different hard isthe trialanderror search', 1);('reward characteristics12below rst', 1);('basic model formulationmarkov decision process point', 1);('essential property detail elements', 1);('describe latestrl research focuses on211', 1);('markov decision processas', 1);('mathematical model sequential decision problem', 1);('markov decision process mdp', 1);('thetheoretical basis', 1);('decision at2a basedon', 1);('current state st2s environment wouldrespond action', 1);('agent transform tothe', 1);('state st12s reward', 1);('6tuple notationm', 1);('sat\x1a0r', 1);('wheresis state space', 1);('ais', 1);('action space', 1);('tst1jstatdenotes', 1);('transitionprobability state sttost1after', 1);('action at\x1a0denotes', 1);('initial state distribution', 1);('rstatdenotes', 1);('thereward value', 1);('action atat state stand 201denotes discount factor', 1);('mdpsetting', 1);('experience agent1', 1);('agentenvironmentfig', 1);('diagram mdp observing', 1);('current state st theagent', 1);('decision environment wouldtransform', 1);('state st1with reward', 1);('s1a1r1s2a2r2statrt apolicy', 1);('probability function \x19atjstwhichmeans probability', 1);('action atat state st', 1);('weare', 1);('optimal policy \x19\x03ajsthat maximizes', 1);('total reward possibletrajectories', 1);('policy that\x19\x03 arg max\x19e \x18\x19txt1rt 1note', 1);('environments dynamics', 1);('transition distribution', 1);('tst1jstat', 1);('whichmeans environment', 1);('currentstatestand action', 1);('independent history whichis', 1);('necessary information agentto', 1);('decision policy \x19atjstis', 1);('current statethere', 1);('cases agent cannotaccess', 1);('fullobservable state', 1);('mdp pomdp pomdp', 1);('settingthe agent', 1);('local observation ot2o distribution observation pot1jst1atisdependent', 1);('previous action 69though', 1);('process policy\x19atjotand', 1);('validfor observations', 1);('formulatedas\x19atjotot\x001\x01\x01\x01ot\x00h1which leverages historyinformation mitigate impact', 1);('able tojournal', 1);('representative works', 1);('trlcategory subcategory method highlights publicationarchitectureenhancementfeature representationgtrxl', 1);('replace lstm vmpo gated transformerxl icml', 1);('transformerbased rl', 1);('fwp neurips', 1);('actorlearner distillation iclr', 1);('combine', 1);('contrastive loss hybird', 1);('lstmtransformer iclr', 1);('representationiris', 1);('world model', 1);('autoencoder transformer arxiv 2022transdreamer', 1);('transformerbased', 1);('stochastic world model arxiv 2022vqm', 1);('latent', 1);('representations transition model', 1);('state action spaces videos arxiv 2021mineclip', 1);('automatic evaluation metric arxiv', 1);('bcdt', 1);('returntogo bc', 1);('rtg neurips', 1);('distribution trajectory beam search', 1);('cluster', 1);('trajectories average cluster returns arxiv 2022rcsl', 1);('theoretical', 1);('analysis capabilities limitations', 1);('rlodt', 1);('blends', 1);('markovianlike', 1);('inductive bias', 1);('eccv', 1);('incontext reinforcement learning arxiv 2022boot', 1);('selfgenerate', 1);('utilize q', 1);('function relabel', 1);('arxiv 2022pretrainingchibit', 1);('pretrain wikipedia', 1);('arxiv 2022lid', 1);('pretrained lms', 1);('general scaffold', 1);('investigate', 1);('generalizable models', 1);('representation matters ofine', 1);('powerlaw', 1);('performance trend', 1);('neuips', 1);('multimodal', 1);('multitask multiembodiment model arxiv 2022switch', 1);('sparsely', 1);('model trajectory value estimator arxiv 2022gdt', 1);('randomly', 1);('changes environment dynamics causal reasoning arxiv 2021applicationrobotic', 1);('manipulationtosil', 1);('transformer osil corl', 1);('modify', 1);('multimodal transformer arxiv 2022hiveformer', 1);('historyaware', 1);('multiview transformer', 1);('goals voxel observation', 1);('promptsituation transformer', 1);('multipreference learning arxiv', 1);('gamesgata', 1);('construct update', 1);('build', 1);('knowledge graph', 1);('questions arxiv 2020ootd', 1);('tbgs iclr', 1);('equip gata', 1);('equip bert', 1);('model recurrent function', 1);('longrange history', 1);('agent', 1);('drivingsplt', 1);('disentangling', 1);('policy world models', 1);('demonstration data', 1);('multiple modes', 1);('fusion', 1);('front view', 1);('lidar cvpr', 1);('safetyenhanced', 1);('framework multimodal view sensors', 1);('true states', 1);('pomdp', 1);('common modelingmethod', 1);('games navigation on212', 1);('elements reinforcement learningexcept', 1);('agent environment fourmain subelements', 1);('system policy rewardfunction value function model', 1);('policy species theaction agent', 1);('current state coreof', 1);('knowledge source thatguides agents action', 1);('whereas', 1);('reward function animmediate return', 1);('action value functionspecies total reward agent', 1);('value thehighest reward', 1);('good nal resultswhich', 1);('cumulative reward model ofthe environment planning method', 1);('agentdecide action', 1);('possible future situationsthere', 1);('quantities interest', 1);('nd optimal policy', 1);('different methods actionvalue function', 1);('q\x19stat', 1);('statevalue functionv\x19stand difference', 1);('advantage functiona\x19stat', 1);('statevalue', 1);('v\x19stmaps', 1);('state tothe', 1);('state stunder thepolicy\x19', 1);('actionvalue function', 1);('q\x19statmaps', 1);('apair stateaction', 1);('reward advantagefunctiona\x19statis', 1);('lowervariance tothe actionvalue function distinguish impact stateand action', 1);('classesof methods', 1);('dynamic', 1);('compute optimal policy', 1);('optimize policythrough policy gradient method eg', 1);('reinforce', 1);('npg', 1);('value iteration method toconduct policy value function eg', 1);('dueling dqn', 1);('actorcritic methods', 1);('critics value improvethe actors policy eg', 1);('ddpg', 1);('sac', 1);('morel', 1);('mopo', 1);('model environment thatsimulates', 1);('simulatedmdp use modelfree methods', 1);('mdp recently', 1);('interests wherean agent extract return', 1);('policies fromsome', 1);('trajectory rolloutsof arbitrary policies', 1);('formally', 1);('static datasets', 1);('dfstatst1rtig', 1);('whereiis theindex actions states', 1);('behavior policy stat\x18d\x19 \x01 thenext states rewards', 1);('dynamicsst1rt\x18t\x01jstatrstat contrast onlinerl methods ofine', 1);('possible domainswhere', 1);('data online suchas healthcare', 1);('inventory maintenance', 1);('itcannot interact environments explore', 1);('new statesand transitions', 1);('need pessimisticvalue policy constraint', 1);('access outofdistribution phenomena', 1);('theunseen state agents', 1);('mistakes policydiverges', 1);('training policy', 1);('thedistribution shift213', 1);('challenges reinforcement learningthere', 1);('agent needs trialanderror search tond optimal policy', 1);('reward signalhowever', 1);('lot danger interactingwith realworld environment', 1);('agents action maycorrelate agents actions eg car', 1);('inthe lane state', 1);('correlate history whichmeans environment', 1);('mdpformulation', 1);('agent needs', 1);('longterm dependencies consequence actioncan', 1);('transitions environment', 1);('temporal credit assignmentproblem', 1);('transformer architecturetransformer', 1);('network architecture formachine learning tasks eld', 1);('rst overall structure diagram isshown', 1);('encoder adecoder', 1);('selfattention pointwise', 1);('layers encoder maps input sequence oftokens latent representations decoder uses togenerate', 1);('results autoregressive mannerwhich consumes', 1);('additional input', 1);('wewill describe component transformer detail221', 1);('selfattentionselfattention', 1);('core component transformer models pairwise relations tokensin order calculate selfattention input tokenrepresentation', 1);('vectors thequery vector q key vector k value vector vwithdimensiondqdkdv pack vectors derivedfig', 1);('structure', 1);('original transformer image 85from', 1);('different inputs', 1);('different matricesqkv compute attention map followsattention', 1);('qkv', 1);('qktpdkv', 1);('2eq2 illustrates rst compute score betweeneach pair', 1);('different input vectors', 1);('theminto probabilities', 1);('softmax operation', 1);('sum value weight', 1);('corresponding probabilitythe selfattention layer encoder module decoder module', 1);('layer decodercan access positions', 1);('current predictedposition', 1);('whilethe', 1);('encoderdecoder attention layer decoder moduleallows', 1);('position decoder query positionsin input sequence', 1);('key matrixkand value matrix', 1);('vcoming', 1);('encoder moduleand query matrix', 1);('qcoming', 1);('previous layerin order boost performance vanilla selfattention layer multihead attention technique', 1);('compute inputrepresentation tokens', 1);('different subspaces', 1);('especiallygiven', 1);('input vector number head h allinputs rst', 1);('groups vectorsand', 1);('query group matrix fqighi1 thekey group matrix fkighi1 value group matrixfvighi1 group hvectors dimensiondq0dk0dv0dmodelh multiattention processis', 1);('q0k0v0 concat', 1);('head 1\x01\x01\x01headhw0where headiattention', 1);('qikivi', 1);('6where theq0k0v0are concatenation correspondinggroup matrixfqighi1fkighi1fvighi1and thew0is theprojection weight222', 1);('feedforward networka', 1);('feedforward network', 1);('selfattention layerit', 1);('linear transformation layers nonlinear activation function betweenffnx', 1);('w2\x1bw1x', 1);('parameters twolinear transformation layers \x1bdenotes nonlinearactivation function', 1);('gelu', 1);('relu', 1);('positional encodingsince', 1);('input sequence', 1);('process invariant position positional', 1);('original input embeddingvanilla transformer positional', 1);('aspepos2i sinpos100002id model 5pepos2i1 cospos100002id model 6wherepos position iis dimension wayeach dimension positional', 1);('corresponds asinusoid', 1);('easy model', 1);('relative positions', 1);('possible extrapolateto', 1);('sequence length inference aremany choices positional encodings learnedpositional', 1);('relative positional encoding90224', 1);('residual connectionin', 1);('information ow inputto output residual connection', 1);('layer encoderand decoder modules', 1);('xattention x', 1);('7wherexis input selfattention layer arealso variants residual connection theprelayer normalization', 1);('preln', 1);('thelayernormalization attention layer differentnormalization algorithms', 1);('ransformer based rltransformerbased rl trl', 1);('goal leverage thepowerful representation ability transformer architecturefor', 1);('alot attention', 1);('thesuccess transformer architecture domains wherethe sequential information process', 1);('critical performanceit', 1);('ideal candidate architecture', 1);('memory agentthese', 1);('agents environments', 1);('due tothe transformers', 1);('superior performance', 1);('transformer tool architecture enhancement', 1);('inherent defects', 1);('sequential decision process', 1);('rlit', 1);('transformer architecturein', 1);('rid limitationsof', 1);('treatingthe sequential decision making progress', 1);('problems asthe sequence', 1);('process behavior cloningstructure perspective sequence', 1);('regularization conservatism need', 1);('future reward methods', 1);('simple transformerstructure', 1);('performance traditionalrl algorithms', 1);('complex optimization processes whicharises interest', 1);('manyworks decision transformer', 1);('tasks reasons success andfailure methods etc section introducethe', 1);('name themarchitecture enhancement', 1);('thecharacteristics algorithms transformerarchitecture', 1);('architecture enhancementin', 1);('powerfultransformer structure', 1);('problems traditionalrl framework', 1);('specic part transformer participation dividethis subsection', 1);('representation andenvironment representation', 1);('introduce methods transformer', 1);('representations multimodalinput agent utilizes representations tomake decisions value', 1);('environment representation', 1);('howto leverage transformer architecture dynamicsand reward', 1);('feature representationfor partially', 1);('markov decision processespomdps', 1);('simple observation notenough', 1);('select optimal action', 1);('agent oftenneeds', 1);('history past observations', 1);('nmost', 1);('rnns lstm', 1);('memory agent', 1);('recent work', 1);('demonstrates thattransformer gains', 1);('domains thanthe', 1);('rnns', 1);('natural questionthat incorporate transformer architecture intothe', 1);('mdps', 1);('loynd', 1);('working memory graph wmg', 1);('variable number factor vectors outputs', 1);('actorcritic networkjournal', 1);('7transformer variantsfeature', 1);('representation environment representationtransformer', 1);('generic framework transformer architecture enhancement', 1);('transformer architectureto enhance', 1);('right examples', 1);('transformer architecture enhanceenvironment representations', 1);('headattentionlayer normlayer normtrxllxmulti headattentionposition wisemlplayer normlayer normtrxlilxmulti headattentionposition wisemlplayer normlayer normgtrxllxgating layergating layerposition wisemlpfig', 1);('architecture center', 1);('trxli', 1);('layer normalization input stream submodules', 1);('right gtrxlreplaces', 1);('residual connection', 1);('wmg', 1);('a3c', 1);('policy gradientr\x12j\x12', 1);('e\x191xt0r\x12log\x19atjht\x12a\x19obstat', 1);('r\x12h\x19ht\x128where\x19atjht\x12is policy head htis output ofwmghis entropy', 1);('wmgalso', 1);('stores history information', 1);('memos', 1);('isthe basis', 1);('wmgs', 1);('leverage thetransformer architecture', 1);('different wmg parisotto', 1);('standard transformer architecture', 1);('xl', 1);('gated transformerxl gtrxl', 1);('improves stability learning speed shownin', 1);('previous', 1);('originaltransformer model suffers', 1);('critical instability', 1);('nd difcult optimize thecanonical transformer results', 1);('poor performancethus', 1);('residual connections gatingmechanisms', 1);('theskip stream residual connection calledidentity map', 1);('reordering', 1);('layer hasmany options highway connection', 1);('sigtanh', 1);('gated recurrent', 1);('98for experiment', 1);('transformer network', 1);('lstmbased', 1);('grutype', 1);('architecture functions dropin replacement', 1);('powerful memory transformers', 1);('rae', 1);('old memories', 1);('compressive transformeras dropin replacement', 1);('lstm impalaalgorithms', 1);('compresses makesuse past observations', 1);('irie', 1);('notice lineartransformers', 1);('equivalent outer', 1);('fwps112', 1);('formulation general thansome linear transformer', 1);('withrecurrent connections', 1);('fwps rfwps', 1);('anduse dropin replacement', 1);('algorithm99 obtains', 1);('powerful performance', 1);('atari2600', 1);('small model size', 1);('disadvantage transformeris signicant computation cost', 1);('important inactorlatency', 1);('actorlearner distillation ald', 1);('procedureand use', 1);('capacity actor', 1);('data largecapacity transformer learner learning optimize theactor policy distillation lossl\x19ald', 1);('es\x18\x19adkl\x19a\x01jsjj\x19l\x01js', 1);('9for training use', 1);('strong ability', 1);('recent dependencies', 1);('banino', 1);('novel agent', 1);('contrastive bert', 1);('transformerfor architecture', 1);('thelstm benets transformers ability process longcontextual dependencies transformer reducethe memory size', 1);('contrastive approach', 1);('relic116', 1);('performance agents experimentson', 1);('suite', 1);('onpolicy offpolicyjournal', 1);('r2d2', 1);('objective contrastive loss312', 1);('environment representationin', 1);('data efciency modelbasedmethods', 1);('recently', 1);('ways toutilize world model', 1);('pure representation learning119 lookahead search', 1);('learning imagination121', 1);('agent 122which', 1);('learning imagination', 1);('powerful transformer tothe', 1);('rnnbased dreamer', 1);('transdreamer', 1);('introduce rst', 1);('stochastic world modeltransformer', 1);('statespace model tssm', 1);('able tosupport', 1);('effective stochastic', 1);('parallel computation ability', 1);('empiricallytransdreamer', 1);('tasks thatrequire longterm', 1);('complex memory interactions whichillustrates', 1);('tssm', 1);('futurein succession', 1);('micheli', 1);('study transformer ability learning imagination pattern', 1);('observable environmentmicheli', 1);('imagination', 1);('inner speech iris', 1);('theimaginary world', 1);('discrete autoencoder', 1);('gptlike', 1);('autoencoder maps', 1);('raw pixels', 1);('amountof image tokens transformer simulates environment dynamics', 1);('dynamics learning sequencegeneration problem training', 1);('real interactionwith world', 1);('environment dynamicsand agents', 1);('imaginary thepolicy learning opts learning objectives hyperparameters', 1);('dreamerv2', 1);('iris', 1);('outperforms line methods', 1);('rlon atari', 1);('100k benchmark', 1);('autoencoders encode actions andstates environment discrete latent variables andtrain', 1);('ozair', 1);('tree search', 1);('mcts126', 1);('future actions observationswhile', 1);('beam search', 1);('reducethe performance degradationa taskspecic reward function', 1);('important guidingthe agents', 1);('craftedby humans', 1);('hard dene inthe realworld', 1);('complex simulation environment', 1);('fan', 1);('contrastive videolanguage model', 1);('reward signal', 1);('domain adaptation techniques tasks', 1);('reward function', 1);('gvrwhich', 1);('maps language goal', 1);('gandcurrent', 1);('vto', 1);('scalar reward trainingit', 1);('infonce', 1);('whichencourages behavior', 1);('current snippet followthe language description', 1);('strong performance highreward snippet', 1);('languagedescriptions means', 1);('good automaticmetric', 1);('trajectory optimizationconsidering rl', 1);('problem conditional', 1);('works aseries', 1);('new studies', 1);('figure7', 1);('subsection rst', 1);('brief introduction thedt', 1);('algorithms introduce', 1);('theinner limitations optimality conditions', 1);('present works', 1);('traditional reinforcement learningconcepts', 1);('natural way', 1);('performancethen introduce', 1);('mechanism isepidemic', 1);('nlp cv rl', 1);('training process', 1);('finallywe', 1);('multiple environments mayinclude unseen', 1);('trap suboptimum usageof transformer architecture conditional', 1);('agents strongexpansibility generalization321', 1);('conditioned bcchen', 1);('decision transformer dtmethod', 1);('treats policy learningprocess sequence', 1);('figure8a', 1);('trajectory asa sequence state action returnstogo', 1);('different time steps', 1);('amenable autoregressive training generation br1s1a1br2s2a2brtstat 10the', 1);('cumulative rewards', 1);('current timestep till end episode brtptt0trt0', 1);('onestep reward rt', 1);('guides model generate actions', 1);('duringtraining', 1);('sample trajectories ofine datasetsand minimize crossentropy loss discrete actionsor', 1);('theperformance evaluation initiate modelwith specic target return', 1);('environment state thenthe', 1);('action observes', 1);('new state rewardupdates', 1);('rewardand repeats episode termination', 1);('similar behavior', 1);('imitation learning performs wellin sparse settings', 1);('stability issues relatedto', 1);('longterm credit assignment', 1);('janner', 1);('trajectory transformer tt', 1);('uses transformer architectureto model trajectory distributions', 1);('additionallymodels state reward transition trajectoryand discretizes dimension', 1);('quick brown fox jumps lazyinput', 1);('architecturefig', 1);('generic framework transformer trajectory optimization middle structure combination', 1);('gdt', 1);('different purposes', 1);('input form', 1);('part mainlyfor', 1);('structure transformer', 1);('increase generalization', 1);('causal transformer', 1);('linear decoderemb pos enc acausal', 1);('decision transformer', 1);('input causal transformer outputsthe', 1);('timestep actions btrajectory', 1);('architecture dimension states actions', 1);('input intothe causal transformer outputs', 1);('dimension states actions', 1);('rtgsin', 1);('ndimensional', 1);('states andmdimensional actions', 1);('trajectory asequence state action reward discretization s1ts2tsnta1ta2tamtrttt1', 1);('training maximize loglikelihood eachtoken sequence autoregressive manner', 1);('oncelearning', 1);('trajectory distribution use beam searchalgorithm', 1);('signal nd', 1);('compared dt', 1);('methodtt predicts state action reward tokens usesan', 1);('additional beam search algorithm planning whichcan', 1);('action planning', 1);('tothe modelfree methods supports possibility thatthere need', 1);('highcapacity sequence model', 1);('torl problemsthough methods', 1);('aprediction task', 1);('rvsbecome', 1);('popular due simplicity', 1);('strong overallperformance', 1);('point outthat stochastic environment return reward', 1);('corresponding trajectory', 1);('probabilistic model onthe', 1);('theyshow', 1);('training goals independentof environment stochasticity', 1);('optimal policies', 1);('alpropose method', 1);('short environmentstochasticityindependent representations', 1);('itrst', 1);('learns discrete representation', 1);('clusterassignment trajectory model learns predictthe average return', 1);('representation whichwould condition', 1);('nalactions method', 1);('actions agentsand', 1);('independent stochasticity environmentin succession', 1);('brandfonbrener', 1);('providea rigorous study capabilities limitations', 1);('learning algorithms', 1);('rcslthey', 1);('necessary assumptions', 1);('toobtain nearoptimal policy strict thosefor', 1);('rcslrequires', 1);('deterministic dynamics knowledge thetarget return condition', 1);('value beconsistent distribution returns datasethowever', 1);('capable learning goodpolicy dataset', 1);('highreturn trajectories environment stochastic', 1);('simple havestrong performance', 1);('act asthe general learning paradigm', 1);('introductionof transformer structure training paradigm', 1);('rcslmakes rl', 1);('strong expansibility generalization', 1);('general agents withjournal', 1);('10good performance multitask settings', 1);('hard fordp methods322', 1);('canonical rlsince', 1);('online component policies', 1);('taskspecic interactions theenvironment online', 1);('datavia exploration', 1);('forstandard online algorithms helpful access theofine data online performance', 1);('needscareful consideration design overall pipeline', 1);('zhenget', 1);('online decision transformer odt', 1);('incorporates online', 1);('algorithm order balance theexplorationexploitation tradeoff', 1);('odt', 1);('policy entropy', 1);('easg', 1);('\x18 \x00log\x19\x12ajsgsubject toh \x12ajsg\x15 12where', 1);('hyperparameter gis', 1);('thepolicy entropy', 1);('level sequencesrather transitions', 1);('empirically odt', 1);('extra signicantgains', 1);('procedure demonstratesthe effectiveness', 1);('competitive stateoftheart approaches', 1);('similarly liet', 1);('ofine behavior cloningwith online', 1);('algorithm focus', 1);('robotic manipulation problem adoptthe phasic approach', 1);('slwith', 1);('task reduction intrinsic reward', 1);('incorporate markovianlike inductive bias transformeris', 1);('sequences states actionsand rewards', 1);('strong connections', 1);('due potential causal relations', 1);('suffer excess information dilute', 1);('essential relations', 1);('markovianlike dependencies tokensfrom scratch', 1);('weak relationship nonadjacent tokens', 1);('alleviate issues', 1);('stateactionreward transformer starformer', 1);('components astep transformer learning local markovian representations', 1);('triple stateactionreward asequence transformer', 1);('longterm dependencynotice', 1);('starformer', 1);('stepwise reward', 1);('performance degradation itis', 1);('theinference stageafterward', 1);('additional interaction theenvironment hypothesize reason methods', 1);('trial error', 1);('data show learning progress', 1);('thedata contains learning eg', 1);('expertpolicies data learning context size istoo', 1);('policy improvement', 1);('laskinet', 1);('algorithm distillation ad', 1);('learns anincontext policy improvement operator learning history', 1);('algorithm ofine training dataset needs tobe', 1);('policyimprovement causal transformer uses learninghistory context model actions learning lossfunction followsl\x12 \x00nxn1t\x001xt1logp\x12aantjhnt\x001ont 13in', 1);('particular training', 1);('samples multiepisodic subsequence length c', 1);('dmlab watermazer', 1);('stores sequential transitions contextqueue', 1);('transformer environmentthrough method', 1);('new tasks', 1);('network parameters', 1);('historical contextfor ofine', 1);('suboptimal training', 1);('isthat ofine dataset coverage', 1);('stateactionreward transitions137', 1);('amount trainingdata', 1);('deteriorate performance model 139except training', 1);('additional model learning environment dynamics', 1);('data coverage', 1);('novel algorithm', 1);('bootstrapped transformerboot', 1);('rst generates data', 1);('modelitself uses data train model', 1);('ability decision transformerarchitecture data', 1);('model consistentwith', 1);('original ofine data', 1);('boot', 1);('onlytrains part', 1);('condence scores', 1);('data condence', 1);('asc 1t0nm 2txtt\x00t01logp\x12 tj t 14wherenm dimensions state actionsimilar', 1);('great effectiveness theextensive experiments', 1);('high training time consumption dueto pseudo data', 1);('point ofinedataset contains suboptimal trajectories', 1);('dtmethod', 1);('nd optimal method meansit lacks', 1);('ability ability', 1);('optimalpolicy suboptimal trajectories', 1);('approaches eg', 1);('qlearning decision transformer qdtleveraging qlearning', 1);('thequality dataset relabel', 1);('tokens inthe ofine training data', 1);('qlearning cql', 1);('framework learns', 1);('bounds thetrue', 1);('qfunctions', 1);('value function', 1);('qdt', 1);('acrossall environments eg', 1);('maze2d hopper halfcheetah', 1);('qlearning dthave', 1);('poor performance environments meansqdt', 1);('strength approaches323', 1);('pretrainingtowards', 1);('models havemade', 1);('impressive success', 1);('natural language process', 1);('essential techniquefor', 1);('high computation costs', 1);('expressive models transformers', 1);('treating rl', 1);('advantage pretrainingtechnique', 1);('large offtheshelf', 1);('rldatasets', 1);('natural language alsoprovide', 1);('drastic improvements convergence speeds andnal policy performances ofine', 1);('language eg', 1);('view sequence', 1);('domains orderto', 1);('language model modeltrajectories', 1);('objective tomaximize similarity language embeddingsand trajectories input representationslcos\x003nxi0maxjciiej 15wheree', 1);('e1evare', 1);('language embeddings withvocabulary size', 1);('vii1i', 1);('3nare trajectories inputtokens andcz1z2 z1jjz1jj2\x01z2jjz2jj2is cosine similarityfunction', 1);('show visual initialization', 1);('performance improvement evenhas', 1);('negative effect illustrates', 1);('large differencebetween image', 1);('point outthat', 1);('general scaffoldimproves combinatorial generalization policy', 1);('outofdistribution tasks', 1);('ablation experiments', 1);('babyai', 1);('need convertthe states actions history', 1);('natural language strings tobenet', 1);('language sequentialinput', 1);('investigates impact', 1);('different modalities studies whatinformation', 1);('languagedata shows transformer', 1);('largelychanges representations', 1);('attention thedownstream tasks', 1);('points reason', 1);('image data attains', 1);('bad performance 41is', 1);('large gradients gradient', 1);('language data performs', 1);('contextlike informationwhich', 1);('mask state andaction tokens reconstruct', 1);('data theyfocus', 1);('decision prediction', 1);('maskdpand', 1);('general agents', 1);('nd maskratios', 1);('states actions', 1);('study effect', 1);('fordownstream tasks', 1);('objectives ableto', 1);('tasks andwhat components objectives importantthey leverage', 1);('ofine dataset mainlyconsider', 1);('tasks lowdataimitation learning ofine', 1);('subset correspondingcontrastive loss', 1);('brac147', 1);('rl sac', 1);('rl resultson gymmujoco d4rl', 1);('learning objective', 1);('performance policy learning bestrepresentation learning objective', 1);('optimal alldownstream tasks324', 1);('generalizationit', 1);('largescalegeneralist agents training', 1);('massive datasets', 1);('theytend use', 1);('small models', 1);('tasks differenttasks', 1);('atarisuite', 1);('actorcritic methods152', 1);('algorithm bothrequire', 1);('separate training game', 1);('works tryto', 1);('neural network agent play', 1);('atarigames', 1);('rl74', 1);('ofine temporal difference methods', 1);('ndthat decision', 1);('scalability performance', 1);('contains expert nonexpert', 1);('produceexpert actions time', 1);('generation problem language models', 1);('inferencetime method assumes abinary classier', 1);('pexperttjand', 1);('bayesrule', 1);('generate expertlevel returns actionsprtjexperttp\x12rtjpexperttjrt16where expert proportional', 1);('future returnpexperttjrt\x11exp\x14rt', 1);('experiment', 1);('rule isalso', 1);('size model thebetter performance', 1);('new gamesvia', 1);('reed', 1);('generalist agent', 1);('ofine data proposejournal', 1);('12an agent', 1);('work multimodal multitask multiembodiment generalist policy trains thewidest variety relevant data', 1);('possible order processthe multimodal data', 1);('agent rst serialize data sequence tokens', 1);('thetransformer model output distribution nextdiscrete', 1);('training lossl\x12 \x00jbjxb1lxl1mbl logp\x12sbljsb1sbl\x00117wherebis training batch sequences maskingfunctionmbl', 1);('token index lis fromthe text', 1);('action agent', 1);('training dataset', 1);('nearoptimal agentexperience', 1);('realworld environmentsand', 1);('information experttrajectories infer', 1);('token evaluation', 1);('lin', 1);('highcapacity sequentialmodels', 1);('problem indicatethree signicant algorithmic', 1);('high computationcost model capacity', 1);('negative impact singletask training', 1);('value estimator whichsuffers', 1);('poor sample complexity', 1);('trajectory transformer switchtt', 1);('layer switchlayer', 1);('layers router functionto route input specic expert', 1);('efcient computation promotes', 1);('multitask learning', 1);('models valuedistribution trajectory', 1);('value tomitigate effect', 1);('value estimator andincorporate uncertainty reward practiceswitchtt uses categorical distribution minimizesthe crossentropy loss', 1);('class discretedistribution planning phase uses weightedsum discrete atoms value trajectoryvx', 1);('nxi1pixzi', 1);('18whereziis discrete value piis correspondingprobability', 1);('replay buffer generatedby', 1);('algorithm contains expertand nonexpert trajectories', 1);('experimental results on10 tasks gymminigrid environment eg', 1);('fourroomdoorkey keycorridor', 1);('show switchtt', 1);('lot ofalgorithms eg hindsight multitask', 1);('future trajectory information', 1);('hindsight information', 1);('trains policies output', 1);('future tokenswhile', 1);('satisfying statistics future state informationthey dene information', 1);('im', 1);('problems asto', 1);('conditional policy \x19ajszby', 1);('thedivergence statistics', 1);('information zmin\x19ez\x18pz \x18\x1a\x19z', 1);('di\x08', 1);('z 19wherei\x08 denotes information statistics trajectory practice identity function rewardfunctionrsa', 1);('introduce generic template', 1);('generalized decision transformergdt', 1);('function anticausalaggregator adapt', 1);('different choices', 1);('i\x08', 1);('infigure 9ain succession', 1);('different tasks', 1);('goal waypoint', 1);('dynamics prediction170', 1);('initialstate inference', 1);('variety inference tasks singleagent', 1);('task corresponds', 1);('unique maskingscheme order', 1);('generalist agent performswell arbitrary sequence inference', 1);('trainedby random', 1);('scheme masks', 1);('probability pmask\x18uniform', 1);('sucha', 1);('training method', 1);('wellon inference tasks', 1);('minigridenvironment framework', 1);('generalist agent andeven outperforms', 1);('models train thegiven tasks means singletask models', 1);('uniedmultitask training improves performanceanother difculty generalization', 1);('generalize unseen tasks', 1);('common method ofinemetareinforcement learning eg', 1);('maml', 1);('macaw175', 1);('quick adaptation', 1);('framework adaptation innlp', 1);('promptbased decision transformer promptdt', 1);('focuses powerof architecture inductive bias', 1);('text descriptionas', 1);('large human labor annotate177', 1);('trajectory segments promptand force agent imitate demonstrations', 1);('9b experimentseg', 1);('cheetahdir', 1);('antdir', 1);('dial', 1);('thatgiven fewshot trajectories', 1);('strongmeta ofine', 1);('focus training agentssuch robust changes environmentdynamics', 1);('causal reasoning enrich trainingdataset counterfactual trajectories', 1);('theagents adaptation structural changes', 1);('mathematicallyfollowed', 1);('model transitions asps0jsa\x08with', 1);('structural featuresof environment', 1);('counterfactual trajectories', 1);('\x08 order train onmore', 1);('effective counterfactual trajectories', 1);('atemeasurement', 1);('order trajectoriesate', 1);('cf txt1epcf', 1);('rt\x00epsource rt 20journal', 1);('linear decoderemb pos enc', 1);('transformer prompt', 1);('structure generalized decision transformer', 1);('function \x08 b', 1);('structure promptdt', 1);('augmentationand history sequence input', 1);('corresponding stateempirical results gymminigrid environment suite 173prove efcacy gymminigrid environment suite173there', 1);('generalizationfrom perspectives', 1);('anymorph', 1);('traininga policy generalize', 1);('new agent morphologies', 1);('attentionneuron', 1);('sudden random', 1);('task etc4', 1);('pplications trlthere', 1);('specic applications', 1);('popular environments includingrobotic manipulation', 1);('games visionlanguagenavigation autonomous', 1);('need process multimodal input acomplex environment', 1);('utilize theability transformer architecture woulddescribe problem formulation eld', 1);('corresponding solutions', 1);('transformer architecture41', 1);('robotic manipulationfor', 1);('oneshot robotic imitation learning importantto', 1);('goalintentionfrom visual demonstrations incorporate withcurrent observations policy network', 1);('al52 explore taskdriven', 1);('oneshot learningstrategy use transformer nonlocal selfattention block182 multihead version extract relational featuresas input state vector policy function transformer', 1);('resnet', 1);('process isbenecial', 1);('spatial temporal information inthe attention module', 1);('test time transformerfeatures', 1);('important details', 1);('inverse dynamics loss whichforces transformer model', 1);('inparticular', 1);('context observations rst randomizedand', 1);('representation model generatefeatures twhich', 1);('logisticmixture distribution', 1);('inverse loss islinvd\x12 \x00lnkxi1 k t t1\x01logistic \x16i t t1\x1bi t t1 21results multiagent', 1);('showthat network design inverse loss canhelp policies', 1);('testtime furtherablations', 1);('transformer models', 1);('levelattention mechanism succession', 1);('zhao', 1);('verystrong similarity train test oneshotimitation learning', 1);('osil', 1);('setting introduce', 1);('robotic manipulation benchmark', 1);('new methodmosaic', 1);('compared tosil', 1);('policy network adopts temporalcontrastive loss objective', 1);('thatthe representations', 1);('adjacent frames assimilar', 1);('possible cases', 1);('ielrep logexpqtwkexpqtwk', 1);('pf\x001i1qtwki', 1);('22whereqkare anchor', 1);('positive respectivelywis projection matrix', 1);('fis', 1);('total frame countexperiments', 1);('robosuite', 1);('metaworld', 1);('powerful learning efciency performance', 1);('general multitaskpolicy', 1);('netuningin humanrobot interaction setting', 1);('natural languageis', 1);('intuitive ways express human intent tothe robot', 1);('easy task', 1);('robotto generate target trajectory human instructionsand commands', 1);('rigid templates witha static', 1);('actions commands', 1);('188provide exible', 1);('interface usercan reshape', 1);('robotic trajectory', 1);('thehuman commands', 1);('trajectory generationprocess sequence', 1);('natural toleverage', 1);('powerful language model transformer', 1);('specifically bucker', 1);('rst employ', 1);('latte', 1);('whichincludes language image encoder geometry encoderand multimodal transformer decodermodels encode human instructions', 1);('rich semantic representations', 1);('language modelcan', 1);('requirement number trainingexamples align geometrical trajectory datawith language', 1);('multimodal attention mechanism', 1);('show thecombination', 1);('large language model withthe multimodal transformers', 1);('intuitive interfaces robots humans', 1);('traditional methods kinesthetic teaching', 1);('enhancethe architecture', 1);('actual images encoder 189and', 1);('trajectory 3d space velocity ratherthan', 1);('xy', 1);('similarlysharma', 1);('transformer architecturemodel', 1);('semantic costmap guides planner', 1);('focus designinggeneric', 1);('trajectories notice thatrecent', 1);('problem sequential tasks requiringto track object', 1);('previous actions', 1);('new model', 1);('modelsthe human instructions', 1);('multiple views history', 1);('amultimodal transformer', 1);('simultaneouslyencodes instructions', 1);('current visual observations proprioception predicts 7dofactions', 1);('experiments rlbench', 1);('demonstrates thestrong generalization ability', 1);('history input contributes longterm tasks', 1);('multiple viewsinput contributes task', 1);('high precision orwith', 1);('similarly shridhar', 1);('aperceiver transformer', 1);('3d structure voxelpatches language goals', 1);('6dof actionsfor taskspecic settings robot needs', 1);('thetask constraints', 1);('jain', 1);('adapt users differentpreferences', 1);('able generalize test time unseenpreferences', 1);('user demonstration', 1);('assumingthat', 1);('task structureand preference', 1);('demonstrations trajectory', 1);('current state learns imitate preferencein demonstrationsminm\x18m \x18dmsa\x18dmlcea\x19s 23wheremis preference', 1);('dis', 1);('multipreference datasetand', 1);('contains stateaction pairs', 1);('validates property', 1);('kitchenenvironment realworld', 1);('franka panda', 1);('robotic arma', 1);('common use transformer robotic manipulation domain', 1);('extraction modulesfor', 1);('transformer architecture betterextract representations visuallanguagetext input fordownstream', 1);('dualarm robot', 1);('textbased gamestextbased', 1);('interactive simulations wherean agent plays game processing text observationsand', 1);('text commands', 1);('formally tbgs', 1);('sparse rewards completinga sequence subgoals', 1);('onthe environments complexity game length verbosity199', 1);('tbgs textworld', 1);('jericho', 1);('textworld qa', 1);('poor explore entirespaces', 1);('games billionpossible actions', 1);('ammanabrolu', 1);('exists oracle agent', 1);('rst use question', 1);('qa tbgs', 1);('nextaction network', 1);('experience ofthe oracle agent use knowledge graphs statespaces', 1);('agents showcase', 1);('kga2c203', 1);('able tackle', 1);('combinatorial actionspace rst time order', 1);('sample efciency', 1);('ammanabroluet', 1);('language model', 1);('qbertnetunes albert squad', 1);('jerichoqadataset', 1);('specic textgame domain', 1);('giventhe', 1);('update knowledge graph thecurrent state', 1);('compared kga2c qbert', 1);('asymptotic performance', 1);('qa', 1);('learning thusimproves sample', 1);('point priorworks', 1);('heuristic exploits gamesinherent structure example', 1);('kga2c', 1);('heuristics update', 1);('kg', 1);('tryto design general', 1);('effective representations inan autonomous way', 1);('heuristics learning', 1);('learns construct update thegraphstructure beliefs', 1);('datadriven mannerjournal', 1);('transformer version', 1);('lstmdqnwith', 1);('dynamic belief graph', 1);('action selection', 1);('game dynamics', 1);('twomodules graph updater action selector thegraph updater', 1);('regimes rst method involves decoder modelto reconstruct text observations belief graph', 1);('problem sequencetosequence tasks', 1);('modell\x00lotxi1logpoitjo1toi\x001tgtat\x001 24wheregtis belief graph', 1);('at\x001is', 1);('previous actionotis observation', 1);('method reformulates therst method contrastive prediction task learns tomaximize', 1);('mutual information predictedgtand text observations', 1);('ot', 1);('action selectorit encodes belief graph', 1);('gtand', 1);('text observation', 1);('otinto', 1);('representations aggregates usinga bidirectional', 1);('greatperformance outperforms baseline groundtruthgraphs evinces effectiveness', 1);('representationsin succession', 1);('successful playalso', 1);('instructions embeddedwithin text', 1);('actions agentso', 1);('ultimate goal', 1);('sensitive presence orabsence instructions', 1);('temporal logic ltl', 1);('formal language whichprovides mechanism monitor progress', 1);('instructions completion', 1);('specically tuli', 1);('rst use', 1);('toaugment reward dene episode termination', 1);('whenthe ltl', 1);('bonus reward givenand penalty', 1);('completean instruction', 1);('terminal state', 1);('theoriginal architecture adapt', 1);('instructions whichincludes', 1);('instructions ingameupdate process', 1);('\x19ajog whereg beliefstate', 1);('augmentation results', 1);('large performanceimprovements', 1);('effective instruction', 1);('dynamics model', 1);('partiallyobservable states noisy text dynamics', 1);('objectoriented mdp oomdp', 1);('factorizesworld states object states', 1);('objectoriented text dynamics ootd', 1);('model includesgraph representation objects', 1);('independent transitionlayers', 1);('belief states', 1);('rewardsand observations', 1);('ootdfig', 1);('ltlgata', 1);('image 60which', 1);('action selectormodels', 1);('beliefstates objects reward model maps sampledobject states rewards', 1);('settingthe transition model rst uses graph decoder thecomplex', 1);('map object statesinto memory graph ht\x001given states', 1);('objectszt\x001 z1t\x001zkt\x001 encodes ht\x001into noderepresentations et\x001 e1t\x001ekt\x001with', 1);('rgcn210', 1);('actionootd uses', 1);('bidaf', 1);('group transitionlayers', 1);('belief state objects', 1);('icm', 1);('framework179 reward model', 1);('asprrtjztgt rzpoolt ggt 25wheregtis goal gis', 1);('function ris', 1);('mlp empiricalresults textworld', 1);('modelfreebaselines terms sample efciency performance43', 1);('navigationlearning', 1);('navigation photorealistic environment withvisuallinguistic clues', 1);('agents need', 1);('language instructions perceive', 1);('navigation actions', 1);('target goal', 1);('thevln', 1);('future observations', 1);('dependent current state action agent visual', 1);('corresponds partial instruction', 1);('agentto memorize navigation state', 1);('localize theuseful information', 1);('current decision making order topromote development', 1);('eld alsoseveral datasets simulators', 1);('natural languagejournal', 1);('r2r', 1);('touchdown', 1);('cvdn', 1);('vnla', 1);('hanna216', 1);('remote objects', 1);('reverie217', 1);('onsince instruction', 1);('characterizes thetrajectory visual state shares', 1);('various relationships withlanguage instructions', 1);('problematic severalmethods', 1);('instructions scratch', 1);('pretrain anencoder align representations language instructionsand visual states alleviate ambiguity ofinstructions', 1);('prevalent formally', 1);('xfxigmi1', 1);('instructioncontainsliword tokens xi xi1xili agentlearns navigate policy \x19given trainingdatasetdef xgwhere', 1);('expert trajectorymax\x12l\x12 x log\x19\x12 jx', 1);('txt1log\x19\x12atjstx26where\x12is', 1);('policy parameter', 1);('singlemodal encoders crossmodal encoder constructthe backbone', 1);('transformer andpropose', 1);('main tasks pretrain policy', 1);('andaction prediction', 1);('similar pretrainingin', 1);('negative loglikelihoodlmlm \x00es\x18p x\x18delogpxijxnis 27where recovery process', 1);('thetrajectory history bylap\x00eas\x18p x\x18delogpajxcls 28where', 1);('cls', 1);('topredict action', 1);('lpretraining lmlm lap', 1);('press', 1);('thelanguage instructions offtheshelf', 1);('vilbert', 1);('instructiontrajectory pairs measure compatibility arealso lot methods', 1);('large number imagetext pairs learngeneric crossmodal representations', 1);('vl bertin', 1);('hong', 1);('point insteadof', 1);('model learning navigatewith modication', 1);('vln hong', 1);('recurrent visionandlanguage', 1);('original architecture arecurrent function memorize history representationsand', 1);('language tokens keys values butnot queries selfattention modules', 1);('thecomputation consumption', 1);('experiments r2r', 1);('reverie', 1);('vl bert', 1);('plusrecurrent function', 1);('complex encoderdecoder models', 1);('stateoftheart resultsfurthermore', 1);('encode history', 1);('recurrent stateswhich', 1);('essential information', 1);('subsequent trajectory', 1);('aware multimodal transformer hamt', 1);('uses acrossmodal transformer', 1);('longrange dependencies', 1);('current observation instruction thehistory sequence adopts hierarchical version thetransformer mitigate computation expenses', 1);('learns representations', 1);('views thespatial relationships', 1);('panorama andthe temporal relationships', 1);('different panoramas 228in order', 1);('visual representations', 1);('proxy tasks', 1);('spatialrelationship prediction', 1);('sprel', 1);('itm', 1);('sequential action prediction', 1);('rlil', 1);('objective\x12 \x12\x161ttxt1r\x12log\x19aht\x12rt\x00vt\x15\x161t\x03t\x03xt1r\x12log\x19a\x03t\x12 30through method', 1);('strong performance bothin', 1);('unseen environments tasks', 1);('similarlypashevich', 1);('multimodal transformer model', 1);('et', 1);('full episode history andleverage synthetic instructions interface betweenthe human agent', 1);('easilyand generalize betterafterward', 1);('complex andstochastic environment audiovisionlanguage environment', 1);('realworld navigation', 1);('theypresent avlen', 1);('guides agent localize audiosource', 1);('realistic visual world agents', 1);('audiovisual information', 1);('seekhelp oracle navigation instructions formof', 1);('natural language', 1);('querythe oracle', 1);('audiovisual cues reachthe goal', 1);('multimodal hierarchicalreinforcement learning framework', 1);('highlevel policy', 1);('select modality use', 1);('lowlevel policy', 1);('corresponding different modality', 1);('experiments matterport', 1);('3d simulator', 1);('audio events', 1);('thesoundspaces framework', 1);('clear improvement inperformance', 1);('existence oracle', 1);('challenging cases44', 1);('autonomous drivingfor', 1);('aspointtopoint navigation', 1);('urban setting keepinga', 1);('safe distance', 1);('dynamic agents followingtrafc rules', 1);('categories approachesjournal', 1);('rgb depthsoundpose sensorprevious', 1);('image encoderaudio encoderpolicy encoder decoderactorcriticactionsamplerpolicy encoder decoderactorcriticactionsamplerobservationstateencoderenvironmentlanguage instructionclipembeddingmulti layer transformeractionpredictiongoal descriptormemory storagefusionfig', 1);('avlen', 1);('whichis combination query policy \x19q', 1);('navigation policy \x19lto learning', 1);('learning andreinforcement learning development', 1);('nocrash', 1);('series ofworks', 1);('sam', 1);('stp3', 1);('intermediate representations', 1);('neat', 1);('adoptingthe neural attention elds reasoning scene structurehowever', 1);('learning methods', 1);('data lack robust interpretabilitythe performance', 1);('ofine handcraftedtrajectory', 1);('researchers interest example', 1);('roach', 1);('rlbased', 1);('model toprovide demonstration', 1);('il', 1);('toromanoff', 1);('rainbowiqnapex', 1);('visual inputthe', 1);('inputboth images cameras', 1);('natural question tointegrate representations', 1);('respective advantages', 1);('prakash', 1);('multimodal fusiontransformer transfuser', 1);('model integrates representations', 1);('different modalities selfattentiontransformer predicts future waypoints autoregressive manner', 1);('transfuser', 1);('imitation learning policy learningto imitate expert trajectoryargmin\x19exw\x18dlw\x19x 31wherewfxtytgtt1is expert trajectory', 1);('xis', 1);('thehighdimensional observations loss function', 1);('l1lossl2loss', 1);('thetransfuser process harms sensor scalability limitedto fusion', 1);('singleview image whichmay problem', 1);('sensorfusion transformer interfuser', 1);('able fuse information multimodal multiview sensors', 1);('whats', 1);('byhumans information processing', 1);('output theintermediate', 1);('safety mind mapwhich', 1);('dynamicagents trafc signs', 1);('constrain theactions', 1);('safe action', 1);('interfuser', 1);('world model reconstruct', 1);('returnsrewards states policy model reconstruct theaction sequencegreat performance', 1);('complex adversarial', 1);('urban scenariostowards training dataset algorithms need learnbehavioral priors', 1);('option isthe ofine', 1);('conservative policy taskspecic behaviors', 1);('problems wherereward labels', 1);('methods oftenmake', 1);('fundamental assumption data', 1);('ofunimodal expert trajectory', 1);('natural precollecteddata', 1);('suboptimal contains', 1);('multiple modes behavior order train models', 1);('multimodal policy behavior', 1);('shaullah', 1);('behavior transformers bet', 1);('multimodal data', 1);('betdivides', 1);('action center kmeans', 1);('residual action uses transformerto map observation categorical distribution kdiscrete action bins', 1);('focal', 1);('loss 241lfocalpt \x001\x00pt logpt 32and', 1);('extra head', 1);('offset loss akin tothe', 1);('multitask loss 242mtloss ahajiikj1 kxj1ibacj\x01jjhai\x00hajijj2233where', 1);('idenotes iverson', 1);('ground truth class action aexperiments', 1);('bet', 1);('allthe modes demonstration datamore', 1);('methods stochastic safetycritical domainshowever methods', 1);('model states actionsas', 1);('optimistic behavior', 1);('separated latent trajectory transformer splt transformer', 1);('policy worlddynamics outcomes', 1);('followingthe', 1);('domainsplt trains', 1);('discrete latentvaes', 1);('world policy model', 1);('range candidate behaviors', 1);('sdvand', 1);('different potential response theenvironment', 1);('conductsrobust planning test time intuition pickingjournal', 1);('18a policy robust', 1);('realistic possible future inthe environment', 1);('experiments carla', 1);('able outperform baseline approaches', 1);('variety autonomous', 1);('works trajectory predictionleverage', 1);('247to model longrange dependencies prediction incomplex trafc situations transformer', 1);('usedto model relationship', 1);('dynamic agents static elements eg lanes', 1);('light dynamic state', 1);('benecial consistent prediction withthe', 1);('vae', 1);('differentpossible modes future trafc trajectories45', 1);('applicationautomatic', 1);('methods policy learning', 1);('252however methods', 1);('symptom inquiryand disease diagnosis policy max reward ratherthan', 1);('diagnostic logic', 1);('nooptimal reward functions', 1);('diagnosis process', 1);('assequence generation', 1);('symptoms diagnoses', 1);('automatic diagnosisas sequence', 1);('train autoregressivemanner', 1);('attention mask mechanism', 1);('automatic diagnosis', 1);('explicit symptomcan', 1);('explicit symptoms implicitsymptom', 1);('explicit symptoms', 1);('implicit symptoms autoregressive mannerin order mitigate discrepancy order ofsymptoms sequence', 1);('disorder realisticsymptom inquiry', 1);('orderless training mechanisms sequence shufe', 1);('experiments onmuzhi', 1);('dxy', 1);('synthetic', 1);('modelthe maintenance problem', 1);('maintenance decisions minimize costs', 1);('thereare', 1);('failure predictionand', 1);('useful life', 1);('estimation human', 1);('maintenance decisions', 1);('likelihood offailure', 1);('rul motivated', 1);('success ofdt', 1);('khorasgani', 1);('problems ofine', 1);('generate optimal maintenance actions continuation repair', 1);('historyof observations actions', 1);('rtgs khorasgani', 1);('action state', 1);('common todevelop', 1);('estimation models industries whenthe', 1);('input intothe decision model', 1);('accuracy simplify', 1);('otkatk\x001rhkrulk', 1);('34wheretis length history window futurehorizon', 1);('experiments cmapss', 1);('generate optimal maintenance actions', 1);('e xtension discussionthe', 1);('hot topic eld ofrl', 1);('strong performance', 1);('tremendous potential', 1);('natural followupdevelopment direction', 1);('systems multiagentreinforcement learning', 1);('potential transformer', 1);('challengesstill need', 1);('future prospects needto', 1);('section rst introduce transformer multiagent', 1);('insights future prospects51', 1);('extensionthe', 1);('transformers tosolve multiagent', 1);('rl marl', 1);('rst work', 1);('261which studies paradigm ofine', 1);('thelargescale datasets', 1);('treatingthe', 1);('decision making process sequence', 1);('activatesa novel pathway', 1);('systems diversedatasets', 1);('problem sinceonline exploration', 1);('multiagent decision transformer madt', 1);('autoregressive transformer architectureshown', 1);('local statefor agent formulate trajectory', 1);('i x1xtxtwherext stoitait35wherestdenotes', 1);('global state oitdenotes localobservation agent iat time step tand', 1);('similar ait', 1);('andthey', 1);('crossentropy', 1);('ofine pretraininglosslce\x12 1ttxt1pat logpatj tat\x12 36whereatis ground truth action tincludesfs1to1tgandadenotes output', 1);('theofine dataset', 1);('usingthe training method online environment sincethe agent', 1);('select actions', 1);('similar ofine dataset', 1);('bad actions', 1);('thusmeng', 1);('transformer backboneof actor critic networks', 1);('algorithm 77experimental results', 1);('high sample efciency generalisabilityenhancementsafterward', 1);('wen', 1);('multiagent transformer mat', 1);('perspective encoderdecoder transformer architecture', 1);('datasetsn', 1);('head attentionadd normfeed forwardadd norm actor networkpadded agent observation embeddingpadded agent padded', 1);('embedding critic network actor losseg ppo losscritic losseg huber loss', 1);('observationagents actionglobal stateall agents actionscausal transformern', 1);('agenta bfig', 1);('bthe encoderdecoder architecture', 1);('inputs sequence agents observations andgenerates agents optimal action autoregressive mannersince agents interactions', 1);('important multiagentsystems', 1);('joint performance wouldimprove', 1);('agent transformerpolicy training', 1);('themultiagent advantage decomposition theorem', 1);('whichtransforms joint policy optimization sequentialpolicy search process', 1);('eq37 mat', 1);('solves theproblem sequence', 1);('enjoysthe monotonic performance improvement guaranteeai1n\x19oai1n nxm1aim\x19oai1m\x001aim 37whats', 1);('adopts ofine imitation learningmat', 1);('online trials errors', 1);('ppolikely', 1);('smac multiagent mujoco', 1);('research football 269demonstrate', 1);('superior performance andhigh data efciency', 1);('excellent fewshot learnerin order tackle partial observability', 1);('rnnsare', 1);('memory information singleagent settings', 1);('rnns yang', 1);('agenttransformer memory atm', 1);('network introduces', 1);('marland entitybound', 1);('layer', 1);('incorporate semanticinductive bias action space', 1);('rstprovides agent memory buffer store pastinformation updates buffer rstinrstout queue', 1);('different entities uses', 1);('qvaluesor', 1);('action probabilitiesqqhaa 38wherehais', 1);('agent correlatedto action experiments', 1);('qmix', 1);('qplex', 1);('algorithms representative algorithms onsmac results', 1);('atms', 1);('ability speedup improvement performance', 1);('needs map actions differentagents', 1);('hard impossible someenvironmentsfurthermore', 1);('model calleduniversal', 1);('policy decoupling transformer updet', 1);('multiple tasks time', 1);('centralized', 1);('executionctde architecture limitation input oroutput dimension', 1);('possibleeach agents value function', 1);('qfunctionthrough', 1);('credit assignment function', 1);('fq\x19stut fqt1qtn', 1);('39whereutis candidate actions qtiis', 1);('qfunction', 1);('agent iat time step', 1);('individual entity observations actiongroups learns relationship matchedentity observationentity', 1);('entitybound', 1);('layer272', 1);('nal loss', 1);('entire framework isthe', 1);('td', 1);('74l\x12 bxi1ydqni\x00qsu\x122 40wherebrepresents batch size ydqni denotes thetarget', 1);('combining', 1);('transformer architecture policy', 1);('updet vdn', 1);('qmix274 qtran', 1);('experimental', 1);('smac263', 1);('capability terms bothperformance speed52', 1);('challengesalthough', 1);('tasks theseworks rst steps', 1);('domain stillhave', 1);('room improvementas', 1);('methods highprobability failure stochastic environments manypreconditions need', 1);('20optimal policy deterministic dynamics', 1);('target return target return', 1);('corresponding thedistribution dataset', 1);('return mitigate', 1);('negative effectof uncertainty training dataset stochasticenvironment results', 1);('inferior traditionalrl algorithm', 1);('appropriate condition themodel', 1);('return input agent isthe visual goals', 1);('long way', 1);('transformer structurethen transformer architecture', 1);('standard transformer', 1);('atransformer structure', 1);('toimprove architecture', 1);('standard transformerarchitecture', 1);('incorporate markovianlike inductive biasinto transformer', 1);('disentangle thepolicy world dynamics generation safer outcomesthese improvements', 1);('taskdriven mannerrather', 1);('general transformer architecturewith high performance', 1);('open problem network', 1);('reinforcement learning algorithm', 1);('mlp', 1);('time transformer', 1);('expensive example thetransformer generalization task', 1);('needsto train largescale data structure needs', 1);('possible situationsalthough methods', 1);('network structurethrough compression distillation', 1);('able deploy', 1);('prospectsin', 1);('transformer capabilities', 1);('potential directions', 1);('problems simpliesmany limitations', 1);('losestheir advantages', 1);('advantages traditionalrl algorithms sequence', 1);('transformeris problem', 1);('time notedthat', 1);('future sequence introduction', 1);('optimal results manyenvironments', 1);('condition whatfunction use', 1);('promising directions future research', 1);('process underthe condition training dataset expert datais', 1);('worth studyingfor', 1);('algorithms rst problem thatneeds', 1);('computational consumption oftransformers', 1);('secondly', 1);('transformerstructure adapt memory module', 1);('worth studyingwhats addition', 1);('lstm pomdp', 1);('conditions otherroles transformer', 1);('worthy furtherstudy time', 1);('dataset contains learning data', 1);('large enoughall reinforcement learning algorithms', 1);('thetransformer structure', 1);('possible design generalmodel', 1);('tasks hasnot', 1);('yetfor realworld applications addition', 1);('thetransformer structure multimodal multitask featureextraction', 1);('thetransformer structure decision network isnecessary', 1);('causes errors andpossible', 1);('negative consequences', 1);('critical security', 1);('practical applicationsnote', 1);('experimental environments', 1);('different methods survey', 1);('different isdifcult', 1);('comparedifferent methods', 1);('followup research', 1);('baseline metric architectureenhancement methods', 1);('environments critic observations', 1);('span entireepisode', 1);('betterchoice memory information', 1);('important makeoptimal decisions agent trajectory optimizationsince methods', 1);('rl d4rldataset', 1);('environment whichhas', 1);('large ofine trajectories order verifythe generalization', 1);('necessary train single agent fordifferent environments', 1);('games suite andgymminigrid environment', 1);('different tasks differentapplications representative testenvironment', 1);('herein addition content', 1);('hasgreat potential elds', 1);('fullystudied6 c', 1);('onclusionin', 1);('survey article', 1);('comprehensive overviewof transformers involvement', 1);('realistic application', 1);('architecture transformer', 1);('thenwe', 1);('works transformer architectureenhancement methods trajectory optimization methodsand', 1);('perspective onthe', 1);('open problems eld', 1);('possible domainsfor', 1);('promisingfuture directions researchreferences1', 1);('p nadkarni', 1);('ohnomachado', 1);('w w', 1);('chapman', 1);('natural language processing introduction journal', 1);('americanmedical informatics', 1);('brown', 1);('mann n ryder subbiah j kaplan p dhariwal neelakantan p shyam g sastry askell', 1);('models fewshot learners', 1);('j devlin mw chang k lee k toutanova bert pretraining', 1);('deep bidirectional transformers language understanding arxiv preprint arxiv181004805', 1);('chen radford r child j wu h jun luan', 1);('sutskever generative', 1);('dosovitskiy', 1);('beyer kolesnikov weissenborn x zhait unterthiner dehghani minderer g heigold gellyet', 1);('transformers', 1);('imagerecognition scale arxiv preprint arxiv201011929', 1);('z liu lin cao h hu wei z zhang lin', 1);('guoswin', 1);('vision transformer', 1);('bahdanau k cho bengio neural', 1);('machine translation', 1);('learning align', 1);('arxiv preprintarxiv14090473', 1);('p parikh tckstrm das j uszkoreit', 1);('decomposable attention model', 1);('natural language inference arxivpreprint arxiv160601933', 1);('ramesh pavlov g goh', 1);('gray c', 1);('voss radfordm chen sutskever zeroshot', 1);('texttoimage generationinicml', 1);('jb alayrac j donahue p luc miech barr hassonk lenc mensch k millican reynolds', 1);('flamingoa', 1);('visual language model fewshot learning arxiv preprintarxiv220414198', 1);('reed k zolna e parisotto g colmenarejo novikovg barthmaron gimenez sulsky j kay j springenberget', 1);('generalist agent arxiv preprint arxiv220506175', 1);('r sutton g barto reinforcement', 1);('learning introduction', 1);('mit', 1);('singh r kumar v p singh reinforcement', 1);('learning inrobotic applications', 1);('comprehensive survey', 1);('articial intelligence review', 1);('singla n rafferty g radanovic n heffernanreinforcement', 1);('learning education', 1);('opportunities', 1);('arxiv preprint arxiv210708828', 1);('liu k', 1);('k ngiam', 1);('celi x sun feng', 1);('reinforcement', 1);('learning clinical decision support criticalcare', 1);('comprehensive review journal', 1);('internet', 1);('r kiran sobh v talpaert p mannion al sallabs yogamani p prez deep', 1);('reinforcement learning forautonomous', 1);('ieee transactions intelligenttransportation systems', 1);('levine kumar g tucker j fu ofine', 1);('review perspectives', 1);('problemsarxiv preprint arxiv200501643', 1);('prudencio r maximo e', 1);('colombini', 1);('surveyon ofine reinforcement learning', 1);('taxonomy', 1);('review openproblems arxiv preprint arxiv220301387', 1);('siebenborn', 1);('belousov j huang j peters howcrucial', 1);('transformer decision transformer arxiv preprintarxiv221114655', 1);('n mishra rohaninejad x chen p abbeel', 1);('simpleneural attentive metalearner arxiv preprint arxiv170703141', 1);('k irie schlag r csords j schmidhuber going', 1);('beyondlinear transformers recurrent', 1);('fast weight programmersneurips', 1);('rae potapenko jayakumar', 1);('hillier p lillicrap compressive', 1);('transformers longrange', 1);('chen k lu rajeswaran k lee grover laskinp abbeel srinivas mordatch decision', 1);('transformerreinforcement learning', 1);('janner q li levine ofine', 1);('reinforcement learningas', 1);('big sequence', 1);('r loynd r fernandez celikyilmaz swaminathan', 1);('hausknecht working', 1);('memory graphs', 1);('e parisotto r salakhutdinov efcient', 1);('actorlearner distillation arxiv preprintarxiv210401655', 1);('banino p badia j walker scholtes j mitrovic', 1);('blundell coberl contrastive', 1);('bert reinforcement learning arxiv preprint arxiv210705431', 1);('v micheli e alonso', 1);('fleuret transformers', 1);('sampleefcient world models arxiv preprint arxiv220900588', 1);('chen yf wu j yoon ahn transdreamer reinforcement', 1);('learning transformer world models arxiv preprintarxiv220209481', 1);('ozair li razavi antonoglou van den oord', 1);('vinyals vector', 1);('j sun da huang', 1);('lu yh liu', 1);('zhou gargplate visuallygrounded', 1);('transformers procedural tasks', 1);('fan g wang jiang mandlekar yang h zhu tangda huang zhu anandkumar minedojo buildingopenended', 1);('agents internetscale knowledgearxiv preprint arxiv220608853', 1);('k paster mcilraith j ba', 1);('whydecision', 1);('stochastic environments arxivpreprint arxiv220515967', 1);('brandfonbrener bietti j buckman r laroche', 1);('bruna', 1);('learning work ofine reinforcement learning arxiv preprintarxiv220601079', 1);('q zheng zhang grover online', 1);('decision transformer arxiv preprint arxiv220205607', 1);('j shang k kahatapitiya x li ryoo starformertransformer', 1);('stateactionreward representations visualreinforcement learning', 1);('wang j oh e parisotto spencer r steigerwaldd strouse hansen filos e brooks', 1);('incontextreinforcement', 1);('learning algorithm distillation arxiv preprintarxiv221014215', 1);('k wang h zhao x luo k ren', 1);('zhang li bootstrapped', 1);('transformer ofine reinforcement learning arxivpreprint arxiv220608569', 1);('yamagata khalil r santosrodriguez qlearningdecision', 1);('leveraging', 1);('forconditional sequence', 1);('ofine rl arxiv preprintarxiv220903993', 1);('reid yamada gu', 1);('ofinereinforcement learning arxiv preprint arxiv220112122', 1);('li x puig', 1);('wang e akyurek torralba j andreasand mordatch pretrained', 1);('arxiv preprint arxiv220201771', 1);('different modality ofine reinforcement learning', 1);('liu h liu grover p abbeel masked', 1);('scalable generalizable decision making', 1);('yang nachum representation', 1);('matters ofine', 1);('sequential decision making', 1);('kh lee nachum yang', 1);('lee freeman', 1);('xus guadarrama fischer e jang h michalewski', 1);('multigame', 1);('decision transformers arxiv preprint arxiv220515241', 1);('q lin h liu', 1);('sengupta switch', 1);('trajectory transformerwith distributional value approximation multitask reinforcement learning arxiv preprint arxiv220307413', 1);('h furuta matsuo gu generalized', 1);('decision transformer ofine hindsight information', 1);('arxiv preprintarxiv211110364', 1);('j lin r georgescu sun bignells milani k hofmann hausknecht dragan', 1);('unimask unied', 1);('inference sequential decision problems arxivpreprint arxiv221110869', 1);('xu shen zhang lu zhao j tenenbaum', 1);('gan prompting', 1);('decision transformer fewshot policygeneralization', 1);('boustati h chockler', 1);('mcnamee transfer', 1);('learningwith causal counterfactual reasoning decision transformersarxiv preprint arxiv211014355', 1);('dasari gupta transformers', 1);('oneshot visual imitation', 1);('figueredo haddadin kapoor maand r bonatti latte language', 1);('trajectory transformer arxivpreprint arxiv220802918', 1);('guhur chen r garcia tapaswi laptev', 1);('schmid instructiondriven', 1);('historyaware policies roboticmanipulations arxiv preprint arxiv220904899', 1);('manuelli fox perceiveractor amultitask', 1);('transformer robotic manipulation arxiv preprintarxiv220905451', 1);('v jain lin e undersander bisk rai transformersare', 1);('adaptable task planners arxiv preprint arxiv220702442', 1);('adhikari x yuan ct zelinka rondeaur laroche p poupart j tang trischler', 1);('hamiltonlearning', 1);('dynamic belief graphs generalize textbasedgames', 1);('p ammanabrolu e tien hausknecht riedl howto', 1);('eaten grue', 1);('structured', 1);('exploration strategiesfor textual worlds arxiv preprint arxiv200607409', 1);('g liu adhikari farahmand p poupart learningobjectoriented', 1);('li p vaezipoor q klassen sanner amcilraith learning', 1);('gamesarxiv preprint arxiv221104591', 1);('li x li', 1);('carin j gao towards', 1);('learninga generic agent visionandlanguage navigation', 1);('majumdar shrivastava lee p anderson parikhand batra improving', 1);('visionandlanguage navigation withimagetext pairs web', 1);('chen p', 1);('schmid laptev history', 1);('awaremultimodal transformer visionandlanguage navigationneurips', 1);('paul k roychowdhury cherian avlen audiovisuallanguage', 1);('navigation 3d environments arxivpreprint arxiv221007940', 1);('r villaor z huang pande j dolan j schneideraddressing', 1);('optimism bias sequence', 1);('n shaullah z j cui altanzaya', 1);('pintobehavior', 1);('cloning', 1);('stone arxivpreprint arxiv220611251', 1);('prakash k chitta geiger multimodal', 1);('fusion transformer endtoend autonomous', 1);('h shao', 1);('wang r chen h li liu safetyenhancedautonomous', 1);('interpretable sensor fusion transformer arxiv preprint arxiv220714024', 1);('z wang schaul hessel h hasselt lanctot', 1);('freitas dueling', 1);('network architectures', 1);('r j williams simple', 1);('algorithmsfor connectionist reinforcement learning', 1);('machine', 1);('r sutton mcallester singh mansour policygradient', 1);('methods reinforcement learning function approximation', 1);('kakade', 1);('natural policy gradient', 1);('v mnih k kavukcuoglu silver rusu j veness gbellemare graves riedmiller k fidjeland g ostrovskiet', 1);('humanlevel', 1);('reinforcement learning nature', 1);('h van hasselt guez silver deep', 1);('p lillicrap j j hunt pritzel n heess erez tassad silver wierstra continuous', 1);('reinforcement learning arxiv preprint arxiv150902971', 1);('j schulman', 1);('wolski p dhariwal radford klimovproximal', 1);('policy optimization algorithms arxiv preprintarxiv170706347', 1);('haarnoja zhou k hartikainen g tucker ha j tanv kumar h zhu gupta p abbeel', 1);('soft', 1);('actorcriticalgorithms applications arxiv preprint arxiv181205905', 1);('r kidambi rajeswaran p netrapalli joachimsmorel modelbased', 1);('yu g thomas', 1);('yu ermon j zou levine', 1);('finnand mopo modelbased', 1);('ofine policy optimizationneurips', 1);('yu kumar r rafailov rajeswaran levine', 1);('finn combo conservative', 1);('policy optimization', 1);('j chen li q chen', 1);('zhou x liu diaformer automatic', 1);('symptoms sequence generation', 1);('k arulkumaran p deisenroth brundage abharath deep', 1);('brief survey', 1);('ieeesignal processing magazine', 1);('vaswani n shazeer n parmar j uszkoreit', 1);('jones ngomez kaiser polosukhin attention', 1);('z lin feng', 1);('n santos yu', 1);('xiang', 1);('zhouand bengio', 1);('selfattentive sentence embeddingarxiv preprint arxiv170303130', 1);('hendrycks k gimpel gaussian', 1);('error linear unitsgelus arxiv preprint arxiv160608415', 1);('agarap deep', 1);('linear units reluarxiv preprint arxiv180308375', 1);('j gehring auli grangier yarats n dauphinconvolutional', 1);('sequence sequence learning', 1);('p shaw j uszkoreit vaswani selfattention', 1);('relative position representations arxiv preprint arxiv180302155', 1);('ba j r kiros g e hinton layer', 1);('normalizationarxiv preprint arxiv160706450', 1);('baevski auli adaptive', 1);('input representations forneural language', 1);('arxiv preprint arxiv180910853', 1);('shen z yao gholami', 1);('mahoney k keutzerrethinking', 1);('batch normalization transformers', 1);('j xu x sun z zhang g zhao j lin understanding', 1);('hochreiter j schmidhuber', 1);('long shortterm memoryneural computation', 1);('j chung', 1);('gulcehre k cho bengio empirical', 1);('recurrent neural networks sequence modelingarxiv preprint arxiv14123555', 1);('espeholt h soyer r munos k simonyan v mnih wardy doron v firoiu harley dunning', 1);('deeprl importance', 1);('z dai z yang yang j carbonell q v', 1);('r salakhutdinov transformerxl attentive', 1);('xedlength context arxiv preprint arxiv190102860', 1);('z yang z dai yang j carbonell r r salakhutdinovand q v', 1);('xlnet generalized', 1);('forlanguage understanding', 1);('j russell articial', 1);('modern approach', 1);('pearsoneducation inc', 1);('boutilier r reiter', 1);('price symbolic', 1);('rstorder mdps', 1);('ijcai', 1);('q nguyen j salazar transformers', 1);('normalization selfattention arxiv preprintarxiv191005895', 1);('r xiong yang k zheng zheng', 1);('xing h zhangy lan', 1);('wang liu', 1);('layer normalization thetransformer architecture', 1);('r k srivastava k greff j schmidhuber', 1);('highway networks arxiv preprint arxiv150500387', 1);('van', 1);('oord n kalchbrenner', 1);('espeholt vinyalsa graves', 1);('conditional', 1);('image generation pixelcnndecoders', 1);('beattie j z leibo teplyashin ward wainwrighth kttler lefrancq', 1);('v valds sadik', 1);('lab arxiv preprint arxiv161203801', 1);('abdolmaleki j springenberg clark h soyerj', 1);('rae noury ahuja liu tirumala', 1);('vmpoonpolicy', 1);('maximum posteriori policy optimization discreteand', 1);('continuous control arxiv preprint arxiv190912238', 1);('schlag k irie j schmidhuber linear', 1);('fast weight programmers', 1);('j schmidhuber learning', 1);('control fastweight memories', 1);('analternative', 1);('dynamic recurrent networks', 1);('neural computation', 1);('g bellemare g ostrovski guez p thomas', 1);('munos', 1);('action gap', 1);('new operators reinforcement learning', 1);('schwarzschild gupta ghiasi goldblum', 1);('goldstein', 1);('uncanny similarity recurrence deptharxiv preprint arxiv210211011', 1);('j mitrovic', 1);('mcwilliams j walker', 1);('buesing', 1);('blundell representation', 1);('invariant causal mechanismsarxiv preprint arxiv201007922', 1);('tassa doron muldal erez li', 1);('casas budden abdolmaleki j merel lefrancq', 1);('deepmindcontrol', 1);('suite arxiv preprint arxiv180100690', 1);('schwarzer anand r goel r hjelm courvilleand p bachman dataefcient', 1);('reinforcement learning selfpredictive representations arxiv preprint arxiv200705929', 1);('ye liu kurutach p abbeel gao mastering', 1);('hafner lillicrap j ba norouzi dream', 1);('behaviors latent imagination arxiv preprintarxiv191201603', 1);('hafner lillicrap norouzi j ba mastering', 1);('atariwith discrete world models arxiv preprint arxiv201002193', 1);('hessel j modayil h van hasselt schaul g ostrovskiw dabney horgan', 1);('piot azar silver rainbowcombining', 1);('reinforcement learning inaaai', 1);('yarats kostrikov r fergus image', 1);('augmentation isall need', 1);('regularizing', 1);('deep reinforcement learning frompixels', 1);('kaiser babaeizadeh p milos', 1);('osinski r h campbellk czechowski erhan', 1);('finn p kozakowski levine', 1);('reinforcement learning atari arxiv preprintarxiv190300374', 1);('r coulom efcient', 1);('selectivity backup operators montecarlo tree search', 1);('international conference computers andgames', 1);('r reddy speech', 1);('understanding systems summary results ofthe veyear research effort carnegiemellon university', 1);('k h fan wu xie r girshick momentum', 1);('visual representation learning', 1);('carroll j lin', 1);('r georgescu sun bignells milani k hofmann hausknecht dragan', 1);('towards', 1);('exible inference sequential decision problems viabidirectional transformers arxiv preprint arxiv220413326', 1);('kumar x', 1);('peng levine rewardconditioned', 1);('policies arxiv preprint arxiv191213465', 1);('nair dalal gupta levine accelerating', 1);('online reinforcement learning ofine datasets arxiv preprintarxiv200609359', 1);('levine reinforcement', 1);('learning control probabilisticinference', 1);('review arxiv preprint arxiv180500909', 1);('j fu kumar nachum g tucker levine d4rldatasets', 1);('deep datadriven reinforcement learning arxivpreprint arxiv200407219', 1);('li gao j yang h xu wu phasic', 1);('selfimitative reduction sparsereward', 1);('r g morris spatial', 1);('presenceof local cues', 1);('fujimoto meger precup offpolicy', 1);('deep reinforcement learning', 1);('r qin gao x zhang z xu huang z li', 1);('zhangand yu neorl', 1);('realworld benchmark ofinereinforcement learning arxiv preprint arxiv210200714', 1);('z lan chen goodman k gimpel p sharma r soricut albert', 1);('lite bert', 1);('learning languagerepresentations arxiv preprint arxiv190911942', 1);('z liu z fan wang p yu augmenting', 1);('sequential recommendation pseudoprior items', 1);('acm sigirconference', 1);('research development information retrieval', 1);('kumar zhou g tucker levine conservative', 1);('k cobbe', 1);('hesse j hilton j schulman leveragingprocedural', 1);('generation benchmark reinforcement learning inicml', 1);('yu quillen z r julian k hausman', 1);('finn', 1);('levine metaworld', 1);('benchmark evaluation multitask meta reinforcement learning', 1);('e todorov erez tassa mujoco', 1);('physics engine', 1);('ieeersj', 1);('international conference onintelligent robots systems', 1);('yt hui chevalierboisvert bahdanau bengiobabyai', 1);('arxiv preprint arxiv200712770', 1);('x puig k ra boben j li wang fidler torralbavirtualhome simulating', 1);('household activities', 1);('programs incvpr', 1);('wu g tucker nachum behavior', 1);('ofinereinforcement learning arxiv preprint arxiv191111361', 1);('g cuccu j togelius p cudrmauroux playing', 1);('atari withsix neurons arxiv preprint arxiv180601363', 1);('bastani pu solarlezama', 1);('policy extraction', 1);('g bellemare naddaf j veness bowling thearcade', 1);('learning environment evaluation platform generalagents journal', 1);('articial intelligence', 1);('v mnih k kavukcuoglu silver graves antonogloud wierstra riedmiller playing', 1);('reinforcement learning arxiv preprint arxiv13125602', 1);('e parisotto j', 1);('ba r salakhutdinov actormimic deepmultitask', 1);('reinforcement learning arxiv preprintarxiv151106342', 1);('rusu g colmenarejo', 1);('gulcehre g desjardins j kirkpatrick r pascanu v mnih k kavukcuoglu r hadsellpolicy', 1);('distillation arxiv preprint arxiv151106295', 1);('oord li vinyals representation', 1);('learning withcontrastive predictive', 1);('arxiv preprint arxiv180703748', 1);('pomerleau efcient', 1);('training articial neural networksfor autonomous navigation', 1);('neural', 1);('krause gotmare', 1);('mccann n keskar jotyr socher n', 1);('rajani gedi generative', 1);('sequence generation arxiv preprint arxiv200906367', 1);('ouyang j wu x jiang almeida', 1);('c l', 1);('wainwrightp mishkin', 1);('zhang agarwal k slama ray', 1);('training', 1);('instructions humanfeedback arxiv preprint arxiv220302155', 1);('r shachter probabilistic', 1);('inference inuence diagramsoperations research', 1);('toussaint robot', 1);('trajectory optimization', 1);('r agarwal schuurmans norouzi', 1);('optimisticperspective ofine reinforcement learning', 1);('v sanh webson', 1);('raffel h bach', 1);('sutawikaz alyafeai chafn stiegler', 1);('scao raja', 1);('multitask', 1);('zeroshot task generalizationarxiv preprint arxiv211008207', 1);('j wei bosma v zhao k guu', 1);('lester n dua dai q v', 1);('finetuned', 1);('language models zeroshot learners arxiv preprint arxiv210901652', 1);('fedus', 1);('zoph n shazeer switch', 1);('scalingto', 1);('trillion parameter models', 1);('simple efcient sparsityarxiv preprint arxiv210103961', 1);('pinto p abbeel generalized', 1);('hindsight reinforcement learning', 1);('eysenbach x geng levine r r salakhutdinovrewriting', 1);('history inverse rl', 1);('inference policyimprovement', 1);('ding', 1);('florensa p abbeel phielipp goalconditioned', 1);('imitation learning', 1);('n rhinehart r mcallister k kitani levine precogprediction', 1);('goals visual multiagent settingsiniccv', 1);('ha j schmidhuber', 1);('world models arxiv preprintarxiv180310122', 1);('p christiano z shah mordatch j schneider blackwellj tobin p abbeel', 1);('zaremba transfer', 1);('real world learning', 1);('inverse dynamics modelarxiv preprint arxiv161003518', 1);('r shah krasheninnikov j alexander p abbeel dragan preferences', 1);('implicit state world arxiv preprintarxiv190204198', 1);('chevalierboisvert', 1);('willems pal minimalisticgridworld', 1);('environment openai gym', 1);('finn p abbeel levine modelagnostic', 1);('fast adaptation', 1);('p liu', 1);('yuan j fu z jiang h hayashi g neubigpretrain', 1);('systematic survey', 1);('natural language processing arxiv preprintarxiv210713586', 1);('shridhar j thomason gordon bisk', 1);('han r mottaghi', 1);('zettlemoyer fox alfred', 1);('everyday tasks', 1);('j pearl causality cambridge', 1);('university press', 1);('trabucco phielipp g berseth anymorph', 1);('learningtransferable polices', 1);('agent morphology', 1);('tang ha', 1);('sensory neuron transformerpermutationinvariant neural networks reinforcement learning', 1);('x wang r girshick gupta k nonlocal', 1);('k x zhang ren j sun deep', 1);('residual learning forimage recognition', 1);('salimans karpathy x chen p kingma pixelcnn improving', 1);('logisticmixture likelihood modications arxiv preprintarxiv170105517', 1);('z mandi', 1);('liu k lee p abbeel towards', 1);('generalizable oneshot visual imitation learning', 1);('icra ieee', 1);('finn xie dasari zhang p abbeel', 1);('levine oneshot', 1);('humans viadomainadaptive', 1);('arxiv preprint arxiv180201557', 1);('zhu j wong mandlekar r martnmartn', 1);('robosuitea modular simulation framework benchmark', 1);('arxiv preprint arxiv200912293', 1);('figueredo haddadin kapoor', 1);('bonatti reshaping', 1);('robot trajectories', 1);('natural languagecommands study multimodal data alignment', 1);('transformers arxiv preprint arxiv220313411', 1);('radford j', 1);('kim', 1);('hallacy ramesh g goh agarwal g sastry askell p mishkin j clark', 1);('learningtransferable visual models', 1);('natural language supervisioninicml', 1);('p sharma', 1);('sundaralingam v blukis', 1);('paxton hermansa torralba j andreas fox correcting', 1);('robot plans withnatural language feedback arxiv preprint arxiv220405186', 1);('manuelli fox cliport', 1);('wherepathways robotic manipulation', 1);('e jang irpan khansari kappler', 1);('ebert', 1);('lynchs levine', 1);('finn bcz zeroshot', 1);('task generalization withrobotic imitation learning', 1);('james z r arrojo j davison rlbench therobot', 1);('learning benchmark learning environment', 1);('jaegle borgeaud jb alayrac', 1);('doersch', 1);('ionescud ding koppula zoran brock e shelhamer', 1);('perceiver', 1);('io general architecture', 1);('inputs outputs arxiv preprint arxiv210714795', 1);('han r batra n boyd zhao hutchinson', 1);('zhao learning', 1);('generalizable visiontactile robotic graspingstrategy', 1);('deformable objects', 1);('transformer arxiv preprintarxiv211206374', 1);('r jangir n hansen ghosal jain x wang lookcloser bridging', 1);('egocentric thirdperson', 1);('transformers robotic manipulation', 1);('r yang zhang n hansen h xu x wang learningvisionguided', 1);('quadrupedal locomotion endtoend crossmodal transformers arxiv preprint arxiv210703996', 1);('h kim ohmura kuniyoshi transformerbased', 1);('deepimitation learning dualarm robot manipulation', 1);('ct kdr x yuan', 1);('kybartas barnes e finej moore hausknecht', 1);('e asri adada', 1);('textworlda', 1);('learning environment', 1);('games workshop oncomputer', 1);('j urbanek fan karamcheti jain humeau e dinant rocktschel kiela szlam j weston learning', 1);('tospeak act fantasy text adventure game arxiv preprintarxiv190303094', 1);('hausknecht p ammanabrolu ct x yuaninteractive', 1);('ction games colossal adventure', 1);('x yuan ct j fu z lin', 1);('pal bengio', 1);('trischler interactive', 1);('language learning question', 1);('arxiv preprint arxiv190810909', 1);('p ammanabrolu hausknecht graph', 1);('natural language action spaces arxivpreprint arxiv200108837', 1);('p ammanabrolu riedl playing', 1);('textadventure gameswith', 1);('reinforcement learning arxiv preprintarxiv181201628', 1);('p rajpurkar j zhang k lopyrev p liang', 1);('100000questions machine comprehension text arxiv preprintarxiv160605250', 1);('yu dohan mt luong r zhao k chen norouziand q v', 1);('qanet combining', 1);('local convolution withglobal selfattention reading comprehension arxiv preprintarxiv180409541', 1);('pnueli', 1);('temporal logic programs', 1);('annualsymposium foundations computer', 1);('science ieee', 1);('diuk cohen', 1);('littman', 1);('representation efcient reinforcement learning', 1);('trouillon j welbl riedel gaussier g bouchardcomplex', 1);('simple link prediction', 1);('schlichtkrull n kipf p bloem r', 1);('berg titov', 1);('welling modeling', 1);('relational data graph convolutionalnetworks', 1);('eswc springer', 1);('seo kembhavi farhadi h hajishirzi bidirectional', 1);('attention ow machine comprehension arxiv preprintarxiv161101603', 1);('p anderson q wu teney j bruce johnson n snderhauf reid gould van den hengel visionandlanguage', 1);('interpreting', 1);('h chen suhr misra n snavely artzi touchdown', 1);('natural language navigation spatial reasoning invisual street environments', 1);('j thomason murray cakmak', 1);('zettlemoyervisionanddialog', 1);('k nguyen dey', 1);('brockett', 1);('dolan visionbasednavigation', 1);('imitation learningwith', 1);('indirect intervention', 1);('k nguyen h daum iii', 1);('anna visual navigationwith', 1);('natural multimodal assistance', 1);('imitation learning arxiv preprint arxiv190901871', 1);('qi q wu p anderson x wang', 1);('shen', 1);('anda v', 1);('hengel reverie remote', 1);('visual referringexpression', 1);('real indoor environments', 1);('misra j langford artzi mapping', 1);('instructions andvisual observations actions reinforcement learning arxivpreprint arxiv170408795', 1);('x wang', 1);('xiong h wang', 1);('beforeyou leap', 1);('bridging', 1);('plannedahead visionandlanguage navigation ineccv', 1);('li q xia bisk celikyilmaz j gao n smithand choi robust', 1);('navigation language', 1);('arxiv preprint arxiv190902244', 1);('j lu batra parikh lee vilbert pretraining', 1);('taskagnostic visiolinguistic representations visionandlanguagetasks', 1);('yc chen', 1);('yu el kholy', 1);('ahmed z gan chengand j liu uniter universal', 1);('imagetext representation learning', 1);('g li n duan fang gong jiang unicodervl auniversal', 1);('encoder vision language crossmodal', 1);('su x zhu cao', 1);('lu', 1);('wei j dai vlbertpretraining', 1);('generic visuallinguistic representations arxivpreprint arxiv190808530', 1);('hong q wu qi', 1);('rodriguezopazo gould vlnbert', 1);('recurrent visionandlanguage bert navigation incvpr', 1);('h tan bansal lxmert learning', 1);('crossmodalityencoder representations transformers arxiv preprintarxiv190807490', 1);('k fang toshev', 1);('feifei savarese scene', 1);('memory transformer', 1);('agents longhorizon tasks incvpr', 1);('arnab dehghani g heigold', 1);('sun lu', 1);('ci c andc', 1);('schmid vivit', 1);('video vision transformer', 1);('pashevich', 1);('schmid', 1);('sun episodic', 1);('transformer forvisionandlanguage navigation', 1);('chen z alhalah k grauman semantic', 1);('chen u jain', 1);('schissler v gari z alhalah v kithapu p robinson k grauman soundspaces audiovisual', 1);('navigation 3d environments', 1);('dosovitskiy g ros', 1);('codevilla lopez v koltuncarla', 1);('open urban driving simulator', 1);('corl pmlr2017', 1);('codevilla e santana lpez gaidon exploringthe', 1);('limitations behavior', 1);('zhao liang h huang g van', 1);('broeck', 1);('soatto sam squeezeandmimic', 1);('networks conditionalvisual', 1);('policy learning', 1);('chen p wu h li j yan tao stp3 endtoendvisionbased', 1);('k chitta prakash geiger neat neural', 1);('attentionelds endtoend autonomous', 1);('z zhang liniger dai', 1);('van gool endtoendurban', 1);('reinforcement learning coach iniccv', 1);('toromanoff e wirbel', 1);('moutarde endtoend', 1);('modelfree reinforcement learning', 1);('urban driving', 1);('radulescu niv ballard holistic', 1);('role structure attention', 1);('trends', 1);('j macqueen classication', 1);('analysis multivariate observations 5th', 1);('berkeley symp math statist probability', 1);('ty lin p goyal r girshick k p dollr focal', 1);('dense object detection', 1);('r girshick fast', 1);('tang r r salakhutdinov multiple', 1);('futures predictionneurips', 1);('kostrikov nair levine ofine', 1);('arxiv preprint arxiv211006169', 1);('j ngiam', 1);('caine v vasudevan z zhang ht', 1);('chiangj ling r roelofs bewley', 1);('liu venugopal', 1);('scenetransformer', 1);('multiple agenttrajectories arxiv preprint arxiv210608417', 1);('liu j zhang', 1);('fang q jiang', 1);('zhou multimodalmotion', 1);('quintanar fernndezllorca parra r izquierdo', 1);('sotelo predicting', 1);('vehicles trajectories', 1);('urban scenarioswith transformer networks', 1);('information 2021ieee', 1);('intelligent vehicles symposium iv ieee', 1);('yuan x weng ou k kitani agentformer agentaware', 1);('transformers sociotemporal multiagent forecastinginiccv', 1);('l l', 1);('yang liang', 1);('zeng ren segal', 1);('urtasun endtoend', 1);('contextual perception predictionwith interaction transformer', 1);('g lee j seo bhat kang j francis jadhavp p liang lp morency diverse', 1);('admissible trajectory', 1);('multimodal context understanding ineccv', 1);('z hou', 1);('liu r zhao z ou liu x chen zhengimperfect', 1);('multilevel', 1);('sequential reward', 1);('dialog management arxiv preprintarxiv210404748', 1);('teixeira v maran dragoni', 1);('interplay ofa conversational ontology ai planning health dialoguemanagement', 1);('acm', 1);('z wei q liu', 1);('peng h tou chen xj huang kfwong x dai taskoriented', 1);('dialogue system automaticdiagnosis', 1);('acl', 1);('xu q zhou k gong x liang j tang', 1);('lin endtoend', 1);('relational dialogue system automaticdiagnosis', 1);('k liao q liu z wei', 1);('peng q chen', 1);('huang taskoriented', 1);('dialogue system', 1);('automatic diseasediagnosis', 1);('hierarchical reinforcement learning arxiv preprintarxiv200414254', 1);('zheng k ristovski farahat', 1);('gupta', 1);('long shortterm memory network', 1);('useful life estimation inicphm', 1);('q wang zheng farahat serita', 1);('gupta remaining', 1);('useful life estimation', 1);('functional data analysis inicphm', 1);('saxena k goebel turbofan', 1);('engine degradation simulation data', 1);('nasa ames prognostics data repository', 1);('yang j wang', 1);('overview multiagent reinforcement learning game theoretical perspective arxiv preprintarxiv201100583', 1);('meng wen yang', 1);('zhang wenh zhang j wang', 1);('xu ofine', 1);('multiagentdecision transformer', 1);('big sequence model conquers starcraftii tasks arxiv preprint arxiv211202845', 1);('yu velu e vinitsky wang bayen wuthe', 1);('surprising effectiveness ppo', 1);('cooperative multiagentgames arxiv preprint arxiv210301955', 1);('samvelyan rashid', 1);('de witt g farquhar n nardellit g rudner cm hung p h torr j foerster', 1);('whiteson', 1);('starcraft multiagent challenge arxiv preprintarxiv190204043', 1);('r sanjaya j wang yang measuring', 1);('nontransitivityin chess', 1);('algorithms', 1);('wen j g kuba r lin', 1);('zhang wen j wang', 1);('yang multiagent', 1);('reinforcement learning sequence', 1);('problem arxiv preprint arxiv220514953', 1);('j g kuba r chen wen wen', 1);('sun j wang yangtrust', 1);('region policy optimisation multiagent', 1);('arxiv preprint arxiv210911251', 1);('j g kuba wen', 1);('meng h zhang mguni j wangy yang', 1);('settling', 1);('variance multiagent policy gradients', 1);('witt', 1);('peng p kamienny p torr', 1);('bhmerand whiteson deep', 1);('continuous cooperative control arxiv preprintarxiv200306709', 1);('k kurach raichuk p sta', 1);('zaj', 1);('bachem', 1);('espeholt', 1);('riquelme vincent michalski bousquetet', 1);('research football novel reinforcement learningenvironment', 1);('proceedings aaai', 1);('hausknecht p stone deep', 1);('observable mdps', 1);('aaai fall symposium series', 1);('omidshaei j pazis', 1);('amato j p j vian deepdecentralized', 1);('multitask multiagent reinforcement learning partial observability', 1);('yang g chen', 1);('wang x hao j hao p hengtransformerbased', 1);('memory multiagent reinforcement learning action', 1);('x nian irissappane roijers dcrac deep', 1);('recurrent actorcritic multiobjective', 1);('observable environments', 1);('internationalconference autonomous agents multiagent systems', 1);('rashid samvelyan', 1);('schroeder g farquhar j foersterand whiteson qmix monotonic', 1);('value function factorisationfor', 1);('icml pmlr2018', 1);('j wang z ren liu yu', 1);('zhang qplex duplexdueling', 1);('arxiv preprint arxiv200801062', 1);('zhu x chang x liang updet universal', 1);('transformers arxiv preprint arxiv210108001', 1);('p sunehag g lever gruslys', 1);('czarnecki v zambaldim jaderberg lanctot n sonnerat j z leibo k tuylset', 1);('valuedecomposition', 1);('cooperative multiagent learning arxiv preprint arxiv170605296', 1);('j k e hostallero k', 1);('kim qtranlearning', 1);('factorize transformation', 1);('cooperative multiagent reinforcement learning', 1);