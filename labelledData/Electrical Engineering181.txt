('eeg', 43);('ecg', 32);('transformer', 23);('figure', 18);('emotion recognition', 14);('fcn', 14);('single-modality models', 12);('amigos', 12);('physiological signals', 11);('version', 7);('international conference', 7);('bert', 6);('fused model', 6);('icassp', 6);('multimodal emotion recognition', 5);('transformer-based', 5);('transformers', 5);('affective computing', 5);('input signal', 5);('output layer', 5);('computational linguistics', 5);('emotional state', 4);('processing', 4);('multimodal physiological signals', 4);('single- modality models', 4);('cls', 4);('f7', 4);('f3', 4);('relu', 4);('f1', 4);('pre-training', 4);('ieee access', 4);('oct.', 4);('proceedings', 4);('june', 4);('ieee', 4);('grenoble alpes', 3);('emotion prediction', 3);('nlp', 3);('trans-', 3);('] use', 3);('v ae', 3);('emotion recognizer', 3);('emotion', 3);('learning rate', 3);('adam', 3);('dropout value', 3);('fused', 3);('nov.', 3);('july', 3);('ieee transactions', 3);('speech', 3);('acm', 3);('grenoble', 2);('multiple physiological signals', 2);('such models', 2);('wearable devices', 2);('such signals', 2);('late-fusion approach', 2);('computer vision', 2);('bert-like', 2);('rec- ognize emotions', 2);('multimodal emo- tion recognition', 2);('emo- tion', 2);('second-level model', 2);('different datasets', 2);('different modalities', 2);('single-modality model', 2);('rst step', 2);('sig- nals', 2);('fully-connected network', 2);('single-modality emotion recognition model', 2);('predicted emotion', 2);('raw signal', 2);('output value corresponds', 2);('single-modality emotion recognizers', 2);('part', 2);('va- lence', 2);('dataset [', 2);('band-pass lter', 2);('single- modality emotion recognizer', 2);('bci2000', 2);('emotion recognition model', 2);('activation function', 2);('recognition performances', 2);('acc.arousal f1valence acc.valence f1', 2);('2= 0:999andl2weight decay', 2);('ray', 2);('model predicts', 2);('single-modality mod- els', 2);('comparison', 2);('multimodal situations', 2);('new ways', 2);('symposium', 2);('louis-philippe morency', 2);('annual meeting', 2);('self-supervised learning', 2);('jan.', 2);('mar', 2);('video', 2);('acoustics', 2);('may', 2);('scientic data', 2);('emotion recognition with pre-trained transformers using multimodal signals author version juan vazquez-rodriguez1', 1);('grgoire lefebvre1', 1);('julien cumin1', 1);('james l. crowley2', 1);('labs', 1);('france', 1);('cnrs', 1);('grenoble inp', 1);('lig', 1);('france abstract', 1);('demon- strate', 1);('multimodal scenario', 1);('emotion recogni- tion performances', 1);('multi- modal inputs', 1);('state-of- the-art dataset', 1);('keywords affective computing', 1);('machine learning', 1);('introduction', 1);('new possibilities', 1);('emotional health', 1);('users [', 1);('medical-grade sensors', 1);('wearable sensors', 1);('transformer-', 1);('different physiological signals', 1);('ex- plore', 1);('individual sen- sor modalities', 1);('complementary information', 1);('further-', 1);('reliable estimates', 1);('address multimodal emotion recognition [', 1);('majority use signals', 1);('al-', 1);('emotion recognition [', 1);('common problem', 1);('train deep- learning models [', 1);('possible approach', 1);('prob- lem', 1);('techniques [', 1);('multiple signal modalities raises', 1);('ne-tune dif- ferent single-modality models', 1);('individual models', 1);('process physiological signals', 1);('lan-', 1);('processing sequences', 1);('se- quences', 1);('logical signal processing [', 1);('attention mechanism', 1);('different parts', 1);('attention-', 1);('processing physiological signals', 1);('informa- tion', 1);('an-', 1);('bene- t', 1);('speech processing', 1);('main contributions', 1);('emotion recognition performances.arxiv:2212.13885v1 [ eess.sp ]', 1);('dec', 1);('related', 1);('contrary', 1);('traditional techniques', 1);('gaussian', 1);('bayes', 1);('neighbours', 1);('support vector machines', 1);('sensor signals', 1);('descrip- tors', 1);('santamaria', 1);('employ models', 1);('convolutional neural networks', 1);('cnn', 1);('harper', 1);('southern [', 1);('recurrent neu-', 1);('networks', 1);('rnn', 1);('cnns', 1);('powerful alternative', 1);('convolutional', 1);('recurrent net-', 1);('effective tool', 1);('emo- tions', 1);('multimodal signals', 1);('audio and/or text', 1);('inputs [', 1);('text [', 1);('multiple physiological signal modal- ities [', 1);('multiple signal modalities', 1);('perfor- mance', 1);('multimodal approaches employ', 1);('recog- nize emotions', 1);('rahman', 1);('] report', 1);('process text', 1);('audio modalities', 1);('middle-fusion process', 1);('siriwardhana', 1);('au- dio', 1);('text modalities', 1);('different features', 1);('khare', 1);('visual parts', 1);('sarkar', 1);('etemad', 1);('] pre-train', 1);('cnn-based', 1);('different transformations', 1);('vazquez- rodriguez', 1);('ap- proaches', 1);('only employ', 1);('physiological modality', 1);('ross', 1);('liu', 1);('variational autoencoders', 1);('extract representations', 1);('physiolog-ical signal', 1);('second-level classier', 1);('yang', 1);('lee', 1);('different signals', 1);('input level', 1);('dif- ferent approaches', 1);('multimodal pre- training approaches', 1);('physiologi- cal signals', 1);('mul- timodal', 1);('dont use', 1);('multimodal physiological scenario', 1);('approach', 1);('classication task', 1);('multiple modali- ties', 1);('important problem', 1);('processing level', 1);('different signal modalities', 1);('op- tion', 1);('compromise be- tween', 1);('fea- tures', 1);('intermediate layers', 1);('early-fusion', 1);('temporal dimension', 1);('computa- tional complexity', 1);('powerful hardware', 1);('expensive multimodal model', 1);('late- fusion approach', 1);('powerful hardware resources', 1);('similar reasoning', 1);('major difculty', 1);('general represen- tation', 1);('how-', 1);('early-fusion limits', 1);('avail- ability', 1);('multimodal approach', 1);('per- 2author', 1);('late fusion approach', 1);('individual sensor modalities', 1);('input sig- nal', 1);('masked value prediction', 1);('recognition training', 1);('electroencephalo- gram', 1);('fusion approaches', 1);('con- catenation', 1);('effective technique', 1);('ecg single-modality emotion recognition', 1);('approach de-', 1);('brief description', 1);('original paper', 1);('infor- mation thanks', 1);('self-attention mechanisms', 1);('model rst encodes', 1);('con-', 1);('neural network', 1);('feature sequence', 1);('pre-training phase', 1);('original signal', 1);('fine-tuning phase', 1);('path b', 1);('vec- torcls e', 1);('parame- ters', 1);('1d-cnn parameters', 1);('eeg single-modality emotion recognition', 1);('pre- vious subsection', 1);('multi- channel', 1);('one-channel input', 1);('1d-cnn encoder', 1);('secondly', 1);('temporal segments', 1);('dif- ferent channels', 1);('masked-point predictor', 1);('input channels', 1);('aside', 1);('fused-signals emotion recognition model', 1);('aver- age', 1);('single-modality recognizers', 1);('employ concatenation', 1);('different sizes', 1);('conve- nient', 1);('output sizes', 1);('multimodal emotion classier', 1);('version raw physiological', 1);('values', 1);('cnnfeature sequence clstransformersignal representations cls', 1);('masked values predictorpredicted masked values', 1);('pre-training fcn emotion classierpredicted emotion', 1);('fine-tuning figure', 1);('single-modality emotion recognizer', 1);('eeg signalecg emotion recognizereeg emotion recognizerlast', 1);('hidden- layer outputsconcatenationmulitmodal', 1);('emotion classier', 1);('predicted emotion figure', 1);('late-fusion', 1);('fcns', 1);('fig-', 1);('experiments', 1);('binary emotion predic- tion', 1);('low levels', 1);('experimental setup', 1);('de- tails', 1);('datasets', 1);('emo- tional videos', 1);('self-assessment form', 1);('binary emotion classication', 1);('average value', 1);('low classes', 1);('single-channel signal', 1);('left arm', 1);('t7', 1);('p7', 1);('o1', 1);('o2', 1);('p8', 1);('t8', 1);('f4', 1);('f8', 1);('de- 4author', 1);('low-pass lter', 1);('cut-off frequency', 1);('data', 1);('way-eeg-gal', 1);('large-eeg-bci', 1);('de- velop', 1);('brain-computer interfaces', 1);('sam- ples', 1);('cross- validation', 1);('4.1.2 signal', 1);('butterworth', 1);('cut-off frequencies', 1);('downsample signals', 1);('normalize signals', 1);('unit- variance', 1);('10-second segments', 1);('ecg emotion recognizer', 1);('vazquez-rodriguez', 1);('sec-', 1);('eeg emotion recognizer', 1);('kernel sizes', 1);('chan- nels', 1);('congura- tion', 1);('receptive eld', 1);('input points', 1);('based', 1);('preliminary studies', 1);('receptive eld size', 1);('outputs values', 1);('pre-', 1);('linear decay', 1);('predicts binary emotions', 1);('activation functions', 1);('differ- ent networks', 1);('binary cross-entropy loss', 1);('2= 0:999and', 1);('l2weight', 1);('predicts emotions', 1);('tune toolkit [', 1);('validation data', 1);('multimodal emotion recognizer', 1);('multimodal emotion recognition model', 1);('archi- tecture', 1);('binary emotion', 1);('opti- mization', 1);('tune toolkit', 1);('different experi- ments', 1);('emotion recog- nition', 1);('% condence interval', 1);('rst report', 1);('pre- 5author', 1);('arousal acc.arousal f1valence acc.valence f1 ecg', 1);('] 0.88\x065:4e\x0030.87\x065:4e\x0030.83\x067:8e\x0030.83\x067:4e\x003', 1);('0.89\x065:0e\x0030.89\x065:0e\x0030.85\x063:8e\x0030.85\x063:9e\x003 training strategy', 1);('section 3.2.1', 1);('strategy im- proves emotion recognition performances', 1);('represen- tations', 1);('fusion strategy', 1);('late-fusion approach improves performance', 1);('valence accuracy', 1);('eec', 1);('figures', 1);('different samples', 1);('ex- periments', 1);('show channels', 1);('wrong class', 1);('correct class', 1);('wrong prediction', 1);('infor- mation', 1);('previous example', 1);('fusion model', 1);('right modal- ity', 1);('incorrect predictions', 1);('looking', 1);('shows chan- nels', 1);('pre-training pre-', 1);('effectiveness', 1);('superior perfor- mance', 1);('score improves', 1);('\x065:1e\x003to 0.89\x065:0e\x003', 1);('late-fusion strategy', 1);('re- sults', 1);('experimen- tal protocol', 1);('present 6author', 1);('amigos dataset baseline arousal f1 valence f1 eeg', 1);('eeg+ecg+grs', 1);('eeg+ecg', 1);('showcase results', 1);('acceptable performance', 1);('baseline uses', 1);('galvanic skin', 1);('gsr', 1);('conclusion', 1);('perspectives', 1);('architec- ture', 1);('signicant accuracy', 1);('late-fusion multimodal approach improves per- formances', 1);('overall', 1);('state-of-the-art performance', 1);('emotion recognition.future', 1);('pre- training', 1);('indi- vidual modalities', 1);('incor- porate information', 1);('dif- ferent modalities', 1);('valuable avenues', 1);('emotion recognition perfor- mances', 1);('phys- iological signals', 1);('traditional modalities', 1);('im- ages', 1);('models behave', 1);('acknowledgements', 1);('miai multidisciplinary ai', 1);('univ', 1);('miai', 1);('anr-19-p3ia-0003', 1);('references', 1);('f. abdat', 1);('c. maaoui', 1);('a. pruski', 1);('human-computer interaction', 1);('facial ex-', 1);('uksim', 1);('computer modeling', 1);('simulation', 1);('wasifur rahman', 1);('md kamrul hasan', 1);('sangwu lee', 1);('ami-', 1);('bagher zadeh', 1);('chengfeng mao', 1);('ehsan hoque', 1);('integrating multimodal information', 1);('large pretrained transformers', 1);('pro-', 1);('online', 1);('s. siriwardhana', 1);('t. kaluarachchi', 1);('m. billinghurst', 1);('s. nanayakkara', 1);('transformer-based self supervised feature fu-', 1);('shamane siriwardhana', 1);('andrew reis', 1);('rivindu weerasekera', 1);('suranga nanayakkara', 1);('jointly fine-tuning', 1);('self supervised', 1);('improve multimodal speech emotion recognition', 1);('arxiv:2008.06682 [ cs', 1);('eess ]', 1);('aug.', 1);('aparna khare', 1);('srinivas parthasarathy', 1);('shiva sun-', 1);('cross-modal transformers', 1);('ieee spoken language', 1);('technology workshop', 1);('slt', 1);('a. miranda correa', 1);('m. k. abadi', 1);('n. sebe', 1);('i. pa-', 1);('dataset', 1);('affect', 1);('personality', 1);('mood', 1);('individuals', 1);('groups', 1);('s. siddharth', 1);('t. jung', 1);('t. j. sejnowski', 1);('utilizing deep learning towards multi-modal bio-sensing', 1);('vision-based affective computing', 1);('kyle ross', 1);('paul hungler', 1);('ali etemad', 1);('unsuper-', 1);('multi-modal representation learning', 1);('wearable data', 1);('ambient intelligence', 1);('humanized computing', 1);('dumitru erhan', 1);('aaron courville', 1);('yoshua bengio', 1);('pascal vincent', 1);('does unsupervised pre-training', 1);('deep learning', 1);('thirteenth', 1);('articial intelligence', 1);('statistics', 1);('jmlr', 1);('ashish vaswani', 1);('noam shazeer', 1);('niki parmar', 1);('jakob uszkoreit', 1);('llion jones', 1);('aidan n. gomez', 1);('kaiser', 1);('illia polosukhin', 1);('attention', 1);('need', 1);('ad-', 1);('neural information processing systems', 1);('genshen yan', 1);('shen liang', 1);('yanchun zhang', 1);('fan liu', 1);('fusing transformer model', 1);('temporal features', 1);('ecg heartbeat classication', 1);('ieee interna-', 1);('tional conference', 1);('bioinformatics', 1);('biomedicine', 1);('bibm', 1);('jacob devlin', 1);('ming-wei chang', 1);('kenton lee', 1);('kristina toutanova', 1);('deep bidi-', 1);('language understanding', 1);('american chapter', 1);('language technologies', 1);('short papers', 1);('minneapolis', 1);('minnesota', 1);('chen sun', 1);('austin myers', 1);('carl v', 1);('kevin murphy', 1);('cordelia schmid', 1);('videobert', 1);('joint model', 1);('language representation learning', 1);('ieee/cvf', 1);('iccv', 1);('wen-chin huang', 1);('chia-hua wu', 1);('shang-bao luo', 1);('kuan- yu chen', 1);('hsin-min wang', 1);('tomoki toda', 1);('speech recognition', 1);('simply fine-tuning bert', 1);('acous-', 1);('juan vazquez-rodriguez', 1);('grgoire lefebvre', 1);('julien cumin', 1);('james l. crowley', 1);('transformer-based self-supervised learning', 1);('arxiv:2204.05103 [ cs', 1);('q-bio ]', 1);('apr', 1);('martin gjoreski', 1);('blagoj mitrevski', 1);('mitja lutrek', 1);('matja gams', 1);('inter-domain', 1);('arousal recognition', 1);('informatica', 1);('lin shu', 1);('yang yu', 1);('wenzhuo chen', 1);('haoqiang hua', 1);('qin li', 1);('jianxiu jin', 1);('xiangmin xu', 1);('emo-', 1);('recognition', 1);('heart rate data', 1);('smart bracelet', 1);('sensors', 1);('l. santamaria-granados', 1);('m. munoz-organero', 1);('g. ramirez-gonzlez', 1);('e. abdulhay', 1);('n. arunk-', 1);('deep convolutional neural network', 1);('detection', 1);('physiological signals dataset', 1);('r. harper', 1);('bayesian deep learning framework', 1);('end-to-end prediction', 1);('heartbeat', 1);('shizhe chen', 1);('qin jin', 1);('multi-modal conditional at-', 1);('fusion', 1);('dimensional emotion prediction', 1);('multimedia', 1);('amsterdam', 1);('netherlands', 1);('e. ghaleb', 1);('m. popa', 1);('s. asteriadis', 1);('multimodal', 1);('temporal perception', 1);('audio-visual cues', 1);('intelligent interaction', 1);('acii', 1);('sept.', 1);('d. y', 1);('choi', 1);('d.-h. kim', 1);('b. c.', 1);('multimodal at-', 1);('network', 1);('continuous-time emotion recogni-', 1);('eeg signals', 1);('b. xing', 1);('h. zhang', 1);('k. zhang', 1);('l. zhang', 1);('x. wu', 1);('x. shi', 1);('s. yu', 1);('s. zhang', 1);('exploiting eeg signals', 1);('au-', 1);('feature fusion', 1);('video emotion recogni-', 1);('yuki matsuda', 1);('dmitrii fedotov', 1);('yuta takahashi', 1);('yu-', 1);('arakawa', 1);('keiichi yasumoto', 1);('wolfgang minker', 1);('emotour', 1);('physiological', 1);('audio-visual features', 1);('proceed-', 1);('joint', 1);('pervasive', 1);('ubiq-', 1);('computing', 1);('computers', 1);('york', 1);('ny', 1);('usa', 1);('com-', 1);('machinery', 1);('yao-hung hubert tsai', 1);('shaojie bai', 1);('paul pu liang', 1);('j. zico kolter', 1);('ruslan salakhutdinov', 1);('multimodal transformer', 1);('unaligned multimodal language sequences', 1);('computa-', 1);('linguistics', 1);('florence', 1);('italy', 1);('p. sarkar', 1);('a. etemad', 1);('ecg-based emotion recognition', 1);('wei liu', 1);('wei-long zheng', 1);('bao-liang lu', 1);('multimodal deep learning', 1);('neu-', 1);('information processing', 1);('cham', 1);('springer', 1);('h. yang', 1);('c. lee', 1);('attribute-invariant variational learning', 1);('physiology', 1);('matthew d. luciw', 1);('ewa jarocka', 1);('benoni b. edin', 1);('multi-channel eeg', 1);('gerwin schalk', 1);('dennis j. mcfarland', 1);('thilo hinter-', 1);('niels birbaumer', 1);('jonathan r. wolpaw', 1);('general-purpose brain-computer interface', 1);('bci', 1);('bio-medical engi-', 1);('a. l. goldberger', 1);('l. a. amaral', 1);('l.', 1);('j. m. haus-', 1);('p. c. ivanov', 1);('r. g. mark', 1);('j. e. mietus', 1);('g. b. moody', 1);('c. k. peng', 1);('h. e. stanley', 1);('physiobank', 1);('physiotoolkit', 1);('physionet', 1);('components', 1);('new re- search resource', 1);('complex physiologic signals', 1);('circu-', 1);('e215220', 1);('murat kaya', 1);('mustafa kemal binli', 1);('erkan ozbay', 1);('hilmi yanar', 1);('yuriy mishchenko', 1);('large electroen- cephalographic motor imagery dataset', 1);('electroen- cephalographic brain computer interfaces', 1);('dec.', 1);('richard liaw', 1);('eric liang', 1);('robert nishihara', 1);('philipp moritz', 1);('joseph e. gonzalez', 1);('ion stoica', 1);('platform', 1);('distributed model selection', 1);('training', 1);('arxiv:1807.05118 [ cs', 1);('stat ]', 1);