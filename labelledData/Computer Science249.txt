('tla', 21);('rl', 16);('additionally', 10);('reinforcement learning', 9);('arxiv', 9);('fast controller', 8);('slow controller', 7);('td3', 6);('slow policy', 6);('fast policy', 5);('response time', 4);('action repetition', 4);('biedenkapp', 4);('tlac', 4);('temporl', 4);('invertedpendulumv2', 4);('tlao', 4);('continuous control tasks', 3);('unit time', 3);('ai', 3);('slow network', 3);('slow agent', 3);('mountaincarcontinuousv0', 3);('standard deviation', 3);('average', 3);('auc', 3);('icml', 3);('aaai', 3);('layer focus', 2);('open loop control', 2);('openai', 2);('different environments', 2);('braylan', 2);('recent work', 2);('temporally layered architecture tla', 2);('algorithms training', 2);('agent needs', 2);('different response times', 2);('hansen', 2);('sharma', 2);('task horizon', 2);('atari', 2);('yu', 2);('mdps', 2);('mdp', 2);('learning', 2);('architecturein setting', 2);('fast action', 2);('fast network act', 2);('evaluation threshold', 2);('stable learning', 2);('action repetition percentage', 2);('pendulumv1', 2);('fast timestep', 2);('nips', 2);('ravindran learning', 2);('tenenbaum', 2);('learning robot control', 2);('corr', 2);('neurips', 2);('temporally layered architecture adaptive distributed continuous controldevdhar pateluniversity massachusetts amherstamherst', 1);('usadevdharpatelcsumassedujoshua russellcollege computer information sciencesuniversity massachusetts amherstamherst', 1);('usajgrussellcsumassedufrancesca walshuniversity massachusetts amherstamherst', 1);('usafnwalshumassedutauhidur rahmanuniversity california san diegola jolla ca', 1);('usatrahmanucsdeduterrance sejnowskisalk', 1);('biological studiesla jolla ca', 1);('statesterrysalkeduhava siegelmannuniversity massachusetts amherstamherst', 1);('usahavacsumasseduabstractwe', 1);('system temporallyadaptive', 1);('fast slow controller', 1);('different timescale design', 1);('draws architecture human brain executes actions differenttimescales', 1);('environments demands', 1);('control design widespreadacross biological systems increases survivability accuracy', 1);('certain uncertainenvironments', 1);('persistent exploration adaptive control', 1);('explainable temporal behavior compute efciencyand', 1);('different algorithms training', 1);('tla closedloop', 1);('exploration forthe', 1);('fast controller closedloop control', 1);('actornotat timestep b', 1);('partially', 1);('open loopcontrol', 1);('action defers', 1);('method suiteof', 1);('strong baselineskeywords', 1);('control\x01realtime\x01continuous control1 introductiondeep', 1);('remarkable success control tasks', 1);('real timegames', 1);('constant frequency', 1);('average human response time', 1);('response frequency increasesthe energy consumption processing inputs', 1);('fast', 1);('jerky behaviourwhich', 1);('damage discomfort humans', 1);('response timeincreases task horizon making difcult train agentarxiv230100723v1 csne', 1);('dec', 1);('layered architecture adaptive distributed continuous controla closed', 1);('loop control b open loop', 1);('controlfigure', 1);('closed', 1);('loop control', 1);('fast controller acts ontop', 1);('slow controller time step', 1);('fast actions threshold', 1);('repetitions slowlong actions', 1);('multiple timesteps b open loop control', 1);('top fastcontroller', 1);('controller processing inputson hand', 1);('response time forces', 1);('macro actions', 1);('incontinuoustime', 1);('poor performance', 1);('changes environment', 1);('optimizing', 1);('challenging macroactions sequences', 1);('different actions number macro actions', 1);('exponential lengthof action sequencethus', 1);('algorithms response time timestep frameskip actionrepetition', 1);('important hyperparameterwhose optimal value', 1);('complexenvironments optimal value', 1);('episodes paperwe', 1);('framework leverage phenomena', 1);('agent adapt actionfrequencywe', 1);('inspiration brain', 1);('types environments', 1);('different situations brains design', 1);('context modulate response time accuratelyrespond number', 1);('familiar unfamiliar complex environments design', 1);('energy insituations', 1);('necessary building history', 1);('speedaccuracy tradeoff', 1);('heitz', 1);('multiple independent systems process environment', 1);('multiple layers biological neural network activate control muscle groups', 1);('central nervous system tradeoff speed accuracy situationdemands', 1);('nakahira', 1);('biological design', 1);('temporally layered architecture tla fig', 1);('architecture layers', 1);('different networks', 1);('different response frequencies', 1);('adaptive behavior', 1);('exponential increase number actions network constantresponse frequency', 1);('agent use combination adapt responsefrequency', 1);('abstract hierarchical temporal knowledge intolayers focus', 1);('different timeframes', 1);('nature architecture', 1);('fog', 1);('contributions are1we', 1);('different temporal context2we introduce', 1);('different advantage', 1);('rlapproaches3we', 1);('performance fewerdecisions', 1);('action repetition environments', 1);('open loop setting alsodemonstrate', 1);('comparable performance', 1);('layered architecture adaptive distributed continuous control4we', 1);('show robust performance simulation', 1);('setting layer', 1);('viability approach', 1);('realtime setting communication orother delays2', 1);('background21 reinforcement', 1);('learningthe goal reinforcement learning', 1);('behavior environment standardreinforcement learning setting environment', 1);('markov decision process mdp mdp', 1);('saprd', 1);('actions ps\x02a\x02s 01isa transition function', 1);('rs\x02a ris', 1);('reward function d0s 01is', 1);('initial state distribution 201is discount factorthe agent', 1);('policy \x19s\x02a 01\x19sa', 1);('pratajsts', 1);('1the objective agent', 1);('optimal policy \x19\x03 parameters maximises', 1);('e1xt0', 1);('trtj\x19egtj\x19 2wherertis reward time andgtis return time policy', 1);('j\x19', 1);('actorcritic methods actor policy', 1);('policy gradient algorithmsilver', 1);('j\x19 er', 1);('\x19 sraq\x19saja\x19 s 3whereq\x19sa', 1);('egtjstsata\x19is', 1);('actionvalue function', 1);('qfunction qfunction', 1);('action aat statesand', 1);('policy \x19thereafter value', 1);('temporal difference learning', 1);('sutton barto', 1);('qstat', 1);('\x02rt maxaqst1a\x00qstat\x034where 201is learning rate', 1);('qlearning', 1);('differentiable functionapproximator', 1);('q\x12sa', 1);('paper use', 1);('deterministic policy gradient', 1);('fujimotoet', 1);('qfunctions', 1);('critics uses pessimistic value', 1);('training policy', 1);('delayed reinforcement learningunlike', 1);('setting environment', 1);('theenvironment realtime environment environment', 1);('time agent', 1);('actionthus perspective agent', 1);('delay time action', 1);('aware delay compensatorypolicies spinal cord reexes', 1);('biological agents', 1);('sandrini', 1);('agent adapt', 1);('realtime environment sensory processing delayscan', 1);('detrimental damage agentif agent', 1);('processing states', 1);('actions action frequency', 1);('equal processingtime', 1);('time step time', 1);('aninput statein', 1);('setting action at1', 1);('processing state st', 1);('state st1', 1);('weuse realtime markov reward process rtmrp', 1);('ramstedt pal', 1);('augmentthe state space', 1);('xs\x02a', 1);('environment transition function', 1);('attimet agent', 1);('unable view', 1);('current state', 1);('policy becomes\x16\x19a0jsa', 1);('prata0jst\x001sat\x001a', 1);('layered architecture adaptive distributed continuous controlthus', 1);('fast actor', 1);('action twill persistfor shorter period time23', 1);('timestepin', 1);('problems agents timestep', 1);('environmentin simulation timestep', 1);('response time agent', 1);('fast changes environment use timestep response time', 1);('inthis section', 1);('constant response time', 1);('aspects control problemvary choice value1performance', 1);('length timestep', 1);('agents performance', 1);('environmental changeshowever', 1);('response speed', 1);('means agent slices episode states', 1);('alonger task horizon', 1);('actionvalue propagation', 1);('slow convergence optimalperformance', 1);('mcgovern', 1);('faster', 1);('processing inputs', 1);('actuation whichrequire energy expenditure', 1);('setting energy', 1);('response speedof agent3memory', 1);('size deep', 1);('reinforcement learning algorithms', 1);('experience replay memory learningmnih', 1);('creation memories', 1);('thusa', 1);('small memory size', 1);('bottleneck performance', 1);('performance theresponse speed increases ip side memory size', 1);('response time mightresult efcient memory use4network', 1);('sizenetwork complexity', 1);('agent uses neural network', 1);('policy theneural network size', 1);('policys complexity', 1);('small neural network', 1);('afaster processing time', 1);('simple policy contrast', 1);('neural networkwould increase processing time', 1);('complex policies', 1);('paragraph policy complexity increases processing time', 1);('response time isdecreased5reward', 1);('distribution', 1);('property environment', 1);('reward', 1);('certain state environment', 1);('important states areoften', 1);('goal state failure state setting return episode independentof response time agent', 1);('perspective agent temporal density thereward rewardstate transitions', 1);('response time agentbecomes', 1);('total state transitions increase results task horizon', 1);('therl problem difcult', 1);('especially', 1);('environments sparse rewards goal state apositive reward states', 1);('zero reward', 1);('explore zerorewardstateaction transitions', 1);('goal state eg mountaincar problem', 1);('moore', 1);('related workthe', 1);('multiple controllers', 1);('knowledge novelhowever', 1);('below31 action repetition frame', 1);('form partial openloop control agent', 1);('asequence actions', 1);('intermediate states', 1);('mixedloopcontrol setting', 1);('sequence actions', 1);('reinforcement learning sequence actions', 1);('number possibleaction sequences length lis exponential l result research area focuses', 1);('possible numberof actions states', 1);('tan', 1);('mccallum ballard', 1);('exponential numberof action sequences works', 1);('action sequences sequence action numberof', 1);('possible actions', 1);('linear lbuckland', 1);('lawrence', 1);('kalyanakrishnan', 1);('srinivaset', 1);('anadditional actionrepetition policywhich', 1);('number time steps repeat', 1);('action lead', 1);('number action decision points episode use approach benchmark inour experiments', 1);('layered architecture adaptive distributed continuous controlmcgovern', 1);('randlv', 1);('training time', 1);('dqn', 1);('mnihet', 1);('amount frames', 1);('increasedhowever approaches', 1);('predictable sequence actions', 1);('tocontinuous domain approaches', 1);('additional exploration contrast', 1);('architecture ourapproach', 1);('layer monitor act', 1);('closedloop temporal abstraction method', 1);('continuous domain', 1);('aactorrepeat decision action', 1);('stateaction value thecritic', 1);('passes critic addition actor decision networks32', 1);('residual layered rlrecently jacq', 1);('lazymdps rl', 1);('top suboptimal base policy toact', 1);('rest actions base policy', 1);('interpretable states agent chooses', 1);('similarlyfor', 1);('continuous environments residual', 1);('residual policy suboptimal base policy thenal action addition actions', 1);('silver', 1);('johannink', 1);('residual rl', 1);('training approach', 1);('temporal residual learningwhere', 1);('frequency network', 1);('frequency network gain benets ofmacroactions residual learning33', 1);('delayedaware', 1);('rlthe', 1);('brooks leondes', 1);('katsikopoulos', 1);('observation delays action delays perspective agentand', 1);('state space', 1);('travnik', 1);('reaction time', 1);('safety realtime environment', 1);('firoiu', 1);('games human reaction time', 1);('predictive model environment undo delayramstedt', 1);('pal', 1);('delayaware versions softactorcritic algorithmhaarnoja', 1);('performance realtime control tasks', 1);('chen', 1);('similar setupto', 1);('performance realtime control tasks work use', 1);('multiple controllers states', 1);('options frameworkthe', 1);('options framework', 1);('precup sutton', 1);('common framework temporal abstraction', 1);('rl options', 1);('3tuples hi\x19 i', 1);('whereiis', 1);('initiation states denes states option', 1);('\x19is option policy', 1);('duration option denes probability option termination inany', 1);('options', 1);('knowledge environment', 1);('successor representation', 1);('machado', 1);('2021or connectedness graph', 1);('chaganty', 1);('learning similarvein research', 1);('dabney', 1);('exploration worktakes advantage phenomenon', 1);('methodsin', 1);('section describe', 1);('fig1 tla', 1);('different timesteps order abstract', 1);('different temporalinformation task experiments simplicity', 1);('multiple thefaster timestep tslown\x01tfast', 1);('fast agent', 1);('layered architecture adaptive distributed continuous controla pendulumv1', 1);('invertedpendulumv2figure', 1);('region shows', 1);('closedloop', 1);('network betterexplore environment', 1);('converge optimal policy', 1);('optimal policy', 1);('slow networkmight', 1);('average return', 1);('timesteps train', 1);('fast network top thefaster agent', 1);('environment actions', 1);('agent achievethis', 1);('action combination function', 1);('ca\x02a rthat', 1);('outputs nal action', 1);('theenvironment dene', 1);('cascasaf', 1);('clipasafaminamax 6whereaminandamax', 1);('maximum action environment asdenotes slowaction', 1);('slow policy afdenotes', 1);('fast policy\x19fafjsas', 1);('praftafjastassts', 1);('actions way', 1);('action becomesas', 1);('repetitions actions', 1);('slow network setting', 1);('exploration thefast network default', 1);('slow network behaviour promotes', 1);('reward states', 1);('networkto train', 1);('identify emergency states', 1);('fast action followsaf\x1aafifjcasaf\x00asj\x15thresh0 otherwise8by', 1);('action inuence nal action', 1);('jerky behavior', 1);('longsmooth actions action', 1);('fast action zero vector abovecondition', 1);('true dimensions', 1);('eachaction dimension', 1);('useful settings dimension', 1);('different joint thataction repetition joint', 1);('components incentivize', 1);('learning1fast action penalty', 1);('fast actions', 1);('nonzero value output', 1);('unseen state transitions', 1);('wouldhinder training', 1);('fast policy negate', 1);('drastic changes', 1);('tomaximize return nd', 1);('fast policy stabilizes training implementthis', 1);('negative reward proportional magnitude', 1);('fast policy output actions withmultiple dimensions', 1);('average magnitude dimensions2fast state augmentation', 1);('pa', 1);('hurdle training', 1);('fast policy effect output actionis', 1);('dependent output', 1);('often', 1);('combination function', 1);('clip sum ofactions', 1);('maximum minimum', 1);('possible action reduces sensitivity nal action theoutput', 1);('output slowpolicy', 1);('fast policy inputs output', 1);('layered architecture adaptive distributed continuous controlenvironment auc avg returntd3 temporl tlao tlac td3 temporl tlao tlacpendulum', 1);('average return results', 1);('tlaoand tlac', 1);('loop forms', 1);('highlight results', 1);('thanthe baseline', 1);('td3figure', 1);('average return average action frequency wrt threshold', 1);('algorithm theinvertedpendulumv2 environment', 1);('reduces average actions', 1);('openloop', 1);('slow controller gate computation fastcontroller', 1);('fast controller actions', 1);('binary gate output g2f01g', 1);('slow policy becomes\x19sasgjs', 1);('prast', 1);('asgjsts 9at step nal action', 1);('environment isatgt\x01ast 1\x00gt\x01aft 10whereaftis action', 1);('fast controller time', 1);('fast controller computation', 1);('slow policy naive approach', 1);('introduce rewardpenalty', 1);('actions reward', 1);('becomesrt\x1agt\x01rt 1\x00gt\x01rt\x012 ifrt\x140gt\x01rt 1\x00gt\x01rt\x0105otherwise115', 1);('experimentsto', 1);('algorithms measure performance', 1);('mujocotaskstodorov', 1);('openai gym', 1);('brockman', 1);('neural networks areimplement', 1);('pytorch', 1);('paszke', 1);('turnbased environmentsopenloop', 1);('hard problem', 1);('previous works', 1);('discrete environments thatbenet', 1);('step sizes', 1);('pendulum', 1);('continuous environment', 1);('anadversarial example algorithm', 1);('outperform baseline', 1);('algorithms theto', 1);('td3 temporl', 1);('continuous control environments', 1);('pendulumv1 mountaincarcontinuousv0from', 1);('classic control problems', 1);('invertedpendulumv2 mujoco', 1);('openai gym7temporally layered architecture adaptive distributed continuous controlenvironment', 1);('repetitions decisionstd3 temporl tlao tlac td3 temporl tlao tlacpendulum', 1);('action repetition percentage decisionsfor implementations', 1);('hyperparameters consistent', 1);('fujimoto', 1);('al2018 actor critic networks', 1);('adam', 1);('kingmaand ba', 1);('twolayer neural networks', 1);('actor andcritica', 1);('actor policy', 1);('iterations random policy', 1);('steps exploration anoise ofn001is', 1);('ddpg', 1);('lillicrap', 1);('version comparisonin order measure training convergence speed measure', 1);('average area training curvesof task', 1);('report performance task', 1);('trials algorithm onpendulumv1', 1);('task task thetemporl', 1);('timesteps maketla', 1);('comparable set timestep', 1);('equal default timestep timestep slowcontroller', 1);('times default timestep', 1);('repeat action', 1);('timesteps fastcontroller', 1);('action thresholdedthe learning curves task', 1);('figure', 1);('nd thecloseloopform', 1);('tla tlac', 1);('outperforms methods', 1);('mountaincarcontinuousv0environment', 1);('difcult task', 1);('lot exploration', 1);('policy converges action', 1);('heretoo tlac', 1);('tla tlao', 1);('td3 pendulum', 1);('hypothesize thatthis', 1);('due largertimesteps', 1);('mountaincarcontinuousv0is', 1);('temporl tlao', 1);('due ability', 1);('notethat', 1);('top suboptimal policy', 1);('outputs zero action', 1);('td3 tlaos', 1);('reveals learningis', 1);('diverges environment', 1);('due reward', 1);('negative states reward penalty', 1);('eq', 1);('future work511 action', 1);('repetitions decisionswe', 1);('measure percentage actions', 1);('method action repetition percentage demonstrateshow good method', 1);('sequences states agent', 1);('reductionin performance', 1);('good measure jerkiness policywe', 1);('measure average number decisions policy', 1);('different different policies denea decision number timesteps', 1);('part agent', 1);('tlacand td3', 1);('number decisions', 1);('equal number timesteps openloop algorithm number ofdecisions', 1);('compute savings', 1);('number decisions notprovide', 1);('complete information compute savings', 1);('fast controller adaptive compute capability', 1);('decisiontwo networks', 1);('number timesteps512 threshold vs', 1);('returnthe tla', 1);('different controllers', 1);('architecturewe threshold', 1);('fast action order', 1);('layered architecture adaptive distributed continuous controlfigure', 1);('examples', 1);('tla td3auc return ar auc return arinverteddoublependulumv2', 1);('ar', 1);('ar auc', 1);('bold \x06corresponds', 1);('changes average actionfrequency', 1);('tla fig', 1);('demonstrates variance average return average action frequency respect tothe threshold', 1);('similar plots', 1);('mujoco', 1);('particular case', 1);('maximum possible reward', 1);('theslow', 1);('interestingly tla', 1);('frequency range', 1);('temporal generalization gap', 1);('asit performs', 1);('performance deteriorates middle513', 1);('temporal', 1);('context statesthe', 1);('important states', 1);('fast action requiredfig', 1);('environment nd', 1);('fast network', 1);('pendulum upright', 1);('critical maximize rewards52', 1);('realtime environmentssince', 1);('setting layer differentsystem', 1);('similar biological reexes', 1);('independent brain', 1);('dene control policy withthe brain layer focuses', 1);('different temporal context', 1);('different reaction time', 1);('realtime environments', 1);('realtime setting', 1);('chenet', 1);('equal timestep layer states environments augmentedby', 1);('previous action', 1);('environment delay', 1);('fortla', 1);('timestepfor fair comparison', 1);('equal original timestep', 1);('slow layertimestep', 1);('double length', 1);('agent act', 1);('\x02max action', 1);('individual environment', 1);('performance efciencyour results', 1);('equal average return', 1);('action repetition environments action repetition complexcontinuous environment', 1);('closedloop variant', 1);('able increase action repetitionwhile', 1);('environments ahigher', 1);('learning curves experiments supplementary material6', 1);('discussion', 1);('brains ability utilize environmental information', 1);('multipletimescales instance brain', 1);('reaction time strategy', 1);('environmentsenergy constraints task demands', 1);('balance reexes', 1);('motor movements', 1);('layered architecture adaptive distributed continuous controland', 1);('efcient responses', 1);('tlas', 1);('detect emergencysituations', 1);('unknown environments', 1);('long periods timethe brains design', 1);('process environmental information parallel', 1);('control makechoices design', 1);('environment review ofhuman', 1);('van der meer', 1);('ability process sensory informationwhile', 1);('species navigate evolve', 1);('large array complexenvironments', 1);('adaptable intelligent design researchers', 1);('biological designs utilizethese neural architectures', 1);('intelligent adaptive systemsthe', 1);('affordance competition hypothesis cisek', 1);('specic theory motor planning', 1);('tlabut', 1);('future designs', 1);('new architectures theory', 1);('mammalian brain creates motor plan', 1);('changingenvironment theory predicts animal representation', 1);('multiple motor plans chooses', 1);('thatoutcompetes plans action plans', 1);('multiple timescales goals action costs environmentalinformation', 1);('representations model predicts animal chooses', 1);('environments demands animals goal theory', 1);('clear behaviorsupportedmodel discrete', 1);('uid movement plan environment movesforward time eg lion', 1);('hunt gazelle', 1);('direction lion', 1);('new prey theory brain creates', 1);('multiple motor plans', 1);('onewins comparison competition motor plans costbenet ratioanother example brains ability adapt environmental demands', 1);('gravity wherethe human gait switches', 1);('kram', 1);('thiswork environment', 1);('different response times nature phenomenonresults reduction', 1);('relative locomotion speed response times', 1);('body sizes', 1);('able navigate', 1);('requiredifferent response frequencies', 1);('important step', 1);('aithe', 1);('benet shorter task horizon', 1);('temporal details policy', 1);('sincethe', 1);('layers activate', 1);('complex environments', 1);('automaticallycategorizes states', 1);('urgent action states biology evolution', 1);('rise tosuch', 1);('control reexes activate', 1);('urgent situations', 1);('knowledge distillation', 1);('tlaoapproach', 1);('fast slow reexes', 1);('macroactions reexes', 1);('aid learning', 1);('moreindepth exploration', 1);('future workour', 1);('similar techniques', 1);('earlyexit architectures', 1);('scardapaneet', 1);('contrast earlyexit scenario realtime control setting output network needs bedifferent', 1);('different times', 1);('aware processing actuation delay pickingan output', 1);('early action needs', 1);('inour realtime experiments hope pave way', 1);('future research', 1);('agents canunderstand impact time optimal action7', 1);('conclusionin', 1);('adaptive response timein reinforcement learning framework', 1);('smooth control realtime setting', 1);('aslow controller', 1);('fast controller monitors intervenes', 1);('slow controller gate', 1);('efcient control', 1);('wedemonstrate', 1);('convergence action repetition closedloop approach', 1);('decision fasterconvergence partiallyopen loop approach', 1);('real time setting processingand actuation delays', 1);('account show approach outperforms', 1);('current approaches', 1);('actions work demonstrates', 1);('adaptive approach similarbenets', 1);('important direction', 1);('future research articiallyintelligent', 1);('layered architecture adaptive distributed continuous controlacknowledgmentsthis', 1);('advanced', 1);('projects', 1);('darpaunder agreement hr00112190041', 1);('reect positionor policy', 1);('governmentreferencesa biedenkapp r rajan', 1);('hutter lindauer temporl learning', 1);('braylan hollenbeck e meyerson r miikkulainen frame', 1);('powerful parameter learning playatari', 1);('competency video', 1);('brockman v cheung', 1);('pettersson j schneider j schulman j tang', 1);('zaremba openai', 1);('gym 2016d', 1);('brooks', 1);('leondes technical', 1);('note markov decision processes stateinformation', 1);('oper res', 1);('buckland p lawrence transition', 1);('chaganty p gaur', 1);('small world', 1);('aamas', 1);('chen xu', 1);('li zhao delayaware', 1);('cisek cortical', 1);('mechanisms action selection affordance competition hypothesis', 1);('philosophical transactionsof', 1);('royal society b', 1);('biological', 1);('dabney g ostrovski barreto temporallyextended', 1);('\x0fgreedy exploration', 1);('abs200601782 2021v', 1);('firoiu ju j', 1);('human speed', 1);('deep', 1);('reinforcement learning action delay', 1);('abs181007286 2018s', 1);('fujimoto h', 1);('hoof meger addressing', 1);('function approximation error actorcritic methods', 1);('abs180209477 2018t', 1);('haarnoja zhou k hartikainen g tucker ha j tan v kumar h zhu gupta p abbeel levinesoft', 1);('actorcritic algorithms applications', 1);('abs181205905 2018e', 1);('hansen g barto zilberstein reinforcement', 1);('openloop closedloop control', 1);('innips', 1);('p heitz', 1);('speedaccuracy tradeoff history physiology methodology behavior', 1);('frontiers', 1);('iriartedaz differential', 1);('locomotor performance', 1);('small large terrestrial mammals journal ofexperimental biology', 1);('pt', 1);('jacq j ferret pietquin geist lazymdps towards', 1);('interpretable reinforcement learning learningwhen act', 1);('abs220308542 2022t', 1);('johannink bahl nair j luo kumar loskyll j ojea e solowjow levine residualreinforcement', 1);('international conference', 1);('robotics automation icra', 1);('pages60236029 2019s', 1);('kalyanakrishnan aravindan v bagdawat v bhatt h goka gupta k krishna v piratla', 1);('abs210203718 2021k', 1);('v katsikopoulos e engelbrecht markov', 1);('decision processes delays asynchronous', 1);('trans autom', 1);('p kingma j ba adam', 1);('method stochastic optimization', 1);('abs14126980 2015r', 1);('kram domingo p ferris effect', 1);('walkrun transition speed', 1);('thejournal', 1);('experimental biology', 1);('p lillicrap j j hunt pritzel n heess erez tassa silver wierstra continuous', 1);('abs150902971 2016m c', 1);('machado barreto precup temporal', 1);('abstraction reinforcement learning successorrepresentation', 1);('abs211005740 2021a', 1);('mccallum h ballard reinforcement', 1);('learning selective perception', 1);('layered architecture adaptive distributed continuous controla mcgovern r sutton h fagg roles', 1);('reinforcement learning 1997v', 1);('mnih k kavukcuoglu silver rusu j veness g bellemare graves riedmiller fidjelandg ostrovski petersen', 1);('beattie sadik antonoglou h', 1);('kumaran wierstra legg', 1);('hassabis humanlevel', 1);('nature', 1);('2015a w', 1);('moore efcient', 1);('technical', 1);('report university', 1);('cambridge', 1);('nakahira q liu j sejnowski j', 1);('doyle diversityenabled', 1);('sweet spots', 1);('architectures andspeedaccuracy tradeoffs sensorimotor control', 1);('proceedings', 1);('national academy', 1);('accessed20220812a paszke gross', 1);('massa lerer j bradbury g chanan killeen z lin n gimelshein', 1);('antigaa desmaison kopf e yang z devito raison tejani chilamkurthy', 1);('steiner', 1);('fangj bai chintala pytorch', 1);('imperative style highperformance', 1);('learning library', 1);('h wallachh larochelle beygelzimer', 1);('alchbuc e fox r garnett', 1);('advances neural informationprocessing systems', 1);('curran associates inc', 1);('url', 1);('precup r sutton temporal', 1);('abstraction reinforcement learning', 1);('ramstedt', 1);('pal realtime', 1);('randlv learning', 1);('macroactions reinforcement learning', 1);('sandrini serrao p rossi romaniello g cruccu j', 1);('willer', 1);('limb exion reex humansprogress neurobiology', 1);('scardapane scarpiniti e baccarelli uncini', 1);('exits neural networkscognitive', 1);('computation', 1);('sharma srinivas', 1);('abs170206054 2017d', 1);('silver g lever n heess degris wierstra riedmiller deterministic', 1);('policy gradientalgorithms', 1);('silver k r allen j', 1);('p kaelbling residual', 1);('policy learning', 1);('abs181206298 2018a', 1);('srinivas sharma', 1);('ravindran dynamic', 1);('sutton g barto reinforcement', 1);('learning introduction', 1);('mit', 1);('press 2018m', 1);('tan costsensitive', 1);('reinforcement learning adaptive classication control', 1);('todorov erez tassa mujoco', 1);('physics engine', 1);('ieeersj internationalconference intelligent robots systems', 1);('ieee', 1);('doi 101109iros20126386109j b', 1);('travnik k mathewson r sutton p pilarski reactive', 1);('reinforcement learning asynchronousenvironments', 1);('frontiers robotics ai', 1);('van der meer z kurthnelson redish information', 1);('theneuroscientist', 1);('xu h zhang taac temporally', 1);('abstract actorcritic', 1);('continuous control', 1);