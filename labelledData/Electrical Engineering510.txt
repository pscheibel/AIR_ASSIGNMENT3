('ua v', 23);('fig', 12);('metaverse', 11);('mdcs', 7);('dqnppo', 7);('ppo', 6);('ppoppo', 6);('dqnppoppoppofig', 5);('comparison', 5);('ua vbased', 4);('continuous agent', 4);('dqn', 3);('rician', 3);('gaussian', 3);('rl', 3);('mb', 3);('reward data size', 3);('aerial vehicle', 2);('ua vto', 2);('time slot', 2);('qlearning', 2);('rodriguezramos', 2);('nin channel mand time slot tis', 2);('mdc', 2);('discrete agent', 2);('j schulman', 2);('\x02 sample', 2);('data size', 2);('time nish mission withdata size', 2);('dqnppoalgorithm', 2);('international conference', 2);('resource allocation', 2);('communications', 2);('arxiv230101474v1 eesssy', 1);('jan', 1);('vaided metaverse wirelesscommunications reinforcement learningapproachpeiyuan si1 wenhan yu1 jun zhao1 kwokyan lam1 qing yang21school computer', 1);('engineeringnanyang technological', 1);('singapore2university', 1);('texas', 1);('statespeiyuan001', 1);('wenhan002 entuedusgjunzhao kwokyanlam ntuedusg', 1);('qingyangunteduabstract metaverse', 1);('users immers iveexperience support 5g high data rate communicati ontechnique', 1);('huge amount data', 1);('physical world needs', 1);('virtual world', 1);('immersive expe rience users', 1);('requirements cove rageto', 1);('5g signal suf ferssevere attenuation', 1);('expensive mainta inthe coverage', 1);('unmanned', 1);('promis ingcandidate technique', 1);('future implementation', 1);('metavers', 1);('eas lowcost highmobility platform communicationdevices paper', 1);('proximal policy optimiz ationppo', 1);('cooperative reinforcement', 1);('ingmethod channel allocation trajectory control', 1);('synchronize data', 1);('physical world th evirtual world', 1);('simulation', 1);('benchm arkapproachesindex', 1);('terms metaverse ua v', 1);('ppoi ntroductionthe', 1);('implementation 5g communication technology maturingarvr devices', 1);('recent years', 1);('aims tocreate virtual world kinds activities includin geducation', 1);('internet', 1);('support ofarvr applications online users', 1);('immersi veservices', 1);('similar inperson activities radingof virtual items', 1);('job opportunitiesto support', 1);('applications data synchronizati onand', 1);('wide wireless network coverage', 1);('practical problems', 1);('involvewearable wireless devices rst problem 5g communication technology', 1);('lowlatency data transmission', 1);('necessary update allthe', 1);('eg environment informa tionto', 1);('ofine tradingrecords', 1);('problem 5g network suffer shigher costs coverage area', 1);('severe signalattenuation', 1);('efcient deplo ybase stations', 1);('suburban low population density nwild areas', 1);('applicable traditional base stat', 1);('network coverage', 1);('data synchronization', 1);('suburban area', 1);('ability carrycommunication devices', 1);('numerous workson', 1);('communication scenarios', 1);('traditional appl ications eg research communication resource allocat ionua', 1);('trajectory control internet vehicles', 1);('optimization problems', 1);('segment ghttime', 1);('discrete time slots convenience ofcomputation resource allocation variables need', 1);('global localoptimal', 1);('methods ensure convergence ofthe solution', 1);('number time slots results tothe increment algorithm complexity', 1);('intege rcharacteristic channel allocation variables results mixedinteger', 1);('ifthe variables', 1);('work cases reinforcement learning', 1);('rlis', 1);('optimization problems thanconvex methods', 1);('feasible solution', 1);('good performance', 1);('global optimal', 1);('hard nd', 1);('number variables', 1);('cui', 1);('multiagent reinforcement learning reso urceallocation algorithm multiua', 1);('fa stconvergence', 1);('luong', 1);('thenetwork state decision movement', 1);('network performance', 1);('gazebobased', 1);('rein forcement learning framework', 1);('movingplatform novel experiment', 1);('ddpg ua vcontrolling', 1);('researchfor communication optimization problems discretechannels', 1);('continuous resource allocation discret e andcontinuous action spaces need', 1);('solve2discretecontinuous hybrid action space reinforcement', 1);('arningproblems multiagent architecture', 1);('f uet', 1);('multiagent reinforcement learnin garchitectures hybrid action spaces', 1);('qlearning dqn', 1);('agents work parallel manner togenerate joint actions', 1);('jiang', 1);('hybrid ctionalgorithm', 1);('massive access control', 1);('thediscrete action selection backoff', 1);('que uingproblems generate', 1);('continuous action access classbarringthe agents', 1);('hybrid action space reinforcement learning algorithms work parallel manner whichdoes', 1);('interagent relationship paperwe', 1);('hybrid reinforcement learning architecture tooptimize discrete channel allocation', 1);('variable thecontinuous trajectory', 1);('agents wo rkin sequential manner', 1);('alternative optimiz ation algorithms ie output agent input ofanother agent', 1);('compared', 1);('works paperconsiders interagent relationship', 1);('converg enceperformance advantage scenario traditiona lconvex optimization number variables notincrease number time slots increases ismore', 1);('friendly timesequential problemscontribution contributions paper followsa', 1);('ppobased', 1);('cooperative hybrid actionreinforcement learning architecture', 1);('ppoppo ua venabled metaverse', 1);('data synchronization proposedproximal policy optimization', 1);('discrete action agents', 1);('continuous action agents', 1);('agents work sequential mannerthe simulation shows comparison', 1);('duellingdqn veries advantage proposedppoppo algorithmthe rest paper', 1);('sectionii', 1);('system model doubleagentpolicy generation model implementation', 1);('present edin section', 1);('iii', 1);('iv', 1);('showsthe simulation results', 1);('corresponding explanation', 1);('theconclusion', 1);('viii ystem modelas', 1);('uplink datacollection system', 1);('llarea', 1);('coverage 5g base station', 1);('nmetaverse', 1);('data collectors', 1);('collectdelayinsensitive local data ofine digital curre', 1);('weather information', 1);('bymetaverse users sensors', 1);('location ofmdcnis', 1);('haveenough energy', 1);('transmission powerto synchronize local data', 1);('mobile base station', 1);('mbs', 1);('local data', 1);('mdcs mchannelseach mdc', 1);('mdcsare', 1);('able share', 1);('bynm number', 1);('andthe location', 1);('oncethe', 1);('mbs mdcs', 1);('clear historicaldata', 1);('ready future data collection pap erwe', 1);('local data size receiver', 1);('ua', 1);('settingsaccording', 1);('experimental characterization thevehicletoinfrastructure radio channels', 1);('suburban env ironments', 1);('yusuf', 1);('smallscale fadingof', 1);('23the channel gain', 1);('ua v mdc', 1);('24hnmt radicalbigntgnmt 1wherentdenotes largescale average channel gainat time slot andgnmtdenotes smallscale fadingcoefcient', 1);('bynt 0dnt 2andgnmt radicalbiggkk1gradicalbigg1k1g 3where0denotes channel gain reference distanced0 1mdenotes path loss exponent varies from2', 1);('2gdenotes thedeterministic', 1);('los', 1);('channel component g', 1);('kdntdenotes', 1);('ua v mdcnin', 1);('bydnt radicalbigxnxuavt2ynyuavt2h24the channeltonoiseratio', 1);('cnr', 1);('bynmt hnmtb25where2denotes power additive', 1);('noiseawgn receiver signal interference', 1);('nois eratio', 1);('sinr mdc', 1);('nin channel min time slot tis givenbynmt pnmtnmt1nm1summationtexti1pimtimt 6wherepnmdenotes transmission power', 1);('mdcs thusthe', 1);('transmission rate', 1);('blog21', 1);('system modelchannel', 1);('allocation trajectory controlenvironmentuavmdrmdrdiscrete ppo continuous ppocombined actionrewarduav', 1);('1it x t\x01 \x01criticchtaactorbpcriticchtaactorbpcriticchtaactorbpcriticchtaactorbpchtafig', 1);('doubleagent', 1);('policy generation modeliii', 1);('ouble agent policy generation modelin', 1);('section introduce doubleagent policy gener ation model', 1);('ppo ppoppo', 1);('channel allocationand', 1);('trajectory control', 1);('2the objective minimize total', 1);('constrain tof', 1);('channel allocationindicator matrix', 1);('trajectory xuavtyuavteach agent focuses specic type', 1);('variable andthe values variables', 1);('previous step step thediscrete proximal policy optimization', 1);('agent generat esthe channel allocation', 1);('agent trajectory generatio nthe', 1);('outpu tof', 1);('agents interact environment getreward', 1);('allocationin', 1);('subsection introduce action space stat espace reward settings discrete agent channelallocation1 action', 1);('discrete agent intuitively', 1);('channelallocation indicator', 1);('itcan', 1);('onehot matrixieinmt01denotes channel mis', 1);('mdcn', 1);('example number users', 1);('numberof channels', 1);('i11ti12ti13ti21ti22ti23ti31ti32ti33ti41ti42ti43t', 1);('8whose dimension', 1);('nm', 1);('onehot denition', 1);('itisintuitive', 1);('increases dimension action space ducethe dimension redene channel allocation indicato rmatrix', 1);('int01m underthis', 1);('int', 1);('nis assignedwith channel andint', 1);('assignedwith channel', 1);('t2i t3i t4i t5i t6i taction\x01\x02 \x030m 1\x04\x01\x05\x02 \x031m 1\x04\x01\x01\x01\x01\x05\x05\x05\x05\x02 \x032m 1\x04\x02 \x033m 1\x04\x02 \x034m 1\x04\x02 \x035m 1\x04fig', 1);('action encodingas', 1);('action agent', 1);('channel allocation indicator matrix', 1);('theencoded', 1);('byachtnsummationdisplayn1intm1n192 state', 1);('current state paper thestate discrete agent', 1);('channel gain', 1);('current step state thediscrete agent', 1);('b yschturestht 10whereuresdenotes matrix', 1);('mdcsandhtdenotes', 1);('matrix channel gain tthstep3', 1);('reward discrete agent', 1);('optimization objective paper', 1);('nish thedata collection mission ie minimize number ste psin episode', 1);('intuitively', 1);('steps agent', 1);('the4less reward', 1);('pena ltyrtimetwith', 1);('negative value step', 1);('connectionbetween reward objective agent', 1);('nishthe mission', 1);('time limit', 1);('tmax', 1);('penalty rtimetis', 1);('accordingto data size', 1);('current step givehigher reward actions result', 1);('transmis sionrate reward discrete agent', 1);('byrchtrtimetunsummationdisplayn1msummationdisplaym1tslotrnmt ttmaxrfailt max11b', 1);('continuous agent trajectory optimizationthe', 1);('rlagent', 1);('action state reward', 1);('areaslot', 1);('x 2tvslot max2t', 1);('vaction', 1);('space t1trajta action space tfig', 1);('action space', 1);('continuous agent1 action', 1);('continuous agent atrajtdetermines location ofua', 1);('step atrajtis', 1);('asatrajtaxtaytaxtayttslotvmaxtslotvmax 12whereaxtaytdenote movement', 1);('xaxis andyaxis respectively2 state', 1);('similar discrete agent includ esthe', 1);('current channel gain htand', 1);('mdcsurest', 1);('current horizontal location', 1);('ua vxuavtyuavtis', 1);('strajt', 1);('isgiven bystrajturesthtxuavtyuavt', 1);('reward continuous agent', 1);('reward thecontinuous agent', 1);('additionalpenalty agent location', 1);('exceeds reasonabl eregion regularize trajectory decision reward thecontinuous agent', 1);('byrtrajtbraceleftbiggrcht x uavtxminxmaxyuavtyminymaxrchtrpenaltyt otherwise14iv', 1);('mplementation proximal policyoptimization ppoppo', 1);('stateofart policy reinforcement learning algorithm supports discrete', 1);('continuous action sspaces section introduce', 1);('preliminary andimplementation', 1);('algorithm discrete agent channe lallocation', 1);('trajectory optimiza tiona implementation', 1);('continuous discrete ppo1 critic network', 1);('critic network', 1);('responsible togive scores actor', 1);('current state', 1);('thearchitectures', 1);('continuous critic netwo rksare', 1);('lay ersloss function', 1);('continuous discrete critic networksare', 1);('byjtraj bracketleftbigvtrajstrajtparenleftbigrtrajtvtrajstrajt1parenrightbigbracketrightbig2 15jch bracketleftbigvchschtparenleftbigrchtvchscht1parenrightbigbracketrightbig2 16whereltrajtandlchtdenote loss function thecritic network', 1);('continuous discrete agent', 1);('respective lyvtrajstrajt1andvchscht1are state value', 1);('old critic networks trajandchrespectivelywhich', 1);('interaction environmentvtrajstrajtandvchschtare state value estimations', 1);('current critic networks trajandch', 1);('training iteration2', 1);('actor network', 1);('architecture ofdiscrete', 1);('continuous actor network', 1);('different due hedifference action spacethe', 1);('continuous actor network trajectory control anetwork value approximation outputs head andahead denotes', 1);('variables iexyandxy denotes xaxis', 1);('action axtaytis', 1);('nx2xandny2ythe', 1);('discrete actor network channel allocation anetwork classication outputs probabilitie spraof action agent sample action', 1);('action probabilities greedy ie outputaction', 1);('pr', 1);('awith probability1', 1);('probability explorationthe output action discrete actor network encodedwhich', 1);('onehot indicators', 1);('calculationloss functions actor networks implementationadopt trick', 1);('simplify calculation whi chis', 1);('algorithm', 1);('conneted layerssoftmax layerstatehead\x01head\x01 head\x02head\x02trajectory2 n\x01', 1);('from2 n\x01', 1);('fromfully conneted layerssoftmax layerstatechannel allocationpr', 1);('probability\x03', 1);('greedyfig', 1);('actor network architecturealgorithm', 1);('ppoppoinitiate remaining', 1);('mdcs ua v', 1);('location networkparameters discrete', 1);('continuous agent1foriterationt12do2', 1);('discrete', 1);('currentstate policy chparenleftbigachtschtparenrightbigto', 1);('channelallocation indicator matrix', 1);('it3', 1);('continuous agent trajectorycontrol', 1);('current state andpolicytrajparenleftbigatrajtstrajtparenrightbigit4', 1);('agent', 1);('interact environment', 1);('reward rchtandrtrajtfor discrete agent', 1);('continuous agent respectively5', 1);('update', 1);('state strajtstrajt1schtscht16', 1);('save', 1);('trajectoryparenleftbigschtachtrchtscht1vchschtparenrightbigandparenleftbigstrajtatrajtrtrajtstrajtvtrajstrajtparenrightbig7 foreveryiiterations do8', 1);('shufe', 1);('data order', 1);('batch size bs9 forj01tbs1do10', 1);('calculate', 1);('loss functions critic actor networks update network parameters gradientascent11 end for12 end for13end forv', 1);('imulation resultsthe', 1);('doubleagent reinforcement learning approach', 1);('tes tedand', 1);('benchmark scenarios', 1);('discrete agents', 1);('ifig', 1);('complete data collectin gmission', 1);('benchmark algorithms', 1);('beginning ofthe training process', 1);('algorith mstable', 1);('parameter settingparameter physical meaning valuenumber', 1);('channels 3default number users', 1);('5bandwidth b 5mhztransmission power', 1);('5wfrequency f 28ghz 5g spectrumpower', 1);('5108wmaximum speed', 1);('10msmission area size l 200mare', 1);('unstable reasonable policy', 1);('environment f', 1);('episodes proposedppoppo algorithm shows tendency convergence whilethe benchmark', 1);('starts nish themission', 1);('shorter time period', 1);('stable thanthe', 1);('algorithm shows poorconvergence performance task', 1);('able converge within5000 episodes', 1);('similar performance', 1);('commonimplementation advantage function0', 1);('mbfig', 1);('time experiment witha', 1);('similar parameter setting', 1);('algorithms need moretime nish data', 1);('algorithm shows', 1);('similar convergenceperformance', 1);('unstable training process ie omesudden increase', 1);('superior stabilit yof', 1);('policyupdate constraint', 1);('kldivergence', 1);('pena ltybetween', 1);('old policy policy', 1);('policy policy', 1);('corresponding rewards training processes', 1);('considerthe reward', 1);('agent guidance exactobjective function implementation reinforcement60', 1);('mblearning', 1);('algorithm tendencies reward', 1);('generate dfrom', 1);('different formulas', 1);('mbthe', 1);('time comparison case witheight users', 1);('similar average performance', 1);('ppoppoalgorithm', 1);('stability ie', 1);('time somet imesjumps', 1);('large values', 1);('stability intoconsideration', 1);('duelli ngdqnppo algorithm general', 1);('able converge experiment donot', 1);('candidate doubleagent reinforcem', 1);('algorithmvi c', 1);('onclusionin', 1);('doubleagent reinforcement architecture data', 1);('meta', 1);('algorithm discrete continuousagents', 1);('different action space state spa cework cascade manner channel allocation', 1);('ua v0', 1);('mb0', 1);('and8userstrajectory control form', 1);('action iterati onour experiments', 1);('time mission stability', 1);('infuture', 1);('transmission power allocatio nand test performance stateofart reinforceme', 1);('wang j zhao mobile edge computing metaverse', 1);('g wirelesscommunications articial intelligence blockchain survey', 1);('convergence', 1);('arxiv preprint arxiv220914147', 1);('lh lee braud p zhou', 1);('wang xu z lin kuma', 1);('bermejo p hui', 1);('acomplete', 1);('survey technological singularity virtual eco system andresearch agenda arxiv preprint arxiv211005352', 1);('p si j zhao h han ky lam liu resource', 1);('ocation andresolution control', 1);('metaverse mobile augmented r', 1);('ealityarxiv preprint arxiv220913871', 1);('j chua', 1);('yu j zhao resource', 1);('allocation mo bilemetaverse', 1);('internet vehicles', 1);('6g wireless com munications', 1);('reinforcement learning approach arxiv preprintarxiv220913425', 1);('hc han', 1);('visual culture immersive taverse tovisual cognition education', 1);('cognitive affective perspectiveson immersive', 1);('technology education', 1);('igi global', 1);('je yu exploration', 1);('educational possibilities', 1);('physical education', 1);('technologies', 1);('murat yilmaz tuna hacalo', 1);('paul clarke examini', 1);('ng use ofnonfungible tokens', 1);('nfts', 1);('mechanism taversein', 1);('european conference', 1);('software process improvement sp', 1);('ringer2022 pp', 1);('jungherr', 1);('schlarb extended reach game enginecompanies companies', 1);('epic', 1);('unity technolo', 1);('platforms extended reality applications', 1);('th e', 1);('metaversesocial media', 1);('society vol', 1);('agiwal roy n saxena', 1);('generation 5g wire lessnetworks', 1);('comprehensive survey', 1);('ieee communications su', 1);('tutorials', 1);('j refonaa g g sebastian ramanan lakshmi effectiveidentication', 1);('black money fake currency', 1);('nfc io', 1);('communication computingand internet', 1);('ic3iot ieee', 1);('j kohli kohli', 1);('preference online', 1);('radingaccount versus', 1);('ofine', 1);('account different age grou', 1);('fromthe desk editor', 1);('p v manedeshmukh designing wireless sensor', 1);('net work toprotect', 1);('agricultural', 1);('animals', 1);('imanagers journal oninformation technology vol', 1);('q wu zeng r zhang joint', 1);('trajectory commu nicationdesign multiua', 1);('wireless networks', 1);('ieee tran', 1);('wireless communications', 1);('lu p si gao h han z liu wu gong tra', 1);('resource optimization ofdmbased ua vpowered iot ne', 1);('ieee transactions', 1);('communications netwo', 1);('j ng', 1);('w b', 1);('lim hn dai z xiong j huang niy', 1);('xshua', 1);('leung', 1);('miao joint', 1);('auctioncoalition format ion framework communicationefcient', 1);('enabledinternet vehicles', 1);('ieee transactions intelligent tr', 1);('ansportationsystems vol', 1);('j cui liu nallanathan multiagent', 1);('reinfo rcement', 1);('ieee transact', 1);('ions onwireless', 1);('p luong', 1);('gagnon ln tran', 1);('labeau deep', 1);('ua vas', 1);('ieee transactions wireless communications', 1);('no11 pp', 1);('sampedro h bavle p de la puen', 1);('pcampoy', 1);('deep reinforcement learning strategy', 1);('platform journal', 1);('intelligent robo', 1);('systems', 1);('h fu h tang j hao z lei chen', 1);('fan deep', 1);('mu ltiagentreinforcement learning discretecontinuous hybrid ction spacesarxiv preprint arxiv190304959', 1);('n jiang deng nallanathan deep', 1);('reinforcem ent learningfor discrete', 1);('continuous massive access control optimiz ation', 1);('icc20202020 ieee', 1);('communication iccieee', 1);('chu j lee kim h kim yoon h chung revi', 1);('ofine payment function cbdc', 1);('require', 1);('christodorescu', 1);('w c', 1);('gu r kumaresan minaei ozdayib price raghuraman saad', 1);('shefeld xu', 1);('owardsa twotier hierarchical infrastructure ofine payment system forcentral bank digital currencies arxiv preprint arxiv20', 1);('yusuf e tanghe', 1);('challita', 1);('experimental', 1);('c haracterization ofv2i radio channel', 1);('suburban environment', 1);('eu', 1);('antennas propagation eucap ieee', 1);('pp1524 c', 1);('r zhang', 1);('3d trajectory optimization', 1);('rici', 1);('venabled', 1);('data harvesting', 1);('ieee transactions wirele', 1);('wolski p dhariwal radford k', 1);('proximal', 1);('policy optimization algorithms arxiv preprint arxi v1707063472017', 1);