('ieee', 24);('ctc', 20);('wer', 12);('inter ctc', 11);('conformer', 9);('icassp', 8);('international conference', 8);('processing icassp', 8);('efcient conformer', 7);('lrs3', 7);('figure', 6);('lrs2', 6);('lrs23', 6);('proceedings', 5);('noise robustness', 4);('ctcbased', 4);('conformerstage', 4);('audiovisual efcient conformer', 4);('patch attention', 4);('lm', 4);('snr', 4);('acoustics speech', 4);('asru', 4);('resnet18', 3);('lrs2 lrs3', 3);('asr', 3);('patch size', 3);('flops', 3);('lrw', 3);('speech recognition', 3);('maet', 3);('ablation', 3);('eff conf', 3);('modesnr', 3);('computer vision', 3);('automatic speech recognition', 3);('joon', 3);('ieeecvf', 3);('conference computer vision patternrecognition pages', 3);('speech', 3);('neural networks', 2);('large scale', 2);('datasets sufcient', 2);('lip reading sentences', 2);('amount ofcomputation', 2);('efcient conformerencoder', 2);('afouras', 2);('prajwal', 2);('close gap', 2);('vsr', 2);('s2s', 2);('patch', 2);('similar performance', 2);('millions', 2);('ta160', 2);('ta320', 2);('conformerblocks', 2);('blocks post', 2);('inter ctcresidual', 2);('kernel size', 2);('encoder layers', 2);('attention complexity', 2);('attention', 2);('mhsa', 2);('residual module', 2);('residual modules', 2);('lip reading', 2);('audiovisual models', 2);('ctcs2s lrw lrs23', 2);('language', 2);('greedy search', 2);('error rate', 2);('triantafyllos afouras joon', 2);('advances', 2);('international conference onacoustics', 2);('proceedings ieee', 2);('audiovisual efcient conformer robust speech recognitionmaxime burchi radu timoftecomputer vision lab caidas ifi', 1);('university w urzburg', 1);('germanyfmaximeburchiradutimofte', 1);('automatic speech recognition asr', 1);('large improvements', 1);('recent years availability', 1);('itpossible train', 1);('powerful deep neural networks reachingvery', 1);('low word', 1);('error rate wer', 1);('academic benchmarkshowever', 1);('impressive performance', 1);('clean audiosamples drop performance', 1);('noisyspeech work', 1);('efcient conformer connectionist temporal classication ctcbased', 1);('architecture processing audio visual modalities', 1);('previous lip reading methods', 1);('backend top', 1);('visual frontend andby', 1);('losses blocks condition intermediate block', 1);('residual modules relax conditional independence assumption', 1);('attention moreefcient simpler attention mechanism', 1);('patchattention experiment', 1);('lrs2 lip reading sentences', 1);('lrs3datasets', 1);('audio visualmodalities', 1);('speech presenceof environmental noise', 1);('training steps', 1);('ouraudiovisual efcient conformer avec', 1);('model achievesstateoftheart performance', 1);('code', 1);('available httpsgithubcomburchimavec1', 1);('introductionendtoend automatic speech recognition', 1);('ondeep neural networks', 1);('standard stateoftheart approaches', 1);('recent years', 1);('7the availability', 1);('possible train power40 ms ratevisual', 1);('ms ratevisual', 1);('frontend conv3d resnet18 audio frontend stft conv2d audio conformerstage', 1);('3audiov isual', 1);('fusion moduleaudiov', 1);('stagevisual backendaudio backend80', 1);('ms ratectc loss40 ms rate80 ms rate80 ms ratefigure', 1);('architecture model', 1);('loss andtakes', 1);('raw audio waveforms lip movements thespeaker inputsful', 1);('weron', 1);('academic benchmarks', 1);('librispeech', 1);('neural', 1);('recurrent neural networks rnn', 1);('neural networks cnn', 1);('transformers', 1);('rawaudio waveforms melspectrograms audio', 1);('totranscribe speech text', 1);('recently gulati', 1);('transformer architecture', 1);('model local', 1);('global dependencies', 1);('convolution attention', 1);('speechrecognition performance', 1);('concurrently nozaki', 1);('33arxiv230101456v1 cscv', 1);('jan', 1);('speech recognition conditioningintermediate encoder block', 1);('burchi', 1);('groupedattention speech recognition', 1);('inspiredfrom', 1);('computer vision backbones', 1);('multiple stages stagecomprises number', 1);('blocks progressivelydownsample project audio sequence', 1);('approaches breakingthe stateoftheart', 1);('major pitfall', 1);('rapid deterioration performance thepresence ambient noise parallel', 1);('audio visualspeech recognition vsr', 1);('lot ofresearch attention', 1);('ability use image processing techniques', 1);('speech recognition systems', 1);('preceding', 1);('visual modality oflip movements', 1);('systems respect noise', 1);('xu', 1);('twostage approach rst', 1);('separate target voicefrom background noise', 1);('speakers lip movementsand transcribe', 1);('audio signal', 1);('oflip movements', 1);('petridis', 1);('uses hybrid architecture training', 1);('lstmbased', 1);('s2smodel', 1);('early fusionstrategy', 1);('usesconformer backend networks', 1);('frontend networks', 1);('recognition performanceother', 1);('visual speech recognition vsronly', 1);('lip movements transcribe', 1);('languageinto text', 1);('important line ofresearch use crossmodal distillation', 1);('zhao', 1);('lip reading performance', 1);('model trainedon largescale', 1);('auxiliary tasks', 1);('subwords units', 1);('charactersto transcribe sequences', 1);('time andmemory requirements', 1);('burden modelin work focus design noise robustspeech recognition architecture processing audio andvisual modalities', 1);('ctcbased efcient conformer', 1);('visual modality lip movements', 1);('audiovisual efcient conformera vec', 1);('arethe rst work', 1);('losses betweenblocks', 1);('visual speech recognition performance show', 1);('residual modules allowsto', 1);('autoregressive nonautoregressive', 1);('helpsto counter', 1);('common failure case audiovisualmodels', 1);('ignore visual modality way weforce prefusion layers', 1);('attentionby efcient simpler attention mechanism thatwe', 1);('complexity contributions work', 1);('architecture processingboth audio visual modalities condition intermediate', 1);('block featureson', 1);('residual modulesto relax conditional independence assumption ofctc models', 1);('werbetween', 1);('autoregressive nonautoregressive methods', 1);('efcient conformergrouped', 1);('attention efcient simpler attention mechanism', 1);('patchattention', 1);('complexity experiment', 1);('lrs2 lrs3datasets', 1);('stateoftheart results', 1);('audioand visual modalities2', 1);('methodin', 1);('section describe', 1);('audiovisualefcient conformer', 1);('network model', 1);('main components audio encoder visual encoderan audiovisual fusion module audiovisual encoderthe audio visual encoders', 1);('modalityspecic frontend networks transform input modality temporal sequences', 1);('backend networks model local', 1);('global temporal relationships', 1);('full model', 1);('blocks addition theoutput', 1);('complete architecture modelis', 1);('model architectureaudio', 1);('frontend audio frontend network rsttransforms', 1);('raw audio waveforms', 1);('fourier', 1);('windows 20ms step size 10ms 80dimensionalmelscale log lter banks', 1);('bya 2d convolution stem extract local temporalfrequencyfeatures', 1);('20ms frame rate signal audiofrontend architecture', 1);('audio frontend', 1);('parameterstadenotes input audio sample lengthstage', 1);('layers output shapefouriertransfstft', 1);('window length160 hop length', 1);('scale', 1);('conv2d', 1);('lters 22stride', 1);('linear', 1);('1180visual frontend visual frontend network 29transforms input video frames temporal sequences', 1);('a3d', 1);('convolution stem kernel size 5\x027\x027is rst', 1);('video video frame', 1);('output spatialaverage', 1);('temporal', 1);('tothe backend network input dimension', 1);('linear layerthe visual frontend architecture', 1);('visual frontend', 1);('parameterstvdenotes number input video framesstage', 1);('layers output shapestemconv3d', 1);('lters 1\x0222stridemaxpoo3d 1\x02321\x0222stride64', 1);('tv2222res', 1);('global average pooling tv512proj linear', 1);('tv256backend', 1);('networks backend networks', 1);('severalstages stage comprises number', 1);('relative positionalencodings temporal sequence', 1);('widerfeature dimensions', 1);('amount computationwhile', 1);('performance use', 1);('stages theaudio backend network downsample audio signal toa', 1);('milliseconds frame rate', 1);('stages necessaryto downsample visual signal frame rate', 1);('shows hyperparameter backend', 1);('backend', 1);('networks hyperparameters', 1);('interctcblocks', 1);('params', 1);('stages', 1);('num blocks', 1);('feature dim', 1);('blocks', 1);('2audiovisual fusion module', 1);('fusion strategy', 1);('audiovisual featuresand', 1);('model complexity acoustic visual', 1);('backend networks', 1);('fedinto joint feedforward network', 1);('size 2\x02dmodel rst', 1);('linearlayer output size dff 4\x02dmodel', 1);('swish', 1);('activation function', 1);('dimension dmodel', 1);('audiovisual', 1);('encoder audiovisual encoder asingle stage backend network', 1);('encoder outputs arethen', 1);('layer maximize sum probabilities', 1);('target alignments22', 1);('patch attentionthe efcient conformer', 1);('multihead selfattention mhsa', 1);('grouped mhsa', 1);('temporal elements', 1);('scaleddotproduct attention', 1);('quadratic computational complexity respect sequence lengththis', 1);('network asymmetric complexitywith', 1);('attention layers', 1);('ops latterlayers shorter sequence length work', 1);('attention simpler moreefcient attention mechanism', 1);('patch attentionfigure', 1);('vision transformer mvit', 1);('video andimage recognition patch attention', 1);('variants complexities', 1);('querykey value output linear projections nanddare thesequence length', 1);('dimension respectivelyattentionvarianthyperparameterfull', 1);('attentioncomplexityregular on\x01d2n2\x01dgrouped', 1);('size', 1);('on\x01d2', 1);('onk\x01d2', 1);('aaabbbcccupsample cattentionb cbfigure', 1);('patch multihead selfattention', 1);('input sequence', 1);('applyingmultihead selfattention output sequence', 1);('on2\x01dtoonk2\x01dwherekdenes', 1);('attention equivalentto', 1);('regular attention k', 1);('input sequence projection querykey', 1);('1dxin 1withqkv', 1);('xwqxwkxwv2wherewqwkwv2rd\x02dare', 1);('query key valuelinear projections parameter matrices', 1);('relativesinusoidal positional', 1);('lowerresolution asmhsa', 1);('x concat o1ohwo3withohsoftmax\x12qhkthsrelhpdh\x13vh', 1);('relative position score matrix thatsatisfysrelij', 1);('qietj\x00ieis', 1);('linear projection ofa', 1);('standard sinusoidal positional', 1);('matrix positions', 1);('\x00nmax\x001tonmax\x001 attention output sequence', 1);('initial resolution', 1);('neighbor upsamplingxoutupsamplenearest 1dmhsa', 1);('5in consequence temporal element patchproduce attention output', 1);('local temporal relationships', 1);('convolution modules whileglobal relationships', 1);('weuse', 1);('1dimensional patches work patch attentionaudio backend', 1);('conformer stagemodule giga flops0010203stage', 1);('d180 n500 stage', 1);('d256 n250 stage', 1);('d360 n125attention', 1);('attention g3 patch attention k3 feedforwardfigure', 1);('audioonly', 1);('backend modules', 1);('flops billioncould', 1);('image video data using2d 3d patches', 1);('thecomputational', 1);('complexity attention variant shownin', 1);('path', 1);('query key value output', 1);('unchanged similar previous work', 1);('patchattention rst audio backend stage', 1);('model recognition performancefigure', 1);('shows amount', 1);('attentionmodule variant respect', 1);('sequence length nand model', 1);('attention variants', 1);('rst audio backend stage23', 1);('intermediate ctc predictionsinspired', 1);('losses encoder blocks', 1);('speech recognition performance', 1);('encoder networks condition intermediate block', 1);('audio visual andaudiovisual encoders', 1);('early predictions relax conditional independence assumption', 1);('duringboth', 1);('training inference intermediate prediction', 1);('recognitionwe use method', 1);('donot share layer parameters losses lthblockoutputxoutlis', 1);('feedforward network withresidual connection softmax activation functionzlsoftmax', 1);('linear xoutl', 1);('zl', 1);('7wherezl2rt\x02vis probability distribution theoutput vocabulary intermediate', 1);('target sequence yaslinterl\x00logpyjzl 8withpyjzl', 1);('x\x192b\x001ctcytyt1zt\x19t', 1);('blockinter ctc residual module conformer blocklinearsoftmaxlinearctc', 1);('intermediate', 1);('conformerblock', 1);('condition prediction nal block itintermediate', 1);('lossfor computation nal losswhere\x192vtare paths tokens', 1);('bctc', 1);('manytoone map', 1);('removes blanks', 1);('labelsfrom paths total training objective', 1);('asfollowsl 1\x00\x15lctc\x15linter10withlinter1kxk2interblockslinterk 11where interblocks', 1);('interctc', 1);('blocks with\x15set', 1);('experiments31 datasetswe', 1);('datasets thiswork', 1);('lip reading sentences2 lrs2', 1);('training evaluationlrw dataset', 1);('audiovisual word recognition dataset', 1);('video segments', 1);('asingle word vocabulary', 1);('training samples', 1);('utterancesper class validation test', 1);('dataset composedof', 1);('videos clips', 1);('bbc', 1);('television whereas', 1);('video clips', 1);('ted tedxtalks', 1);('corresponding subtitles withword alignment boundaries', 1);('pretrainsplit trainval', 1);('training 28hours', 1);('whereas lrs3', 1);('utterances inthe', 1);('utterances thetrainingvalidation', 1);('utterances inthe test', 1);('hours videos', 1);('speakerhave 224\x02224pixels resolution', 1);('25fps 16khz audio32 implementation', 1);('detailspreprocessing', 1);('rotation scale', 1);('lip regions', 1);('boxes 96\x0296pixels facilitate recognition', 1);('retinaface', 1);('face alignment network fan', 1);('facial landmarks', 1);('facial', 1);('landmarks thelrw', 1);('clean comparison methods bytepair', 1);('tokenizer isbuilt', 1);('pretrain trainval splits', 1);('vocabulary size', 1);('7data augmentation', 1);('specaugment', 1);('onthe audio melspectrograms training', 1);('frequency masks mask size parameterf', 1);('time masks adaptive size ps', 1);('mask videos time axis', 1);('maximum mask duration', 1);('size 88\x0288and', 1);('video trainingwe', 1);('central crop withhorizontal', 1);('test time', 1);('setup', 1);('rst pretrain visual encoder onthe', 1);('crossentropy loss recognizewords', 1);('visual encoder', 1);('for30 epochs frontend weights', 1);('initialization training', 1);('audio', 1);('visual encoders', 1);('noam', 1);('10kwarmup steps peak learning rate', 1);('theadam optimizer', 1);('l2', 1);('trainableweights model train models globalbatch size', 1);('gpus', 1);('batch size', 1);('nvidia a100', 1);('gpusare', 1);('audiovisual experiments whilertx', 1);('ti', 1);('theaudioonly', 1);('videos shorterthan', 1);('seconds training', 1);('weaverage models weights', 1);('stochastic weight averaging', 1);('comparison wer lrs2 lrs3', 1);('nonpubliclyavailable datasets', 1);('audioonly ao visualonly vo audiovisual v', 1);('werao vo vusing publicly', 1);('datasets petridis', 1);('ctcs2s lrw lrs2', 1);('zhang', 1);('s2s lrw lrs23', 1);('ctc v', 1);('xuet', 1);('s2s lrw lrs3', 1);('lfmmi lrs2', 1);('s2s lrs23', 1);('ctc lrw lrs23', 1);('neural lm ctc lrw lrs23', 1);('nonpublicly', 1);('datasets afouras', 1);('s2s mvlrs lrs23', 1);('s2s mvlrs lrs2', 1);('shillingford', 1);('ctc lrvsr', 1);('makino', 1);('transducer youtube31k', 1);('transducer youtube90k', 1);('s2s mvlrs tedx', 1);('ctcs2s lrw vspeech lrs23', 1);('similarly', 1);('ngram', 1);('statistical language model', 1);('atransformer neural language model 6gram', 1);('usedto generate list hypotheses', 1);('beam search anexternal', 1);('transformer lm', 1);('rescore nal listthe 6gram', 1);('pretrain andtrainval transcriptions', 1);('concerning', 1);('gpt3 small', 1);('librispeech lmcorpus', 1);('05m steps', 1);('batch size 01m tokensand netune', 1);('wers audiovisual efcient conformer', 1);('stateoftheart methods', 1);('lrs2and lrs3', 1);('stateoftheart performances', 1);('model competes', 1);('recent autoregressive methods', 1);('s2scriterion', 1);('similar results stilllack', 1);('uses auxiliary losses', 1);('networks foundour audiovisual network converge', 1);('steps intermediate', 1);('losses visualencoder', 1);('audiovisual layers helpprefusion layers', 1);('ablation studieswe', 1);('ablation study', 1);('understandthe improvements terms complexity', 1);('broughtby architectural modication report numberof operations', 1);('number multiplyandadd operations network process', 1);('audiovideo clip', 1);('inverse real', 1);('factor inv rtf', 1);('intel core', 1);('cpu', 1);('thread ablations', 1);('models 200epochs', 1);('conformer visual backend', 1);('backend network use byte pairencodings tokenization', 1);('tofurther downsample temporal sequences', 1);('impactingthe computation', 1);('backend network visualonlymodel', 1);('modelcomplexity training time number model parameters', 1);('study visual backend networkvisualbackendparamsmillionlrs2testlrs3testflopsbillioninvrtfconformer', 1);('conf', 1);('526referencethe authors', 1);('year period hundreds thrown outoutputsblock 3theotho', 1);('pa people anyyour perandconndries aboutent threghowblock', 1);('paperss overaiyearpaiodandhundredsthat thrououtowblock', 1);('theauthorslookedatpaperswitenoverainghtyearperiodand hundredsthattobeen throwoutblock', 1);('paperswrittenover10year period hundredshadtobethrownoutfigure', 1);('output', 1);('visualonly', 1);('intermediatectc prediction', 1);('blocks sentence', 1);('residual modules blocksto relax conditional independence assumption', 1);('3conformer blocks', 1);('example intermediate blockpredictions', 1);('external language model test', 1);('seethat output', 1);('intermediate predictions', 1);('previous layers', 1);('model renes output framelevelpredictions', 1);('insertion deletion errors inaddition substitution errors study impact', 1);('multimodal learning measuringthe performance audiovisual model', 1);('networks multimodal inputs canoften', 1);('modes case speechrecognition', 1);('problem lip readingwhich', 1);('model ignore visual information', 1);('counter problemby', 1);('prefusion layers transcribe input', 1);('impact inter ctc', 1);('audiovisual model', 1);('wer lrs2 lrs3', 1);('modality settinginter', 1);('ctcaudiovisual eval modemasked', 1);('audio maskno', 1);('199patch multihead selfattention experiment', 1);('attention patch attention rstaudio encoder stage objective increase themodel efciency simplicity', 1);('grouped', 1);('reduceattention complexity', 1);('long sequences rst encoderstage', 1);('shows impact attention varianton', 1);('model performance complexity', 1);('westart efcient conformer', 1);('theattention mechanism nd', 1);('rst backend', 1);('study audio backend attentionattentiontypegroup', 1);('patch sizelrs2testlrs3testflopsbillioninvrtfregular', 1);('noise robustnesswe', 1);('measure model noise robustness', 1);('various typesof noise', 1);('audiovisual efcient conformerwith', 1);('werevolution', 1);('ao', 1);('vo', 1);('models respect', 1);('multiple signal', 1);('noiseratio snr', 1);('white noise babble noise thenoisex corpus', 1);('nd processing audio andvisual modalities', 1);('speechrecognition robustness respect babble noise', 1);('babble noise training', 1);('previous works', 1);('nd canfurther', 1);('noise robustness test timerobustness', 1);('various types noise', 1);('various types', 1);('audio noise', 1);('sounds andmusic', 1);('counterpart presence variousnoise types conrm hypothesis audiovisual model', 1);('able use visual modality aid speechrecognition audio noise', 1);('present inputsnr dbword', 1);('lrs2 ao lrs2 av lrs2 av lrs2 vo lrs3 ao lrs3av lrs3 av lrs3a babble', 1);('noisesnr dbword', 1);('lrs2 ao lrs2 av lrs2 av lrs2 vo lrs3 ao lrs3av lrs3 av lrs3b', 1);('white noisefigure', 1);('withbabble noise measure noise robustness evaluatingour models presence babble', 1);('22comparison methods', 1);('ourmethod results', 1);('shows thatour audiovisual model', 1);('presence babble noise', 1);('snragainst', 1);('comparison', 1);('babble noisemethod', 1);('comparison petridis', 1);('testwer function', 1);('white noisemethod', 1);('conclusionin', 1);('efcient conformer ctcbased', 1);('architecture processing audio visualmodalities', 1);('ctclosses', 1);('comparable results recentautoregressive lip reading methods', 1);('patchattention simpler efcient attention mechanismto', 1);('attention rst audio encoder stageour', 1);('stateoftheart performance', 1);('andlrs3 test', 1);('exploreother techniques', 1);('noise robustnessof model', 1);('recent lip readingmethods', 1);('various audio noises', 1);('crossmodal distillation pretrainedmodels', 1);('visual frontend network complexity', 1);('recognition performanceand experiment', 1);('rnntransducer', 1);('learning objective', 1);('applicationsacknowledgmentsthis work', 1);('alexander', 1);('vonhumboldt foundation', 1);('avhreferences1 triantafyllos afouras joon', 1);('chung andrew seniororiol vinyals andrew zisserman deep', 1);('audiovisualspeech recognition', 1);('transactions pattern analysisand machine intelligence', 1);('chung andrew zisserman lrs3ted', 1);('largescale dataset visual speech recognition arxiv preprint arxiv180900496', 1);('chung andrew zisserman asr', 1);('crossmodal', 1);('yannis assael brendan shillingford shimon whitesonand nando de freitas lipnet endtoend', 1);('arxiv preprint arxiv161101599', 1);('tom brown benjamin mann nick ryder melanie subbiah jared kaplan prafulla dhariwal arvind neelakantan pranav shyam girish sastry amanda askell', 1);('models fewshot learners', 1);('neural information processing systems', 1);('adrian bulat georgios tzimiropoulos', 1);('alignment problemand adataset', 1);('3d facial landmarks', 1);('proceedingsof ieee', 1);('maxime burchi valentin vielzeuf efcient', 1);('ieee automatic speechrecognition understanding', 1);('chung andrew zisserman lip', 1);('reading thewild', 1);('asian conference computer vision pages 87103springer', 1);('chung', 1);('zisserman lip', 1);('reading prole201710', 1);('ronan collobert', 1);('puhrsch gabriel synnaevewav2letter', 1);('speech recognitionsystem arxiv preprint arxiv160903193', 1);('jiankang deng jia guo evangelos ververas irene kotsia stefanos zafeiriou retinaface singleshot', 1);('linhao dong shuang xu bo xu speechtransformera', 1);('norecurrence sequencetosequence model speechrecognition', 1);('haoqi fan bo xiong karttikeya mangalam yanghao lizhicheng yan jitendra malik christoph feichtenhofer multiscale', 1);('vision transformers', 1);('alex graves santiago fern', 1);('faustino gomez', 1);('schmidhuber connectionist', 1);('sequence data recurrent neuralnetworks', 1);('icml', 1);('alex graves abdelrahman mohamed geoffrey hinton speech', 1);('recurrent neural networksin2013', 1);('international conference acoustics speechand signal processing pages', 1);('anmol gulati james qin chungcheng chiu niki parmar yu zhang jiahui yu wei han shibo wang zhengdong zhang yonghui wu', 1);('conformer convolutionaugmented', 1);('transformer speech recognition arxivpreprint arxiv200508100', 1);('pengcheng guo florian boyer xuankai chang tomokihayashi yosuke higuchi hirofumi inaguma naoyukikamo chenda li daniel garciaromero jiatong shi', 1);('alrecent developments espnet toolkit', 1);('wei han zhengdong zhang yu zhang jiahui yu chungcheng chiu james qin anmol gulati ruoming pang', 1);('wu contextnet improving', 1);('convolutional neuralnetworks', 1);('global context arxiv preprint arxiv200503191', 1);('awni hannun carl case jared casper bryan catanzarogreg diamos erich elsen ryan prenger sanjeev satheeshshubho sengupta adam coates', 1);('deep', 1);('endtoend speech recognition arxiv preprintarxiv14125567', 1);('kaiming xiangyu zhang shaoqing ren jian sundeep', 1);('residual learning image recognition', 1);('kenneth heaeld kenlm faster', 1);('languagemodel queries', 1);('statistical machine translation pages', 1);('pavel izmailov dmitrii podoprikhin timur garipov dmitryvetrov andrew gordon wilson averaging', 1);('generalization arxivpreprint arxiv180305407', 1);('shigeki karita nanxin chen tomoki hayashi takaakihori hirofumi inaguma ziyan jiang masao somekinelson enrique yalta soplin ryuichi yamamoto xiaofeiwang', 1);('comparative study transformer vs rnn inspeech applications', 1);('ieee automatic speech recognition understanding', 1);('diederik p kingma jimmy ba adam', 1);('method forstochastic optimization arxiv preprint arxiv14126980', 1);('samuel kriman stanislav beliaev boris ginsburg jocelyn huang oleksii kuchaiev vitaly lavrukhin ryan learyjason li yang zhang quartznet deep', 1);('automaticspeech recognition 1d timechannel', 1);('separable convolutions', 1);('conferenceon acoustics speech', 1);('taku kudo john richardson sentencepiece', 1);('simple language', 1);('independent subword tokenizer detokenizer neural text processing', 1);('emnlp', 1);('jaesong lee shinji watanabe intermediate', 1);('loss regularization', 1);('acoustics speechand', 1);('ieee202128 jason li vitaly lavrukhin boris ginsburg ryan learyoleksii kuchaiev jonathan cohen huyen nguyen', 1);('teja gadde jasper', 1);('endtoend convolutional neural acoustic model arxiv preprint arxiv190403288', 1);('pingchuan stavros petridis maja pantic endtoend', 1);('audiovisual speech recognition conformersinicassp', 1);('pingchuan stavros petridis maja pantic visualspeech', 1);('multiple languages', 1);('wild arxivpreprint arxiv220213084', 1);('somshubra majumdar jagadeesh balam oleksii hrinchukvitaly lavrukhin vahid noroozi boris ginsburg citrinet closing', 1);('gap nonautoregressive autoregressive endtoend models', 1);('automatic speech recognition arxiv preprint arxiv210401721', 1);('takaki makino hank liao yannis assael brendanshillingford basilio garcia otavio braga olivier siohan recurrent', 1);('neural network transducer audiovisualspeech recognition', 1);('automatic speech recognition understanding workshop', 1);('pages 905912ieee', 1);('jumon nozaki tatsuya komatsu relaxing', 1);('conditional independence assumption', 1);('intermediate predictions arxiv preprintarxiv210402724', 1);('vassil panayotov guoguo chen daniel povey sanjeevkhudanpur librispeech', 1);('asr corpus', 1);('public domain audio books', 1);('international conferenceon acoustics speech signal processing', 1);('daniel', 1);('yu zhang chungcheng chiu youzhengchen bo li william chan quoc v', 1);('yonghui wuspecaugment', 1);('large scale datasets', 1);('stavros petridis themos stafylakis pingchuan georgios tzimiropoulos maja pantic audiovisual', 1);('speechrecognition hybrid ctcattention architecture 2018ieee', 1);('spoken language', 1);('technology workshop', 1);('slt', 1);('kr prajwal triantafyllos afouras andrew zissermansubword', 1);('level lip reading visual attention', 1);('proceedings ieeecvf', 1);('recognition', 1);('prajit ramachandran barret zoph quoc v lesearching', 1);('activation functions arxiv preprintarxiv171005941', 1);('dmitriy serdyuk otavio braga olivier siohan audiovisual', 1);('worth 32x32x8 voxels', 1);('in2021 ieee automatic speech recognition understanding', 1);('brendan shillingford yannis assael matthew', 1);('hoffman thomas paine', 1);('hughes utsav prabhu hankliao hasim sak kanishka rao lorrayne bennett', 1);('allargescale visual speech recognition arxiv preprintarxiv180705162', 1);('chung andrew senior oriol vinyals andrewzisserman lip', 1);('reading sentences', 1);('george sterpu', 1);('saam naomi harte attentionbased', 1);('audiovisual fusion robust', 1);('acm', 1);('multimodal interaction', 1);('andrew varga herman jm steeneken assessment', 1);('forautomatic speech recognition', 1);('ii', 1);('noisex92 database andan experiment study effect additive noise speechrecognition systems', 1);('ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan n gomez', 1);('kaiser illiapolosukhin attention', 1);('neuralinformation processing systems', 1);('bo xu cheng lu yandong guo jacob wang discriminative', 1);('multimodality speech recognition', 1);('computer vision patternrecognition', 1);('jianwei yu shixiong zhang jian wu shahram ghorbanibo wu shiyin kang shansong liu xunying liu helenmeng dong yu audiovisual', 1);('recognition overlappedspeech lrs2 dataset', 1);('qian zhang han lu hasim sak anshuman tripathi erikmcdermott stephen koo shankar kumar transformertransducer', 1);('streamable speech recognition model withtransformer encoders rnnt loss', 1);('xingxuan zhang feng cheng shilin wang spatiotemporal', 1);('convolutional sequence learning forlip reading', 1);('proceedings ieeecvf internationalconference computer vision', 1);('ya zhao rui xu xinchao wang peng hou haihong tangand mingli', 1);('hearing', 1);('improving', 1);('speech recognizers', 1);('proceedings aaai', 1);('articial intelligence', 1);