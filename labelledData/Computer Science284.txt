('assumption', 6);('lipschitz', 4);('nota thomas', 4);('lemma', 3);('tsitsiklis', 3);('future rewards', 3);('corollary', 2);('equation', 2);('theorem', 2);('furthermore', 2);('barto', 2);('mdp', 2);('bertsekas tsitsiklis', 2);('sutton', 2);('reinforcement learning optimization pr oblemarxiv preprint arxiv191002140', 1);('ssutton discounted', 1);('naik roshan shari niko yasui hengshuai yao richa', 1);('press 2018abhishek', 1);('sutton andrew g barto reinforcement learning introduction mit', 1);('internationalconference learning representations', 1);('optimality rates convergence', 1);('global', 1);('po licy gradient methods', 1);('wang qi cai zhuoran yang zhaoran wang neural', 1);('optimization', 1);('siam', 1);('convergence gr adientmethods errors', 1);('p bertsekas john n tsitsiklis gradient', 1);('adv antageestimation arxiv preprint arxiv150602438 2015dimitri', 1);('continuous control', 1);('highdimensional', 1);('schulman philipp moritz sergey levine michael jordan p', 1);('policy gradient gradient arxivpreprint arxiv190607073 2019john', 1);('nota philip thomas', 1);('mismatch ac torcritic algorithms arxiv preprint arxiv201001069 2020chris', 1);('combes', 1);('tachet', 1);('seijen shimon whites', 1);('zhang romain laroche harm', 1);('information processing systems', 1);('advances', 1);('r konda john n tsitsiklis actorcritic', 1);('machine learning', 1);('international conference', 1);('proceedings', 1);('natural actorcritic algorithms', 1);('thomas bias', 1);('pages10571063 2000philip', 1);('advances neural information processing systems', 1);('gradient methods reinforcement learning func tion approximation', 1);('policy', 1);('sutton david mcallester satinder p singh yishay', 1);('practical algorithmsreferencesrichard', 1);('results signicantimplications', 1);('approximation conjunctionwith strategies', 1);('certain minimum rate results helpclarifythe roleofdiscountingin policygradientmethods andprovidea solidtheoretical foundation use', 1);('convergence optimal policy', 1);('approximation especia', 1);('providingareasonablejustication use', 1);('approximation upperboundedbysomenite valueproportionalto1', 1);('3that error', 1);('conclusionswe', 1);('4with parameters psvmaxldandq', 1);('iteration ivextenddoublevextenddoublevextenddoublevextenddoublevextenddoublesummationdisplayssvisdisvextenddoublevextenddoublevextenddoublevextenddoublevextenddouble1isvmaxldisvmaxldthus', 1);('4is satisedwe havefrom', 1);('steepest ascentdirection', 1);('statement thetheorem', 1);('parameterization becausethe', 1);('error termthereforewecanapplytheorem 1ifwecanshowthat therelevantassumptionshold', 1);('jand', 1);('approximation bewritten terms ascent direction respect', 1);('jproofwe', 1);('limit point iis stationary point', 1);('anite value limiji', 1);('jiconverges', 1);('conditions satisedsummationdisplayi0isummationdisplayi02i ic1i 21wherecis', 1);('methodi1iiii 20assume ithe step size i0 discount factor i01 andthe', 1);('letibe', 1);('certain conditionstheorem', 1);('optim alpolicy', 1);('approximation results', 1);('main result shows', 1);('convergence discounted approximationwe', 1);('smaxssvextenddoublevextenddoublevextenddoublevextenddoublevsdsvextenddoublevextenddoublevextenddoublevextenddouble svmaxmaxssvextenddoublevextenddoublevextenddoublevextenddoubledsvextenddoublevextenddoublevextenddoublevextenddoublesvmaxmaxssvextenddoublevextenddoublevextenddoublevextenddoublevextenddouble1t1summationdisplayt1prstsvextenddoublevextenddoublevextenddoublevextenddoublevextenddouble', 1);('error vector esuch vextenddoublevextenddoublevextenddoublevextenddoublevextenddoublesummationdisplayssvsdsvextenddoublevextenddoublevextenddoublevextenddoublevextenddouble1le 19proofwe havevextenddoublevextenddoublevextenddoublevextenddoublevextenddoublesummationdisplayssvsdsvextenddoublevextenddoublevextenddoublevextenddoublevextenddouble', 1);('lefor', 1);('th e errorvector proportional', 1);('18prooffor allsvextenddoublevextenddoublevextenddoublevextenddoublevextenddoublet1summationdisplayt1prstsvextenddoublevextenddoublevextenddoublevextenddoublevextenddoublet1summationdisplayt1vextenddoublevextenddoublevextenddoublevextenddoubleprstsvextenddoublevextenddoublevextenddoublevextenddoublet1summationdisplayt1ltwe everythingwe need', 1);('svextenddoublevextenddoublevextenddoublevextenddoublevextenddoublet1summationdisplayt1prstsvextenddoublevextenddoublevextenddoublevextenddoublevextenddoubleld', 1);('state distribution', 1);('ldfor', 1);('exists nite', 1);('existence f constantboundltat timestep tis sucient purposes havecorollary', 1);('nite tt7the', 1);('ltexists', 1);('s0vextenddoublevextenddoublevextenddoublevextenddoubleprs0s0vextenddoublevextenddoublevextenddoublevextenddouble 0therefore induction', 1);('notice', 1);('ith parameter stiprststisummationdisplayst1ssummationdisplayat1aprst1st1at1st1pstst1at1summationdisplayst1ssummationdisplayat1aparenleftbigat1st1pstst1at1iprst1st1parenrightbigsummationdisplayst1ssummationdisplayat1aparenleftbigprst1st1pstst1at1iat1st1parenrightbigsummationdisplayst1ssummationdisplayat1aiprst1st1summationdisplayst1ssummationdisplayat1iat1st1salt1salif partial derivative respect parameter bounde thepartial derivative', 1);('consider', 1);('lo ose', 1);('exists need show constan exists donot need', 1);('lttherefore', 1);('lt1', 1);('17proofassumethatforagiventimestep forallsvextenddoublevextenddoubleprst1svextenddoublevextenddoublelt1for', 1);('svextenddoublevextenddoublevextenddoublevextenddoubleprstsvextenddoublevextenddoublevextenddoublevextenddoublelt', 1);('thatfor alls', 1);('ltsuch', 1);('allt exists nite', 1);('helpful lemmalemma', 1);('sequence discount factors satisfyassumption', 1);('quantity proportional 1by', 1);('entire expression', 1);('multiplicand boundedthen', 1);('expression becaused0s 0due coecient', 1);('summationdisplayssvsds 1summationdisplayssvst1summationdisplayt1prsts16notice d0s', 1);('error vector contains coecient', 1);('directionsummationdisplayssvsdsbracehtipupleftbracehtipdownrightbracehtipdownleftbracehtipuprighterror vector 156we', 1);('jbracehtipupleftbracehtipdownrightbracehtipdownleftbracehtipuprightascent', 1);('objective ie', 1);('sumof ascent direction', 1);('1and product ruleby', 1);('all01j summationdisplayssdsvssummationdisplayssvsds 14proofthis', 1);('expression havecorollary', 1);('evstvst1this', 1);('bellman', 1);('all01j summationdisplayssdsvs 13proofconsider andt rearrange', 1);('h ow', 1);('form gradient', 1);('summationdisplayssdsvs 11wheredsd0s1t1summationdisplayt1prsts 12we', 1);('true policy gradient b ias anupper', 1);('approximation b e viewedas', 1);('section show', 1);('bias discounted approximationin', 1);('approxima tion ofthe policy gradient ascent direction error term allowin g', 1);('limit point xiisa stationary point fthe goal paper decompose', 1);('fxi elsefxiconverges toa nite value limifxi', 1);('assumptions', 1);('letxibe', 1);('assumptions havetheorem', 1);('magnitude ofthe error vector wi magnitude proportional limitwimust decay', 1);('satisfying positive scalars pandqbadblwibadbl iparenleftbigpqbadblfxibadblparenrightbig 9assumption 4will', 1);('wiis error vector', 1);('satisfying positive scalars c1andc2c1badblfxibadbl2 fxisi badblsibadbl c2badblfxibadbl 8assumption', 1);('siis ascent direction', 1);('ifor index step size tandare', 1);('usetfor index sequence step sizewe', 1);('positive satisessummationdisplayi0isummationdisplayi02i 74bertsekas', 1);('step size iis', 1);('lbadblxxbadbl', 1);('lwe', 1);('dierentiable scalar function onrnsuch', 1);('results te rms ofascent directions policy gradient methods', 1);('ascent descent directions', 1);('sequencei1iij 5wheresij andwiis zero vector convergence results', 1);('convergence properties se quences theform4xi1xiisiwi 4wherexiis parameter vector iis step size siis descent direction forsome objective function fxi andwiis vector errors example directgradient ascent', 1);('r ange settingsin', 1);('strengthening convergence properties gradient descent', 1);('methods', 1);('theconvergence', 1);('3way tradeo bias variance representatio n error23', 1);('terms biasvariance tradeosutton', 1);('objective use discoun', 1);('approximation b', 1);('2019therefore paper view', 1);('naik', 1);('reects thetrue goals practitioners episodic setting wellden', 1);('sutton barto', 1);('reinforcement learning', 1);('app roximation', 1);('incomplete review incorrect uses', 1);('scope pap er3an', 1);('standard neural network architec tures remedyfor theorypractice gap', 1);('convergence proofs policy gradient meth ods', 1);('som etimes2while', 1);('thomas2014however', 1);('2000prior work', 1);('nota', 1);('approximation gradient anyobjective', 1);('thomas2014', 1);('mistaken gradient discountedobjective3ithasbeenshownthatnotonlyisthisassumptionincorrect', 1);('actionvaluefunction expression dene approximation asebracketleftbiggt1summationdisplayt0qstatlnstatvextendsinglevextendsinglevextendsinglevextendsinglevextendsinglebracketrightbigg 3this approximation', 1);('policy gradient substitutes', 1);('sanda alnasl2thediscountedapproximation', 1);('continuous exists som e constantlsuch', 1);('qj ebracketleftbiggt1summationdisplayt0qstatlnstatvextendsinglevextendsinglevextendsinglevextendsinglevextendsinglebracketrightbigg', 1);('jcan', 1);('jsutton', 1);('gradient methods attempt', 1);('policy gradient methodspolicy', 1);('value functions givenbyvandq', 1);('thediscounted', 1);('returns fromtimetare', 1);('relative immediate rewards', 1);('scalar value decreases importanceof', 1);('ebracketleftbiggt1summationdisplayitrivextendsinglevextendsinglevextendsinglevextendsinglevextendsinglestbracketrightbigg qstat ebracketleftbiggt1summationdisplayitrivextendsinglevextendsinglevextendsinglevextendsinglevextendsinglestatbracketrightbiggthediscount', 1);('policy aregiven byvst', 1);('returns stateaction pair', 1);('qfunctionq', 1);('policy actionvalue function', 1);('returns startingin aparticular2state', 1);('thestatevalue', 1);('terminal state rewardis', 1);('stis', 1);('ebracketleftbiggt1summationdisplayt0rtvextendsinglevextendsinglevextendsinglevextendsinglevextendsinglebracketrightbigg', 1);('jgiven', 1);('agent objective agent nd parameterswhich optimize function', 1);('policy vector parameters', 1);('atstis', 1);('practical applicationsactionsareselectedbytheagentaccordingtoa policy suchthat', 1);('rewards episodic setting thedefault choice', 1);('tand', 1);('end episode agen stuckin state time', 1);('special state', 1);('simplifythe mathematical treatment', 1);('rtrstatst1', 1);('action transitions', 1);('st', 1);('d0 timestep tthe agent observes state', 1);('s0', 1);('initial state', 1);('foreach', 1);('anepisodebegins', 1);('initial statedistribution', 1);('rmaxr', 1);('maximum reward', 1);('transition boundedby', 1);('distribution rewards', 1);('rs drmaxrmax', 1);('current state action', 1);('atransition function determines probability distributionover', 1);('psa ds', 1);('actionsavailable agent', 1);('ais', 1);('possiblestatesof environment', 1);('sis', 1);('saprd', 1);('decision processmdp', 1);('markov', 1);('background21 notationin rl', 1);('step ize theresultingpolicy gradientmethod convergeto locallyoptimal policy', 1);('increasing1 rate', 1);('standard results convergence gradientmethods errors', 1);('form magnitude upperboundedbyavalueproportionalto1 where01isthediscount factor show', 1);('weshow', 1);('object ive', 1);('approximation po licy gradientcan', 1);('optimal policyin paper show', 1);('approximation gradient anyobjectivefunction', 1);('schulman', 1);('approximation policy gradient1it been1the', 1);('thomas2014notaand thomas', 1);('policy gradient algorithms', 1);('thatthe gradient', 1);('gra dient ofa', 1);('policy gradient', 1);('additional benet', 1);('thomas2014 frequently', 1);('sum futurerewards approach reduces variance estimators introducesbias', 1);('common approach', 1);('high variance sum', 1);('estimators gradient c suerfrom high variance', 1);('unbiased', 1);('stochastic estimator gener', 1);('sum rewards', 1);('gradient objective function', 1);('performance ofan agentspolicyby', 1);('algorithmsthat attempt', 1);('rl', 1);('gradient methods class reinforcement learning', 1);('introductionpolicy', 1);('standard g uarantees ofgradient ascent', 1);('method recovers', 1);('thatthe discount factor', 1);('approximation follo', 1);('convergence behavior prope rties thispaper show', 1);('pproximation policy gradient gradient objecti functionlittle', 1);('th e discountedapproximation', 1);('approximation policy gradient', 1);('popular policy gradient methods reinforcement lea', 1);('policy gradient methodschris notajanuary', 1);('convergence', 1);('dec', 1);('arxiv221214066v1 cslg', 1);