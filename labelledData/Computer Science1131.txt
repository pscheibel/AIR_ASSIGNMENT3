('reinforce', 22);('sample trajectories', 4);('return g\x12', 4);('comparison', 4);('baseline algorithm', 4);('openai', 3);('reinforcealgorithm', 3);('lunar lander', 3);('cartpole', 3);('shallow network', 3);('comparing', 3);('backward curriculum', 3);('training process', 2);('lunar landerv2', 2);('reverse learning', 2);('markov', 2);('\x19\x12atjstteqepisodelengthfort 1tott\x001docompute', 2);('\x19\x12atjstteqepisodelengthfortt\x001tot 1docompute', 2);('g\x00vrlog\x19\x12atjstupdateoptimizerend', 2);('cart', 2);('cart pole', 2);('original algorithm', 2);('curriculum reinforcement learningfigure', 2);('comparison reinforce reinforcewith', 2);('kaya', 2);('advances', 2);('backward curriculum reinforcement learningkyung min ko1sajad khodadadian2siva theja maguluri3abstractthe', 1);('current reinforcement learning algorithm', 1);('trajectories train agentthe', 1);('agentlittle guidance agent explore muchas', 1);('possible appreciation reinforcement learning', 1);('sample efciencythe', 1);('important factor thatdecides performance algorithm', 1);('pasttasks', 1);('techniques changingthe structure network increase sampleefciency', 1);('work proposenovel reverse curriculum reinforcement learningreverse curriculum learning', 1);('training theagent', 1);('backward trajectory episoderather', 1);('thisgives', 1);('strong reward signal theagent', 1);('sampleefcient manner', 1);('minorchange algorithm', 1);('orderof trajectory training agent', 1);('thereforeit', 1);('stateofart algorithms1', 1);('introductionin', 1);('recent years development', 1);('gpu', 1);('tremendous advances', 1);('learning result reinforcement learning', 1);('reinforcement', 1);('rl', 1);('powerful technique trains agent maximize reward gainso agent', 1);('problem aremany applications reinforcement learning usedsuch robotic manipulation tasks', 1);('li', 1);('atarigamesmnih', 1);('multiagent systems', 1);('hwang', 1);('alpha', 1);('reinforcement learnings', 1);('natural reward function usuallysparse', 1);('agent theagent', 1);('conventional reinforcement learning problems', 1);('agent blindto goal task property', 1);('optimal policy', 1);('guidance thehuman', 1);('large amount computationalpower samples learnin work introduce novel reverse curriculum reinforcement learning uses trajectories agent inreverse order train agent', 1);('goal thetask result agents', 1);('sample efcient way', 1);('strong reward signals', 1);('method need modify theneural network structures', 1);('previousknowledge train agent', 1);('reverse learning canbe', 1);('stateoftheart algorithms suchas', 1);('pposchulman', 1);('a3csewak', 1);('simple stepsfirst', 1);('trajectory agent', 1);('usual ipthe order trajectory episode train agentour reverse learning', 1);('changes order trainingprocess train', 1);('ourreverse learning method', 1);('reinforcesutton', 1);('a2ckonda tsitsiklis', 1);('algorithms oncartpolev1', 1);('discreteaction spaces', 1);('effectsof return normalization techniqueschaul', 1);('2021variation learning rate structure neural networkon performance reverse learning2', 1);('related worksthere', 1);('havesimilarities reverse learning method', 1);('imitation learningciosek', 1);('demonstrationof experts', 1);('goalof task', 1);('approach use samples', 1);('train agents', 1);('imitation learninglimits agents', 1);('advantage exploration', 1);('moreoverit', 1);('lots work', 1);('need nd experts toperform', 1);('task record process trainour agent', 1);('reverse learning doesnt need extrawork need reverse trajectory trainin case', 1);('backward trajectory anexpert imitation learning', 1);('curriculum', 1);('learning whichmodies schedule learning process aparxiv221214214v1 csai', 1);('dec', 1);('curriculum reinforcement learningplied', 1);('various machine learning tasks', 1);('basic idea ofcurriculum learning training', 1);('example rstthen', 1);('level difculty solvethe problemnarvekar', 1);('reverse learning training agent', 1);('end theepisode', 1);('episode followingthe concept curriculum learning', 1);('complete task', 1);('goal state ratherthan', 1);('previously', 1);('curriculum learning', 1);('ball intoa goal', 1);('asada', 1);('similar work proposedby', 1);('barnes', 1);('solves difcult robotic problemsbaranes', 1);('oudeyer', 1);('full task space limits application variousproblems', 1);('tovarious machine learning tasks', 1);('method doesntrequire', 1);('previous knowledge train agent3', 1);('preliminariesin', 1);('nite discrete time horizon', 1);('decision process', 1);('mdp', 1);('msapr', 1);('\x1a 0t dene', 1);('sas', 1);('aas', 1);('anaction setps\x02a\x02sris transition probability ris', 1);('reward function', 1);('reward factor\x1a0is', 1);('initial state distribution', 1);('tis', 1);('decision processmdpat time step agent', 1);('transition probability', 1);('pthe', 1);('goal reinforcement learning nd optimalpolicy\x19\x12atjstthat maximizes reward gain workwe reverse order agents trajectory', 1);('tand', 1);('ittb use train agent goal state sgto initialstates0', 1);('strong reward signal whichgives meaningful guidance goal4', 1);('reverse curriculum learning41 reverse curriculum learning reinforcereinforce', 1);('basic policy gradient algorithms', 1);('stateofart policy gradientalgorithms rst', 1);('reverse curriculum learningon method', 1);('basic policy gradient algorithmwhich', 1);('simple structure uses', 1);('returns log probability', 1);('action tocompute gradient', 1);('sparse reward function lowers performance', 1);('reverse', 1);('curriculum learningcan', 1);('updatingits gradient end episode', 1);('natural sparse rewardfunction', 1);('strong reward signal', 1);('algorithm', 1);('sample trajectory', 1);('policy network', 1);('ip orderof episode compute loss function reverse orderthe major difference', 1);('reverses order episode', 1);('reinforcecollect', 1);('\x12oldgrlog\x19\x12atjstupdateoptimizerend foralgorithm', 1);('backward curriculum reinforcecollect', 1);('\x12oldgrlog\x19\x12atjstupdateoptimizerend for42', 1);('reverse curriculum learning reinforcewith baselinethe', 1);('main problem', 1);('high variance', 1);('divergence ofpolicy network parameters', 1);('common way', 1);('thevariance subtract baseline', 1);('weaver tao', 1);('inthe reinforce', 1);('algorithm value function', 1);('appropriate baseline subtract return', 1);('g agent', 1);('bothpolicy network value network loss function willbe\x12 \x12oldg\x00vrlog\x19\x12atjst baseline willdecrease variance', 1);('step size gradientin reverse', 1);('baseline algorithmwe rst', 1);('policy ip theorder episode compute loss function shownon algorithm 4algorithm', 1);('reinforce baselinecollect', 1);('curriculum reinforcement learningalgorithm', 1);('backward reinforce baselinecollect', 1);('experimental resultwe', 1);('reinforcement learning environment', 1);('openai gym brockman', 1);('test performanceof backward curriculum learning algorithm onreinforce', 1);('baseline algorithmsfor experiments', 1);('environments default structure network isa multilayer perceptron network', 1);('layers 128neurons baseline', 1);('effect ofreturn normalization', 1);('structure ofnetworks51', 1);('cart pole environmentin', 1);('section rst', 1);('backward curriculumalgorithm', 1);('algorithm goal thecartpolev1 algorithm balance pendulum cartfrom', 1);('discrete action domain theforce', 1);('cart environment', 1);('fallingdown pole cartthe comparison backward curriculum algorithm andoriginal', 1);('conspicuous difference thatpoint backward curriculum algorithm', 1);('original algorithm solves environmentwithin', 1);('baseline algorithm analyze effect backward curriculum learning algorithm gure', 1);('backward curriculum algorithm', 1);('baseline algorithmnot', 1);('high variancethat disturbs agent', 1);('goal statewe', 1);('baseline thereinforce algorithm', 1);('high varianceproblem', 1);('performance algorithm', 1);('pole environment', 1);('gure 3figure', 1);('cart poleenvironment52 lunar lander environmentthe', 1);('test algorithms isthe', 1);('lunarlanderv2', 1);('thegoal', 1);('landdown goal region sky', 1);('downthe agent action space', 1);('right main engines agentreaches', 1);('section comparedthe performance backward curriculum algorithm andthe', 1);('experimentedwith effect return normalizationas', 1);('algorithmreaches goal state average', 1);('scores within2000 episodes', 1);('solves environmentwithin', 1);('goal environment', 1);('ofthe training process', 1);('agent reachthe goal', 1);('thereinforce baseline', 1);('algorithm nishes task', 1);('reverse method', 1);('comparedthe performance reverse', 1);('baseline gure', 1);('observe ingure', 1);('goal score butreverse curriculum learning', 1);('variance uses', 1);('return normalizationeven', 1);('sample efciency', 1);('problemwith high variance', 1);('originalreinforce loss function \x12 \x12oldgrlog\x19\x12atjstthe step size gradient', 1);('gand', 1);('loglog\x19\x12atjst log action probability', 1);('acceptable return high magnitude', 1);('increase step size gradient', 1);('thechoice', 1);('step size', 1);('crucial reinforcement learning sincea', 1);('small step size slows convergence rate whilea', 1);('large step size', 1);('oscillations divergence ofpolicy networks', 1);('due overshootingpirotta', 1);('return normalization methodto stabilize high variance', 1);('basic idea return normalization making', 1);('scale magnitude return toavoid', 1);('problems subtractingthe average return', 1);('standard deviation return', 1);('equation 1schaul', 1);('2021as result', 1);('return normalization', 1);('stable increasethe performance algorithm', 1);('original algorithm backward curriculum algorithm', 1);('lunar landerenvironmentas', 1);('observe gure', 1);('return normalization learning curve high variance someof learning rates result divergence', 1);('applyingreturn normalization', 1);('variance signicantlyand agent learning rate', 1);('lunar landerenvironmentfigure', 1);('environmentdemonstrates return normalization optimizedthe performance', 1);('variance shownin gure 9returnnorm return\x00averagereturnstandarddeviationof return1figure', 1);('return normalization backward curriculum', 1);('reinforce lunar lander', 1);('comparison deep shallow networkthe', 1);('depth network', 1);('important factorsthat', 1);('performance algorithm general adeep neural network', 1);('computation power', 1);('number layers yield parametersto compute gradient', 1);('shallow network thebenet', 1);('quick computation', 1);('layers shallow', 1);('networks havedifferent benets', 1);('simple environments', 1);('simple environment agentcan', 1);('simple heuristics', 1);('computation efforts', 1);('effect', 1);('return normalization backward curriculumreinforce', 1);('usingreturn normalizationdeeper networks', 1);('simple environment maylead', 1);('causes computational waste', 1);('performance thecartpole environment', 1);('networksour baseline network structure', 1);('baseline network deepernetork', 1);('baseline originalreinforce baseline method observe difference', 1);('networks result', 1);('backward curriculum method didntimprove performance', 1);('network networkalready', 1);('accurate approximation', 1);('agentstart training process', 1);('goal thetask', 1);('effect performance', 1);('performance shallow', 1);('gure11 agent shallow networks', 1);('episode samples agent deepernetworks', 1);('conclude backward', 1);('conclusionin', 1);('novel reverse curriculum reinforcement learning addresses', 1);('natural sparse rewardfunction problem method', 1);('reverses order ofthe episode', 1);('training process willlet agent', 1);('goal task beginningfigure', 1);('backward curriculum learning originalreinforce baseline', 1);('deep shallow networktherefore replaces', 1);('natural sparse reward functionwith', 1);('strong reward signal optimize sampleefciency', 1);('method reversecurriculum learning doesnt', 1);('steps modify structure code', 1);('current stateofart algorithms', 1);('reinforce reinforce', 1);('baseline algorithm testour method', 1);('baseline stateofart algorithmswe', 1);('reverse curriculum learning usesfewer samples', 1);('testedthe effect return normalization depth networkwe', 1);('reverse curriculum learning', 1);('simple tasks', 1);('future work', 1);('current stateofart algorithms test variousenvironments7', 1);('acknowledgementthis', 1);('georgia techs', 1);('summer undergraduate engineering program', 1);('sure', 1);('part ofthe', 1);('national science foundation', 1);('nsfreferencesasada noda tawaratsumida hosoda kpurposive', 1);('behavior acquisition', 1);('real robot', 1);('reinforcement learning', 1);('machine', 1);('oudeyer py active', 1);('learning inversemodels', 1);('goal exploration inrobots', 1);('robotics autonomous systems', 1);('g cheung v pettersson', 1);('schneider jbackward curriculum reinforcement learningschulman j tang j zaremba', 1);('gymarxiv preprint arxiv160601540 2016ciosek', 1);('k imitation', 1);('learning reinforcement learningarxiv preprint arxiv210804763 2021haarnoja', 1);('zhou abbeel p levine softactorcritic offpolicy', 1);('maximum entropy', 1);('reinforcement learning stochastic actor', 1);('internationalconference', 1);('machine learning pp', 1);('pmlr2018holcomb porter', 1);('k ault v mao g', 1);('j overview', 1);('deepmind alphago zero aiinproceedings', 1);('international conference onbig data education pp', 1);('ks tan', 1);('chen cc cooperativestrategy', 1);('robot soccersystems', 1);('ieee transactions fuzzy systems', 1);('hong dumitras shallowdeep', 1);('understanding', 1);('international conference machine learning pp33013310', 1);('pmlr', 1);('v tsitsiklis j actorcritic', 1);('neural information processing systems 121999li', 1);('th su lai', 1);('hu jj walkingmotion', 1);('generation synthesis control', 1);('pgrl lpi fuzzy logic', 1);('ieee transactions', 1);('onsystems man', 1);('cybernetics part', 1);('cybernetics', 1);('v kavukcuoglu k silver graves aantonoglou wierstra riedmiller playingatari', 1);('deep reinforcement learning arxiv preprintarxiv13125602 2013narvekar', 1);('peng', 1);('leonetti sinapov j taylorm e stone p curriculum', 1);('learning reinforcement learning domains framework survey arxivpreprint arxiv200304960 2020pirotta', 1);('restelli bascetta', 1);('adaptive', 1);('stepsize policy gradient methods', 1);('advances neuralinformation processing systems', 1);('ostrovski g kemaev borsa returnbased', 1);('normalisation trick', 1);('rlarxiv preprint arxiv210505347 2021schulman', 1);('j wolski', 1);('dhariwal p radford', 1);('proximal', 1);('policy optimization algorithmsarxiv preprint arxiv170706347 2017sewak', 1);('actorcritic', 1);('models a3c', 1);('deep reinforcement learning', 1);('springer', 1);('r mcallester singh mansour policy', 1);('gradient methods reinforcement learning withfunction approximation', 1);('1999weaver l', 1);('tao n', 1);('optimal reward baseline', 1);('reinforcement learning arxiv preprintarxiv13012315', 1);