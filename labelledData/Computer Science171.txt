('v\x19 p', 95);('mdps', 79);('1\x00 t', 74);('theorem', 57);('robust average-reward', 35);('bellman', 35);('1\x00 t+1', 33);('v\x03 p', 31);('optimal policy', 22);('vt', 22);('blackwell', 21);('algorithm', 21);('s\x03 t', 21);('value function', 20);('proc', 20);('mdp', 19);('xu', 17);('puterman', 16);('nilim', 16);('el ghaoui', 16);('mannor', 16);('p\x19', 16);('pt+1', 16);('iyengar', 15);('tewari', 14);('bartlett', 14);('hence', 14);('s.', 14);('p\x19 v', 14);('markov', 13);('decision processes', 13);('\x19\x141x t=0', 13);('a.', 13);('lemma', 13);('v\x19 pt+1', 13);('discount factor', 12);('machine learning', 12);('t+1\x00 t', 12);('international conference', 11);('pmlr', 11);('optimal robust policy', 10);('zhang', 10);('+ t\x1bpas', 10);('h.', 10);('average-reward setting', 9);('uniform convergence', 9);('p.', 9);('pa', 9);('rvi', 9);('l.', 9);('=x a\x19', 9);('transition kernel', 8);('lim', 8);('based', 8);('icml', 8);('p2pep', 8);('s1', 8);('h\x19 p', 8);('restatement', 8);('value function converges', 7);('robust setting', 7);('wiesemann', 7);('general uncertainty', 7);('robust', 7);('relative value function', 7);('stationary model', 7);('bagnell', 6);('kalathil', 6);('wang', 6);('non-robust setting', 6);('p1', 6);('policy \x19', 6);('\x00 r', 6);('vt+1', 6);('> \x0e', 6);('figure', 6);('m.', 6);('d.', 6);('neural information processing systems', 6);('z.', 6);('p\x19 v\x00i', 6);('i+p\x19 v+', 6);('lv', 6);('zou', 5);('zhou', 5);('stationary policy \x19', 5);('pa s', 5);('rst show', 5);('pis', 5);('assumption', 5);('hong', 5);('= min', 5);('robust vi', 5);('v0', 5);('r.', 5);('robust reinforcement learning', 5);('advances', 5);('w.', 5);('+x a\x19', 5);('pt+1+', 5);('worst-case average reward', 4);('derman', 4);('cumulative reward', 4);('ng', 4);('schneider', 4);('articial intelligence', 4);('ho', 4);('petrik', 4);('robust value function', 4);('stationary policies', 4);('average-reward criterion', 4);('robust average- reward', 4);('robust policy evaluation', 4);('limit method', 4);('robust optimal control problem', 4);('hordijk', 4);('yushkevich', 4);('rational function', 4);('direct approach', 4);('e\x19', 4);('average-reward value function', 4);('p=', 4);('p0', 4);('+ \x1bpas', 4);('support function', 4);('limit approach', 4);('kl-divergence', 4);('specically', 4);('worst-case transition kernels', 4);('t\x001do', 4);('worst-case average-reward', 4);('denote', 4);('min \x142n t\x150pe\x14', 4);('luo', 4);('sutton', 4);('consider', 4);('k.', 4);('j.', 4);('reinforcement learning', 4);('nips', 4);('\x19 [', 4);('s\x19', 4);('n\x121\x00 \x13n', 4);('k\x14lj 1\x00 2j', 4);('\x00s\x19 n', 4);('v\x19\x03', 4);('v\x19j p', 4);('s0=s\x15', 4);('\x15 = min', 4);('\x19\x141x t=1', 4);('+\x1bpa \x13s', 4);('u \x13s', 4);('worst-case performance', 3);('model mismatch', 3);('robust dis-', 3);('satia', 3);('lave jr', 3);('kuhn', 3);('rustem', 3);('tamar', 3);('autef', 3);('yu', 3);('pokutta', 3);('badrinath', 3);('tessler', 3);('efroni', 3);('panaganti', 3);('goyal', 3);('grand-clement', 3);('kaufman', 3);('schaefer', 3);('si', 3);('levine', 3);('nite interval uncertainty', 3);('transition kernels', 3);('certain rate', 3);('rl', 3);('average- reward', 3);('[ \x0e', 3);('p=fpa', 3);('min \x142n t\x150plim n', 3);('robust value iteration', 3);('similar idea', 3);('hu', 3);('robust policy evaluation problem', 3);('robust average-reward g\x19', 3);('time step', 3);('optimal robust average-reward g\x03', 3);('together', 3);('early rewards', 3);('relative value iteration', 3);('average-reward robust', 3);('jain', 3);('bertsekas', 3);('contamination model', 3);('kl', 3);('experimental results', 3);('robust rvi', 3);('comparison', 3);('r=', 3);('learning', 3);('learning representations', 3);('iclr', 3);('in-', 3);('ternational conference', 3);('g.', 3);('maxa\x08 r', 3);('recall', 3);('p2plim', 3);('v\x19 p\x19', 3);('s\x19 n', 3);('exists \x0e0', 3);('v\x19 pt', 3);('t > \x0e', 3);('\x00 tf\x19', 3);('similarly', 3);('pk1', 3);('lim t', 3);('v\x19\x03 p', 3);('according', 3);('v\x19 p=', 3);('\x19\x14 lim n', 3);('pg', 3);('= min \x142n t\x150pe\x14', 3);('i+p', 3);('lv\x00lu', 3);('+\x1bpav \x13s', 3);('intermediate step', 2);('introduction', 2);('decision process', 2);('solving', 2);('worst-case perfor- mance', 2);('fundamental characterizations', 2);('roy', 2);('yang', 2);('russel', 2);('benosman', 2);('van baar', 2);('geist', 2);('eysenbach', 2);('kober', 2);('peters', 2);('stochastic process', 2);('atia', 2);('specic nite interval uncertainty', 2);('robust average-reward value function', 2);('uniform convergence result', 2);('key importance', 2);('design algorithms', 2);('optimal control', 2);('technically', 2);('nite number', 2);('optimal robust value function', 2);('bell-', 2);('vinitsky', 2);('pinto', 2);('hou', 2);('rajeswaran', 2);('huang', 2);('kos', 2);('lin', 2);('pattanaik', 2);('mandlekar', 2);('wherepa sis', 2);('time step t', 2);('stationary policy', 2);('different', 2);('steady-state distribution', 2);('lim n', 2);('npn\x001 t=0', 2);('envi- ronment transits', 2);('p2p', 2);('1x t=0 trtjs0=s #', 2);('nn\x001x t=0rtjs0=s #', 2);('unique xed-point', 2);('appendix', 2);('optimal robust average-reward', 2);('robust average-reward mdps', 2);('discount factor approaches', 2);('algorithms', 2);('convergence lim', 2);('nite interval', 2);('rahimian', 2);('bayraksan', 2);('de-mello', 2);('pfor', 2);('pcan', 2);('current estimate', 2);('t t+1 t+2', 2);('optimal', 2);('past', 2);('.there exists', 2);('< \x0e <', 2);('arg max\x192\x05dg\x19', 2);('+g=x a\x19', 2);('max a\x08 r', 2);('= arg max a\x08 r', 2);('wei', 2);('ross', 2);('chen', 2);('wan', 2);('naik', 2);('proposition', 2);('kullback-lerbler', 2);('different uncertainty', 2);('archibald', 2);('mckinnon', 2);('thomas', 2);('nominal transition kernel', 2);('total variation', 2);('total variation distance', 2);('dtv', 2);('qjjpa s', 2);('total variation model', 2);('dkl', 2);('robust version', 2);('i.', 2);('ma', 2);('c.', 2);('markovian', 2);('e.', 2);('neurips', 2);('b.', 2);('robust markov', 2);('c. p.', 2);('policy iteration', 2);('springer', 2);('n.', 2);('abbeel', 2);('adversarial', 2);('math', 2);('operations', 2);('robotics', 2);('s. h.', 2);('reinforcement', 2);('ad-', 2);('liu', 2);('complexity', 2);('rudin', 2);('model uncertainty', 2);('blanchet', 2);('sigaud', 2);('buffet', 2);('r. s.', 2);('barto', 2);('mdps input', 2);('ep', 2);('[ \x01 ]', 2);('nn\x001x t=0rt', 2);('s0=s', 2);('p+\x1bpas', 2);('\x19\x14x a\x19', 2);('js0=s\x15 =x a\x19', 2);('js ]', 2);('=1 1\x00 g\x19', 2);('p+h\x19 p+f\x19 p', 2);('n\x10 1\x00 \x11n', 2);('clearly', 2);('lj', 2);('k+ks\x19 n', 2);('p+', 2);('puniformly', 2);('y0max xf', 2);('arg maxxf', 2);('such that8jy\x00y0j < \x0e0', 2);('p2pg\x19 p', 2);('= lim', 2);('\x13 =j', 2);('t+1k1+ t\x01t', 2);('k1+k t+1f\x19', 2);('+k t+1f\x19', 2);('k1 \x14h', 2);('pk1\x14kvt\x00', 2);('pk1=\x01t+k', 2);('p=g\x19\x03', 2);('> \x0e0', 2);('pj', 2);('< \x0f', 2);('v\x19 pis', 2);('1nx t=0', 2);('rn\x00ng\x19 p+ng\x19 p\x00ng\x19 p', 2);('v\x19 pcan', 2);('js1=s0\x159 =', 2);('+ min', 2);('part', 2);('state transition matrix', 2);('g 1\x15', 2);('summing', 2);('r\x19+1 n', 2);('r +', 2);('arg maxa', 2);('v \x13s', 2);('> v\x00', 2);('v \x12s', 2);('robust average-reward markov decision processes yue wang,1alvaro velasquez,2george atia,3ashley prater-bennette,4shaofeng zou1', 1);('buffalo', 1);('state university', 1);('york', 1);('innovation ofce', 1);('advanced', 1);('projects', 1);('agency 3university', 1);('florida', 1);('4air force research', 1);('laboratory', 1);('ywang294 @ buffalo.com', 1);('alvaro.velasquez @ darpa.mil', 1);('george.atia @ ucf.edu', 1);('ashley.prater-bennette @ us.af.mil', 1);('szou3 @ buffalo.edu', 1);('abstract', 1);('approximates average-reward', 1);('in- vestigate robust average-reward', 1);('relative value iteration algorithm', 1);('effective mathemat- ical tool', 1);('sequential decision-making', 1);('stochastic envi- ronments', 1);('various factors', 1);('exogenous perturbation', 1);('partial observability', 1);('adversarial attacks', 1);('solution policies', 1);('poor performance', 1);('noteworthy efforts', 1);('copyright', 1);('advancement', 1);('rights reserved.solution', 1);('robust mdp', 1);('different reward op- timality criteria', 1);('agent interacts', 1);('re- ward', 1);('reward formulation', 1);('practical model-free setting', 1);('cen- troid', 1);('large/continuous state', 1);('action spaces', 1);('op- erator', 1);('poor long-term performance', 1);('inventory management', 1);('communication networks', 1);('long-term aver-arxiv:2301.00858v1 [ cs.lg ]', 1);('jan', 1);('2023age performance', 1);('average-reward crite- rion', 1);('compared', 1);('such in- tricacy', 1);('one-to-one correspondence', 1);('limit points', 1);('state-action frequen- cies', 1);('special cases', 1);('necessary conditions', 1);('average- reward settings', 1);('graph structure', 1);('discounted-reward setting', 1);('heretofore', 1);('rst work', 1);('considers robust average-reward', 1);('un- der', 1);('` 1uncertainty', 1);('challenges', 1);('provable theoretical', 1);('previous work', 1);('non-robust average- reward', 1);('main contributions', 1);('standard non-robust setting', 1);('specic tran- sition kernel', 1);('non-robust value function con- verges', 1);('average-reward non-robust value function', 1);('possible transition kernels', 1);('previous point-wise convergence result', 1);('special structure', 1);('possible worst-case transi- tion kernels', 1);('value func- tion converges', 1);('non-robust value function', 1);('average-reward w.r.t', 1);('conver- gence', 1);('motivateour algorithm design', 1);('approximate robust average-reward', 1);('average reward setting', 1);('black-', 1);('value functions', 1);('novel proof', 1);('motivated', 1);('relative value iteration method', 1);('intermedi- ate step', 1);('interme- diate step', 1);('robust rel- ative value function', 1);('greedy approach', 1);('robust value iteration method', 1);('man equation', 1);('related', 1);('model-based', 1);('stud- ies', 1);('model-free setting', 1);('stochas-tic samples', 1);('online fashion', 1);('ofine fashion', 1);('empirical studies', 1);('abdullah', 1);('studies', 1);('specic nite interval uncer- tainty', 1);('\x0e2 [', 1);('optimal robust policy exists', 1);('model-free algorithm', 1);('specic ` 1-norm uncertainty', 1);('` 1-norm uncertainty', 1);('various types', 1);('preliminaries', 1);('problem model', 1);('discounted mdps', 1);('state space s', 1);('action space', 1);('transi- tion kernel', 1);('supon', 1);('withpa s', 1);('reward function r', 1);('s\x02a', 1);('state st', 1);('reward sig- nalr', 1);('afor', 1);('state s', 1);('action aat state swith probability \x19', 1);('-dimensional probability simplex', 1);('s.expected', 1);('following policy \x19', 1);('t=0 trtjs0=s ]', 1);('average-reward mdps', 1);('specic transition kernel', 1);('p\x141', 1);('nn\x001x t=0rtjs0=s\x15', 1);('transition matrix', 1);('reward func- tion', 1);('andp\x19 \x03', 1);('limit matrix', 1);('p\x141x', 1);('cumulative difference', 1);('average value g\x19', 1);('v\x19 p=h\x19 pr\x19', 1);('i\x00p\x19+ p\x19', 1);('i\x00p\x19', 1);('deviation matrix', 1);('rel- ative value functions', 1);('=e\x19\x14 r', 1);('+x s02spa s', 1);('arbitrary transition kernel', 1);('rectangular uncertainty', 1);('iyen-', 1);('p=n', 1);('apa s', 1);('-rectangular uncertainty', 1);('s-rectangular uncertainty', 1);('policy \x19for', 1);('min \x142n t\x150pe\x19', 1);('2n t\x150p.in', 1);('worst-case average- reward', 1);('robust average- reward value function', 1);('t\x19v', 1);('minp2pasp >', 1);('vis', 1);('vonpa', 1);('t\x19', 1);('such contraction result', 1);('max \x192\x05g\x19', 1);('robust case', 1);('algorithms converge', 1);('optimal solutions', 1);('set \x05\x02p', 1);('mild assumption', 1);('compact subset', 1);('as-', 1);('standard uncertainty', 1);('sat- isfy', 1);('huber', 1);('total- variation', 1);('policy \x19and', 1);('such point-wise convergence', 1);('robust value function measures', 1);('uniform', 1);('1and min', 1);('similar convergence result', 1);('special uncertainty', 1);('general com-', 1);('proof technique', 1);('p2mv\x19 p', 1);('m\x12p', 1);('limand min', 1);('demon- strate', 1);('convergence result', 1);('ba- sic idea', 1);('arbitrary initialization', 1);('vi', 1);('general compact uncertainty', 1);('discount factor tis', 1);('tot+1 t+2', 1);('subsequently', 1);('operator w.r.t discount factor tis', 1);('vtof', 1);('pby theorem', 1);('policy evaluation input', 1);('robust average reward', 1);('p. theorem', 1);('great practical importance', 1);('max- imizes', 1);('input', 1);('max a2a\x08', 1);('arg maxa2a\x08', 1);('discount fac- tor tis', 1);('one-step robust', 1);('theorem establishes thatvtin', 1);('performance un- der', 1);('+\x01\x01\x01and0 +', 1);('towards', 1);('sensitive term', 1);('1is optimal', 1);('intuitively', 1);('following', 1);('robust average- reward setting', 1);('optimal policies', 1);('p=g\x03 pg', 1);('any\x0e < < 1,9\x19\x032\x05\x03', 1);('result implies', 1);('sec- tion', 1);('additional advantage', 1);('average reward', 1);('steady state', 1);('contra- diction', 1);('difference function', 1);('policies \x19and\x17', 1);('similar technique', 1);('nite num- ber', 1);('difference functionf\x19', 1);('uniform conver- gence', 1);('approximate average-reward', 1);('inter- mediate steps', 1);('rst generalize', 1);('relative value function measures', 1);('similar structure', 1);('fundamental difference', 1);('average-reward ones', 1);('sharp contrast', 1);('man operator', 1);('problem settings', 1);('de- rive', 1);('equivalent optimality condition', 1);('greedy policy\x19\x03is', 1);('the\x19in eq', 1);('greedy policy', 1);('algo-', 1);('algo- rithm converges', 1);('all-ones vector', 1);('span semi-norm', 1);('= maxsw', 1);('solves thealgorithm', 1);('robust rvi input', 1);('\x0fand arbitrary s\x032s', 1);('v0\x00v0', 1);('certain class', 1);('unichain setting', 1);('major technical novelty', 1);('s0 >', 1);('the-', 1);('satises w', 1);('+ max afr', 1);('+\x1bpa s\x03', 1);('g = max afr', 1);('remark', 1);('con- vergence', 1);('expectation w.r.t.\x19', 1);('convergence results', 1);('examples', 1);('numerical', 1);('support function\x1bpas', 1);('various uncertainty', 1);('value itera- tion methods v.s', 1);('value iteration methodon', 1);('garnet', 1);('uniform distribu- tion', 1);('reward functions r', 1);('a\x18uniform [', 1);('nominal transition ker- nel', 1);('different algorithms', 1);('value iteration', 1);('greedy policies', 1);('robust average- reward policy evaluation', 1);('contamination', 1);('pa s+rp0', 1);('adversarial model', 1);('nominal transition kernel p', 1);('probability 1\x00r', 1);('arbitrary kernel p0with probability', 1);('problem \x1bpas', 1);('v+rminsv', 1);('distance metric', 1);('dif- ference', 1);('to- tal variation', 1);('distributions pandqis', 1);('=1 2kp\x00qk1', 1);('s= fq', 1);('=p >', 1);('v\x00 rmin\x16\x150fmaxs', 1);('kullbackleibler', 1);('probability distributions', 1);('dis- tributionsp', 1);('=p sq', 1);('duality result', 1);('=\x00min \x150n', 1);('+ log\x10 p > e\x00v \x11o', 1);('robust methods', 1);('worst-case reward', 1);('direct method', 1);('relative value iteration converge', 1);('optimal robust policies', 1);('theoretical results', 1);('conclusion', 1);('dynamic pro-', 1);('op- timal policy', 1);('optimal robust solution', 1);('proof techniques', 1);('acknowledgment', 1);('national science foundation', 1);('grants ccf-2106560', 1);('ccf-2007783', 1);('ccf-2106339', 1);('ccf-1552497.references abdullah', 1);('m. a.', 1);('ren', 1);('ammar', 1);('h. b.', 1);('milenkovic', 1);('wasserstein', 1);('arxiv preprint arxiv:1907.13196', 1);('t.', 1);('operational', 1);('research society', 1);('g. k.', 1);('beckus', 1);('alkhouri', 1);('velasquez', 1);('steady-state', 1);('expected reward multichain mdps', 1);('k. p.', 1);('robust reinforce-', 1);('least squares policy iteration', 1);('performance guarantees', 1);('a. y', 1);('j. g.', 1);('d. p.', 1);('dynamic programming', 1);('opti-', 1);('mal control', 1);('ii', 1);('belmont', 1);('athena scientic', 1);('discrete', 1);('annals', 1);('mathematical statistics', 1);('learning innite- horizon average-reward markov decision processes', 1);('constraints', 1);('arxiv preprint arxiv:2202.00150', 1);('finite', 1);('academic', 1);('twice', 1);('maximum', 1);('arxiv preprint arxiv:2103.06257', 1);('arxiv preprint arxiv:1811.00215', 1);('fast bellman', 1);('partial', 1);('l1-robust markov', 1);('opti- mality', 1);('handbook', 1);('pang', 1);('x.', 1);('lan', 1);('yin', 1);('wasserstein constraint', 1);('arxiv preprint arxiv:2006.00945', 1);('l. j', 1);('kullback-leibler', 1);('robust optimization', 1);('optimization online', 1);('papernot', 1);('goodfellow', 1);('duan', 1);('neural network policies', 1);('p. j', 1);('probability ratio test', 1);('ann', 1);('statist', 1);('g. n.', 1);('mathe-', 1);('d. l.', 1);('a. j', 1);('informs', 1);('computing', 1);('survey', 1);('international journal', 1);('delving', 1);('adversarial at- tacks', 1);('deep policies', 1);('kernel-based', 1);('z.-w.', 1);('liao', 1);('shih', 1);('m.-l.', 1);('m.- y', 1);('sun', 1);('adversarial attack', 1);('deep reinforcement learning agents', 1);('joint conferences', 1);('ijcai', 1);('zhu', 1);('garg', 1);('fei-fei', 1);('savarese', 1);('adversarially', 1);('robust policy learning', 1);('active', 1);('construc- tion', 1);('physically-plausible perturbations', 1);('ieee/rsj', 1);('intelligent robots', 1);('systems', 1);('iros', 1);('ieee', 1);('robustness', 1);('decision problems', 1);('uncertain transition matrices', 1);('generative model', 1);('arxiv preprint arxiv:2112.01506', 1);('tang', 1);('bommannan', 1);('chowd-', 1);('robust deep reinforcement learning', 1);('autonomous agents', 1);('multiagent systems', 1);('davidson', 1);('sukthankar', 1);('gupta', 1);('adversarial reinforcement learning', 1);('interna-', 1);('tional conference', 1);('m. l.', 1);('markov decision processes', 1);('discrete stochastic dynamic programming', 1);('t. h.', 1);('effective', 1);('robust op- timization', 1);('siam', 1);('optimization', 1);('ghotra', 1);('ravindran', 1);('epopt', 1);('robust neural network policies', 1);('model ensembles', 1);('functional analysis', 1);('mcgraw-hill', 1);('engineering', 1);('r. h.', 1);('ro-', 1);('constrained-mdps', 1);('soft-constrained robust policy optimization', 1);('arxiv preprint arxiv:2010.04870', 1);('j. k.', 1);('r. e.', 1);('uncertain transition probabilities', 1);('f.', 1);('distri-', 1);('ofine contextual bandits', 1);('o.', 1);('john wiley', 1);('a. g.', 1);('cambridge', 1);('massachusetts', 1);('mit', 1);('scaling', 1);('function approximation', 1);('action robust reinforcement learning', 1);('continuous control', 1);('ininternational', 1);('p. l.', 1);('bounded', 1);('average reward criterion', 1);('computational learning theory', 1);('parvate', 1);('jang', 1);('bayen', 1);('populations', 1);('arxiv preprint arxiv:2008.01825', 1);('average-reward markov decision processes', 1);('online robust reinforcement learning', 1);('policy gradient method', 1);('con-', 1);('c.-y', 1);('jahromi', 1);('m. j.', 1);('sharma', 1);('model-free', 1);('innite-horizon average-reward markov decision processes', 1);('mathematics', 1);('operations re-', 1);('distributionally robust markov decision processes', 1);('towards the-', 1);('understandings', 1);('robust markov decision pro-', 1);('asymptotics', 1);('arxiv preprint arxiv:2105.03863', 1);('distributionally', 1);('robust counterpart', 1);('ieee transactions', 1);('automatic', 1);('k. w.', 1);('on-policy', 1);('deep reinforce- ment learning', 1);('inter-', 1);('national conference', 1);('bai', 1);('q.', 1);('qiu', 1);('glynn', 1);('finite-sample regret bound', 1);('distributionally robust ofine tabular reinforcement learning', 1);('artical intelligence', 1);('statistics', 1);('aistats', 1);('pmlr.review', 1);('robust discounted mdps', 1);('brief review', 1);('t\x19is', 1);('policy', 1);('[ r', 1);('robust optimal', 1);('important problem', 1);('\x19\x03= arg max \x19v\x19', 1);('robust value iteration approach', 1);('arg maxa\x08 r', 1);('equivalence', 1);('time-varying', 1);('stationary', 1);('equivalence result', 1);('stationary transition kernel models', 1);('analog result', 1);('2n t\x150p', 1);('transition kernel model', 1);('different time step', 1);('time steps', 1);('npn\x001 t=0rt', 1);('s0=si', 1);('\x19\x02p1 t=0 trt', 1);('s0=s\x03', 1);('p. bye\x14', 1);('= min \x142n t\x150pe\x19', 1);('1x t=0 trtjs0=s # = min', 1);('p2pe\x19', 1);('robust-average reward', 1);('robust average-reward problem', 1);('stationary transition kernel model', 1);('arbitrary stationary policy \x19', 1);('nn\x001x t=0rtjs0=s # = min', 1);('similar result', 1);('js0=s\x15 = min', 1);('p=x', 1);('arg minp2pas', 1);('pa s3', 1);('fpa s', 1);('\x01 =x a\x19', 1);('ep\x19', 1);('a0=a', 1);('] =x a\x19', 1);('js0=s ] =x a\x19', 1);('js0=s\x15 +ep\x19', 1);('\x19 [ r1\x00g\x19', 1);('pjs0=s', 1);('] +ep\x19', 1);('\x19\x14 \x1bpa1', 1);('\x19\x14 r1\x00g\x19', 1);('p s0=s\x15', 1);('v\x19 pjs0=s\x15', 1);('\x19\x14 r0\x00g\x19', 1);('p+r1\x00g\x19 pjs0=s\x15', 1);('s2', 1);('js0=s ]', 1);('2the proof', 1);('multiple minimizers.=ep\x19', 1);('js0=s\x15 \x14min', 1);('stationary transition kernel sequence \x14=', 1);('v\x19 p.', 1);('non-robust average-reward', 1);('equation shows', 1);('p=g\x19 p\x19', 1);('stationary kernel', 1);('worst-case kernel', 1);('proves eq', 1);('vector v', 1);('bykakits matrix norm', 1);('kak= supx2rdkaxk1 kxk1', 1);('p=h\x19 pr\x19', 1);('following proposition', 1);('if\x05andpare', 1);('pkis', 1);('such thatkh\x19', 1);('pk\x14hfor', 1);(',1 1x n=1', 1);('nx', 1);('clearlys\x19', 1);('specic \x19', 1);('p. lemma', 1);('exists \x0e2', 1);('\x05\x02p\x02 [ \x0e', 1);('pk\x14h', 1);('exists \x0e', 1);('1\x00\x0e \x0eh\x14k <', 1);('constant k.', 1);('1\x00 h\x141\x00\x0e \x0eh\x14k', 1);('n+1r \x141 \x121\x00 \x13n hn+1\x14hkn \x0e', 1);('mn', 1);('becauseka+bk\x14kak+kbkfor inducedl1norm', 1);('1x n=1mn=h \x0ek 1\x00k', 1);('weierstrass m-test', 1);('on\x05\x02p\x02 [ \x0e', 1);('constant l', 1);('=pn n=1', 1);('t\x19 n', 1);('lipschitz', 1);('l1 norm', 1);('constant l.', 1);('por', 1);('=nx n=1', 1);('nn\x121\x00 \x13n\x001\x001', 1);('moreover krt\x19', 1);('k\x14nx n=1n\x121\x00 \x13n\x0011 2hn+1', 1);('h1\x00 ln', 1);('=nx n=1n\x121\x00 \x13n1 2hn+2', 1);('\x12 1\x00h1\x00 \x13 ln', 1);('=nx n=1n\x121\x00 \x13n\x0011 2hn+1\x00nx n=1n\x121\x00 \x13n1 2hn+2 =1 2h2\x00n\x121\x00 \x13n1 2hn+2+nx n=2\x121\x00 \x13n\x0011 2hn+1 \x141 2h2+h2 21\x00 h1 1\x001\x00 h =h2 2+h2 21\x00 h1 1\x001\x00 h', 1);('\x141 1\x00h1\x00 h2 2+h2 21\x00 h1 1\x001\x00 h', 1);('\x141 1\x00k\x12h2 \x0e2+h2 \x0e2k 1\x00k\x13', 1);('j \x14j 2\x00 1j', 1);('k\x14nx n=1 \x121\x00 \x13n', 1);('n+1r \x14nx n=1\x121\x00 \x13n hn+1 \x14nx n=1knh \x14hk 1\x00k', 1);('k = 2\x00', 1);('\x14\x121 \x0e2hk 1\x00k+1 \x0e1 1\x00k\x12h2 \x0e2+h2 \x0e2k 1\x00k\x13\x13 j 1\x00 2j', 1);('1\x00 2j', 1);('\x0e2hk 1\x00k+1 \x0e1 1\x00k\x10 h2 \x0e2+h2 \x0e2k 1\x00k\x11\x11', 1);('pand', 1);('n\x0f', 1);('k < \x0f', 1);('k \x14ks\x19', 1);('k \x142\x0f+ks\x19 n', 1);('k \x142\x0f+lj 1\x00 2j', 1);('= maxf\x0e', 1);('k <', 1);('cauchys', 1);('pk=kh\x19 pr\x19k\x14his', 1);('k \x14k', 1);('k \x14', 1);('1\x00 2j+kf\x19', 1);('kj 1\x00 2j', 1);('1x n=1', 1);('n+1r\x19 \x14', 1);('1x n=1\x121\x00 \x13n hn+1 \x14h \x0e1\x00 h1 1\x001\x00 h \x14h \x0ek 1\x00k', 1);('p\x02\x05', 1);('function f', 1);('max xlim y', 1);('= lim y', 1);('jforjy\x00y0j < \x0e0', 1);('iff', 1);('j=jmax xf', 1);('\x00max xf', 1);('lim y', 1);('= max xf', 1);('= max xlim y', 1);('factor approaches 1.theorem', 1);('.the robust', 1);('robust average-reward on\x05', 1);('j =jvt+1', 1);('j =j', 1);('j +', 1);('\x12 t\x1bpas', 1);('\x00 t\x1bpas', 1);('j+ t', 1);('\x12 \x1bpas', 1);('t+1k1+ tmax s\x1ax a\x19', 1);('a1-lipschitz function', 1);('\x14x a\x19', 1);('tk1 =kvt\x00', 1);('pv\x19 p', 1);('lets\x03', 1);('arg maxsj', 1);('psuch', 1);('\x19\x14p1 t=0 trtjs0=s\x15', 1);('worst-case transition kernel ofv\x19', 1);('j = min', 1);('t+1k1 =k', 1);('k1 \x14', 1);('pt+1k1+kf\x19 pt+1', 1);('\x00 t+1f\x19', 1);('+ t+1kf\x19', 1);('h+l+ t+1l+ sup \x19', 1);('h+ 2l+cfis', 1);('+ t\x01t', 1);('following lemma', 1);('pk1=', 1);('max \x19g\x19', 1);('max \x19v\x19', 1);('firstly', 1);('j \x14jvt+1', 1);('j + max a\x12', 1);('\x13 \x00max a\x12', 1);('\x13 \x14j', 1);('j + max', 1);('optimal robust', 1);('j\x14 maxxjf', 1);('j \x14j', 1);('j+ tmax', 1);('t+1k1+ tmax s', 1);('support function \x1bpas', 1);('t+1k1\x14kj t\x00 t+1j', 1);('stationary deterministic polices', 1);('deterministic optimal robust policy', 1);('exists \x19\x032\x05dsuch', 1);('p=g\x19\x03 p', 1);('assume', 1);('deterministic policies', 1);('=g\x19\x03 m', 1);('> g\x191', 1);('p\x15', 1);('all\x19\x03 i', 1);('dene \x05\x03=f\x19\x03 i', 1);('d=g\x19\x03 i', 1);('p\x00g\x191 p.', 1);('set \x05dis nite', 1);('\x0f < d', 1);('exists \x0e0 <', 1);('\x19\x03 iand\x19j', 1);('deterministic policy\x192\x05d', 1);('such thatv\x03', 1);('possible optimal robust polices', 1);('\x19\x03 mg', 1);('set \x05\x03', 1);('exists \x19\x03 j2\x05\x03', 1);('= max \x192\x05dv\x19', 1);('direct approach recall', 1);('relative function', 1);('4the proof', 1);('p2pv\x19 p=', 1);('\x19\x14p1 t=0', 1);('p+g\x19 p\x00g\x19 p', 1);('wherern=pn t=0rt', 1);('p2pandn', 1);('p\x15ng\x19 p', 1);('\x15lim n', 1);('rn\x00ng\x19 p', 1);('v\x19 p\x15min p2pep', 1);('p2pv\x19 p', 1);('p2ph\x19 pr\x19', 1);('h\x19 pis', 1);('compact set', 1);('stationary worst-case transition kernel', 1);('pbypg', 1);('\x15 \x14epg', 1);('\x15 =v\x19', 1);('pgdenotes', 1);('worst-case transition kernel', 1);('p. hence', 1);('.for anysand\x19', 1);('+1x t=1', 1);('s0=s\x15=', 1);('min \x142n t\x150p', 1);('p+e\x14', 1);('+ min \x142n t\x150p8 <', 1);('p02pmin', 1);('2n t\x151p8 <', 1);('p02p8', 1);('s0 min \x14=', 1);('2n t\x151p', 1);('e\x14', 1);('s0min pa s', 1);('s02paspa s', 1);('vectors v', 1);('letb', 1);('v\x00v', 1);('arg minp2pasp >', 1);('andpv=fpa s', 1);('v.', 1);('rewrite eq', 1);('g 1\x15r\x19+', 1);('p\x19 vare', 1);('g 1=gp\x19', 1);('vr\x19+p\x19 v', 1);('multiplying', 1);('ng 1\x15', 1);('v=1', 1);('v=', 1);('g 1\x15lim n', 1);('r\x19 = lim n', 1);('\x19\x14nx t=0rt\x15 =g\x19', 1);('p\x19 v1', 1);('g 1=r +', 1);('p v\x00i', 1);('p vare', 1);('stationary transition kernel', 1);('avesuch thatg', 1);('p=g p', 1);('andp avebyp', 1);('p vis', 1);('worst-case transition', 1);('p vv\x14p v', 1);('g 1\x14r +', 1);('g 1\x14', 1);('j\x001r +', 1);('ng 1\x14', 1);('g 1\x14lim n', 1);('nep ave', 1);('\x14nx t=0rt\x15 =g', 1);('thusg=g\x03 p', 1);('+\x1bpa s', 1);('rjsj', 1);('= max \x19fr\x19+\x1bp\x19', 1);('\x00v= max \x19fr\x19\x00g+\x1bp\x19', 1);('equation eq', 1);('p=g\x03 p', 1);('optimal robust policy.theorem', 1);('update operator', 1);('arg maxs', 1);('arg mins', 1);('= max', 1);('+\x1bpau \x13s', 1);('=\x1bpav \x13s', 1);('\x00\x1bpav \x13s', 1);('> u', 1);('v \x13s= arg minp2pav \x13sp > vandpav', 1);('u \x13s= arg minp2pav \x13sp > u', 1);('> u \x14', 1);('u \x13s=', 1);('v \x12s=', 1);('qigthen nx i=1pixi\x00nx i=1qixi =nx i=1', 1);('xi\x00nx i=1', 1);('xi \x14nx i=1', 1);('maxfxig\x00nx i=1', 1);('minfxig =nx i=1', 1);('+\x12nx i=1', 1);('\x00nx i=1', 1);('\x13 minfxig =\x12 1\x00nx i=1bi\x13 sp', 1);('\x14\x12 1\x00nx i=1bi\x13 sp', 1);('> \x15 >', 1);('nx i=1bi\x15\x15', 1);('lis', 1);('aj-step contraction operator', 1);('ljv\x00lju', 1);('relative value iteration converges', 1);('optimal equation', 1);('section 1.6.4', 1);