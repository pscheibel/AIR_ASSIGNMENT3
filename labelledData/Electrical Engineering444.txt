('tts', 10);('asr', 7);('international conference', 7);('voice conversion', 5);('vc', 5);('ieee', 5);('stylettsvc', 4);('llatent', 4);('icassp', 4);('melspectr', 3);('figure', 3);('per', 3);('rtf', 3);('acoustics speech', 3);('processingicassp ieee', 3);('oice conversion', 2);('adversarial training', 2);('ttsbased', 2);('zhang', 2);('automatic speech recognition', 2);('yourtts', 2);('againvc', 2);('vqmivc', 2);('training', 2);('step', 2);('mutual information', 2);('adain', 2);('tacotron', 2);('ground truth', 2);('f0', 2);('mi', 2);('english', 2);('higan', 2);('objective', 2);('acc', 2);('eis', 2);('lmiorlcycle', 2);('machine learning pmlr', 2);('voice conversion vector quantization', 2);('processing icassp ieee', 2);('stylettsvc oneshot voice conversion knowledge transfer fromstylebased tts modelsyinghao aaron li cong han nima mesgaranidepartment electrical engineering columbia', 1);('usaabstractoneshot', 1);('speech fromany source speaker arbitrary target speaker onlya seconds reference speech target speakerthis relies', 1);('speakers identityand speech content task', 1);('novel approach learning disentangledspeech representation', 1);('learning stylebasedtexttospeech', 1);('models cycle consistent adversarial training', 1);('high delity similarity learning', 1);('additional melspectrogram encoderthrough teacherstudent knowledge', 1);('novel dataaugmentation scheme approach results disentangledspeech representation', 1);('input text', 1);('thesubjective', 1);('evaluation shows approach', 1);('previous stateoftheart oneshot voiceconversion models naturalness similarityindex', 1);('terms v', 1);('representations texttospeech', 1);('introductionv', 1);('technique converts onespeakers voice anothers voice', 1);('linguistic prosodic information phonemes prosodyrecent advances', 1);('particular type voice conversion oneshot voiceconversion type voice conversion', 1);('asanytoany voice conversion aims', 1);('speech fromany source speaker arbitrary target speaker', 1);('onlya seconds reference audio target speakerto', 1);('unseen speakers voice', 1);('speakersvoice unseen training model needs', 1);('representation speech', 1);('potential sourcesand target speakers', 1);('learning disentangledrepresentations speech speaker identity', 1);('crucial forsuccessful oneshot voice conversionseveral techniques', 1);('instance normalization2', 1);('vector quantization', 1);('learning fromasr', 1);('methods albeit', 1);('effective guaranteethat', 1);('sourcespeaker information', 1);('mellotron', 1);('hand use phoneme alignmentand pitch curve source speech resynthesize thespeech target speaker', 1);('phoneme alignment', 1);('pitch curve', 1);('speech reect speech content andprosody source audio', 1);('sourcespecic information', 1);('suffer fromtwo', 1);('essential problems', 1);('major drawback', 1);('ttsbasedmodels', 1);('input text sequence ofphonemes generate alignment limits potentialfor applications realtime inference', 1);('hasmade attempt address problem training', 1);('additional melspectrogram encoder', 1);('phoneme alignmentand text representation', 1);('equivalent training', 1);('system show herethis way encoder training optimal', 1);('method generalization problemsince', 1);('reconstructspeech pitch phoneme alignment sourcespeaker', 1);('speech willsound', 1);('natural similar target speakers inputpitch phoneme alignment', 1);('different speakerin paper', 1);('nonparalleloneshot voice conversion framework', 1);('styletts17', 1);('texttospeech model address', 1);('generalization problems rst traininga', 1);('styletts', 1);('speech decoder cycle consistency lossfunction adversarial objectives train melspectrogram encoder', 1);('representations reconstruct decoder output', 1);('representationsfrom phoneme alignment speakers trainingset', 1);('speech input', 1);('unlikethe', 1);('previous method', 1);('technique notforce', 1);('phonemealignment representations subjective human evaluationshows model outperforms', 1);('previous stateoftheart oneshot voice conversion model', 1);('twoother baseline models', 1);('ieeearxiv221214227v1', 1);('dec', 1);('decoder', 1);('encoderphonemesinput melspectr', 1);('aligner pitch extractorresynthesized melspectr', 1);('encoder', 1);('training inferenceinput', 1);('pitch extractorrefer', 1);('encoderadainrefer', 1);('encoderadain resblockdecoderadainresblockdecoderconverted melspectr', 1);('inference schemes', 1);('orange pretrainedand', 1);('training decoder', 1);('synthesize target speech referencemelspectrogram pitch curve energy phoneme alignment text input melspectrogram b', 1);('trainingand inference procedures text aligner text encoder', 1);('melspectrogram encoderfor unseen source target speakers', 1);('convolutional layers', 1);('rnn', 1);('transformers model capabilityto', 1);('realtime inference fasterthanrealtimevocoderour work', 1);('multiple contributions', 1);('thatthe cycle consistency adversarial objective', 1);('decoder melspectrogram encoder forvc applications ii introduce novel data', 1);('voice conversion results input andtarget training iii', 1);('models voice conversion applications', 1);('alternative solution', 1);('mimaximization', 1);('objective audio samples modelare', 1);('available httpsstylettsvcgithubio', 1);('methods21 stylettsstyletts', 1);('nonautoregressive texttospeech model integrates style information adaptive instance normalization', 1);('stylettsframework', 1);('modules text encoder styleencoder discriminator text aligner pitch extractor speechdecoder duration predictor prosody predictor', 1);('speech decoder voice conversion describe modules objectives', 1);('train speechdecoder use duration prosody predictors relevant', 1);('forsimplicity', 1);('text aligner pitch extractorare', 1);('training overview ofstyletts decoder training', 1);('1atext encoder', 1);('input phonemes text encoder', 1);('tencodes', 1);('tinto latent representation htexttt use thesame text encoder', 1);('19style encoder', 1);('input melspectrogram x encoder extracts style code ssx voice conversion application sis', 1);('equivalent speaker', 1);('style encoder', 1);('starganv2vc20', 1);('domainspecic linear projection layersdecoder decoder', 1);('gsynthesizes', 1);('melspectrogramxghtext\x01dalignspxnxfrom input audio x wherehtext\x01dalignis', 1);('latent representation phonemessis style code target speaker pxis pitch contour andnxis log norm energy xper frame decoderconsists', 1);('residual blocks', 1);('equation 1with style code sis', 1);('g thepxandnxare', 1);('output fromevery residual block input', 1);('residual blockadain xs', 1);('l\x1bsx\x00\x16x\x1bxl\x16s', 1);('maps sis stylevector\x16\x01and\x1b\x01denotes channel', 1);('l\x1bandl\x16are', 1);('linear projections', 1);('adaptive gain bias', 1);('style vector sdiscriminator employ discriminator', 1);('das', 1);('speakers training', 1);('thediscriminator', 1);('architecture style encoderbut domainspecic linear projection layer eachspeaker domainspecic layer', 1);('discriminator tocapture', 1);('speaker training settext aligner pitch extractor text aligner', 1);('aisbased', 1);('task thelibritts corpus', 1);('pitch extractor', 1);('fis', 1);('pretrainedjdc network', 1);('libritts', 1);('f0estimated', 1);('text aligner asthe', 1);('pitch extractor asthe', 1);('stylettsvcfor', 1);('text input train additionalencoderethat encodes melspectrogram xintohensuchthatghtext\x01dalignspxnx', 1);('gexspxnx thatis', 1);('encoder learns', 1);('decoder synthesize speech therepresentations', 1);('text encoder phonemealignment encoder', 1);('1d residual blockswith instance normalization', 1);('henhtext\x01dalignin case encoder', 1);('model mayproduce unnatural speech effect', 1);('input xin extract thepitchpinfxinand energy nint logsnpn1xnt2where xntrepresents nthmel thetthframenthe number mels speech content henexin compute style code ssxrefto synthesize xtrgfrom thetarget speaker', 1);('pinandninare 1dimensional', 1);('information thanpitch volume', 1);('replicate effectsofhtext\x01dalignfor', 1);('possible speech', 1);('ghenisalso', 1);('representation phonemes containno speaker information overview', 1);('training objectiveswe', 1);('train model', 1);('steps rst train decoderwith cycle consistency loss function train theencoder', 1);('aforementioned objective', 1);('melspectrogram x2xysrc referencexref2xytrg source speaker ysrc2y targetspeakerytrg2y train model', 1);('lossfunctionsmel reconstruction loss', 1);('melspectrogram x2xand', 1);('corresponding text t2t decoder', 1);('withlrecext\x02kx\x00ghtext\x01dalignspxnxk1\x03 2where htextttis', 1);('phoneme representationdalignis attention alignment', 1);('textalignerssxis style code xpxfxis thepitch', 1);('xandnxis energy x use monotonicversion attention alignment', 1);('time attention alignments', 1);('monotonic containspeaker informationstyle reconstruction loss', 1);('meaningful stylecode', 1);('speaker embeddings', 1);('style reconstruction', 1);('similar 20lstyextxref\x02ksxref\x00sxtrgk1\x03 3where xtrgghtext\x01dalignsxrefpxnx', 1);('melspectrogram style code xrefwith thephonemes alignment pitch energy information xencoder loss training encoder', 1);('representations phoneme alignment encoder loss arbitrary target speaker training setlenextxrefkx\x00gexsxrefpxnxk14where xghtext\x01dalignsxrefpxnxthe convertedspeech', 1);('text representation phoneme alignmenthere xcan', 1);('ground truth training setor', 1);('data xg\x10htext\x01dalignsxrefpxnx\x11when xis', 1);('htextand dalignare text andalignment', 1);('speech sample xandxrefis', 1);('reference audio', 1);('different xref xis', 1);('speech sample', 1);('thisnovel', 1);('data augmentation', 1);('explores input targetspace', 1);('robust models', 1);('techniquephoneme loss', 1);('ex', 1);('original phoneme content employ phoneme loss functionto maximize', 1);('representations phonetic content alinear projection', 1);('pfor', 1);('frame inputlmiext1ttxi1ce\x00dalign\x01tip\x01exi\x015wheretis number frames', 1);('ce\x01denotes', 1);('crossentropy loss functioncycle consistency loss', 1);('sure decoder generalizes', 1);('different input style codes', 1);('independent text pitchand energy', 1);('employ cycle consistency loss functionlcycle', 1);('ext\x02', 1);('x\x00g\x00hsxpxtrgnxtrg\x01 1\x036wherepxtrgis pitch curve nxtrgis energy', 1);('speech xtrg training decoder hhtext\x01dalignandxis ground truth dalignis attentionalignment', 1);('speech xtrg training theencoder hexandxxin equation 4adversarial loss use', 1);('adversarial objectives theoriginal crossentropy loss function adversarial', 1);('extytrglog', 1);('dlxysrc\x00dlxysrc', 1);('18whered\x01ydenotes output discriminator thespeakery2yxtrgghsxrefpxnxthe convertedspeech xghspxnxthe', 1);('listhe', 1);('total number layers', 1);('danddldenotes', 1);('outputfeature map lthlayer withnlfeatures values xandhare equation', 1);('theencodereor decoder', 1);('dis', 1);('trainedfull objectives', 1);('full objective functions training thedecoder', 1);('full objective functions encoder areminepmaxdlen\x15cyclelcycle \x15milmi\x15advladv\x15fmlfm103', 1);('experiments31 datasetswe', 1);('vctk', 1);('thevctk', 1);('speakers withvarious accents', 1);('trainingand rest', 1);('unseen speakers', 1);('speakers training validation', 1);('comparison mos', 1);('condence intervalsbetween', 1);('different modelsmethod', 1);('mosn mospground truth', 1);('\x06009366 \x06010yourtts', 1);('\x06010the samples', 1);('thetext sequences phoneme sequences', 1);('fft', 1);('size of2048 hop size', 1);('window length', 1);('torchaudio', 1);('baseline models32', 1);('training detailswe', 1);('epochs \x15sty 02\x15cycle 1\x15adv 1and\x15fm', 1);('epochs \x15mi', 1);('adamw', 1);('099weight decay \x15', 1);('learning rate 10\x004and batchsize', 1);('melspectrogramsinto segments shortest length', 1);('evaluation results test accuracy', 1);('accphoneme', 1);('error rate', 1);('real time factor', 1);('betweendifferent models', 1);('geforce rtx', 1);('ti gpumethod accper rtfground truth', 1);('evaluationswe', 1);('subjective evaluations', 1);('metrics themean opinion score naturalness', 1);('mosn', 1);('measuresthe naturalness', 1);('opinionscore similarity', 1);('mosp', 1);('evaluates similaritybetween', 1);('reference speech', 1);('nativeenglish speakers', 1);('online survey', 1);('amazon mechanical1httpsgithubcomkyubyongg2pfig', 1);('example', 1);('attention alignment dalignand', 1);('ex\x01h\x001textwhere', 1);('h\x001textis pseudoinverseofhtext representation', 1);('reproduces monotonic alignment', 1);('eacts', 1);('asrmodelturk', 1);('recent baseline models', 1);('stateoftheart model', 1);('yourtts11', 1);('anytoany voice conversion', 1);('allbaseline', 1);('train test speaker', 1);('fair comparison melspectrograms', 1);('khzin evaluationsin', 1);('theorder models', 1);('model labels', 1);('different speakers reading thesame sentence', 1);('ground truth andthe rest', 1);('source input model andthe', 1);('baseline models ensures', 1);('different sampleshave', 1);('different lengths raters nd oneis ground truth method', 1);('similar multiple stimuli', 1);('reference anchor', 1);('mushra', 1);('enablingthe subjects', 1);('subtle difference', 1);('ground truth attention check ratings subject', 1);('mos', 1);('raters droppedin addition subjective evaluations', 1);('performedobjective evaluations', 1);('speaker classication phonemeerror rate', 1);('per asr', 1);('speakersimilarity speech intelligibility', 1);('speaker classication model', 1);('resnet18', 1);('speaker label model', 1);('test speakers', 1);('models samples generatedwith', 1);('different models', 1);('speech waveforms totext', 1);('espnet', 1);('convertedthe text phoneme sequences calculate', 1);('subjective', 1);('evaluation results', 1);('opinion scoresmos', 1);('condence intervals', 1);('ci', 1);('mosn mospproposed', 1);('\x06009367 \x06011wo augmentation', 1);('ablation studyto', 1);('ablation study subjective objective', 1);('lmiandlcycle', 1);('encoder decoder', 1);('addition show latent loss', 1);('al10 hurts performance', 1);('loss theencoder training', 1);('ext\x02kdalign\x01htext\x00exk1\x03', 1);('11in latent representation', 1);('text encoder andphoneme alignment', 1);('data augmentation theencoder loss', 1);('thedata augmentation', 1);('xxand use groundtruth training', 1);('xin equation', 1);('results comparison', 1);('different models ground truth model signicantlyoutperforms baseline models naturalness andsimilarity subjective evaluation experiment modelalso', 1);('otherbaseline models', 1);('yourtts per', 1);('donote difference', 1);('flowbased', 1);('evaluation results speakerclassicationtest accuracy', 1);('phoneme error rate', 1);('betweendifferent training objectivesmethod', 1);('accperbaseline', 1);('104wo augmentation', 1);('jacobian', 1);('matrix inversion', 1);('flowbased yourtts', 1);('ablationstudy results', 1);('decreases naturalness similarity', 1);('speech baseline model', 1);('full objectives alsooutperforms models', 1);('classication accuracy', 1);('per training', 1);('data augmentationalso decreases', 1);('naturalness objective metricsit', 1);('naturalness similarity drop', 1);('in10 hypothesize', 1);('model becauseeis', 1);('version htextwhich', 1);('token embeddings', 1);('exto', 1);('reconstructs monotonic alignmentdalignwith noise', 1);('thisshows', 1);('phoneme representations daligncan', 1);('pseudoinverse htext hand encoder', 1);('elearns', 1);('different representation decoder use toreconstruct', 1);('natural speech', 1);('monotonic alignment text representation', 1);('avoidsthe problems', 1);('speech unclear', 1);('incorrect phonetic content5', 1);('conclusionswe', 1);('model foroneshot voice conversion novel cycle consistency andphoneme', 1);('maximization objectives place latent reconstruction objective framework', 1);('novel dataaugmentation scheme', 1);('explores input output space', 1);('modelachieves stateoftheart performance similarity naturalness subjective objective evaluations whereour models', 1);('various metrics', 1);('mos acc per', 1);('previous models alsodemonstrate', 1);('latent reconstruction loss', 1);('llatentproposed', 1);('speech similarity naturalnessand', 1);('potential explanation effect', 1);('oneshot voice conversion systems suchas', 1);('realtime inference fasterthanrealtime vocoder', 1);('text labels training beprohibitive train', 1);('largescale speech corpora', 1);('need text labels', 1);('wewould', 1);('speaker similarity learning abetter speaker representation reproduce accent ofunseen speakers6', 1);('acknowledgmentsthis', 1);('national institute health', 1);('nihnidcd', 1);('mariejosee henry r kravis7 references1 kaizhi qian yang zhang shiyu chang xuesong yangand mark hasegawajohnson autovc zeroshot', 1);('autoencoder loss', 1);('juchieh chou chengchieh yeh hungyi leeoneshot', 1);('speaker andcontent representations instance normalizationarxiv preprint arxiv190405742', 1);('dayi wu hungyi lee oneshot', 1);('acoustics speechand', 1);('yenhao chen dayi wu tsunghan wu hungyilee againvc', 1);('oneshot voice conversion', 1);('activation guidance adaptive instance normalizationinicassp', 1);('conferenceon acoustics speech', 1);('benjamin', 1);('niekerk leanne nortje hermankamper vectorquantized', 1);('neural networks acousticunit discovery zerospeech', 1);('challenge arxivpreprint arxiv200509409', 1);('disong wang liqun deng yu ting yeung xiao chenxunying liu helen meng vqmivc vector', 1);('unsupervisedspeech representation disentanglement oneshotvoice conversion arxiv preprint arxiv210610132', 1);('huaizhen tang xulong zhang jianzong wang ningcheng jing xiao avqvc oneshot', 1);('arxiv preprint arxiv220210020', 1);('zhonghao li benlai tang xiang yin yuan wan lingxu chen shen zejun ppgbased', 1);('singingvoice conversion adversarial representation learning', 1);('jhenghao lin yist lin chungming chien', 1);('lee s2vc', 1);('framework anytoany voiceconversion', 1);('representations arxiv preprint arxiv210402901', 1);('mingyang zhang yi zhou li zhao haizhou litransfer', 1);('learning speech synthesis voice conversion nonparallel training data', 1);('ieeeacmtransactions audio speech language processing', 1);('edresson casanova julian weber christopher dshulby arnaldo candido', 1);('eren g', 1);('olge andmoacir', 1);('ponti yourtts towards', 1);('zeroshot multispeaker tts zeroshot voice conversion', 1);('adam gabry goeric huybrechts manuel samribeiro chungming chien julian roth giulia comini roberto barrachicote bartek perz jaimelorenzotrueba v', 1);('oice lter', 1);('fewshot', 1);('texttospeechspeaker adaptation', 1);('acoustics speech signalprocessing icassp ieee', 1);('ruobai wang yu ding lincheng li changjie fanoneshot', 1);('icassp20202020 ieee', 1);('processing icassp ieee2020', 1);('huaizhen tang xulong zhang jianzong wang ningcheng zhen zeng edward xiao jing xiaotgavc improving', 1);('autoencoder voice conversion', 1);('ieee automatic speech recognition understanding', 1);('asru ieee', 1);('rafael valle jason li ryan prenger bryan catanzaro mellotron multispeaker', 1);('expressive voice synthesis', 1);('rhythm pitch', 1);('global styletokens', 1);('seungwon', 1);('dooyoung kim myunchul joecotatron transcriptionguided', 1);('speech encoder foranytomany voice conversion', 1);('parallel dataarxiv preprint arxiv200503295', 1);('yinghao aaron li cong han nima mesgaranistyletts', 1);('generative model naturaland diverse texttospeech synthesis arxiv preprintarxiv220515439', 1);('xun huang serge belongie arbitrary', 1);('realtime adaptive instance normalizationinproceedings', 1);('international conference oncomputer vision', 1);('jonathan shen ruoming pang ron j weiss mikeschuster navdeep jaitly zongheng yang zhifengchen yu zhang yuxuan wang rj skerrvryan', 1);('alnatural tts synthesis', 1);('wavenet melspectrogram predictions', 1);('ieee internationalconference acoustics speech', 1);('yinghao aaron li ali zare nima mesgaranistarganv2vc', 1);('voice conversion ininterspeech', 1);('heiga zen viet dang rob clark yu zhang ron jweiss ye jia zhifeng chen yonghui wu libritts', 1);('librispeech texttospeech arxiv preprint arxiv190402882', 1);('sangeun kum juhan nam joint', 1);('detection andclassication', 1);('voice melody', 1);('convolutional recurrent neural networks', 1);('applied', 1);('masanori morise', 1);('harvest highperformancefundamental frequency estimator speech signalsininterspeech', 1);('dmitry ulyanov andrea vedaldi victor lempitskyinstance', 1);('ingredient faststylization arxiv preprint arxiv160708022', 1);('jaehyeon kim sungwon kim jungil kong sungroh yoon glowtts', 1);('generative ow texttospeech', 1);('monotonic alignment search', 1);('advances', 1);('information processing systems', 1);('malik boudiaf j', 1);('rony imtiaz masud ziko ericgranger marco pedersoli pablo piantanida ismail ben ayed', 1);('mutual information viewof metric learning crossentropy vs pairwise lossesineuropean conference computer vision', 1);('springer2020', 1);('jungil kong jaehyeon kim jaekyoung bae higan generative', 1);('adversarial networks efcient andhigh delity speech synthesis', 1);('advances neural information processing systems', 1);('junichi yamagishi christophe veaux kirsten macdonald', 1);('cstr', 1);('vctk corpus', 1);('multispeaker corpus cstr voice', 1);('toolkit version', 1);('yaoyuan yang moto hira zhaoheng ni anjalichourdia artyom astafurov caroline chen chingfeng yeh', 1);('puhrsch david pollack dmitriygenzel donny greenberg edward z yang jason lianjay mahadeokar jeff hwang ji chen peter goldsborough prabhat roy sean narenthiran shinji watanabe soumith chintala vincent quennevilleb', 1);('yangyang shi torchaudio', 1);('building blocksfor audio speech processing arxiv preprintarxiv211015018', 1);('ilya loshchilov frank hutter fixing', 1);('weight decayregularization adam', 1);('shinji watanabe takaaki hori shigeki karita tomokihayashi jiro nishitoba yuya unno nelson enriqueyalta soplin jahn heymann matthew wiesner nanxinchen adithya renduchintala tsubasa ochiaiespnet endtoend', 1);('speech processing toolkit', 1);('proceedings interspeech', 1);