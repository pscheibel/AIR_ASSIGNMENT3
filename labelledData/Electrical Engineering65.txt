('% o', 22);('hsv', 17);('cswin', 16);('computer vision', 14);('places2', 14);('fig', 13);('celeba', 12);('proceedings', 11);('tfill', 10);('deepfill', 9);('rw', 9);('iconv', 9);('aot-gan', 9);('cnn', 8);('ssim', 8);('crfill', 8);('pattern recognition', 8);('ours', 7);('hifill', 6);('masked', 6);('transformer block', 5);('image size', 5);('psnr', 5);('lpips', 5);('zoom-in', 5);('image', 5);('state-of-the-art methods', 4);('stripe window', 4);('local layer', 4);('joint attention', 4);('local layers', 4);('global layer', 4);('rdb', 4);('iout', 4);('pm', 4);('ca', 4);('sn', 4);('pc', 4);('quantitative', 4);('v2 [', 4);('ieee/cvf', 4);('ieee', 4);('% mask', 4);('human vision', 3);('lightweight model', 3);('color space', 3);('global information', 3);('whole image', 3);('receptive elds', 3);('canny', 3);('madf', 3);('rrdb', 3);('igt', 3);('ground truth', 3);('lhsv', 3);('ltotalhsv', 3);('value', 3);('patchmatch', 3);('shift-net', 3);('partial convolutions', 3);('aggregated', 3);('ablation', 3);('structural similarity', 3);('z. lin', 3);('eccv', 3);('new loss function', 2);('furthermore', 2);('transformer [', 2);('repair performance', 2);('early stage', 2);('loss function', 2);('appendix', 2);('edgeconnect', 2);('yu', 2);('edge structure', 2);('zhu', 2);('gen-', 2);('vision', 2);('feature map', 2);('image in- painting', 2);('whole model', 2);('cswin transformer', 2);('transformer blocks', 2);('own multi-head', 2);('vertical stripes', 2);('different receptive elds', 2);('lepe', 2);('edge', 2);('l1', 2);('perceptual', 2);('checkerboard artifacts', 2);('style', 2);('total loss', 2);('ablation experiments', 2);('adversarial loss', 2);('[ logd', 2);('ltotal', 2);('conduct experiments', 2);('comparison methods', 2);('reference state-of-the-art', 2);('region-wise', 2);('gated convolution', 2);('bridging global context interactions', 2);('large mask', 2);('lama', 2);('irregular masks', 2);('re- sults', 2);('hence', 2);('perceptual similarity', 2);('local information', 2);('peak', 2);('signal-to-noise ratio', 2);('ndenotes', 2);('hdenotes', 2);('bold', 2);('underline', 2);('italics', 2);('places2 psnr', 2);('parameters', 2);('x106 mask5 % o', 2);('qualitative', 2);('color deviation', 2);('early training steps', 2);('totalhsv', 2);('generative', 2);('k.', 2);('pro-', 2);('d. chen', 2);('b. guo', 2);('e. shechtman', 2);('j. yu', 2);('j. yang', 2);('x. shen', 2);('x. lu', 2);('t. s. huang', 2);('x. li', 2);('x. liu', 2);('ieee transactions', 2);('image processing', 2);('european conference', 2);('p. isola', 2);('a. efros', 2);('zeng', 2);('contextual transformations', 2);('huge masks', 2);('lightweight image inpainting by stripe window transformer with joint attention to cnn bo-wei chen', 1);('tsung-jung liu', 1);('kuan-hsien liuy', 1);('electrical engineering', 1);('graduate institute', 1);('communication engineering', 1);('chung hsing', 1);('taiwan', 1);('computer', 1);('information engineering', 1);('taichung', 1);('taiwan abstract image', 1);('important task', 1);('admirable methods', 1);('com- puter hardware', 1);('suitable model', 1);('personal use', 1);('special transformer', 1);('traditional convolutional neural network', 1);('further-', 1);('pri- mary colors', 1);('rgb', 1);('inten- sify color details', 1);('extensive', 1);('index terms hsv', 1);('joint attention mechanism', 1);('vision transformer', 1);('introduction image', 1);('main goal', 1);('realistic pixels', 1);('photo restoration', 1);('realistic results', 1);('important points', 1);('adjacent tex- tures', 1);('reasonable structure', 1);('meth- ods target', 1);('traditional diffusion method', 1);('current methods', 1);('gan', 1);('narrow receptive eld', 1);('global infor- mation', 1);('key edge', 1);('utilize auxiliary information', 1);('structure recovery', 1);('edges [', 1);('at- tention', 1);('attention scores com-', 1);('suvorov', 1);('] utilize', 1);('fast fourier convolution', 1);('ffc', 1);('frequency domain', 1);('global receptive elds', 1);('methods im-', 1);('overall repair result', 1);('huge compu- tational cost', 1);('recent years', 1);('cnns', 1);('low resolutions', 1);('computer memory', 1);('lightweight trans-', 1);('stable repair', 1);('specically', 1);('stripe window self- attention', 1);('traditional full self-attention', 1);('stripe', 1);('window self-attention mechanism computes self-attention parallel', 1);('vertical stripe cross-windows', 1);('constant- width stripes', 1);('global attention', 1);('computational cost', 1);('important factor', 1);('differ- ence', 1);('original image', 1);('basic primary colors', 1);('color consistency', 1);('input image', 1);('follow-up experiments', 1);('training details', 1);('experiment results', 1);('ablation studies', 1);('page limit', 1);('quantitative results', 1);('object removal experiments', 1);('im- ages', 1);('supplementary ma-', 1);('.arxiv:2301.00553v1 [ eess.iv ]', 1);('jan', 1);('related work traditional', 1);('traditional', 1);('gener- ally', 1);('dif- fusion method', 1);('diffusion', 1);('methods disseminate', 1);('texture content', 1);('multi-curve information', 1);('patch', 1);('approximate nearest-neighbor', 1);('nearest-neighbor region', 1);('similar nearest- neighbor region', 1);('complete image', 1);('large masks', 1);('deep', 1);('hardware technology', 1);('deep learning model', 1);('gradually', 1);('different modules', 1);('models utilize edge auxiliaries information', 1);('nazeri', 1);('gateconv', 1);('generate edge im- ages', 1);('additional auxiliaries information', 1);('complex structure', 1);('interior space', 1);('edge information', 1);('contextual at- tention', 1);('yi', 1);('attention scores', 1);('similar texture', 1);('attention module', 1);('wide attention', 1);('local re- ceptive eld', 1);('recently', 1);('vit', 1);('transformer', 1);('nlp', 1);('novel transformers', 1);('dong', 1);('zheng', 1);('huge mask', 1);('inpaint plausible textures', 1);('special attention', 1);('re- ceptive eld', 1);('traditional convolution', 1);('basic transformer', 1);('di- vide', 1);('novel stripe-', 1);('special transformer framework', 1);('joint attention local', 1);('lay- ers', 1);('model focuses', 1);('attention information be- tween', 1);('simple up-samples', 1);('major contributions', 1);('stripe window self-attention transformer', 1);('efcient local enhancement position', 1);('original method', 1);('global layers', 1);('overall consistency', 1);('repair results', 1);('color consis- tency', 1);('common dataset', 1);('extensive experiments', 1);('methodology overview', 1);('im', 1);('binary mask', 1);('mboth', 1);('downsam- ple input image', 1);('local residual', 1);('dense block', 1);('] layer', 1);('differ- ent', 1);('conv-relu', 1);('concate- nate', 1);('upsample layers', 1);('h\x02w\x02c', 1);('handware', 1);('16and swto4', 1);('horwchosen', 1);('horizontal stripes', 1);('different', 1);('general multi-head self-attention', 1);('mhsa', 1);('stripe window multi-head self-attention', 1);('sw- mhsa', 1);('low-resolution image', 1);('whole model structure shows', 1);('global', 1);('input images', 1);('imandm', 1);('right side shows', 1);('cswin transformer block', 1);('normalization factor', 1);('residual dense block', 1);('top right corner', 1);('full attention', 1);('redesigned cswin block', 1);('cswin block', 1);('rst feed-forward', 1);('self-attention block', 1);('sw-mhsa', 1);('stripe window self-attention', 1);('self-attention', 1);('residual link', 1);('re- fer', 1);('right side', 1);('self-attention needs', 1);('multiple times', 1);('attention information', 1);('nito', 1);('joint', 1);('different recep- tive elds', 1);('local receptive elds', 1);('con- form', 1);('values inq', 1);('joint attention.3.3', 1);('loss functions', 1);('func- tion', 1);('l1=jiout\x00igtj', 1);('ledge =1 npn i=1jj', 1);('iout medge\x00igt medge', 1);('medge', 1);('+ 10\x03iedge', 1);('edge mask', 1);('iedge', 1);('edge detection [', 1);('visible regions', 1);('vgg-based', 1);('perceptual loss', 1);('generate images', 1);('according', 1);('=1 nnx i=1jj', 1);('gt', 1);('edge =1 nnx i=1jj', 1);('medge\x00hsv gt medge', 1);('=\x15hsv\x03lhsv +\x15hsv edge\x03lhsv edge', 1);('where\x15hsv =', 1);('and\x15hsv edge =', 1);('saturation', 1);('discriminator loss', 1);('ld', 1);('generator loss', 1);('lg', 1);('ld=\x00eigt', 1);('] \x00eioutm [ logd', 1);('] \x00eioutm [ log', 1);('m ]', 1);('lg=\x00eiout', 1);('ladv=ld+lg+\x15gplgp', 1);('patchgan', 1);('g.', 1);('lgp=eigtjj5igtd', 1);('gradient penalty', 1);('\x15gp= 1e\x003', 1);('=\x15l1l1+\x15edgeledge +\x15perclperc +\x15stylelstyle +\x15totalhsvltotalhsv +\x15advladv', 1);('\x15edge =', 1);('\x15perc =', 1);('\x15style =', 1);('\x15totalhsv =', 1);('above loss weights', 1);('experiments', 1);('datasets', 1);('20k images', 1);('original dataset', 1);('5k images', 1);('4k images', 1);('state-of-the-art huge-parameters models', 1);('contextual at-', 1);('deep- fill', 1);('contextual residual aggregation', 1);('im-', 1);('convolution', 1);('contextual trans- formations', 1);('mask-aware dynamic filter-', 1);('auxiliary contextual', 1);('cr- fill', 1);('quantitative comparisons', 1);('ap- proach', 1);('pa- rameters', 1);('evaluation metrics', 1);('training images', 1);('similar resources', 1);('main point', 1);('qualitative comparisons', 1);('compared', 1);('clear textures', 1);('similar results', 1);('mod- els', 1);('training data', 1);('original trans-', 1);('learned', 1);('perceptual im- age patch similarity', 1);('in- uence', 1);('with- outltotalhsv', 1);('\x02256 images', 1);('original transformer', 1);('conclusion', 1);('lightweight joint attention trans-', 1);('wide receptive eld information', 1);('sig- nicant improvements', 1);('small amount', 1);('large models', 1);('ad- vantage', 1);('hard- ware support', 1);('demon- strate', 1);('small models', 1);('large mod- els', 1);('references', 1);('k. nazeri', 1);('e. ng', 1);('t. joseph', 1);('f. z. qureshi', 1);('m. ebrahimi', 1);('adversarial edge learning', 1);('arxiv preprint arxiv:1901.00212', 1);('r. suvorov', 1);('e. logacheva', 1);('a. mashikhin', 1);('a. remi-', 1);('a. ashukha', 1);('a. silvestrov', 1);('n. kong', 1);('h. goka', 1);('lempitsky', 1);('resolution-robust', 1);('fourier convolutions', 1);('ieee/cvf winter', 1);('ap-', 1);('x. dong', 1);('j. bao', 1);('w. zhang', 1);('n. yu', 1);('l. yuan', 1);('general vision transformer backbone', 1);('win- dows', 1);('c. barnes', 1);('a. finkelstein', 1);('d. b. goldman', 1);('correspon- dence algorithm', 1);('structural image editing', 1);('acm trans', 1);('graph', 1);('free-form', 1);('international confer- ence', 1);('contextual atten- tion', 1);('com- puter vision', 1);('m. zhu', 1);('d.', 1);('c. li', 1);('f. li', 1);('e. ding', 1);('z. zhang', 1);('mask awareness', 1);('z. yi', 1);('q. tang', 1);('s. azizi', 1);('d. jang', 1);('z. xu', 1);('contex-', 1);('tual residual aggregation', 1);('ultra high-resolution im- age', 1);('ieee/cvf con-ference', 1);('x. chen', 1);('s. xie', 1);('li', 1);('p. doll', 1);('r. gir-', 1);('scalable vision learn- ers', 1);('c. zheng', 1);('t.-j', 1);('cham', 1);('j. cai', 1);('d. phung', 1);('bridg-', 1);('global context interactions', 1);('high-delity image completion', 1);('ieee/cvf con-', 1);('l. ding', 1);('a. goshtasby', 1);('canny edge detec- tor', 1);('pattern', 1);('x. wang', 1);('k. yu', 1);('s. wu', 1);('j. gu', 1);('liu', 1);('c. dong', 1);('qiao', 1);('c.', 1);('loy', 1);('esrgan', 1);('enhanced', 1);('super- resolution generative adversarial networks', 1);('computer vi- sion', 1);('q. dong', 1);('c. cao', 1);('fu', 1);('incremental', 1);('transformer structure', 1);('po- sitional', 1);('pattern recogni-', 1);('g. liu', 1);('f. a. reda', 1);('k. j. shih', 1);('t.-c. wang', 1);('a. tao', 1);('b. catanzaro', 1);('irregular holes us-', 1);('euro-', 1);('pean conference', 1);('j.-y', 1);('t. zhou', 1);('image-', 1);('to-image translation', 1);('conditional adversarial net-', 1);('z. yan', 1);('m. li', 1);('w. zuo', 1);('s. shan', 1);('ma', 1);('s. bai', 1);('l. wang', 1);('a. liu', 1);('d. tao', 1);('e. r. hancock', 1);('regionwise', 1);('generative adversarial im- age', 1);('ieee transac-', 1);('cybernetics', 1);('h. hukkel', 1);('f. lindseth', 1);('r. mester', 1);('in- painting', 1);('dagm', 1);('german conference', 1);('springer', 1);('j. fu', 1);('h. chao', 1);('high-resolution image', 1);('visualization', 1);('computer graphics', 1);('h. lu', 1);('m. patel', 1);('cr-ll', 1);('erative image', 1);('auxiliary contextual re- construction', 1);('ieee/cvf inter-', 1);('national conference', 1);('z. wang', 1);('a. c. bovik', 1);('h. r. sheikh', 1);('e. p. simon-', 1);('quality assessment', 1);('error visibility', 1);('r. zhang', 1);('o. wang', 1);('unreasonable effectiveness', 1);('deep features', 1);('perceptual metric', 1);('celeba dataset datasets', 1);('whole dataset', 1);('test datasets', 1);('contextual attention', 1);('imputed convolution', 1);('aux-', 1);('contextual', 1);('qualitative comparison', 1);('irregu- lar masks', 1);('param- eters', 1);('tiny masks', 1);('training steps', 1);('similar re- sources', 1);('ground-truth image', 1);('hap- pens', 1);('small masks', 1);('object removal', 1);('conduct object removal experiments', 1);('target removal', 1);('background repair', 1);('grid background', 1);('model needs', 1);('enhance structure', 1);('future research', 1);('inpainting-by-stripe-window-transformer- with-joint-attention-to-cnn', 1);('object', 1);('original', 1);('object removal result.fig', 1);('inpainting', 1);('celeba psnr', 1);