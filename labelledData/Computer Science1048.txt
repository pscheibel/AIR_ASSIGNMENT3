('epfs', 30);('epf', 29);('figure', 20);('fa', 17);('sefce', 16);('antrum', 15);('theorem', 12);('rc', 12);('lemma', 9);('sse', 8);('stackelberg', 7);('us0', 7);('nash', 6);('gis', 6);('hence', 6);('us', 6);('us0g', 6);('p1s', 5);('onestep lookahead', 5);('incentive compatibility', 5);('vs', 5);('bosansk', 4);('bo', 4);('consider', 4);('markov', 4);('p2to', 4);('case', 4);('algorithm', 4);('gpu', 4);('qs\x162\x00us\x162', 4);('function approximation', 3);('sefces', 3);('p2is', 3);('fvi', 3);('bellman', 3);('usis', 3);('furthermore', 3);('schaul', 3);('articial intelligence', 3);('proceedings', 3);('advances', 3);('triangle inequality', 3);('strong induction', 3);('u0sg', 3);('neural networks', 2);('silver', 2);('kiekintveld', 2);('fabased', 2);('game', 2);('p1is', 2);('upper concave envelope', 2);('bottomup fashion', 2);('phase', 2);('equation', 2);('lipschitz', 2);('crucially', 2);('mnih', 2);('training', 2);('onehot vector', 2);('deep', 2);('br', 2);('articial intelligence brown n sandholm', 2);('international conference', 2);('proceedings aaai', 2);('sansk b', 2);('aaai', 2);('machine learning', 2);('computing', 2);('shorthandu0sutargets8hvs02csus0i\x16 ifs2s1hvs02csus0 s0i\x16ifs2s2be', 2);('u0s\x162', 2);('rst equality', 2);('fact reuse proof', 2);('tus0 s0\x160 1\x00tus00 s00\x1600 z', 2);('u0s\x162u0s\x162\x0fj\x001\x00tus0', 2);('induction hypothesis', 2);('small need', 2);('p1assuming', 2);('epfs usfor', 2);('descendent states s0wsthe domain precludes', 2);('policy play s0with probability tand consequent', 2);('s00with probability1\x00tand', 2);('payoff \x1600 denition', 2);('h0hj\x001is', 2);('true s2sds j', 2);('qs\x162\x00u0s\x162 u0s\x162\x00us\x162', 2);('qs\x162\x00u0s\x162', 2);('\x0f tqs0\x160 1\x00tqs00\x1600\x00tus0 s0\x160\x001\x00tus00 s00\x1600 \x0f\x14t', 2);('qs0\x160\x00us0\x160', 2);('qs00\x1600\x00us00\x1600', 2);('line followsfrom expansion denitions', 2);('qsandu0s', 2);('fth line', 2);('truncation operator', 2);('\x001 values', 2);('hjis', 2);('u0sis', 2);('us00g', 2);('cerm', 2);('epf sefce', 2);('learning rate', 2);('traj', 2);('uniform', 2);('function approximation solving stackelberg equilibrium large perfectinformation gameschun kai ling j zico kolter fei fangschool computer', 1);('carnegie mellon universitychunkailcscmuedu', 1);('zkoltercscmuedu feifcscmueduabstractfunction approximation', 1);('critical componentin', 1);('large zerosum games', 1);('attention hasbeen', 1);('generalsum extensiveform games', 1);('competitive cooperative counterparts key challenge formany equilibria generalsum games', 1);('simple analogue tothe state value function', 1);('markov decision processesand', 1);('zerosum games exists paper', 1);('payoff frontier epfa', 1);('generalizationof state value function generalsum games approximate optimal', 1);('extensiveform correlatedequilibrium', 1);('appropriate backup operations andloss functions rst method applies', 1);('thestackelberg setting', 1);('performance guarantees', 1);('additionally', 1);('method guarantees incentivecompatibility', 1);('selfplay approximate bestresponse oracles1', 1);('introductiona', 1);('central challenge', 1);('modern game', 1);('handlelarge game trees', 1);('large traverse oreven specify', 1);('board games', 1);('chess pokersilver', 1);('brown sandholm', 1);('2019morav ck', 1);('bakhtin', 1);('modern video games', 1);('large state action spacesvinyals', 1);('scalable game', 1);('neural networks model state values', 1);('networks ability generalize evaluation statesnever', 1);('morav', 1);('schmid', 1);('methods', 1);('fahave', 1);('stateoftheart performance alsoexhibit humanlike behavior', 1);('kasparov', 1);('generalsum games', 1);('difcult tosolve', 1);('literature centers aroundcopyright', 1);('advancement articialintelligence', 1);('wwwaaaiorg rights reservedon methods', 1);('exact backward induction', 1);('incremental strategy generation', 1);('cern bo', 1);('cermaket', 1);('karwowski', 1);('bosansky cermak', 1);('20151whileexact methods', 1);('large game trees', 1);('large traverse', 1);('ability tackle generalsum games', 1);('practical interest security domains', 1);('wildlife poachingprevention', 1);('fang', 1);('airport patrols', 1);('pita', 1);('al2008 nonnash equilibrium generalsum games thevalue state', 1);('direct application', 1);('zerosum solvers', 1);('infeasiblein paper', 1);('payoff frontier epf', 1);('twoplayer games', 1);('conitzer', 1);('tradeoff player payoffs analogous state value inzerosum', 1);('study pitfalls thatcan', 1);('generalsum games ii', 1);('usingneural networks', 1);('designedbellmanlike loss iii', 1);('guarantees incentivecompatibility performance method approachis rst application', 1);('fa stackelberg', 1);('bestresponse oracles performance guarantees', 1);('experimental', 1);('method aapproximate solutions games', 1);('traverse b generalize', 1);('setting game payoffs', 1);('preliminaries notationa', 1);('perfect information game', 1);('anite game tree game states s2sgiven vertices andaction spaceasgiven', 1);('lanctot', 1);('wang', 1);('zerosum games generalsum', 1);('games2the idea', 1);('letchford', 1);('polynomial time solution', 1);('sses', 1);('namingarxiv221214431v1 csgt', 1);('dec', 1);('2022each state', 1);('p1orp2', 1);('leaf terminalstate2l\x12s ofgis', 1);('byrifor player', 1);('action a2asat states62lleads tos0tas wheres02s', 1);('tis', 1);('deterministic transition function', 1);('cs', 1);('fs0jtas s0a2asgdenote', 1);('immediate children ofs', 1);('state sprecedes states0ifs6s0andsis ancestor s0ing writevif allowingss0an actiona2asleads tos0ifss0andtasvs0with', 1);('slight abuse notation', 1);('tasvs0byas0orsas0', 1);('tree states ss0wheress0', 1);('a2assuch savs0 usethe notationwandawhen relationships reverseda strategy \x19i wherei2 fp1p2g', 1);('state s2', 1);('sito', 1);('distribution actionsas iepa2as\x19ias', 1);('givenstrategies\x191and\x192', 1);('probability reaching2 l', 1);('qi2fp1p2gqs0asvs0s0as02si\x19ias0', 1);('p2lpjs\x191\x192ri', 1);('asshorthandp\x191\x192andri\x191\x192ifsis roota strategy \x192is', 1);('response strategy \x191ifr2\x191\x192\x15r2\x191\x1902for strategies \x1902', 1);('ofbest responses \x191is', 1);('brs', 1);('2\x191the grim strategy arg min\x191max\x192r2\x191\x192ofp1towards', 1);('p2 conversely', 1);('joint altruistic strategyarg max\x191\x192r2\x191\x192is', 1);('p2s', 1);('grim altruistic strategies whichare subgameperfect ie', 1);('optimal thegame', 1);('state3grim altruisticstrategies ignore', 1);('bybackward induction state', 1);('vsandvsthe', 1);('internal values', 1);('p2for', 1);('grim altruistic strategies', 1);('backward induction21', 1);('stackelberg equilibrium perfectinformation gamesin strong stackelberg', 1);('leader andfollower', 1);('p1andp2respectively', 1);('leader commits strategy \x191and follower', 1);('responds leader', 1);('\x1922brs 2\x191such benet', 1);('optimal commitment leader ie pair \x19 \x191\x192such that\x1922brs 2\x191andr1\x191\x192is maximizedit', 1);('wellknown optimal', 1);('noworse leader', 1);('equilibrium oftenmuch', 1);('1a k1k20', 1);('follower payoff', 1);('states whichhave0probability reached4commitment rights', 1);('interactions thep1reneges commitment', 1);('p2plays', 1);('responsewhich detrimental leader setting', 1);('de jongeand zhang', 1);('solutions thesubgame', 1);('leader payoff', 1);('solution leader', 1);('uniform strategythis ensures', 1);('yields followera payoff', 1);('assumptions ofsse nets leader payoff 45stackelberg', 1);('extensiveform correlated equilibriumfor', 1);('paper focus relaxation', 1);('sseknown stackelberg', 1);('actions follower time decision making follower deviates recommendation theleader', 1);('grim strategyin', 1);('sefce p1takes', 1);('actions maximizeits reward subject constraints recommendations', 1);('p2relative', 1);('p2facing', 1);('grim strategy potential deviationdenition', 1);('minimum', 1);('givens2s2s02cs', 1);('dene minimum', 1);('incentive s0 maxs2css6s0vs ie minimum', 1);('p1needs', 1);('p2unders0for', 1);('strategy pair \x19 \x191\x192is asefce incentive compatible ie s2s2a2as\x192as0', 1);('r2tas\x191\x192\x15 tasadditionally', 1);('\x19is optimal', 1);('r1\x191\x192is', 1);('maximizedin section', 1);('describe optimal', 1);('polynomial time', 1);('perfect information games22', 1);('valueswhen', 1);('perfect informationgames valuevsof state', 1);('crucial quantity whichsummarizes utility', 1);('sonward assumingoptimal play players contains sufcient information', 1);('optimal solution', 1);('typically', 1);('statess0ws zerosum games vsvswhile', 1);('cooperative gamesvsvs', 1);('knowing', 1);('true value', 1);('optimal policy', 1);('onestep lookahead vscan', 1);('states backwardinduction', 1);('large standardworkaround', 1);('vswith approximate vswhichis', 1);('tandem search algorithm', 1);('montecarlo', 1);('tree search etc', 1);('anapproximate solution', 1);('rich function class state', 1);('neural network', 1);('modern solvers', 1);('able togeneralize vacross', 1);('large state spaces', 1);('value iteration', 1);('class methods', 1);('fitted value iteration fvi lagoudakis parr2003 dietterich wang', 1);('munos szepesv', 1);('ari2008 idea', 1);('optimize parameterssuch minimize', 1);('regular regression problem5here the5we distinguish', 1);('rl fvi', 1);('transition function isknown', 1);('fviss010\x001\x0011stayk1k2exita toy', 1);('14048unenforceableenforceable b', 1);('vertex s01', 1);('vertex exiting1', 1);('14048epf envelopenot envelope', 1);('root vertex sfigure', 1);('leader follower leaf\x03states vertices edges areactions bd', 1);('x axes follower \x162 leader payoffs', 1);('us\x16', 1);('b pinkregions', 1);('p2too', 1);('pink regions part', 1);('upper concave envelope removedbellman loss measures distance vsand', 1);('v distance 0for alls vmatches optimal v practicesmall errors', 1);('accumulate cascade', 1);('performance function', 1);('loss s23', 1);('related worksome', 1);('state values ingeneralsum games', 1);('relatedto murray gordon', 1);('macdermed', 1);('dermed charles', 1);('theachievable set payoffs', 1);('sse letchford', 1);('stochastic gamesthese methods analytical nature scale poorlyperolat', 1);('greenwald', 1);('qlearninglike', 1);('algorithm generalsum', 1);('stationary strategieswhich', 1);('long range threats likethe', 1);('sse zinkevich greenwald littman', 1);('showa class generalsum', 1);('games valueiterationlike methods', 1);('zhong', 1);('studyreinforcement learning', 1);('setting onlyconsider followers myopic', 1);('castellettipianosi restelli', 1);('issue incentive compatibility', 1);('reinforcement learningand selfplay', 1);('leibo', 1);('recent', 1);('methods accountfor nonstationary environment player', 1);('foerster', 1);('perolat', 1);('game theoretical guarantees terms incentive compatibility', 1);('non zerosum games3', 1);('review solving sefce', 1);('enforceablepayoff frontiersin', 1);('importance valuefunctionvin', 1);('zerosum games section wereview analogue', 1);('generalsum gameswhich term', 1);('payoff frontiers epf introduced letchford conitzer', 1);('state sisafunctionusr7rf\x001g thatus\x162gives themaximum leader payoff', 1);('son condition', 1);('p2obtains', 1);('payoff \x162 leavess2lhave degenerate', 1);('epfs usr2s', 1);('r1sand\x001 everywhere', 1);('tradeoff payoffs', 1);('p1andp2', 1);('nowreview twophase algorithm', 1);('example game', 1);('1a k1k2 0this approach forms basis', 1);('computing epf backward induction theepf', 1);('line segment', 1);('payoffs ofits children', 1);('\x001 everywhere', 1);('becausethe leader', 1);('mix actions compute', 1);('uswe', 1);('case1p1is', 1);('incentive compatibility needs', 1);('p2a', 1);('0undertstays s0', 1);('lefttruncate regions theepf ats0which', 1);('s0thatareenforceable byp1', 1);('p2toexit', 1);('p2from', 1);('commits grimstrategy ats0ifp2chooses', 1);('p2apayoff', 1);('enforceable payoffs degenerate', 1);('blue linesegment', 1);('observe wecan', 1);('payoffs line segment', 1);('ss children union points suchlines', 1);('blue segments', 1);('1b and1c removesf00g', 1);('epf figure', 1);('g1andg2be functions thatgjr7rf\x001g', 1);('upper concave envelope ie inffh\x16jhis concave h\x15maxfg1g2goverrg', 1);('sincevis', 1);('associative commutative use shorthandvf\x01gwhen applyingvrepeatedlyover nite', 1);('functions addition', 1);('g tas lefttruncation gwith threshold t2r ieg t\x16 g\x16if\x16\x15tand\x001', 1);('concave functions anys2s', 1);('epfuscan', 1);('terms itschildren', 1);('epf us0wheres02cs', 1);('usingvand s0us\x16 8hvs02csus0i\x16 ifs2s1hvs02csus0 s0i\x16ifs2s21which', 1);('extracting strategies epf onceushasbeen', 1);('fromthe root', 1);('opt', 1);('coordinates ofthe', 1);('maximum point', 1);('uroot', 1);('payoffs theoptimal\x19', 1);('initialize \x162opt 2which', 1);('p2at', 1);('gdepthrst', 1);('constructionus\x162\x001 point \x162us\x162is convexcombination', 1);('factors correspond optimalstrategy\x19as 2distinct children', 1);('repeat process s0s00with\x1602\x162 0\x16002\x162', 1);('repeat processfors0and\x1602\x162 example', 1);('s\x162 0which', 1);('p2playing', 1);('ats0\x16', 1);('result section 2theorem', 1);('iusis piecewise linear concave number knots6no', 1);('number leaves', 1);('usingbackward', 1);('polynomial time injsj', 1);('games chance', 1);('continueto piecewise linear concavemarkovian property', 1);('state values vsin zerosumgames', 1);('internal vertex singwith', 1);('epfwhile', 1);('optimal strategy branchesof game', 1);('leader vertexwith actions', 1);('terminal states payoffs', 1);('corresponding knots', 1);('us sinceusis', 1);('backward induction', 1);('gandg0which', 1);('acommon subgame', 1);('couldreuse theusfound', 1);('gforus0ing0', 1);('observation underpins inspiration workif sands0are similarin', 1);('usandus0are', 1);('likely similar itshould', 1);('challenges applying fa epfwe', 1);('original problem', 1);('fvi suppose', 1);('state hasfeaturesfsin', 1);('stateshistory design neural network', 1);('fparameterizedby network maps state', 1);('fsto representation', 1);('goodapproximation optimize', 1);('appropriatebellmanlike loss', 1);('usin', 1);('lieu ofus', 1);('design considerationsepfs', 1);('right object', 1);('state scould', 1);('constant memory', 1);('number knots', 1);('belinear number leaves underneath', 1);('state scalar a6knots slope', 1);('training pipeline1sample', 1);('trajectory s1newstnew2update replay buffer', 1);('bwiths1st3fori2f1tgdo4', 1);('sample batch', 1);('sfs1sng\x12b5 compute lossse', 1);('update', 1);('ompute lossse', 1);('usi e', 1);('usjnext e', 1);('fsjnextfor allsjnext2csi4', 1);('compute utargetsiusing equation', 1);('fusjnextg5returnpilusiutargetsismall vector', 1);('lossless summary whichenjoys', 1);('markovian', 1);('encapsulates theepf', 1);('class games', 1);('gkin figure', 1);('1a k1\x002andkk22\x0011 optimalleader payoff', 1);('gkis9\x0011k2', 1);('us0kfigure', 1);('lossless summary s0anduse', 1);('everygk resultant optimal leader payoffscan recoverus0\x162between\x1622\x0011 implies thatno lossless summary', 1);('promises arising fa error considerthe', 1);('2a k1\x0010k2\x001 exactus0is line segment', 1);('points \x00110and1\x00\x0f\x001', 1);('blue line segment', 1);('performing phase', 1);('s0is onceagain uniform policy', 1);('follower utility 1ins00', 1);('payoff 1isimpossible regardless', 1);('leader willingto sacrice', 1);('maximum outcome s00is1\x00\x0fsince', 1);('leader payoff of\x0010 general', 1);('errorcan lead', 1);('low payoffs fact', 1);('promises consider', 1);('case k1\x0030k2 1while', 1);('us0the', 1);('promiseof1ats00isfulllable involves', 1);('cost of\x0030', 1);('3b general problem', 1);('smallrange of\x162', 1);('small \x0fis', 1);('issue generaluscan', 1);('constants eg', 1);('proportionate tomaxsr1s\x00minsr1sminjr2s\x00r2sj existence', 1);('costly payoffs rules', 1);('representations basedss010\x001s00k1k2\x0011\x00\x0f\x00100a', 1);('unfulllable costly promisesss000\x001\x001refuseq1\x00q2accedeb stage game', 1);('thetantrum gamefigure', 1);('sections', 1);('leader follower leaf\x03states vertices edges actions1', 1);('unfulllable promise1', 1);('costly', 1);('b costlypromises', 1);('blue lines', 1);('epfs us0', 1);('green lines', 1);('epfs us0us00', 1);('faerror', 1);('bluesquare 045can', 1);('endpointsofus0with probability', 1);('05black curveson', 1);('space \x162', 1);('small errors incurredby discretization', 1);('huge drops performance5', 1);('fa epf performance guaranteeswe', 1);('design method', 1);('insights section', 1);('discretization overp2payoffs\x162', 1);('us\x162\x001', 1);('p2payoffs', 1);('suitable loss', 1);('network architecture', 1);('small set ofm\x152pointsp s fxjyjg forj2m', 1);('heremis', 1);('complexity neural networke', 1);('representation power', 1);('epf usis', 1);('linear interpolation mpoints', 1);('us\x001if\x162maxjxjor\x162minjxj', 1);('assumption follower payoffs altruistic grimstrategy', 1);('vsandvs', 1);('statesthrough architecture', 1);('allj2m havevs\x14xj\x14vs', 1);('convenient loss', 1);('fsvsvstakes asinputs state', 1);('upper bounds', 1);('vs\x14vsand', 1);('outputs matrix', 1);('rm\x022representingfxjyjgwherex1vsandxmvs', 1);('simplicity use amultilayer feedforward network depth widthwandrelu activations layer', 1);('serious', 1);('applications shouldutilize domain specic architectures', 1);('denoting', 1);('output ofthe', 1);('layer hdfs2rw forj2f2m\x001gandk2mwe setxj\x1b\x10ztxjhdfs bxj\x11\x01\x00vs\x00vs\x01vsykztykhdfs bykandx1vsandxmvs where\x1bx', 1);('herezxjzyk2rwandbxjbyk2rareweights', 1);('parameters fromfeedforward network form network parameters', 1);('byp svand', 1);('byits knots', 1);('iteration line', 1);('functions learning epfs', 1);('usu0swe', 1);('loss mitigate', 1);('costly promisesl1usu0s max\x162jus\x162\x00u0s\x162jl1was', 1);('large loss approximation', 1);('small range \x162egfigure 3b', 1);('achieving', 1);('small loss', 1);('us\x162approximates u0s\x162well', 1);('\x162 design decision', 1);('important example contrast', 1);('l1with', 1);('intuitive loss', 1);('l2usu0s r\x162us\x162\x00u0s\x1622d\x162 observe l2is', 1);('small theexample', 1);('3b fact \x0fis', 1);('small enoughleads', 1);('policy discussedin section', 1);('suboptimal phenomena', 1);('guarantees', 1);('uimplicitly', 1);('denes policy\x19by onestep lookahead', 1);('extracting', 1);('ofine s2 fact', 1);('necessary extract \x19\x01sondemandnonetheless \x19enjoys', 1);('important propertiestheorem', 1);('method s2s2anda2as have\x19sas0', 1);('r2tas', 1);('tastheorem', 1);('fa error ifl1usutargets\x14\x0ffor', 1);('alls2sthenjr1\x19\x00r1\x19\x03jod\x0fwheredis depth', 1);('gand\x19\x03is', 1);('optimal strategyheretasis transition function section', 1);('recallfrom', 1);('\x19to optimal', 1);('incentive compatibility ii', 1);('r1\x19to', 1);('theorems', 1);('approach disentangles criteria', 1);('p2willalways', 1);('recommendations iethere', 1);('unexpected outcomes', 1);('hard constraint', 1);('due choice network architecturewhich ensures', 1);('us\x162', 1);('u conversely theorem', 1);('shows thegoal', 1);('r1subject', 1);('thisdistinction', 1);('dependent convergence trainingthis', 1);('contrast methods', 1);('selfplay reinforcement learning agents incentive compatibility', 1);('apparentconvergence players strategy', 1);('practical implications example', 1);('quality \x19canbe', 1);('r1\x19based', 1);('implicit guarantees', 1);('incentive compatibility tobe', 1);('approximate bestresponse oracle', 1);('expensive training', 1);('rl', 1);('agentthe primary limitation method', 1);('vandvand', 1);('\x19grim1be approximate grim strategy', 1);('denevesto', 1);('follower payoffs swhen', 1);('\x19grim1 ier2s \x19grim1\x192 where\x1922brs 2\x19grim1', 1);('following definition', 1);('approximate minimum', 1);('s0 maxs2css6s0vesfor alls2s', 1);('\x19altbe approximate joint altruistic strategyand resultant', 1);('internal payoffs state', 1);('vsunder', 1);('mild assumption \x19altalways benets', 1);('p2more', 1);('\x19grim1 ie', 1);('vs\x15vesfor', 1);('veandvand', 1);('intuition straightforwardifp2s threats', 1);('termsofmaxsj s\x00 sj', 1);('fa error weaker bounds ifl1usutargets\x14\x0ffor', 1);('alls2 thenjr1\x19\x00opt 2jod\x0fwheredis depth ofgandopt', 1);('max\x162uroot\x162remark key', 1);('technical difculty', 1);('v inour', 1);('experiments \x19grim1can', 1);('generallarge games approximate \x19grim1vby', 1);('overs2 use heuristics', 1);('s1implementation details', 1);('stabilize training target networksarulkumaran', 1);('prioritizedexperience replay', 1);('ii practice', 1);('train loss', 1);('distances xcoordinate knotsinusandu0s ielp\x1622fknotsgus\x162\x00u0s\x1622sincelupper bounds', 1);('l21', 1);('ino mtime practice use brute forcemethod', 1);('operations runs ino m3 iv train', 1);('portionsofus lead loss performance sincepayoffs', 1);('usare pareto', 1);('waste knots learning', 1);('portion v', 1);('specics', 1);('forall implementation details', 1);('appendix6 experimentswe', 1);('synthetic games', 1);('gamedetails', 1);('experiment environments', 1);('appendixcode', 1);('tantrum', 1);('2b repeatedntimes q10q2\x151 rewards accumulatedover stages way', 1);('p1can', 1);('positive payoffs isby', 1);('destructive \x001\x001outcome', 1);('q21p2has usethreats', 1);('rantrum', 1);('haso3nleaves clearthat grim resp altruistic strategy', 1);('resp notthrow tantrum', 1);('hence vandvare', 1);('knowneven whennis', 1);('theraw', 1);('fsis 5dimensional vector rst', 1);('previous stages thelast', 1);('current stateresource', 1);('collection rc', 1);('j\x02jgrid', 1);('witha time horizon n cell contains', 1);('quantities 2different resources r1xyr2xy\x150', 1);('playersbegin', 1);('toan adjacent cell stay', 1);('piis', 1);('resourcei players', 1);('resources whenthe game ends', 1);('p1the', 1);('opportunity threatenp2with', 1);('p2does', 1);('o25nleavesthe', 1);('grim strategy', 1);('p1to', 1);('vandvstill', 1);('search leastforp2 state', 1);('expensive use', 1);('locations b', 1);('current coordinates player andwhose', 1);('resource collectedand number rounds remaining61', 1);('experimental setupgames fixed parameters', 1);('withj 7n', 1);('different games', 1);('rewards', 1);('loggaussian process xytosimulate spatial correlations details', 1);('appendix', 1);('alsoreport payoffs nonstrategic', 1);('p1which', 1);('p2best', 1);('games 1e12states\x01opt \x01sp\x01nonrc', 1);('narc', 1);('na na', 1);('parameter games55', 1);('9510564748494exactpredictedlookahead b', 1);('100k epochs55', 1);('9510564748494exactpredictedlookahead c', 1);('2m epochs265', 1);('25501t argetlookahead failure casefigure', 1);('species trials \x01opt\x01sp \x01nonis averagedifference method optimal', 1);('nonstrategic leader commitment', 1);('failure case', 1);('optimal strategy', 1);('thespecial structure game note subgame perfectequilibrium', 1);('p1zero', 1);('withj 9n', 1);('large use approximates', 1);('vve', 1);('search online ie whensappears training', 1);('p2starting', 1);('ve\x19altis', 1);('runningexact search depth', 1);('greedy algorithm', 1);('rare occasion thatvs', 1);('ves', 1);('vs ves', 1);('report results infigure 4a show difference', 1);('payofffor method optimal', 1);('ii subgameperfect', 1);('iii nonstrategic leader', 1);('allowq1q2tovary betweenstages', 1);('vectors qi211n trajectory uses', 1);('different qi append', 1);('eachplayer training', 1);('iid samples qji\x18exp11 evaluation metric \x14r1\x19opt ie ratio', 1);('optimal \x19for eachn test 100qvectors', 1);('\x14against greedy strategy whichrecommends', 1);('long sufcientthreats remainder game', 1);('p1details appendix', 1);('stress test \x19on', 1);('different testdistributionqji\x18exp1', 1);('discussionfor', 1);('parameter games', 1);('optimal value', 1);('optimal performance', 1);('outperforms baselines', 1);('averagevalue improvement', 1);('perfect equilibrium vacuous', 1);('unable issuethreats', 1);('game tree', 1);('signicantlyoutperform nonstrategic baselinefor', 1);('outofdistribution qsfigure 5a', 1);('performance', 1);('drops nbecomes', 1);('complex performance degrades nincreases', 1);('100thresh020406080100thresh n5n6n7n10bfigure', 1);('featurized rantrum', 1);('\x14 ratio leaders payoff thetrue optimum grd \x14and str\x14denote results baseline greedy method results stress', 1);('withqdrawn distribution training b', 1);('proportion', 1);('\x14\x14 threshform greedy baseline stress test suggests thenetwork', 1);('datafigures 4b 4c shows', 1);('rootfor epochs 100k 2m', 1);('2m training epochs', 1);('onestep lookahead mirrors', 1);('epfin', 1);('portions case', 1);('beginning training', 1);('red markersare', 1);('portions onthe', 1);('2m epochs knots', 1);('blue markers learning', 1);('regionsfigure 4d', 1);('yieldshigh loss', 1);('training failure case raresince', 1);('resultant action stilloptimalin case', 1);('p2was\x162\x00255which', 1);('mdps', 1);('policies nearoptimal', 1);('losses states7', 1);('conclusionwe', 1);('novel method', 1);('fa epfs', 1);('bestof knowledge rst time object', 1);('games performance guaranteeswe hope approach', 1);('current gapbetween', 1);('zerosum generalsum gamesreferencesarulkumaran', 1);('k deisenroth p brundage', 1);('reinforcement learning briefsurvey', 1);('ieee', 1);('processing magazine', 1);('wu lerer brown n', 1);('nopress diplomacy scratch advances neural information processing systems', 1);('34bosansk b', 1);('hansen k lund', 1);('b andmiltersen', 1);('computation stackelberg', 1);('equilibriaof nite sequential games', 1);('acm transactions economicsand computation teac', 1);('124bosansk b', 1);('hansen k miltersen p', 1);('srensen', 1);('computation stackelberg equilibria finite sequential', 1);('corr', 1);('abs150707677bosansky b', 1);('cermak j', 1);('sequenceform', 1);('stackelberg equilibria extensiveformgames', 1);('twentyninth aaai', 1);('libratus', 1);('ai', 1);('nolimit poker', 1);('proceedings twentysixth', 1);('joint', 1);('superhuman ai', 1);('multiplayer poker science', 1);('pianosi', 1);('restelli', 1);('multiobjective fitted qiteration pareto', 1);('frontier approximationin', 1);('networking sensing', 1);('ieeecermak j bosansky', 1);('durkota k lisy v kiekintveld', 1);('strategies computingstackelberg equilibria extensiveform games', 1);('volume 30cerm ak', 1);('j bo', 1);('durkota k lis v kiekintveld', 1);('correlated strategies computingstackelberg equilibria extensiveform', 1);('proceedings thirtieth aaai', 1);('articial intelligence aaai16', 1);('aaai presscern j bo', 1);('incremental strategy generation stackelberg equilibriain extensiveform', 1);('2018acm conference', 1);('economics computation', 1);('jonge zhang', 1);('strategic negotiations forextensiveform games', 1);('autonomous agents multiagentsystems', 1);('charles', 1);('value', 1);('stochastic games', 1);('complete incompleteinformation', 1);('phd', 1);('georgia', 1);('technologydietterich wang x', 1);('batch', 1);('value functionapproximation', 1);('support vectors', 1);('neural information processing systems 14fang f', 1);('nguyen h pickles r lam', 1);('clementsg r', 1);('singh schwedock', 1);('b c', 1);('tambe mand lemieux', 1);('pawsa deployed gametheoreticapplication combat poaching ai magazine', 1);('j n chen r alshedivat whiteson sabbeel p mordatch', 1);('learning', 1);('awareness arxiv preprint arxiv170904326', 1);('j lerer bakhtin brown n', 1);('performance nopress diplomacy', 1);('search', 1);('learning representations greenwald', 1);('k serrano r', 1);('correlatedqlearning icml', 1);('doubleoracle', 1);('stackelberg equilibrium', 1);('approximationin generalsum extensiveform games', 1);('volume 3420542061kasparov', 1);('chess drosophila', 1);('g parr r', 1);('leastsquares', 1);('policyiteration journal', 1);('research 411071149lanctot', 1);('zambaldi v gruslys lazaridou atuyls k p', 1);('j silver graepel', 1);('gametheoretic approach multiagent', 1);('neural information processing systems 30leibo', 1);('j z zambaldi v lanctot marecki j', 1);('multiagent', 1);('reinforcement learning sequential', 1);('social dilemmas arxiv preprint arxiv170203037', 1);('letchford j conitzer v', 1);('extensiveform games', 1);('acm', 1);('electronic', 1);('acmletchford j macdermed', 1);('conitzer v parr r', 1);('andisbell c l', 1);('optimal strategies committo stochastic games', 1);('twentysixth aaai', 1);('conference onarticial', 1);('intelligence macdermed', 1);('narayan k isbell', 1);('c l', 1);('weiss l2011 quick', 1);('polytope approximation', 1);('equilibria stochastic games', 1);('twentyfifth aaai conferenceon articial intelligence mnih v kavukcuoglu k silver rusu venessj bellemare g graves riedmiller fidjeland k ostrovski g', 1);('humanlevel', 1);('reinforcement learning nature 5187540529533morav ck', 1);('schmid burch n lis v morrill dbard n davis waugh k johanson bowlingm', 1);('deepstack expertlevel', 1);('articial intelligence inheadsup nolimit poker science', 1);('r szepesv', 1);('ari c', 1);('finitetime boundsfor fitted value iteration', 1);('research 95murray c', 1);('gordon g', 1);('finding', 1);('equilibria general sum stochastic games', 1);('paszke gross chintala chanan g yang edevito z lin z desmaison antiga', 1);('lerera', 1);('automatic', 1);('pytorchperolat j', 1);('vylder', 1);('hennes tarassov e strub fde boer v muller p connor j burch n anthony tet', 1);('mastering game stratego modelfree multiagent reinforcement learning', 1);('arxiv preprintarxiv220615378', 1);('perolat j strub', 1);('piot', 1);('pietquin', 1);('learning nash', 1);('equilibrium generalsum', 1);('games frombatch data', 1);('articial intelligence statistics', 1);('j jain ord', 1);('onez f', 1);('portway', 1);('tambe mwestern', 1);('paruchuri p kraus', 1);('armorsecurity los angeles', 1);('international airport', 1);('quan j antonoglou silver d2015 prioritized', 1);('experience replay arxiv preprintarxiv151105952', 1);('schmid moravcik burch n kadlec r davidson j waugh k bard n timbers', 1);('lanctot holland z', 1);('player', 1);('games arxiv preprintarxiv211203178', 1);('silver huang maddison', 1);('j guez sifre lvan den driessche g schrittwieser j antonoglou ipanneershelvam v lanctot', 1);('mastering', 1);('deep neural networks tree search nature', 1);('hubert schrittwieser j antonoglou laim guez lanctot sifre', 1);('kumaran graepelt', 1);('general reinforcement learning algorithmthat masters chess shogi', 1);('selfplay science', 1);('babuschkin chung j mathieu jaderberg czarnecki', 1);('dudzik huang georgievp powell r ewalds horgan kroiss danihelka agapiou j oh j dalibard v choi sifrel sulsky vezhnevets molloy j cai buddend paine gulcehre', 1);('wang z pfaff pohlen tyogatama cohen j mckinney k smith schault lillicrap apps', 1);('kavukcuoglu k hassabis dand silver', 1);('alphastar mastering realtimestrategy game starcraft ii', 1);('shi z r yu', 1);('wu singh r joppal fang', 1);('reinforcement learning forgreen security games realtime information', 1);('h yang z wang z jordan', 1);('canreinforcement learning find stackelbergnash equilibriain generalsum markov', 1);('myopic followersarxiv', 1);('preprint arxiv211213521', 1);('zinkevich greenwald littman', 1);('2005cyclic equilibria', 1);('neural information processing systems 18a', 1);('preliminariesthe', 1);('show estimate', 1);('different optimal', 1);('depth statedsbe longest path', 1);('leaf ieds \x1a0 s2lmaxs02csds0', 1);('nited maxs2sdsand niteletusbe', 1);('fors2snl', 1);('\x1ar1s\x162r2s\x001 otherwisefor clarity', 1);('epfs u0swhich', 1);('equal denition', 1);('usdomains epfs', 1);('function hr7r', 1);('dom', 1);('h fxjhx\x001g', 1);('thefollowing', 1);('lemmas ensure', 1);('completeness reader', 1);('overthis section desiredlemma', 1);('us dom u0s dom us dom u0s vsvsthe', 1);('proofs straightforward', 1);('cproof', 1);('u0sandus', 1);('concave envelopes left truncations', 1);('onedimensional functionvcan bewritten', 1);('as24s02csus035\x162 maxs0s002cst201\x160\x16002rt\x1601\x00t\x1600\x162tus0\x160 1\x00tus00\x1600that', 1);('children states s0s002csthis', 1);('1dimensional functions', 1);('letchford conitzer', 1);('s02cs wheres2s2 convenience dene s0 s0whens2s2and\x001 whens2s1 plays role', 1);('ie f \x001 fforanyfr7r', 1);('truncation s2s1and', 1);('goal', 1);('goal show', 1);('l1loss', 1);('low states iemax\x1622vsvsjus\x162\x00u0s\x162j\x14\x0f 2then', 1);('good performance ie leader', 1);('payoff order', 1);('o\x0fdless', 1);('optimal additivelya2', 1);('learned epfs approximately optimalthe', 1);('theorem show', 1);('epfs usare', 1);('uspointwise', 1);('strong induction states', 1);('induction hypothesis iskj max\x1622vsvsjus\x162\x00us\x162j\x14j\x0f8swhereds jby denition', 1);('k0satises', 1);('base case', 1);('inductive caseassume thatk0\x01\x01\x01kj\x001are', 1);('kjusing', 1);('lets2ssuch', 1);('thatds j', 1);('supposek0kj\x001are', 1);('true ju0s\x162\x00u0s\x162j\x14\x0fj\x001for all\x1622vsvsproof', 1);('fix\x162', 1);('show ju0s\x162\x00u0s\x162j\x14\x0fj\x001', 1);('\x1b s0s00t\x160\x1600be parameters whichachieves maximumarg maxs0s002cst201\x160\x16002rt\x1601\x00t\x1600\x162tus0 s0\x160 1\x00tus00 s00\x1600 3and', 1);('efps', 1);('\x1b s0s00t\x160\x1600arg maxs0s002cst201\x160\x16002rt\x1601\x00t\x1600\x162tus0 s0\x160 1\x00tus00 s00\x1600 4which arguments', 1);('u0s\x162andu0s\x162respectively', 1);('epfwith', 1);('equal \x162is', 1);('upper convex envelope s0s00we', 1);('12for simplicity', 1);('suppose u0s\x162u0s\x162', 1);('s0\x160 1\x00tus00 s00\x1600 z \x14u0s\x162 t\x10us0 s0\x160\x00us0 s0\x160\x11 1\x00t\x10us00 s00\x1600\x00us00 s00\x1600\x11 \x14\x0fj\x0015and rst inequality', 1);('j\x01jfollows assumption case', 1);('u0s\x162wastaken', 1);('argmax ie', 1);('ki2', 1);('wherei20j\x001 fact thatds0ds jand operator', 1);('absolute error difference7however 3inequalitiescannot hold', 1);('true ie', 1);('u0s\x162\x14u0s\x162', 1);('suppose u0s\x162u0s\x162\x00\x0fj\x001', 1);('similar derivation', 1);('s0\x160 1\x00tus00 s00\x1600 z \x14u0s\x162 t\x10us0 s0\x160\x00us0 s0\x160\x11 1\x00t\x10us00 s00\x1600\x00us00 s00\x1600\x11 \x14\x0fj\x0016where rst inequality', 1);('j\x01jcomes case 2s assumption', 1);('u0s\x162was', 1);('froman argmax ie', 1);('ki', 1);('wherei20j\x001and depth 3inequalities', 1);('assumption case', 1);('u0s\x162\x15u0s\x162\x00\x0fj\x001combining', 1);('result cases', 1);('\x1bis argmax', 1);('values \x16that lie', 1);('epfsnow', 1);('possible discrepancy givesmax\x1622vsvsjus\x162\x00us\x162j\x14 max\x1622vsvsjus\x162\x00u0s\x162jju0s\x162\x00us\x162j max\x1622vsvsjus\x162\x00u0s\x162jju0s\x162\x00u0s\x162j\x14 max\x1622vsvsjus\x162\x00u0s\x162j max\x1622vsvsju0s\x162\x00u0s\x162j\x14 max\x1622vsvsju0s\x162\x00u0s\x162j\x0f\x14\x0fj7where', 1);('us\x162andu0s\x162', 1);('thefourth fact maxxjfx gxj\x14maxxjfxj maxyjgyj fth assumption', 1);('main theorem', 1);('ds', 1);('dsis', 1);('depth tree thefact base case', 1);('k0is', 1);('ifl1usu0s\x14\x0ffor', 1);('alls2s max\x1622vsvsjus\x162\x00us\x162j\x14\x0fd wheredis depth thegameproof', 1);('l1and', 1);('payoffs induced strategy closetoexpectedtheorem', 1);('close pointwise', 1);('\x19 joint policy', 1);('vsvs7rbe', 1);('given\x1b s0s00t\x160\x1600given byarg maxs0s002cst201\x160\x16002rt\x1601\x00t\x1600\x162tus0 s0\x160 1\x00tus00 s00\x1600 8the', 1);('s62l\x162qs\x162 tqs0\x160 1\x00tqs00\x1600 9theorem', 1);('\x14\x0fdfor alls2sand all\x1622vsvsproof proof', 1);('dsagain lethj qs\x162\x00us\x162', 1);('\x14\x0fj8swhereds j 10by denitionh0is', 1);('induction hypothesis fact thats0s002cshave', 1);('causes element', 1);('true j20dand theorem', 1);('piecing', 1);('togetherlet\x16\x032', 1);('arg max\x162uroot\x162 ie', 1);('follower root optimal policy \x19\x03', 1);('\x162arg max\x162uroot\x162', 1);('follower root \x19 haveuroot\x16\x032\x00qroot\x162uroot\x16\x032\x00uroot\x16\x032zj\x01j\x14\x0fduroot\x16\x032z\x14uroot\x162\x00qroot\x162\x14\x0fd', 1);('uroot\x162\x00qroot\x162', 1);('\x142\x0fdthe inequalities', 1);('denition \x162', 1);('complete proof', 1);('qroot\x162is', 1);('preciselyr1\x19by denitionb', 1);('4the proof', 1);('a3', 1);('approximate bounds', 1);('thanstrict ones', 1);('new boundsfors2snl', 1);('\x1ar1s\x162r2s\x001 otherwisethis case', 1);('vsvs', 1);('stricter truncation s0for eachs02cs dene', 1);('likebefore s0 s0whens2s2and\x001 whens2s1', 1);('a3letqs\x162 vesvs7rbe', 1);('given\x1b s0s00t\x160\x1600given byarg maxs0s002cst201\x160\x16002rt\x1601\x00t\x1600\x162tus0 s0\x160 1\x00tus00 s00\x1600 11the', 1);('recursive equations', 1);('s62l\x162qs\x162 tqs0\x160 1\x00tqs00\x1600 12let induction hypothesis behj', 1);('\x14\x0fj8swhereds j 13by denitionh0is', 1);('induction hypothesis factthats0s002cshave', 1);('causes element exceeddomain bounds', 1);('true j20d', 1);('observe thatqs\x162', 1);('r1\x19when', 1);('\x162 arg max\x162us\x162', 1);('usdom u0s vsvsproof', 1);('rst equality denition show', 1);('dom us vvby', 1);('state swe', 1);('possible casescase 1s2s1by denition', 1);('maxs02csvs0 andvs mins02csvs', 1);('observe thatmaxfdom', 1);('max8dom24s02csus0359 maxs02csmaxfdom', 1);('upperconcaveenvelope largestof largestx coordinates', 1);('us0 similarly', 1);('min8dom24s02csus0359 mins02csminfdom', 1);('upper concave envelopesis', 1);('upper limits', 1);('u0s sinceu0sis', 1);('u0sgwe', 1);('u0sminfdom u0sgu0smaxfdom u0sg\x001', 1);('1case 2s2s 2by denition', 1);('maxs02csvs0andvs maxs02csvs0note difference', 1);('current action work', 1);('dom u0smaxfdom u0sg', 1);('max8dom24s02csus0 s0359 maxs02csmaxfdom', 1);('s0g maxs02csmax\x08vs0vs0 s01 maxs02csvs0vs16where', 1);('fact maxs02csvs0\x15maxs2css6s0vs\x15maxs2css6s0vs s0ie', 1);('part lefttruncation step s02csdom', 1);('s06 haveminfdom', 1);('s0g maxs002csminfdom', 1);('dependent s0', 1);('dom us0', 1);('s0 foralls0 particularconsiders\x03 arg maxs02csmaxfdom', 1);('us0g\x15', 1);('empty setin', 1);('u0s', 1);('s0g min8dom24s02csus0 s0359 mins02csminfdom', 1);('s0g mins02csmaxs002csminfdom', 1);('us00gvs19the', 1);('rst line denition', 1);('line uses argument case', 1);('fact thatu0sis concave show', 1);('u0s\x162\x001', 1);('ready tackle proof', 1);('us dom u0s dom us dom u0s vsvsproof', 1);('u0sanduswith u0sandus', 1);('extensions', 1);('chancefor', 1);('games chance backups', 1);('inmal convolutions', 1);('denote', 1);('chance nodes', 1);('scfors2scand', 1);('probability transition stos0to be\x19cs0s', 1);('case s2sc cases haveus\x16', 1);('ms02cs\x19cs0sus0\x16\x19 cs0sss010\x001\x0011stays0000405exitfigure', 1);('sample game difference', 1);('sse sefcefigure', 1);('s00 b', 1);('epf sse', 1);('forsefce ats', 1);('1bwherelis maximalconvolution operator', 1);('similar infconv operator convex functionsf1mf2\x16 supyff1\x16\x00y f2yjy2rgrefer', 1);('details whylis', 1);('right operator', 1);('linear number knots functions piecewise linear concave sort line segmentsin', 1);('gradients stitch line segments', 1);('order gradients', 1);('thankfully', 1);('applyinglto piecewise linear concave functions', 1);('piecewise linear concave function', 1);('henceepfs usfor', 1);('l1loss theorem', 1);('minor additions omit theproof papere', 1);('comparison epfs sse sefceone', 1);('key disadvantages', 1);('epfs sse', 1);('epfs figure', 1);('pointwise maximums follower nodes upperconcave envelopes handfigure 8c shows', 1);('piecewise linear concave functionfurthermoreas', 1);('equal xcoordinates', 1);('epf ssethis', 1);('p1more', 1);('sse sefce', 1);('vice versasee', 1);('breakdown computational complexity', 1);('different classes games eg', 1);('pure behavioral', 1);('different levels imperfect informationf', 1);('useful', 1);('reasoning sefcefor', 1);('uncomfortable correlation', 1);('perfect information games', 1);('g0', 1);('follower vertexs', 1);('leader vertex s0just twoactions action', 1);('copy game', 1);('followervertexs construction', 1);('entire process', 1);('ndthesse ofg0which', 1);('gthe', 1);('purpose leader vertex', 1);('follower strategies recommendation thefollower', 1);('s0 action s0corresponds', 1);('action follower', 1);('abinary signal', 1);('follower actions', 1);('saandsbbe duplicate followervertices', 1);('different strategies subgame', 1);('sse g0can', 1);('sefce g', 1);('pure probabilities', 1);('saandsbfrom theleader', 1);('probabilities state singfor', 1);('pure action', 1);('example', 1);('follower vertices', 1);('gain intuition', 1);('sefce thesolution', 1);('algorithm section', 1);('g0mimics', 1);('follower vertex', 1);('sefceg', 1);('detailsg1 borrowing techniques rl fviwe', 1);('employ target networks', 1);('gradient descent', 1);('true loss', 1);('frozencopy network use compute', 1);('u0sie utargets', 1);('uis', 1);('main network withweights', 1);('gradient descent key idea target', 1);('epoch candestabilize training update target network', 1);('main network', 1);('bulk loss', 1);('small fraction states focus attention thesestates employ', 1);('state sin replay buffer tobe', 1);('proportionate square root', 1);('loss ie', 1);('lusu0s', 1);('05for convenience', 1);('hyperparameters addons', 1);('scope paper', 1);('future workg2', 1);('modied loss functionour', 1);('l1does', 1);('ourhypothesis', 1);('slow learning', 1);('fact point', 1);('responsible loss', 1);('neighbors itscoordinates', 1);('training local learning', 1);('usmakes', 1);('l1', 1);('absolute pointwise difference tends stabilize trainingthough', 1);('terms ofp\x0finsteadletusandu0sbe', 1);('letx1fx1xk1gandx2fx01x0k2gbe', 1);('thexcoordinates knots', 1);('usandu0srespectively', 1);('xx2x1x2usx\x00u0sx220this', 1);('pointwise differences', 1);('practical', 1);('upper concave envelope lefttruncationtheoretically', 1);('upper concave envelope kpoints', 1);('linear time', 1);('algorithm requiresa signicant number', 1);('ifelse statements', 1);('unsuitable batch operations agpu', 1);('alternative employ runs', 1);('ok3time', 1);('everydistinct pairs points xiyi xjyj check', 1);('point xayawherexa2xixjwhether xayalies belowor line segment xiyixjyj point xayais anysuch line segment ag', 1);('point overall scheme', 1);('codebut runs', 1);('linear time method batch sizes', 1);('important downside howeveris amount', 1);('intermediate calculations b', 1);('cubic terms number ofknots m', 1);('factor mthe number knots', 1);('epfdealing', 1);('different number actions vertex', 1);('points paddingthem', 1);('rectangular tensor', 1);('mask matrix', 1);('point isinactive points', 1);('upper concave envelopes potential points part linesegment', 1);('convenient truncate points', 1);('points performinterpolation', 1);('new point s0g4', 1);('decreasing portions epfkearning', 1);('pareto', 1);('select points example', 1);('8b 8c parts', 1);('yellow regions wouldnot matter', 1);('select maximum point xcoordinate 0insteadif', 1);('slight variation', 1);('epf usr7rf\x001g', 1);('maximum leader payoff', 1);('payoff least\x162rather', 1);('omitting', 1);('real tradeoff payoffs betweenp1andp2', 1);('epfsthat', 1);('experimentswe describe', 1);('u0sbe', 1);('knots fx1y1 xkykg', 1);('order xcoordinates wherex1vs', 1);('thej arg maxiyi', 1);('sampling training trajectoriesone', 1);('design decisions', 1);('sample states trajectories singleplayer setting commonplace use form \x0fgreedy', 1);('work use', 1);('randomthere exceptions', 1);('possible game wassmall', 1);('enumerate states implication leaf', 1);('actualtrajectories experience', 1);('samples uniform trajectories', 1);('work wellfor', 1);('game depth', 1);('cases states middle learning', 1);('meaningful becausetheir children', 1);('statessat mostdmaxto', 1);('replay buffer dmaxis', 1);('learnedfor states', 1);('tree rst parents nd', 1);('version n', 1);('stable learning', 1);('recall setting', 1);('\x18325and depth 50a uniform trajectory leaves states', 1);('start dmax', 1);('epochs games uniform trajectories work', 1);('game deeph', 1);('additional details experimental setuph1 environment detailsfor', 1);('experiments network multilayer', 1);('network width', 1);('relu', 1);('activations andnumber knots', 1);('pytorch', 1);('paszke', 1);('accelerate training', 1);('generation detailsmaps', 1);('reward map', 1);('gaussian', 1);('process query points givenby xycoordinates grid use squareexponential kernel length scale', 1);('standard deviation of01 way', 1);('spatial smoothness rewards realism', 1);('examplesof maps', 1);('procedure gures', 1);('good regions', 1);('p1may', 1);('p2andvice', 1);('antrum generationfor antrum', 1);('values q2were', 1);('training hyperparameterswe', 1);('adam', 1);('kingma ba amsgrad reddi', 1);('suitable use implementation', 1);('pytorch paszke', 1);('al2017 library replay buffer size 1m minibatch size', 1);('target networks parameters', 1);('training epochsh5', 1);('number epochs termination criterionunfortunately', 1);('rare small loss', 1);('thenumber', 1);('table 1figure', 1);('left', 1);('right example reward maps', 1);('p1andp2in rcnum epochs', 1);('prioritized', 1);('frequencyrc 2m', 1);('random state', 1);('na natantrum', 1);('unif', 1);('10rc 17m', 1);('differences', 1);('experimental setups game', 1);('frequency refers', 1);('epochs sample anew trajectoryi', 1);('qualitative discussion optimal strategies antrumwe', 1);('special case q1', 1);('intuitively', 1);('threats possiblethat', 1);('future states', 1);('p1does', 1);('p2\x00nthe', 1);('number times stage game', 1);('payoff player', 1);('p1canget', 1);('p2chooses', 1);('accede average nq2times', 1);('p1cannot', 1);('thatwould lead', 1);('p2losing', 1);('possible threat leader', 1);('beginningin experiments', 1);('p1playsto00at', 1);('leader vertices', 1);('p2accedes', 1);('rst jbnq2cstages probability', 1);('thej 1th vertex plays', 1);('recommendation mix strategies probability n\x00jq2q2it accedes', 1);('clearlythe', 1);('p2is\x00n', 1);('andp2cannot betterhowever result hold settings', 1);('case n', 1);('derivation suggests rst stage thefollower accedes probability', 1);('stage accedes probability 05at rst stage', 1);('suffer \x003if', 1);('yields \x002payoffwhich', 1);('payoff \x001in', 1);('future equivalent threat \x003', 1);('stage itis', 1);('incentive compatible follower accede', 1);('specically', 1);('player accedes', 1);('payoff \x004it', 1);('previous stage hand grim trigger threat', 1);('payoff \x004\x002from past and\x002from', 1);('deviatehowever q2is', 1);('incentive sufcient', 1);('future threat accedingis\x002 follower', 1);('21from acceding8in general', 1);('tight requiren\x00bnqczthreat grim trigger\x15 qzcost', 1);('time roundthat', 1);('accede recommendation', 1);('technically', 1);('previous rounds', 1);('condition satisedfor', 1);('round implies', 1);('previous rounds8this phenomena', 1);('similar difference coarse', 1);('ce', 1);('difference playerhas', 1);('accede followerhigher tree way room leader', 1);('form solution', 1);