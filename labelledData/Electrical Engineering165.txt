('online', 17);('rnn-t', 16);('macro-block dropout', 15);('aed', 14);('sec', 14);('c. kim', 14);('international conference', 14);('speech', 14);('wers', 12);('rnn', 11);('input units', 11);('ieee', 11);('processing', 10);('icassp', 9);('lstm', 8);('dropblock', 8);('acoustics', 8);('fig', 7);('error rates', 6);('librispeech', 6);('neural networks', 6);('cnns', 6);('rnns', 6);('conventional dropout', 5);('wer', 5);('% test-other', 5);('d. gowda', 5);('m. bacchiani', 5);('april', 5);('eds', 5);('asru', 5);('dec.', 5);('proceedings', 5);('conventional dropout approach', 4);('dropout rate', 4);('application', 4);('baseline dropout', 4);('a. garg', 4);('t. n. sainath', 4);('r. garnett', 4);('curran associates', 4);('r. m. stern', 4);('s. mun', 4);('machine learning', 4);('large number', 3);('recurrent neural network', 3);('test-clean andtest-other', 3);('attention-based encoder-decoder', 3);('attention-based encoder decoder', 3);('n=', 3);('standard deviation', 3);('m ]', 3);('experimental results', 3);('shallow fusion', 3);('macro-block dropout approach', 3);('c. han', 3);('z. chen', 3);('a. narayanan', 3);('bengio', 3);('neural information processing systems', 3);('audio', 3);('ieee automatic speech recognition', 3);('automatic speech recognition', 3);('understanding', 3);('may', 3);('k. kim', 3);('usa', 3);('learning representations', 3);('iclr', 3);('advances', 3);('seoul', 2);('new regularization algorithm', 2);('random dropout', 2);('recurrent neural network-transducer', 2);('improve- ment', 2);('end-to-end speech recognition', 2);('speech recognition accuracy', 2);('thanks', 2);('language model', 2);('lm', 2);('model robustness', 2);('environmental mismatch', 2);('minimum', 2);('dropout approach', 2);('square mask', 2);('zero position', 2);('spatial correlation', 2);('neural network layer', 2);('large variation', 2);('macro- block dropout', 2);('bernoulli', 2);('input x', 2);('hadamard', 2);('dropout rate qis', 2);('total number', 2);('delta function', 2);('legend label', 2);('vari- ance', 2);('byq q', 2);('dropout', 2);('shakedrop', 2);('steps', 2);('two-dimensional macro-block dropout', 2);('one-dimensional macro-block dropout', 2);('input unit', 2);('macro-block', 2);('ny', 2);('creates', 2);('absolute value operation', 2);('speech recognition experiments', 2);('tensorflow', 2);('api', 2);('rnn- t', 2);('partition dimension', 2);('scaling', 2);('speech recognition model', 2);('predic- tion network', 2);('lstms', 2);('unit size', 2);('prediction network', 2);('loss [', 2);('log probability', 2);('transformer lm', 2);('adam', 2);('specaugment', 2);('j. kim', 2);('s. kim', 2);('systems', 2);('nov.', 2);('c.-c. chiu', 2);('wu', 2);('r. prabhavalkar', 2);('p. nguyen', 2);('a. kannan', 2);('sequence-to-sequence models', 2);('a. misra', 2);('k. chin', 2);('interspeech', 2);('speech recognition', 2);('d. d. lee', 2);('m. sugiyama', 2);('ieee transactions', 2);('robust', 2);('sept.', 2);('a. graves', 2);('g. hinton', 2);('sig-', 2);('improving', 2);('neural network training', 2);('q. v', 2);('a. krizhevsky', 2);('i. sutskever', 2);('r. fergus', 2);('eds.', 2);('toulon', 2);('france', 2);('conference track', 2);('openreview.net', 2);('s. bengio', 2);('h. wallach', 2);('on-', 2);('line ]', 2);('g. e. hinton', 2);('recurrent neural networks', 2);('u. v', 2);('luxburg', 2);('i. guyon', 2);('lecun', 2);('m. shin', 2);('m. kumar', 2);('macro-block dropout for improved regularization in training end-to-end speech recognition models chanwoo kim1', 1);('sathish indurti1', 1);('jinhwan park1', 1);('wonyong sung2 samsung research1', 1);('korea seoul', 1);('university2', 1);('korea', 1);('jh0354.park g @ samsung.com', 1);('wysung @ snu.ac.kr', 1);('abstract', 1);('difcult problem', 1);('large neural network models', 1);('dropout technique', 1);('effec- tive', 1);('complex co-adaptations', 1);('al- gorithm', 1);('different drop', 1);('constant average dropout rate', 1);('algorithm shows', 1);('% word', 1);('algo- rithm shows', 1);('index terms', 1);('introduction deep', 1);('learning technologies', 1);('remarkable changes', 1);('speech recognition al- gorithms', 1);('past decade', 1);('gaussian mixture model', 1);('gmm', 1);('feed-forward deep neural networks', 1);('ff- dnns', 1);('ff-dnns', 1);('short-term memory', 1);('networks [', 1);('voice assistant devices', 1);('google', 1);('home [', 1);('amazon alexa', 1);('samsung bixby', 1);('samsung electronics', 1);('sebastian seung', 1);('vice president', 1);('evp', 1);('daniel lee', 1);('evp seunghwan cho', 1);('language', 1);('team', 1);('lvt', 1);('samsung research.widely', 1);('tremendous', 1);('conventional speech recognition system', 1);('acoustic model', 1);('de- coder', 1);('weighted finite', 1);('transducer', 1);('wfst', 1);('complete end-to-end all-neural speech recognition system [', 1);('such notable shifts', 1);('model architectures', 1);('conventional approaches', 1);('speech signals [', 1);('data augmentation [', 1);('data', 1);('small power boosting', 1);('spb', 1);('algorithm [', 1);('extreme example', 1);('end-to-end speech recognition systems', 1);('] algorithms', 1);('complete end-to-end systems', 1);('wfst-based', 1);('large vocabulary speech recognition tasks [', 1);('end-to-end speech recognition sys- tems', 1);('possible thanks', 1);('target units', 1);('byte pair encoded', 1);('bpe', 1);('unigram lan- guage model [', 1);('] subword units', 1);('training methodologies', 1);('error rate', 1);('mwer', 1);('training [', 1);('neural network structures', 1);('major issue', 1);('regulariza- tion', 1);('various approaches', 1);('data-augmentation', 1);('model training [', 1);('dropout approach [', 1);('approaches [', 1);('thearxiv:2212.14149v1 [ cs.lg ]', 1);('dec', 1);('semantic information', 1);('convolu-', 1);('nearby activations', 1);('cnn', 1);('l- ters', 1);('nearby elements', 1);('activation units', 1);('motivated', 1);('feed-forward layers', 1);('multiple input units', 1);('simple algorithm', 1);('signif- icant improvement', 1);('large macro-blocks', 1);('con- stant dropout rate', 1);('feed- forward', 1);('ff', 1);('knowl- edge', 1);('big chunks', 1);('new way', 1);('varies inmacro-block dropout', 1);('low-cost regularization algorithm', 1);('macro blocks', 1);('random values', 1);('small computational requirement', 1);('related works dropout', 1);('simple regularization technique', 1);('co-adaptations [', 1);('random mask tensor mwith', 1);('dimension dx', 1);('element m2mfollows', 1);('distribution m\x18bernoulli', 1);('dropout output xoutis', 1);('xout=x m 1\x00q', 1);('by1 1\x00qis ap-', 1);('proportion', 1);('kept input units0.00.20.40.60.81.01.2probability n=', 1);('n=fig', 1);('probability mass function', 1);('pmf', 1);('macro blocks aren= 4andn=', 1);('probability density function', 1);('pdf', 1);('n=1', 1);('dropout ap- proach', 1);('original form', 1);('inference time', 1);('input layer', 1);('ra- tio', 1);('n n=', 1);('n.', 1);('central limit theo- rem', 1);('gaussian', 1);('typical values', 1);('q= 0:2and', 1);('large num- ber', 1);('conclude thatpx mpx\x191\x00q', 1);('of1 1\x00qto', 1);('dense network models', 1);('image classication [', 1);('speech recognition [', 1);('dropconnect', 1);('drop-path [', 1);('shake-shake [', 1);('drop- block', 1);('] regularizations', 1);('] claim', 1);('size number', 1);('time0input size number', 1);('two-dimensional', 1);('one-dimensional', 1);('macro-block dropout cases', 1);('tiny rectangle', 1);('grid corresponds', 1);('larger', 1);('rectangular chunks aremacro-blocks', 1);('region', 1);('blue color', 1);('random numbers', 1);('small variation', 1);('conven- tional dropout case', 1);('denition', 1);('two-dimensional data xwith', 1);('dimen- sion', 1);('nx', 1);('neural-network layer', 1);('macro-blocks', 1);('input dxalong', 1);('new dimension', 1);('px', 1);('py', 1);('nx=pxmxand ny=pymywheremxandmyare', 1);('macro block sizes', 1);('y axes', 1);('illustrates examples', 1);('two-dimensional vari-', 1);('number oftime steps', 1);('input size', 1);('2a shows', 1);('two-dimensional region', 1);('two-dimensional macro-block approach', 1);('mask pattern', 1);('time axis', 1);('one-dimensional macro-block approach', 1);('ap- proach', 1);('small number', 1);('stem plot', 1);('probability of0:84=', 1);('probability of\x004 2\x01 0:820:22=', 1);('such large variation', 1);('two-dimensional macro- block dropout approaches', 1);('experimental cong- uration', 1);('one-dimensional macro-block dropout approach', 1);('two- dimensional approach', 1);('one-dimensional approach corresponds', 1);('weight realization', 1);('con- sistent', 1);('bayesian', 1);('litera- tures', 1);('partition dimen- sions d', 1);('two- dimensional cases', 1);('two- dimensional macro- block dropout cases', 1);('entire algorithm', 1);('algorithm', 1);('infer- ence time', 1);('original dropout', 1);('random ten- sorrwhose dimension', 1);('thebernoulli distribution', 1);('dropout probability', 1);('input', 1);('inputs', 1);('dropout rateq', 1);('ifmode ==inference', 1);('return x', 1);('random tensor rwith', 1);('element rofr', 1);('tensor mby', 1);('nearest-neighbour method', 1);('applies', 1);('xm=x m.', 1);('obtains', 1);('output xoutby', 1);('elementsx m xm', 1);('test setbaseline dropoutmacro-block dropout', 1);('% ity', 1);('resize operation', 1);('well-known nearest-neighborhood interpolation approach', 1);('factor ris', 1);('elements x m', 1);('gru', 1);('equation [', 1);('h [ m ] =o [ m ] \x1bh', 1);('c [ m ]', 1);('time index', 1);('hyperbolic tangent function', 1);('o [ m ]', 1);('output-gate value', 1);('andc [ m ]', 1);('cell value', 1);('ob- vious', 1);('h [ m ]', 1);('negative values', 1);('safe division', 1);('tf.math.divide nonan', 1);('zero.we observe', 1);('sincepx mpx\x191\x00qdoes', 1);('large variance', 1);('conven- tional', 1);('of1 1\x00qand', 1);('one-dimensional approach', 1);('experimental con- guration', 1);('1-dimensional macro-block dropout', 1);('test setbaseline dropout1-d macro-block dropout', 1);('using1 1\x00q test-clean', 1);('speech recognizer', 1);('.our speech recognition system', 1);('in-house us- ingkeras models [', 1);('major components', 1);('transcription network', 1);('joint network', 1);('one-dimensional maro-block dropout approaches', 1);('model test setbaseline dropoutone-dimensional macro-block dropout', 1);('different d', 1);('rnn-ttest-clean', 1);('attention-based encoder decodertest-clean', 1);('% encoder', 1);('layers [', 1);('overall temporal', 1);('rst layer', 1);('transducer output', 1);('compo- nents', 1);('attention block', 1);('encoder structure', 1);('connectionist temporal classication', 1);('ctc', 1);('encoder output', 1);('full network', 1);('lctc-rnn-t', 1);('lctc+ lrnn-t', 1);('ctc-rnn-t', 1);('ctc-ce', 1);('lctc-ce', 1);('lctc+ lce', 1);('global norm [', 1);('tf.clip byglobal norm', 1);('shallow-fusion tech- nique', 1);('subtract log', 1);('speech recogni- tion training database', 1);('] .our formulation', 1);('logpsf\x00 yl x [', 1);('m ] \x01 = logp\x00 yl x [', 1);('l\x01 \x00\x15plogpprior', 1);('+\x15lmlogplm\x00 yl ^y0', 1);('output label index l', 1);('x [', 1);('vector sequence', 1);('zero-th frame', 1);('-st frame', 1);('output label sequence', 1);('output index zero', 1);('probabil- ity', 1);('logplm\x00 yl ^y0', 1);('logp\x00 yl x [', 1);('test set baseline dropout macro-block dropout', 1);('] training', 1);('hours test-clean', 1);('hours test-other', 1);('previous work', 1);('optimizer [', 1);('initial learning rate', 1);('full epoch', 1);('learning rate decreases', 1);('decay rate', 1);('x [ m ] andylare', 1);('input power-mel lterbank', 1);('output label', 1);('input frame index', 1);('decoder output step index', 1);('thepower-mel lterbank', 1);('log-mel lterbank', 1);('previous experimental results [', 1);('power-mel lter- bank', 1);('power-law nonlinearity', 1);('power coefcient of1', 1);('data-augmentation technique', 1);('experiments [', 1);('different dropout rates', 1);('macro- block dropout approaches', 1);('con- ventional dropout', 1);('macro-block dropout approaches', 1);('dif- ferent partition sizes', 1);('themacro-block dropout algorithm shows', 1);('performance improvement', 1);('macro-block dropout shows', 1);('conclusions', 1);('new regularization algorithm re-', 1);('random mask', 1);('bet- ter performance', 1);('macro-block dropout comparedto', 1);('references', 1);('d. lee', 1);('a. kumar', 1);('neural end-to-end au- tomatic speech recognition algorithms', 1);('signals', 1);('computers', 1);('r. j. weiss', 1);('k. rao', 1);('e. gonina', 1);('n. jaitly', 1);('b. li', 1);('j. chorowski', 1);('state-of-the-', 1);('art speech recognition', 1);('s. hochreiter', 1);('j. schmidhuber', 1);('long short-term memory', 1);('neural computation', 1);('t. hughes', 1);('generation', 1);('virtual rooms', 1);('train deep-neural networks', 1);('far-eld speech recognition', 1);('google home', 1);('//dx.doi.org/10.21437/interspeech.2017-1510 [', 1);('j. k. chorowski', 1);('d. bahdanau', 1);('d. serdyuk', 1);('k. cho', 1);('attention-based', 1);('c. cortes', 1);('n. d. lawrence', 1);('//papers.nips.cc/paper/5847- attention-based-models-for-speech-recognition.pdf [', 1);('e. a. p. habets', 1);('j. benesty', 1);('i. cohen', 1);('s. gannot', 1);('j. dmo-', 1);('new insights', 1);('mvdr beamformer', 1);('room acoustics', 1);('lan-', 1);('jan', 1);('ephraim', 1);('h. van trees', 1);('signal subspace approach', 1);('speech enhancement', 1);('audio processing', 1);('power', 1);('power dis- tribution normalization algorithm', 1);('robust speech recogni- tion', 1);('understand-', 1);('interspeech-2014', 1);('s. shon', 1);('domain', 1);('mismatch robust acoustic scene classication', 1);('channel information conversion', 1);('acous-', 1);('j. s. chung', 1);('j. huh', 1);('delving', 1);('environment invariant', 1);('recognition', 1);('proc', 1);('odyssey', 1);('language recognition', 1);('odyssey.2020-49', 1);('u. h. yapanel', 1);('j. h. l. hansen', 1);('mvdr-based', 1);('acoustic front-end', 1);('pmvdr', 1);('ro- bust', 1);('speech communication', 1);('feb.', 1);('power-normalized cepstral coef-', 1);('pncc', 1);('robust speech recognition', 1);('ieee/acm trans', 1);('lang', 1);('process', 1);('july', 1);('t. sainath', 1);('r. nongpiur', 1);('spectral', 1);('distortion model', 1);('training phase-', 1);('sensitive deep-neural networks', 1);('far-eld speech recogni- tion', 1);('e. variani', 1);('efcient', 1);('room simulator', 1);('neural network acoustic models', 1);('interspeech-', 1);('sept', 1);('//dx.doi.org/10.21437/interspeech.2018-2566 [', 1);('k. kumar', 1);('speech recogni- tion', 1);('small power', 1);('a.', 1);('mohamed', 1);('deep recurrent neural networks', 1);('t. kudo', 1);('subword', 1);('neural net- work translation models', 1);('multiple subword candidates', 1);('annual meeting', 1);('as-', 1);('computational linguistics', 1);('papers', 1);('melbourne', 1);('australia', 1);('computa-', 1);('linguistics', 1);('jul', 1);('//www.aclweb.org/anthology/p18-1007 [', 1);('c. chiu', 1);('word error rate train-', 1);('s. indurthi', 1);('small', 1);('end-to-end speech recog- nition', 1);('stream-', 1);('ieee interna-', 1);('tional conference', 1);('d. s.', 1);('w. chan', 1);('zhang', 1);('b. zoph', 1);('e. d. cubuk', 1);('simple data augmentation method', 1);('//dx.doi.org/10.21437/interspeech.2019-2680 [', 1);('s.', 1);('d. k. han', 1);('h. ko', 1);('generative', 1);('adver- sarial network', 1);('acoustic scene training', 1);('svm hyper-plane', 1);('dcase', 1);('n. srivastava', 1);('r. salakhutdinov', 1);('simple way', 1);('//jmlr.org/papers/v15/srivastava14a.html [', 1);('l. wan', 1);('m. zeiler', 1);('s. zhang', 1);('l. cun', 1);('regularization', 1);('s. dasgupta', 1);('d. mcallester', 1);('atlanta', 1);('georgia', 1);('pmlr', 1);('jun', 1);('//proceedings.mlr.press/v28/wan13.html [', 1);('g. larsson', 1);('m. maire', 1);('g. shakhnarovich', 1);('fractalnet', 1);('ultra-deep', 1);('id=s1vab4cex [', 1);('x. gastaldi', 1);('shake-shake', 1);('3-branch residual networks', 1);('workshop track', 1);('pcmyl', 1);('g. ghiasi', 1);('t.-y', 1);('lin', 1);('regulariza- tion method', 1);('convolutional networks', 1);('neu-', 1);('information processing systems', 1);('h. larochelle', 1);('k. grauman', 1);('n. cesa-bianchi', 1);('//papers.nips.cc/paper/8271-dropblock- a-regularization-method-for-convolutional-networks.pdf [', 1);('imagenet', 1);('deep convolutional neural networks', 1);('f. pereira', 1);('c. j. c. burges', 1);('l. bottou', 1);('k. q. weinberger', 1);('//papers.nips.cc/paper/4824-imagenet- classication-with-deep-convolutional-neural-networks.pdf [', 1);('g. e. dahl', 1);('deep neural networks', 1);('linear units', 1);('yamada', 1);('m. iwamura', 1);('t. akiba', 1);('k. kise', 1);('deep residual learning', 1);('ieee access', 1);('gal', 1);('z. ghahramani', 1);('neural information processing sys-', 1);('curran asso-', 1);('//papers.nips.cc/paper/6241-a-theoretically-grounded- application-of-dropout-in-recurrent-neural-networks.pdf [', 1);('t. moon', 1);('h. choi', 1);('h. lee', 1);('rnndrop', 1);('novel dropout', 1);('m. abadi', 1);('p. barham', 1);('j. chen', 1);('a. davis', 1);('dean', 1);('m. devin', 1);('s. ghemawat', 1);('g. irving', 1);('m. isard', 1);('m. kudlur', 1);('j. levenberg', 1);('r. monga', 1);('s. moore', 1);('d. g. murray', 1);('b. steiner', 1);('p. tucker', 1);('vasudevan', 1);('p. warden', 1);('m. wicke', 1);('yu', 1);('x. zheng', 1);('tensorow', 1);('large-scale machine learning', 1);('usenix symposium', 1);('osdi', 1);('savannah', 1);('usenix', 1);('//www.usenix.org/conference/ osdi16/technical-sessions/presentation/abadi [', 1);('f. chollet', 1);('keras', 1);('m. ranzato', 1);('f. j. huang', 1);('boureau', 1);('unsuper-', 1);('computer vision', 1);('pattern recognition', 1);('s. fern', 1);('f. gomez', 1);('j. schmid-', 1);('connectionist', 1);('temporal classication', 1);('labelling', 1);('sequence data', 1);('icml', 1);('york', 1);('acm', 1);('//doi.acm.org/10.1145/1143844.1143891 [', 1);('r. pascanu', 1);('t. mikolov', 1);('training recurrent neural networks', 1);('icml13', 1);('jmlr.org', 1);('iii1310iii1318', 1);('id=3042817.3043083 [', 1);('n. kanda', 1);('x. lu', 1);('h. kawai', 1);('maximum-a-posteriori-based', 1);('end-to-end acoustic models', 1);('ieee/acm trans-', 1);('language processing', 1);('improved', 1);('vocal tract length perturbation', 1);('state-of-the-art end-to- end speech recognition system', 1);('interspeech-2019', 1);('graz', 1);('austria', 1);('//dx.doi.org/10.21437/interspeech.2019-3227 [', 1);('a. vaswani', 1);('n. shazeer', 1);('n. parmar', 1);('j. uszkoreit', 1);('l. jones', 1);('a. n. gomez', 1);('l.', 1);('kaiser', 1);('i. polosukhin', 1);('attention', 1);('s. vishwanathan', 1);('//papers.nips.cc/paper/7181-attention-is-all-you-need.pdf [', 1);('panayotov', 1);('g. chen', 1);('d. povey', 1);('s. khudanpur', 1);('lib-', 1);('asr corpus', 1);('public domain audio books', 1);('k. lee', 1);('e. kim', 1);('s. singh', 1);('l. heck', 1);('end-to-end', 1);('large vocabulary end-to-end speech recognition system', 1);('ieee automatic speech recog-', 1);('d. p. kingma', 1);('j. ba', 1);('stochastic optimization', 1);('san diego', 1);('ca', 1);('//arxiv.org/abs/1412.6980 [', 1);('power-law', 1);('non- linearity', 1);('uniform distribution criterion', 1);('automatic speech recogni- tion', 1);('un-', 1);('signal processing', 1);('robust speech recognition mo-', 1);('auditory processing', 1);('ph.d.', 1);('carnegie mellon', 1);('pittsburgh', 1);('pa usa', 1);