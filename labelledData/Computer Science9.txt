('wikipedia', 16);('qdec-fp', 9);('qdec-fps', 8);('* * * *', 7);('sdr', 5);('shekhar', 5);('brafman', 4);('hm b1', 4);('hm b2', 4);('ht b4', 4);('ht b5', 4);('ht b6', 4);('ht b9', 4);('variant type domain # bts time width height', 4);('hm t1', 4);('hm t3', 4);('ht t6', 4);('ht t9', 4);('ht t11', 4);('complex problems', 4);('guy shani', 4);('free encyclopedia', 4);('online', 4);('articial intelligence', 3);('qdec-pomdp', 3);('2021b ]', 3);('type domain # bts time width height', 3);('simple problems', 3);('proceedings', 3);('automated', 3);('markov', 3);('1-december-2022 ]', 3);('2022a ]', 2);('mdp', 2);('2022b ]', 2);('state space', 2);('ais', 2);('action space', 2);('ris', 2);('pomdp', 2);('2022c ]', 2);('tis', 2);('transition probabilities', 2);('reward function', 2);('observation probabilities', 2);('discount factor', 2);('dec-pomdp', 2);('ai', 2);('{ i }', 2);('agent i', 2);('actio n', 2);('shani', 2);('cpor', 2);('maliah', 2);('individual agent', 2);('new team solution', 2);('ht b10', 2);('ht b7', 2);('ht b8', 2);('team plans', 2);('ht t12', 2);('ht t13', 2);('ht t14', 2);('aaai', 2);('partially', 2);('shashank shekhar', 2);('ronen i. braf-', 2);('may', 2);('decision process', 2);('arxiv:2301.01246v1 [ cs.ai ]', 1);('jan', 1);('2023efcient method', 1);('diverse agents', 1);('qdec-pomdps nitsan soffair ben gurion', 1);('university soffair @ post.bgu.ac.il', 1);('abstract', 1);('sota', 1);('qdec- pomdp', 1);('tackle problems', 1);('dif- ferent types', 1);('new algorithm', 1);('algo- rithm performs', 1);('qdec- fp', 1);('introduction automated', 1);('scheduling [', 1);('str ate- gies', 1);('action sequences', 1);('intelligent agents', 1);('autonomou s robots', 1);('complex multidimensional spaces', 1);('decision theory', 1);('unknown environments', 1);('background', 1);('pis', 1);('action ain stateswill lead', 1);('immediate reward', 1);('policy function', 1);('time period', 1);('observation owhich dependson', 1);('new state', 1);('ac- tiona', 1);('probability o', 1);('rewardrequal tor', 1);('aiis', 1);('joint actions', 1);('joint observa- tions', 1);('ois', 1);('time step', 1);('state updates', 1);('transition function', 1);('agent observes', 1);('observation func', 1);('whole team', 1);('r.', 1);('decision-making process', 1);('multiple agents i n', 1);('dynamic environment', 1);('ac- tions', 1);('uses pol-', 1);('icy trees', 1);('local plans', 1);('eac h node', 1);('agent performs', 1);('ac- tion', 1);('future action selec', 1);('] planner', 1);('current belief state', 1);('determin- istic', 1);('classical problem', 1);('belie f state', 1);('explicit description', 1);('belief state', 1);('algorithm uses', 1);('lazy belief-state maintenance', 1);('] algorithm', 1);('planner uses', 1);('classical projection t oplan', 1);('observation action', 1);('myopic value', 1);('future observations', 1);('cal cu-', 1);('disjunctive action landmarks', 1);('factored', 1);('algorithm [', 1);('2021a ] rst creates', 1);('single- agent team problem', 1);('team solution tree', 1);('ea ch', 1);('local policy', 1);('local problems', 1);('actions ar e', 1);('agents canno t', 1);('qdec-fp qdec-fp', 1);('three-stage process', 1);('multi-agent problems', 1);('rst stage', 1);('team so- lution', 1);('pro- jection', 1);('team solution', 1);('agent plan tre es', 1);('represen t-', 1);('tra ns- forms preconditions', 1);('possible worlds', 1);('qdec- fps', 1);('proposi- tion', 1);('algorithm', 1);('rst step', 1);('pre- pare', 1);('sensory capabiliti es', 1);('team plan', 1);('observatio ns', 1);('subsequent steps', 1);('domains', 1);('box-pushing', 1);('different locations', 1);('light box', 1);('heavy box', 1);('different boxes.4.2', 1);('table-mover', 1);('exact location', 1);('different capabilities', 1);('mani p-', 1);('ta- bles', 1);('4-core pro- cessor', 1);('hm', 1);('ht', 1);('planning process', 1);('maximum tree width', 1);('maximum tree height', 1);('th', 1);('e winner', 1);('qdec', 1);('box-pushing grid', 1);('b1', 1);('additional costs', 1);('back- tracks', 1);('vari- ant', 1);('quality tree', 1);('variant focuses', 1);('agen t', 1);('valid team plans', 1);('cpor.qdec-fps', 1);('quality trees', 1);('1+ back- tracks', 1);('qual ity trees', 1);('agent constrain ts', 1);('backtrack mech- anism', 1);('table-mover t1', 1);('plan trees.qdec-fps type domain # bts time width height', 1);('conclusion', 1);('planning algorithm', 1);('high qual- ity tree plans', 1);('smal l overhead', 1);('complex communication', 1);('certain domains', 1);('references', 1);('r. i. brafman', 1);('g. shani', 1);('re-', 1);('partial information', 1);('ronen i. brafman', 1);('shlomo zilberstein', 1);('qualitative', 1);('partial ob', 1);('multi-agent domains', 1);('shlomi maliah', 1);('ronen brafman', 1);('erez karpas', 1);('observable online con- tingent planning', 1);('landmark heuristics', 1);('twenty- fourth', 1);('international conference', 1);('scheduling', 1);('2021a ]', 1);('deterministic contingent multi-agent planning', 1);('inter-', 1);('national conference', 1);('schedul-', 1);('improved', 1);('multi-agent planning', 1);('partial observability', 1);('ar-', 1);('intelligence', 1);('decentralized', 1);('observable markov deci- sion process', 1);('decision pro- cess', 1);('title=markov decision process', 1);('contrib- utors', 1);('observable markov decision process', 1);