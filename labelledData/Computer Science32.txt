('] [', 41);('computer vision', 28);('rgb', 27);('beijing', 19);('segment scale', 18);('action prediction', 17);('ieee', 17);('china', 14);('pattern recognition', 14);('global scale', 13);('ucf101', 12);('fig', 11);('ieee/cvf', 11);('international conference', 11);('lstm', 10);('e2emsnet', 9);('hand', 9);('hmdb51', 8);('posts', 8);('artificial intelligence', 8);('proceedings', 8);('bit', 7);('kong y', 7);('telecommunications', 6);('temporal information', 6);('short -term', 6);('cnn', 6);('ieee transactions', 6);('video', 6);('ieee transactions on circuits and systems for video technology', 5);('prediction', 5);('wang', 5);('partial video', 5);('recognition', 5);('observation ratio', 5);('temporal', 5);('observation rate', 5);('mtssvm', 5);('-gcn [', 5);('avg', 5);('learning', 5);('circuits', 5);('fu y', 5);('research interests', 5);('jianqin yin', 4);('previous method', 4);('action evolution', 4);('action recognition', 4);('full video', 4);('data processing method', 4);('action categories', 4);('strr', 4);('3d-cnn +', 4);('flow', 4);('blow dry hair', 4);('haircut', 4);('head massage', 4);('pattern analysis', 4);('machine intelligence', 4);('systems', 4);('aaai', 4);('ph.d.', 4);('yuan sun', 3);('zhicheng zhang', 3);('partial videos', 3);('grant', 3);('e -mail', 3);('spatio -temporal representation', 3);('time dimension', 3);('temporal dimension', 3);('compared', 3);('temporal scales', 3);('slowfast', 3);('specifically', 3);('k=', 3);('network structure', 3);('equation', 3);('aapnet', 3);('global', 3);('deepscn', 3);('to', 3);('method input feature', 3);('skeleton lstm', 3);('ours', 3);('2d-cnn +', 3);('experimental results', 3);('different observation rates', 3);('chen', 3);('lu j', 3);('hu j', 3);('ji', 3);('videos [', 3);('tran d', 3);('european conference', 3);('springer', 3);('wu x', 3);('image processing', 3);('professor', 3);('member', 2);('end -to-end', 2);('multi', 2);('class label', 2);('different', 2);('continuous evolution', 2);('modern post', 2);('telecom', 2);('jin tang', 2);('electronic engineering', 2);('secondly', 2);('spatio -temporal information', 2);('2d convolution', 2);('feichtenhofer', 2);('video recognition', 2);('action prediction methods', 2);('temporal scale', 2);('temporal difference', 2);('global scale uses', 2);('action evolution information', 2);('computational load', 2);('local evolution', 2);('global evolution', 2);('human actions', 2);('cnns', 2);('optical flow', 2);('non', 2);('stm', 2);('3d convolution', 2);('tam', 2);('tdn', 2);('full observations', 2);('partial observation', 2);('partial data', 2);('long -term', 2);('-term dependencies', 2);('different granularities [', 2);('problem formulation', 2);('observation rates', 2);('previous section', 2);('local temporal window', 2);('state -of-the-art methods', 2);('dbow', 2);('mmapm', 2);('deep', 2);('rgn', 2);('-kf [', 2);('rspg', 2);('as', 2);('aorap', 2);('aase', 2);('-local [', 2);('akt', 2);('teacher', 2);('-student [', 2);('resnet18', 2);('xinxiao wu', 2);('rgn-kf', 2);('rspg+as', 2);('accuracy rate', 2);('observation rate increases', 2);('observation ratios', 2);('influence', 2);('multi -scale architecture', 2);('ablation study', 2);('observation', 2);('confusion matrix', 2);('progress level', 2);('video recognition [ c ] //proceedings', 2);('tao z', 2);('action prediction [ c ] //proceedings', 2);('early', 2);('sun', 2);('wang x', 2);('zhao h', 2);('gan', 2);('action recognition [ c ] //', 2);('wang h', 2);('torresani', 2);('eccv', 2);('cham', 2);('wang r', 2);('hou j', 2);('fernando', 2);('zhao y', 2);('xiong y', 2);('machine learning', 2);('end-to-end multi-scale network', 1);('videos xiaofa liu', 1);('iee e', 1);('jin tang abstract', 1);('efficient multi -scale network', 1);('action classes', 1);('end -to- end manner', 1);('mod els motion evolution', 1);('different temporal scales', 1);('complexity problems', 1);('insufficient temporal', 1);('spatial information', 1);('scale network', 1);('segment scale leverages temporal difference', 1);('consecutive frames', 1);('finer motion patterns', 1);('2d convolutions', 1);('short', 1);('memory', 1);('small computational cost', 1);('challenging datasets', 1);('extensive experiments', 1);('effective -ness', 1);('index', 1);('multi -scale network', 1);('end-to- end method', 1);('introduction he', 1);('temporal axis', 1);('broader research domain', 1);('human activity analysis', 1);('conventional action recognition', 1);('actions [', 1);('action label', 1);('research attention', 1);('wide application', 1);('high r eal-time requirements', 1);('machine interaction', 1);('security surveillance', 1);('previous work', 1);('promising results', 1);('national natural science foundation', 1);('fundamental research', 1);('universities', 1);('a04-3', 1);('natural science foundation', 1);('hainan province', 1);('corresponding', 1);('xiaofa liu', 1);('liuxiaofamail @ 163.com', 1);('jqyin @ bupt.edu.cn', 1);('zczhang @ bupt.edu .cn', 1);('tangjin @ bupt.edu.cn', 1);('sunyuan @ bu pt.edu.cn', 1);('-stage approach', 1);('predictive model', 1);('separation operati', 1);('action predi ction', 1);('model design', 1);('complete action', 1);('als o', 1);('w e', 1);('end -to-end method', 1);('end -to-end network', 1);('local spatio -temporal information representati', 1);('long-term time sequence fusion', 1);('end -to- end structure', 1);('generation method', 1);('part', 1);('computational consumption', 1);('chieve end -to-end structure', 1);('-stream network s', 1);('3d convolutions', 1);('extract local spatio -temporal', 1);('fuse th e', 1);('historical evolution information', 1);('spatial multi -scale', 1);('image field', 1);('multi -scale research', 1);('video analytics', 1);('v ideos poses', 1);('motion evolution information', 1);('different time scales', 1);('video motion analysis', 1);('method utilizes', 1);('slow pathway', 1);('low frame rate', 1);('fast pathway', 1);('high frame rate', 1);('spatial sema ntics', 1);('fine temporal resolution', 1);('efficient multi -scale model', 1);('long -term temporal difference module s', 1);('long -term motion information', 1);('multi -scale temporal', 1);('frame rate', 1);('th ese method s simplif y', 1);('certain extent', 1);('too', 1);('local evolution information', 1);('adjacent frames', 1);('global evolution information', 1);('video sequence', 1);('timing information', 1);('firstly', 1);('segment scale uses', 1);('long -term time scales', 1);('effective framework', 1);('n summary', 1);('main contribution s lie', 1);('simple end -to-end approach', 1);('ou r knowledge', 1);('segment summarization', 1);('propagation framework', 1);('trade -off', 1);('state -of-the-art performance', 1);('2d convolutions framework', 1);('ii', 1);('related work', 1);('action recognition methods', 1);('output labels', 1);('years [', 1);('methods', 1);('model appearance', 1);('motion information', 1);('late fusion', 1);('-up research', 1);('inputs samp', 1);('fps', 1);('temporal module s', 1);('-local net [', 1);('correlation', 1);('net [', 1);('s [', 1);('2d + 1d paradigm', 1);('computation cost', 1);('wh ich', 1);('1d temporal convolution [', 1);('efficient temporal modules', 1);('tsm', 1);('tea [', 1);('recent work s', 1);('architecture search', 1);('context information', 1);('efficient manner [', 1);('full action executions', 1);('core ideas', 1);('c ertain reference significance', 1);('dynamic ba g-of-words approach', 1);('observations increase', 1);('researchers approach', 1);('various perspectives', 1);('major divisions [', 1);('one-shot mappings', 1);('partial observations', 1);('groundtruth labels', 1);('basic assumption', 1);('action video', 1);('sufficient information', 1);('appropriate overall action class regardless', 1);('-up research work [', 1);('hierarchical extractions', 1);('initial partial observation', 1);('knowledge distillation', 1);('methods distill', 1);('partial observations [', 1);('feature representation', 1);('temporal extrapolation fashion [', 1);('propagate frame -wise residuals', 1);('complete partial observation', 1);('relevant', 1);('ieee transactions on circuits and systems for video technology c. multiple', 1);('action analysis', 1);('periodic motion', 1);('long-term forecasts need', 1);('trend information', 1);('term dependencies', 1);('-term forecasts need', 1);('short -term dependencies', 1);('current difficulty', 1);('dynamic dependencies', 1);('decomposin g', 1);('original data', 1);('gate mechanism', 1);('internal structure', 1);('rnn', 1);('trade -off approach', 1);('iii', 1);('our method', 1);('detail ou r approach', 1);('multiple scales', 1);('end -to-end fashion', 1);('ou r end-to-end framework', 1);('multi -scale', 1);('o f', 1);('action sequences', 1);('problem', 1);('human motion', 1);('incomplete motion', 1);('subsequent work [', 1);('full video [', 1);('xt', 1);('complete action ex ecution', 1);('] x t t', 1);('action execution', 1);('facilitate quantitative experiments', 1);('ea ch', 1);('tk', 1);('] kth k', 1);('/ r k', 1);('fig.1', 1);('observation ratio /', 1);('r k', 1);('= =', 1);('data', 1);('upper part', 1);('complete video', 1);('simulate action evolution', 1);('partial video needs', 1);('feature extraction', 1);('previous partial videos', 1);('local spatio -temporal representations', 1);('s egment', 1);('previous spatio -temporal information', 1);('action duration', 1);('abundant spatio -temporal information', 1);('differences', 1);('data processing', 1);('previous methods', 1);('data processing strategy', 1);('ieee transactions on circuits and systems for video technology c. network', 1);('t o', 1);('model action evolution', 1);('fo r', 1);('short time windows', 1);('global features', 1);('local videos', 1);('segment', 1);('dynamic sequence', 1);('temporal context relationship', 1);('spatial relationship organization', 1);('frame need', 1);('tw o kinds', 1);('local time windows', 1);('redundant information', 1);('image f rame', 1);('dynamic information', 1);('temporal window', 1);('efficient alternative modality', 1);('motion representation [', 1);('spatio -temporal', 1);('extraction module', 1);('prediction problem', 1);('current frame', 1);('tdm', 1);('temporal difference module', 1);('] t t t t t', 1);('+ +=', 1);('difference information', 1);('2d convolutions network', 1);('upsample cnn downsample d i=', 1);('original frame -level representation', 1);('original features ti', 1);('actual experiment', 1);('passes th', 1);('s fuse s', 1);('cnn i=+', 1);('i s', 1);('s fuse', 1);('cnn downsample d i=+', 1);('observed', 1);('human body', 1);('representative actions', 1);('local spa tio- temporal action', 1);('time progresses', 1);('historical sequence', 1);('reconstru ct', 1);('historical global evolution', 1);('overview', 1);('network', 1);('module extracts', 1);('local motion evolution', 1);('temporal model', 1);('chronological order', 1);('global action evolution', 1);('actual scene', 1);('end time', 1);('means th', 1);('overall length', 1);('variable -length input characteristics', 1);('global spatiotemporal characteristics', 1);('historical observations', 1);('i l s out=', 1);('action evolves', 1);('short -term time window', 1);('historical observation', 1);('implemented', 1);('segments progressive', 1);('spatiotemporal relationship', 1);('subsequent segments', 1);('progressive manner', 1);('historical global history', 1);('additional computational consumption', 1);('iv', 1);('experiments', 1);('experiment results', 1);('evaluation datasets', 1);('implementation details', 1);('a. datasets', 1);('video datasets', 1);('human interactions', 1);('high -five', 1);('videos', 1);('realistic scenes', 1);('body parts', 1);('subject appearance', 1);('illumination condition', 1);('v iewpoint', 1);('complex dataset', 1);('large -scale human action recognition dataset', 1);('co mprises', 1);('human facial motions', 1);('static background windows', 1);('comparable datasets', 1);('video clips', 1);('official data splits', 1);('youtube', 1);('video cont ains', 1);('distinct action classes', 1);('overall video clips', 1);('video s', 1);('train/test splits', 1);('implementation deta ils', 1);('thanks', 1);('end -to-end network structure design', 1);('various video datasets', 1);('resnet50', 1);('short -term module', 1);('step size', 1);('differential information', 1);('employ convolutional layers pre', 1);('video frames', 1);('video frame', 1);('shorter side', 1);('nvidia geforce rtx', 1);('official settings', 1);('standard evalu ation protocol', 1);('average accuracy', 1);('model training', 1);('model validation', 1);('groups f', 1);('c. comparison', 1);('state-of-the-art methods', 1);('-scn [', 1);('gcn', 1);('spr', 1);('-net [', 1);('jvs', 1);('jcc', 1);('jfip', 1);('tableillustrate', 1);('significant improvements', 1);('observation rates fr om', 1);('reliable predictions', 1);('actions evolve', 1);('table', 1);('the accuracy', 1);('of different action prediction methods on bit dataset at different observ ation ratios from', 1);('note that the missing value is because the experimental results of the corresponding observ ation rate are not provided in the original paper', 1);('hand-crafted', 1);('ieee transactions on circuits and systems for video technology mtssvm', 1);('jolo', 1);('ocrl', 1);('tableshows', 1);('table shows', 1);('early stages', 1);('competitive performance', 1);('performance improvement', 1);('ucf101datasets', 1);('good performance', 1);('long time windows', 1);('insuff icient', 1);('table ii the accuracy', 1);('of different action prediction methods on hmdb51 dataset at different observ ation ratios from', 1);('note that the missing value is because the experimental results of the corresponding observ ation rate are not provided in the original pa per', 1);('iii the accuracy', 1);('of different action prediction methods on ucf101 dataset at different observ ation ratios from', 1);('note that the missing value is because the experimental results of the corresponding observ ation rate are no t provided in the original paper', 1);('ieee transactions on circuits and systems for video technology aapnet', 1);('spr-net', 1);('jvs+jcc+jfip', 1);('d. ablation', 1);('evaluation results', 1);('illustrates', 1);('different scale architecture', 1);('different settings', 1);('table iv the accuracy', 1);('at different scale settings on the ucf101 dataset', 1);('segment scale +', 1);('module s', 1);('different scale information', 1);('action clips', 1);('complete structure', 1);('average accuracy differe nce', 1);('multi -scale structure', 1);('nd discriminative degree', 1);('long-time scales', 1);('complex actions', 1);('long duration', 1);('conversely', 1);('prediction process', 1);('long -duration actions', 1);('scale settings', 1);('different hyperparameter settings', 1);('comparative experiments', 1);('e. anal', 1);('ysis o f', 1);('different actions', 1);('human -object interaction', 1);('body', 1);('human -human interaction', 1);('playing', 1);('musical instruments', 1);('classification results', 1);('blowing candles', 1);('kitchen', 1);('apply eye makeup', 1);('baby crawling', 1);('pull ups', 1);('punch', 1);('playing guitar', 1);('playing piano', 1);('playing violin', 1);('dunk', 1);('classification layer', 1);('stable prediction performance', 1);('different scenarios', 1);('similar external', 1);('fig6', 1);('n appearance comparison', 1);('mispredicted.89.59090.59191.59292.593 0.10.20.30.40.50.60.70.80.91.0accuracy', 1);('segment-scale two-scales8 ieee transactions on circuits and systems for video technology table v the accuracy', 1);('on ucf101 dataset under several hyperparameters', 1);('note', 1);('limited by resources and time', 1);('our experimental results do not guarante e that all hyperparameters ha ve been adjusted to the optimum', 1);('appearance', 1);('v. conclusion', 1);('network model', 1);('temporal sca les', 1);('end-to-end framework', 1);('2d convolutional layers', 1);('layer fuses', 1);('term evolution', 1);('experimental validation', 1);('method possesses', 1);('powerful local scale', 1);('time scale', 1);('future work', 1);('hyperparameter', 1);('hidden', 1);('decay', 1);('decay rate=0.1', 1);('ieee transactions on circuits and systems for video technology references', 1);('liu j', 1);('shahroudy', 1);('wang g', 1);('skeleton', 1);('line action prediction', 1);('scale selection network [', 1);('hou', 1);('z. li', 1);('p. wa', 1);('w. li', 1);('skeleton optical spectra', 1);('convolutional neural networks', 1);('h. luo', 1);('g. li', 1);('yao', 1);('z. tang', 1);('q. wu', 1);('x. hua', 1);('dense semantics', 1);('networks', 1);('fan h', 1);('malik j', 1);('sequential context networks', 1);('computer vis ion', 1);('li', 1);('representation learning', 1);('instructional video prediction', 1);('syste', 1);('cai y', 1);('li h', 1);('ac', 1);('tion knowledge', 1);('partial videos [ c ] //procee -dings', 1);('recurrent', 1);('action prediction [', 1);('eee transactions', 1);('ryoo', 1);('s.', 1);('human activity prediction', 1);('videos [ c ] //2011', 1);('gao', 1);('hard -to-predict samples [ c ] //', 1);('lai j h', 1);('progressive', 1);('student learning', 1);('early action prediction [ c ] //', 1);('wildes r p. spatiotemporal', 1);('residual propagation', 1);('ieee/cvf internat', 1);('ional conference', 1);('tong z', 1);('difference networks', 1);('efficient action recognition [ c ] //pro', 1);('lin j', 1);('han s. tsm', 1);('efficient video understanding [ c ] //proceedings', 1);('simonyan k', 1);('zisserman', 1);('-stream convolution -al networks', 1);('action recognitio n', 1);('advances', 1);('neural information processing systems', 1);('girshick r', 1);('gupta', 1);('-local neural networks [ c ] //proceedings', 1);('jiang', 1);('m m', 1);('spatiotemporal', 1);('correlation networks [ c ] //p roceedings', 1);('xu', 1);('yang', 1);('3d convolutional neural networks', 1);('human action recognition [', 1);('bourdev', 1);('fergus r', 1);('3d convolutional networks [ c ] //proceedings', 1);('spatiotemporal convolutions', 1);('xie', 1);('huang j', 1);('rethinking', 1);('speed', 1);('-accuracy trade -offs', 1);('video classification [ c ] //proceedings', 1);('li k', 1);('li x', 1);('wang y', 1);('ct', 1);('channel tensorization network', 1);('video classification [', 1);('arxiv preprint arxiv:2106.01603', 1);('liu z', 1);('wan', 1);('g l', 1);('wu', 1);('adaptive module', 1);('li y', 1);('shi x', 1);('tea', 1);('action recognition [ c ] //proceedings', 1);('feichtenhofer c. x3d', 1);('expanding', 1);('efficient video recognition [ c ] //proceedings', 1);('wildes r p. review', 1);('video predictive understanding', 1);('arxiv preprint arxiv:2107', 1);('kit d', 1);('discriminative model', 1);('multiple temporal sca les', 1);('action prediction [ c ] //', 1);('singh g', 1);('saha', 1);('sapienza', 1);('online', 1);('real -time', 1);('multiple spatiotemporal action localisation', 1);('prediction [ c ] //proceedings', 1);('spatial', 1);('temporal relation reasoning', 1);('international journal', 1);('max', 1);('-margin action predictio n machine [', 1);('herath s. anticipating', 1);('jaccard similarity measures [ c ] //proceedings', 1);('ondrick c', 1);('pirsiavash h', 1);('torral', 1);('anticipating', 1);('visual representations', 1);('video [ c ] //', 1);('shi y', 1);('hartley r.', 1);('action anticipation', 1);('rnn [ c ] //', 1);('gammulle h', 1);('denman', 1);('sridharan', 1);('predicting', 1);('learnt model', 1);('action anticipation [ c ] //proceedings', 1);('co', 1);('vision', 1);('chen j', 1);('bao', 1);('group activity prediction', 1);('sequential relational anticipation model [ c ] //european conference', 1);('oord', 1);('dieleman', 1);('zen h', 1);('wavenet', 1);('generativ e model', 1);('raw audio [', 1);('arxiv preprint arxiv:1609.03499', 1);('heng wang', 1);('alexander klser', 1);('cordelia schmid', 1);('liu cheng', 1);('dense trajectories', 1);('cvpr', 1);('jun', 1);('colorado', 1);('ffinria -00583818f [', 1);('zheng z', 1);('ruan q. multi', 1);('-level recurrent residual networks', 1);('action recognition [', 1);('arxiv preprint arxiv:1711.08238', 1);('tempora', 1);('l action detection', 1);('segment networks [ c ] //', 1);('chung j', 1);('ahn', 1);('bengio y', 1);('hierarchical', 1);('multiscale recurrent neural networks [', 1);('arxiv preprint arxiv', 1);('wang j', 1);('wang z', 1);('li j', 1);('multilevel', 1);('wavelet decomposition network', 1);('interpretable time series', 1);('analysis [ c ] //proceedings', 1);('acm sigkdd inter', 1);('national conference', 1);('knowledge discovery', 1);('data mining', 1);('hu h', 1);('qi g j', 1);('scale recurrent neural networks [ c ] //proceedings', 1);('campos v', 1);('jo', 1);('u b', 1);('gir', 1);('skip', 1);('state updates', 1);('recurrent neural networks [', 1);('arxiv preprint arxiv:1708.06834', 1);('lin d. recognize', 1);('dynamics [ c ] //procee -dings', 1);('ieee conf', 1);('jia y', 1);('human interaction', 1);('interactive phrases [ c ] //european conference', 1);('berlin', 1);('heidelberg', 1);('kuehne h', 1);('jhuang h', 1);('garrote e', 1);('hmdb', 1);('large video database', 1);('human motion recognition [ c ] //', 1);('soomro k', 1);('zamir', 1);('shah m. ucf101', 1);('human actions classes', 1);('wild [', 1);('arxiv preprint arxiv:1212.0402', 1);('adversarial', 1);('action prediction networks [', 1);('liu', 1);('gao y', 1);('li z', 1);('prediction network', 1);('auxiliary observ', 1);('ratio regression', 1);('[ c ] //', 1);('multimedia', 1);('expo', 1);('icme', 1);('ambiguousness', 1);('-aware state', 1);('evolution', 1);('video te', 1);('lai', 1);('zheng', 1);('w s', 1);('-local temporal saliency action prediction [', 1);('confidence', 1);('self refinement', 1);('zhao j', 1);('wang r. anticipating', 1);('graph growing', 1);('predict', 1);('ion [ c ] //', 1);('ieee transactions on circuits and systems for video technology xiaofa liu', 1);('b.s', 1);('hohai', 1);('nanjing', 1);('m.s', 1);('mechanical engineering', 1);('shandong', 1);('jinan', 1);('service robot', 1);('astronautics', 1);('satellite navigation technology', 1);('satellite autonomous integrity', 1);('jilin', 1);('changchun', 1);('associate professor', 1);('beiji', 1);('intelligent', 1);('signal detection', 1);('n assistant', 1);('signal processing', 1);('deep learning', 1);