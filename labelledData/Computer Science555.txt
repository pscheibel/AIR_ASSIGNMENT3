('rl', 43);('dises', 30);('drl', 17);('od', 16);('ppo', 14);('figure', 11);('gp', 7);('cgp', 6);('space', 6);('reinforcement learning', 5);('symbolic expression', 4);('symbolic policy', 4);('ii', 4);('ieee', 4);('pmlr', 4);('symbolic expression search dises', 3);('dnns', 3);('equation', 3);('symbolic regression', 3);('iii', 3);('visualization', 3);('10t otal', 3);('international conference', 3);('competitive performance', 2);('dierentiable optimization', 2);('objectlevel abstractions', 2);('dnnbased rl', 2);('agents symbolic', 2);('atari', 2);('symbolic planning', 2);('dnn', 2);('atari retro', 2);('neural network', 2);('similarly', 2);('rollout iw', 2);('symbolic policies', 2);('sdrl', 2);('symbolic expressions', 2);('specically', 2);('sr', 2);('natrees', 2);('sgd', 2);('symbolic trees', 2);('old policy', 2);('circuscharlie', 2);('neural guided search', 2);('reward', 2);('time secs', 2);('ablations', 2);('object', 2);('bprot', 2);('rlapproach', 2);('dsp', 2);('ppo a2c', 2);('pong', 2);('teacher model', 2);('adventureisland2', 2);('adventureisland3', 2);('x5', 2);('x4sub', 2);('x3', 2);('x1 x7 x0figure', 2);('expression tree', 2);('proceedings', 2);('articial intelligence', 2);('symbolic visual reinforcement learninga', 1);('framework objectlevel abstraction', 1);('expression searchwenqing zheng sp sharan zhiwen fan kevin wang yihan xizhangyang wangwzheng', 1);('spsharan zhiwenfan kevinwang1839 yihanx atlaswangutexasedudepartment', 1);('electrical computer engineeringthe', 1);('texas austin austin tx', 1);('usaabstractlearning', 1);('interpretable policies', 1);('challenging task', 1);('complex scenes neuralnetworks', 1);('black boxes dicult', 1);('highlevel domainspecic', 1);('policy learning symbolic planning', 1);('tohighdimensional visual scenes suer scalability issues', 1);('poorlywhen images', 1);('complex object interactions address', 1);('novel symbolic learning approachthat discovers discrete symbolic policies', 1);('byusing', 1);('raw pixellevel inputs', 1);('able leveragethe simplicity scalability advantages symbolic expressions', 1);('incorporatingthe strengths neural networks', 1);('learning optimization experimentsdemonstrate', 1);('able generate symbolic policies simpler moreand', 1);('scalable stateoftheart symbolic', 1);('amount symbolicprior knowledge codes', 1);('available httpsgithubcomvitagroupdiffses', 1);('introductionone', 1);('major goal reinforcement learning', 1);('generalizable interpretable reliable policies', 1);('recent years', 1);('neural networks', 1);('great success', 1);('generalizable rules', 1);('various complex scenarios', 1);('black boxes aredicult', 1);('3to address', 1);('poor interpretability', 1);('promising solution 456789101112131415161718unlike', 1);('parameterize policy', 1);('neural activations weightsin', 1);('continuous highdimensional space symbolic', 1);('composes policy', 1);('discretecombination input operands symbolic mathematical operators example', 1);('cartesian genetic programming', 1);('sdrl9', 1);('complex environments highdimensional inputs', 1);('montezumas revenge compared', 1);('equallyarxiv221214849v1 cslg', 1);('dec', 1);('agents symbolic policies', 1);('interpretablelightweight ecient executedespite', 1);('frameworks scalability', 1);('complex visual scenesremains challenge', 1);('expressive image', 1);('abstractions need', 1);('works thatuse symbolic expressions', 1);('raw input spaces inputs highdimensional imagesinstead numerical states search rules', 1);('evenintractable inputoutput arities', 1);('general mathematical operators eg', 1);('small arities', 1);('intelligent decision behaviors emergeit', 1);('architectures example', 1);('neuralnetwork agents', 1);('matrix multiplications', 1);('orderto scale highdimensional state spaces images', 1);('current works', 1);('dene newoperators', 1);('large arities eg', 1);('cartesian genetic programmingbased', 1);('approaches619 limits input lowresolution images', 1);('complex expressions', 1);('highresolution compleximage inputs', 1);('furthermore', 1);('highdimensional space', 1);('additional dicultyin optimization', 1);('symbolic policies higharity operatorscan', 1);('optimization procedure dicult', 1);('localminima account issues', 1);('complex symbolic policiessome symbolic', 1);('primitives example 6uses', 1);('matrix operators process images', 1);('highlevel abstractions eg ladder doorthese designs', 1);('signicant human expert knowledge', 1);('lack abilityto', 1);('new environments', 1);('poor scalability', 1);('aris lack joint optimization mechanismsfor architecture coecients', 1);('discrete architectures symbolic expressionsare dicult', 1);('continuous optimization nature result symbolicrl frameworks', 1);('architecture learning', 1);('tasks existingsymbolic', 1);('generate endtoend symbolic policies', 1);('thesymbolic architectures', 1);('inputs states output nal actionwithout', 1);('input type exible', 1);('learning endtoend approaches eective nonvisual tasks visual tasks', 1);('certain level offeature learning abstraction', 1);('redundant information inthe images extract key', 1);('lowerdimensional operands followingsymbolic architectures contrast way', 1);('agents handlefeature learning layerwise feedforward abstraction comparison pioneeringsymbolic', 1);('matrix operations', 1);('image patches', 1);('adhocfeature processing steps', 1);('symbolic policies theirkey advantage', 1);('agentsto address', 1);('scalability symbolic', 1);('underobjectlevel representation', 1);('pixel level', 1);('improves architecture andcoecient learning design', 1);('assumption objectlevel2abstractions', 1);('yield simpler symbolic reasoning', 1);('raw pixel', 1);('21to end', 1);('summarizethe image lowdimensional object', 1);('object featureswe use genetic', 1);('symbolic expression thatcomposes action output', 1);('explainable mathematical operatorsunlike', 1);('low eciency bruteforce search2223', 1);('optimization procedure', 1);('ways rst', 1);('gradient descent evolution', 1);('search procedure alternates', 1);('neural networkand', 1);('symbolic trees symbolic evolution process', 1);('keepthe mathematical operators', 1);('small set', 1);('dimensional objectlevelabstractions operand', 1);('primitives contributions', 1);('novel symbolic', 1);('framework learns symbolic expressionsas', 1);('object representations', 1);('highdimensional vision domains lesshuman expert knowledgeto', 1);('complex vision domains', 1);('symbolic evolution approach introduces gradient descent optimizationinto symbolic expression', 1);('neuralguidedsymbolic evolution visual', 1);('taskssystematic comparisons', 1);('extensive experimental resultsin', 1);('related worksthe', 1);('deep neural', 1);('parameterizes policyvia', 1);('rl sdrl', 1);('parameterizes policy', 1);('thedierences model composition', 1);('thesetwo approaches', 1);('interaction environment theenvironments', 1);('observations states', 1);('eitherdrl symbolic', 1);('policies work', 1);('exibility symbolic', 1);('rlapproaches', 1);('modelsthe stateoftheart symbolic', 1);('level methods visual', 1);('domain operatorlevel symbolic', 1);('rlmethodsdirectlyusesymbolsmathoperatorstosubstitutetheneuralnetworkparameterizedpolicies drl', 1);('reasoning level symbolic', 1);('uses symbolsto', 1);('higherlevel abstractions', 1);('agent action selection planning9101112 nonvisual', 1);('domain symbolic', 1);('operatorlevelsymbolic policies control', 1);('state spaces', 1);('54131415161718the operatorlevel symbolic', 1);('majorclass operatorlevel symbolic', 1);('methods adopts', 1);('cartesian genetic programming3terminology short denition referencesectionsymbolic rl', 1);('reinforcement learning approach usessymbolic expressions', 1);('policiessection 1pixellevel representationthe', 1);('raw matrix representation images', 1);('height\x02width\x02channel pixelssection 1objectlevel representationa summary visual objects scene ratherthan', 1);('raw pixel valuessection 1genetic', 1);('gpa', 1);('evolutionary algorithm evolvesa population candidate symbolic expressionsthrough mutationscrossoversetc nallypick', 1);('onesection 2operand math symbols', 1);('variableswith values', 1);('leaf nodes thesymbolic treesection 33operator math symbols', 1);('variables values', 1);('branchingnodes symbolic treesection', 1);('papercgp evolve symbolic policy', 1);('allowfor image processing functions', 1);('class matrix operations example theskewness kurtosis', 1);('statistical vector operations matrix ofpixels eective', 1);('pixel inputs', 1);('matrix operators', 1);('search algorithmon', 1);('bprost', 1);('able play', 1);('atarigames', 1);('comparable humans', 1);('symbolic representation learning', 1);('variational autoencoders', 1);('vaeto', 1);('raw pixels', 1);('recent work', 1);('approach searches symbolic representation improvesthe models eciency', 1);('interpretablethe reasoning level symbolic', 1);('symbolic planning example', 1);('framework 9features plannercontrollermetacontroller architecture lets sublayers thecontroller', 1);('intrinsic rewards', 1);('nsrl', 1);('neurologic reasoning module', 1);('model extractsthe', 1);('logical rules', 1);('complex problems asmontezumas', 1);('revenge', 1);('visual symbolic', 1);('learnable component symbolicexpression', 1);('smarter composition symbolic operatorsoperands', 1);('donethrough generationselection procedure genetic', 1);('othersimilar evolution methods', 1);('independent learning agent4w1y1vx1vy1input', 1);('imagedetected objectsobject featuresactions learned', 1);('forestp02', 1);('p07 p01 y1x2 y5x2 vx2y2 x3vx5 y221 46figure', 1);('inference procedure', 1);('foregroundobjects rst', 1);('object detection module objectfeatures', 1);('symbolic expressionslearns output symbolic expression agent', 1);('sequence prediction modelsuch recurrent neural network', 1);('symbolicgeneration method submodule', 1);('methodin addition visual domain', 1);('approach use symbolic regression estimate value function54', 1);('policy value function', 1);('interpretable policy high computational complexity limits application inhigherdimensional problems', 1);('direct local searchover symbolic policies', 1);('humanreadable policies approach', 1);('setting specic', 1);('syntactic models', 1);('able learnsmoother trajectories neural policies3', 1);('symbolic expression search framework', 1);('preliminaries symbolic expression symbolic regressionon treeforest structured symbolic', 1);('general symbolic equation', 1);('form tree', 1);('inorder traversal thistree', 1);('equivalent string representation equation single symbolic tree', 1);('multidimensional inputs itsoutputis', 1);('singlescalar value order scale', 1);('symbolic tree', 1);('rnadimensional', 1);('action spacewe learnnasuch trees compose forest sense tree', 1);('responsible oneaction', 1);('nadimensionalcontinuous', 1);('action space', 1);('continuousdiscrete spaces', 1);('someof output dimensions', 1);('continuous actionsas', 1);('tree leaf nodes input object', 1);('operand space', 1);('nodes math operators drawnfrom operator space tree node possesses oat value value leaf node isthe', 1);('constant value value', 1);('node execution result ofthe subtree', 1);('possesses oat5value value root node', 1);('navalues', 1);('corresponding actions case discrete action', 1);('environments thesevalues presoftmax probabilities', 1);('common treatment', 1);('models inthe discrete action space', 1);('tiis value ith treeaction concatai1ti', 1);('cont', 1);('action envsample softmaxnai1tidisc action env1with symbolic forest formulation function generates output actionprobabilities', 1);('input observations dene function policy thissymbolic forest aim', 1);('policy function maximizethe total rewardon', 1);('genetic programming based symbolic regression', 1);('section2', 1);('theoperator level', 1);('controls agents actions', 1);('leverage thepowerful robust', 1);('evolutionary algorithm genetic', 1);('submodule ofour', 1);('group candidate symbolic expressions evolvesthem', 1);('new generations variations random mutation crossover', 1);('ascreening', 1);('generationthis mutationselection process', 1);('certain performance metrics', 1);('themaximum number iterations reachednaive symbolic search method', 1);('bruteforce search procedure onetimesymbolic regression tasks', 1);('joint process searchand interaction', 1);('low eciency', 1);('scratch largescaleproblems', 1);('inecient entangledwith environment interaction', 1);('accelerate symbolic search', 1);('entanglementnew mechanism', 1);('separate policy learning symbolic', 1);('dises frameworkcompared drl', 1);('exible exploration', 1);('additional burdenof learning architecture coecients address issue', 1);('burdenof exploration exploitation', 1);('threestage learningapproach neural policy learning symbolic', 1);('rst stage weleverage', 1);('continuous optimization neural networks', 1);('symbolic knowledge distillation', 1);('policy symbolic model expression nal stage weperform', 1);('performance symbolicmodel', 1);('stage yieldstage afneural x\x12 2where aandxare action image observation \x12is neural network coecientsthe', 1);('fneuralacts teacher model', 1);('practice weadopt otheshelf', 1);('algorithm train', 1);('cnnbased', 1);('controller wealso note neural', 1);('reinforcement learning algorithm', 1);('here6after training neural net controller fneural', 1);('stage thesymbolic', 1);('symbolic regression module usinggp', 1);('goal need dene operand operatorsprevious symbolic', 1);('extensive human expert knowledge denehighlevel abstractions', 1);('visual patterns', 1);('complex domainspecicfunctional symbolic toolboxes', 1);('symbolic planning targets etc', 1);('assumption object level', 1);('yield simpler', 1);('transferable symbolic reasoning', 1);('raw pixelfeatures', 1);('object level abstractions itsoperandsafsymbolic x0 3where', 1);('coecient symbolic architecture x0as foreground objectfeatures', 1);('detect objects', 1);('learningthe learning procedure fsymbolicfollows', 1);('classical way symbolic regression', 1);('sra', 1);('type regression analysis searches space mathematical expressions ndan equation', 1);('ts dataset', 1);('dierent', 1);('conventional regression techniques thatoptimize parameters', 1);('model structure', 1);('infers model structuresand parameters data run', 1);('needs dataset', 1);('dxy', 1);('wherex2rn\x02jx0jandy2rn\x02na wherenis number iid samples jx0jis totalnumber object', 1);('nais', 1);('dimension action space symbolicregression procedure thenstage', 1);('fsymbolici isryifsymbolicix 4when thefsymbolicis', 1);('new input image x', 1);('rst uses anobject detection module', 1);('foreground object', 1);('x0 pass featuresinto forest symbolic trees fsymbolic x0to', 1);('action procedure visualizedin', 1);('symbolic expression use', 1);('stage tofurther optimize coecients architectures', 1);('symbolic expression whichwe', 1);('details symbolic fitting', 1);('iion', 1);('object detection', 1);('asoperands symbolic expressions', 1);('options objectdetection', 1);('subsequent symbolic learning', 1);('approachthe object detection submodule', 1);('3we use', 1);('spatially parallel attention component extraction space', 1);('pretrainthe foreground object detection submodule', 1);('object detection algorithm unies spatial attention scenemixtureapproaches', 1);('manual labels', 1);('streams foregroundmodule', 1);('dynamic objects', 1);('main agent entitiesand background module', 1);('static background theenvironment denitions', 1);('training lossfunctions', 1);('optimize movement', 1);('frames under7the assumption game entities', 1);('static backgroundgiven', 1);('raw observations environment training foreground backgroundstreams decompose', 1);('independent objects', 1);('distributions components', 1);('pixelwisemixture model', 1);('complete image distribution', 1);('overcomes scalingissues', 1);('parallel spatial attention making', 1);('suitable scenes', 1);('large numberof objects result', 1);('benets scenemixture spatialattentionmodelsas', 1);('interesting discovery experiments', 1);('odcould', 1);('satisfactory detection results human interpretation perspectiveas', 1);('separate single object', 1);('disjoint components', 1);('howeverthis', 1);('nonsatisfactory detection lead', 1);('algorithm failure succeedingsymbolic learning module', 1);('objects composes arobust symbolic expression', 1);('whole discussions objectdetector submodule section 42on operand selection', 1);('need denethe', 1);('corresponding operand space operator space operand space constructedfrom', 1);('object detectionmodule module generates object class location', 1);('box size', 1);('ready use operands symbolic regression', 1);('detectedobject append', 1);('xcoordinate ycoordinate andthe horizontal vertical components velocity vxandvy', 1);('mdetected', 1);('objects result 4mfeatures', 1);('consecutive frames number objectsmay', 1);('new objects', 1);('edge frame emergefrom center', 1);('number objects', 1);('regardless', 1);('lter top \x16mobjects highestdetection probability type', 1);('rare cases fewerthanmobjects', 1);('zeroson operator selection choice objectlevel abstraction', 1);('pixellevel representation', 1);('simpler symbolic expressions generate complexbehaviors', 1);('representation power object representations', 1);('forexample', 1);('simple operators \x00\x03 andare sucient', 1);('spatialtemporal relationships objects suchas object', 1);('path object b object', 1);('bsuch', 1);('sucient control environments', 1);('gymarcade', 1);('games object avoidance tasks end', 1);('higherorderstatistical operators skewness matrix variance', 1);('operators \x00\x02\x14\x15\x012\x013p\x01explog', 1);('weintentionally', 1);('operator space', 1);('human expert knowledge', 1);('new environments34', 1);('multiaction optimization neural guided symbolic searchthe', 1);('multiaction symbolic policy search', 1);('optimization technique itguides symbolic evolution', 1);('neural network models evolve', 1);('inthenadimensional action spacethe', 1);('traditional evolutionary algorithm genetic', 1);('population osprings applies mixture mutation crossover', 1);('guided searchsymbolic tree neural networkx1y1vx1vy1', 1);('xnynvxnvyn x3vx5', 1);('symbolic tree03', 1);('y1x2 y5', 1);('y1x2 y5x2 vx2y2', 1);('y1x2 y5x2vx2y2', 1);('x3vx5 y2action 1action 2action nfigure', 1);('search procedure', 1);('agents action', 1);('learns symbolic representationaction actionother operations evolve symbolic trees order', 1);('exible evolution method', 1);('dises comparedwith', 1);('previous mutation', 1);('novelmechanisms gradient descent optimization coecients neural guidedmultiaction tree search', 1);('2unlike neural networks', 1);('needs optimizetheircoecients thesymbolicexpressionsneedbothskeletonsthestructureoftheexpressioncomposition coecients constants expression optimization theskeleton', 1);('optimize coecients', 1);('conventional genetic mutation applyinggradient descent coecient symbolic expression symbolic policy isjust', 1);('mathematical operators straightforward', 1);('computationalgraph output respect scalar parameters', 1);('input wordsthe evolution genetic program', 1);('information thederivatives', 1);('equivalent backpropagation', 1);('neural networksour', 1);('dierentiable symbolic tree', 1);('dropin replacementfor neural network', 1);('gradient descent dierentiability', 1);('additional evolution strategy', 1);('evolution options crossover subtreemutation hoist mutation point mutation reproduction use', 1);('probability thesgd', 1);('default ratios', 1);('evolution options', 1);('share therest', 1);('probability evolution', 1);('symbolic expressions starts evolve symbolic regressionresults stage', 1);('warm initializations learning', 1);('atime actions', 1);('agent teacher agentfrom stage', 1);('symbolic treefor', 1);('search approach way symbolic evolution procedureis', 1);('exible neural network', 1);('convergence rate symbolicexpressionthe target function', 1);('thepolicy parameters \x12 coecients', 1);('inputsl\x12 1nnxi1minrt\x12 \x01atcliprt\x12 1\x00\x0f1 \x0f\x01at 5herert\x12is ratio', 1);('new policy', 1);('atis', 1);('advantage experienceat\x00vst rt rt1\x01\x01\x01', 1);('t\x0011rt\x001 t\x00tvst', 1);('discount factor', 1);('relative importance', 1);('future rewards', 1);('thevalues', 1);('1\x00\x0fand1 \x0f where\x0fis', 1);('small positive value', 1);('overall eect loss function', 1);('thepolicy network ie neural network', 1);('action output', 1);('symbolic expressionto', 1);('select actions maximize', 1);('certain rangeof', 1);('policy making', 1);('sudden large changes thatcould destabilize learning process', 1);('default settings', 1);('withoutmodications algorithm', 1);('algorithm', 1);('neuralguided symbolic', 1);('tuningrequire pretrained ppo', 1);('teacher agent \x19\x12', 1);('symbolic expression forest\x19', 1);('ensure optimized', 1);('symbolic policy \x19\x03 1initialize empty experience replay buer', 1);('d2foreach', 1);('iteration do3forith action do4replaceith output \x19\x12iwith expression \x19 i5run', 1);('policy \x19\x12', 1);('d6sample', 1);('minibatch statrtst1fromd7compute advantage', 1);('atfor', 1);('experience8end for9update policy', 1);('old policy parameters \x12', 1);('old \x12 11end for12return', 1);('symbolic expressions nal policy \x19\x03', 1);('experimental settings resultsin', 1);('systematic study', 1);('show hyperparameterssettings example', 1);('symbolic policies section', 1);('performsystematic ablation studies object detection module neural', 1);('search componentsin section', 1);('in10subsub addsub adddiv sub0108 mulx3', 1);('x3sub', 1);('x0 x0 x3x0', 1);('subx2 div0900', 1);('x2mul', 1);('muldiv adddiv addx1', 1);('x3 x1 x0sub', 1);('x0 x2 x1figure', 1);('pongatari2600', 1);('theleaf', 1);('objects positions velocities symbolicexpressions oer potential explanability', 1);('policy subtrees mighthappen constitute', 1);('interpretable meanings example geometricfeatures leaf nodes', 1);('x0x1could', 1);('bexpongxracketypongvypong xy location ofthe pong racket vertical velocity pong', 1);('point ofthe pong racket cis', 1);('horizontal distance timegap', 1);('point racket', 1);('similar logic learnedsection', 1);('visual nonvisual environments', 1);('symbolic policy neural network teacher presentedin section', 1);('experimental settings visualizationswhen', 1);('training teacher model stage', 1);('standard implementation fromstablebaselines3', 1);('reward decay', 1);('learning rate 00005to train', 1);('symbolic policy stage', 1);('select features', 1);('top \x16m 16objects', 1);('probability leadingto', 1);('number samples', 1);('iterationand total number iterations', 1);('symbolic expression withneural', 1);('search stage', 1);('training setting stage thesame symbolic hyperparameter stage', 1);('engineeringfacilitation considerations renormalize rewards', 1);('examples symbolic trees', 1);('fromperformance', 1);('side reward symbolic policy', 1);('visual environments areshown', 1);('model performance', 1);('section 211environment', 1);('dises reward drl rewardcircuscharlienes', 1);('dises drl', 1);('visualizations', 1);('object detection submodule', 1);('consecutive frames detectedforeground', 1);('ablation study421 ablation', 1);('suboptimal unsupervised object detectionmoduleas', 1);('symbolic policy reliant success object detection submoduleit', 1);('objects input operands', 1);('natural question thesymbolic policy', 1);('section testout', 1);('various types suboptimal object detectors measure drop performanceobject', 1);('detection visualizations', 1);('object detection results', 1);('diverse scenarios combination', 1);('dynamic objectsthe', 1);('underfitted object detection module', 1);('part rst case studywe emulate', 1);('intermediate training checkpointsof', 1);('accuracy rewards', 1);('specically adventureisland3nes', 1);('environment sample checkpointsof', 1);('training retrain', 1);('oftheir outputs', 1);('agent samein', 1);('average precision iou', 1);('threshold 50with respect agroundtruth groundtruth', 1);('gymarari style games', 1);('observing', 1);('therewards drops', 1);('module ie', 1);('reward lead anentire collapse algorithm12intermediate', 1);('od checkpoint od avg precision reward30', 1);('checkpoints od', 1);('middle training', 1);('adventureisland3nesenvironmenthandling simulated object missing', 1);('test robustness', 1);('case studyand drop', 1);('crucial objects ones', 1);('charlie', 1);('interact noncrucialobjects results', 1);('shows algorithm', 1);('crucial objectsare', 1);('simulated', 1);('object detection failures', 1);('probabilityreward obtained', 1);('ring', 1);('crucial fire', 1);('pot', 1);('crucial money', 1);('bag', 1);('object splitting', 1);('instances object detection', 1);('spacealgorithm', 1);('itcould', 1);('careful hyperparameter optimization boundary loss coecients', 1);('box sizes object', 1);('attempt overt hyperparameterstowards', 1);('environment setting', 1);('notion trulyendtoend', 1);('framework minimum', 1);('erroneous detection', 1);('capable learning robust rules forthese environments', 1);('signicant performance degredationoverall ablation study results', 1);('acceptable performance', 1);('certain failure cases object detection object representations', 1);('rules display reliability', 1);('unable retainits accuracy', 1);('entire pipeline', 1);('satisfactory rewards13a', 1);('input image circuscharlieb segmented foreground objectsfigure', 1);('erroneous object splitting', 1);('firering charlie circuscharlienes observe', 1);('charlie lion', 1);('ablation', 1);('tuning techniquesin', 1);('parameter tuning', 1);('theperformance improvement', 1);('ablation results', 1);('neural guidedngsearchareprovidedintable5', 1);('search symbolic expressionsmutate', 1);('seconds thenumber generations', 1);('model converges', 1);('slowerwithout neural', 1);('search total generations training', 1);('natree', 1);('generationswithout neural guidance', 1);('ng', 1);('neural guidanceremoving dierentiability', 1);('ablation results dierentiability shownin', 1);('ablation study', 1);('strategy symbolicexpression evolution implementation side', 1);('nodes symbolictree', 1);('trainable use genetic', 1);('evolve expressions', 1);('thetness metric maximize', 1);('environment rewardas', 1);('dierentiable method andalso', 1);('time converge reason', 1);('dierent trees valuesare', 1);('close decision threshold', 1);('easy cross', 1);('subtle dierences continuousvalues symbolic tree nodes lead dierent discrete decisionsconguration', 1);('generationsvanilla gp', 1);('training14methods visualnonvisual outputdimsalgorithm dierentiability primitives expert knowledgerequireddises', 1);('4ndim space', 1);('odgpsgd', 1);('4basic object', 1);('basic math operators \x00\x02\x15sdrl9', 1);('reward q learning specied planninghigh', 1);('level representations ladderplatform rope key door openthe door etcatari', 1);('8ndim space', 1);('math operators jxjpn1ex\x001e\x001skewness xkurtosis x matrix operations rst\x00elementxsplitx', 1);('nsrl10', 1);('ndimspace transformer ilp', 1);('iw', 1);('screen pixels34vaeiw', 1);('ndim', 1);('space variational autoencoderswidth', 1);('screen pixels34dsp', 1);('policy gradient indirectbasic', 1);('math operators \x00\x02sincoslog', 1);('pretrained', 1);('neural networkpirl', 1);('4ndimspacebayesian optimization', 1);('basicobjectfeatures pretrainedneural', 1);('strips', 1);('comparison', 1);('dierent symbolic', 1);('comparison existing symbolic rl methodswe', 1);('important properties symbolic', 1);('theapplicable domains theapproach theamount', 1);('human expert knowledge run approach', 1);('thesimplicity', 1);('vital prominent advantage symbolic', 1);('drl drl', 1);('potential tolearn', 1);('competitive policies symbolic', 1);('merit symbolic', 1);('applicable domains approach andthe', 1);('expert knowledge', 1);('scaleinto diverse', 1);('domains thorough comparison propertiesamong dierent symbolic', 1);('nonvisual rl settingsfirst', 1);('deep symbolic policy dsp', 1);('cartpole mountaincar pendulum dspmethod', 1);('policy gradient maximize performance generatedpolicies way', 1);('optimize symbolic tree dierence', 1);('learninghappens sequence predictor', 1);('section 2since', 1);('incompatible discrete output spaces', 1);('cartpolecontinuousa', 1);('continuous action space version', 1);('cartpole', 1);('conventionaldrl methods', 1);('performance intrinsic behaviorof', 1);('continuous environments', 1);('symbolic methods', 1);('performant solutionsresults', 1);('dises dsp', 1);('bothwhitebox beats', 1);('conventional approaches', 1);('ppo a2c dises', 1);('upper hand15environmentresultant', 1);('policy reward', 1);('dises a2c ppo dsp disescartpole', 1);('10000mountaincar 002\x00072logs2s201759628', 1);('comparison dises dsp', 1);('symbolic rl', 1);('visual rl settingssymbolic', 1);('visual rl', 1);('setting primary aim work', 1);('dises cartesian genetic programming cgp', 1);('andspaceinvaders environments results', 1);('table 9environmentreward obtainedcgpdises', 1);('a2cppopong', 1);('comparison dises cgp', 1);('visual symbolic rl', 1);('environmentsbesides results terms model simplicity', 1);('applies matrix operators pixellevelinputs', 1);('uses foreground object coordinates velocities operandswhich', 1);('comparison deep network humanswhile', 1);('deep neural networks', 1);('notthe intention thiswork section oer', 1);('brief comparison', 1);('disesas', 1);('human players', 1);('symbolic policy dier thedrl performance transferability00', 1);('timesteps training episodes1e7010002000300040005000600070008000rewardcircuscharlienes00', 1);('timesteps training episodes1e7201001020pongatari260000', 1);('timesteps training episodes1e7101102103104seaquestatari2600ppo a2c', 1);('distilled symbolic rulefigure', 1);('performance', 1);('cnnbased rl', 1);('distilledsymbolic expression16441', 1);('performance comparisonthe', 1);('reward performances', 1);('dises ppo', 1);('a2c', 1);('humanplayer symbolic policy human player scores', 1);('frame rate symbolic policies cases', 1);('drlespecially', 1);('rules eg', 1);('environment theracket', 1);('point pong', 1);('rules conciseway', 1);('struggles approximate', 1);('heavy coecients442', 1);('behavior comparisonwe', 1);('show behaviors', 1);('symbolic policies twodataset generation schemes', 1);('symbolic policy displaysdierent trajectory teacher', 1);('initial state', 1);('distilled policy evaluationfigure', 1);('action distributions', 1);('teacher agent', 1);('teacher right443', 1);('policy transfer comparisonwe', 1);('transferability dierent policies domain', 1);('run experiments inthe', 1);('adventureisland2 adventureisland3', 1);('refer figure', 1);('rst train ateacher', 1);('ppo adventureisland3', 1);('test theppo symbolic expression', 1);('thecnn models', 1);('original environment overthe checkpoint', 1);('entire training history contrary', 1);('new environments symbolic policy', 1);('transfers betterwith', 1);('performance lossthe performance gain symbolic policy', 1);('disentanglement objectdetection action inference', 1);('dierent scenarios scenes pixellevel attributesare', 1);('logical specications congruent', 1);('drl od', 1);('newvisual scenes', 1);('thesymbolic expression symbolic expression transfers', 1);('drl45 examples', 1);('equationsmore examples', 1);('symbolic controller expressions', 1);('observe tradeo symbolic expression simplicity agentabilityenvironment complexity', 1);('circuscharlienesand seaquestatari2600', 1);('object information toperform eective control17a', 1);('adventureisland2figure', 1);('testing', 1);('policy transferrability', 1);('images screenshots', 1);('similar identical styles', 1);('adventureisland3 ai3', 1);('symbolic policyis', 1);('ppoa2c', 1);('norsymbolic policiesod submodule', 1);('adventureisland2evaluation adventureisland3 adventureisland2checkpoints ai3', 1);('10m 10m25m50m75m10mbesta2c', 1);('020050100100200ppo teacher', 1);('1950od presci 5221figure', 1);('results transferrabilty case studymuladd subadd divadd mulmul mul0896', 1);('x2add', 1);('divdiv subx3', 1);('x3 x3 x0div', 1);('x7 x6 x0div', 1);('submul muladd subx4', 1);('x6 x3', 1);('0261sub divx2', 1);('x6 x6 x7add', 1);('divdiv mulx3', 1);('circuscharlienesenvironmentdivsub', 1);('muldiv divdiv subdiv addadd subx4', 1);('x8 x4 x6mul', 1);('x7div', 1);('subsub divx6', 1);('x5 x8', 1);('0232add subx3', 1);('x8 x3 x2mul', 1);('muladd divdiv mulx3', 1);('x0 x1 x8sub', 1);('x9 x9 x6mul', 1);('adddiv addx8', 1);('x9 x6 x8div', 1);('x8 x3 x3div', 1);('muldiv submul divdiv subx8', 1);('x0 x7 x3div', 1);('x5 x9 x1mul', 1);('adddiv mulx5', 1);('x4 x2 x2add', 1);('x8 x1 x3sub', 1);('subdiv mulsub divx3', 1);('x3 x6 x2sub', 1);('x4 x1 x5sub', 1);('mulsub addx0', 1);('x0 x1 x5mul', 1);('seaquestatari2600environment185 discussions limitationsthis', 1);('work aims', 1);('current symbolic reinforcement learning', 1);('methods reducinghuman expert knowledge making policy simpler', 1);('scalable complex visualscenes', 1);('knowledge rst work', 1);('dierentiablesymbolic search visual', 1);('domain rst base operator level symbolic policieson object representationsthe', 1);('previous symbolic', 1);('assumptions expert policy', 1);('available distillationand neural', 1);('atariretrogymstyle', 1);('image inputs', 1);('dierentiableoptimization broader applicability', 1);('complex scenarios realworld 3dvision inputs', 1);('future work plan', 1);('theneuralsymbolic coevolution test approach dicult scenarioswe', 1);('interpretable theenvironment', 1);('andor model performance improves note apossible tradeo expression simplicity model performance6', 1);('conclusionthis', 1);('novel symbolicreinforcement learning framework generates', 1);('simple competitive symbolic policiescomposedofsymbolicoperatorsandobjectrepresentations', 1);('comparedwithprevioussymbolicrl', 1);('simpler math operatoer', 1);('reduces need human expert knowledge design process scalesbetter', 1);('complex highdimensional visual inputs', 1);('additionally', 1);('neuralguidedsearch augments symbolic policy evolution process', 1);('exible optimizationand dierentiability', 1);('algorithm approach pavesthe way', 1);('exible symbolic policies', 1);('complex reinforcement learningdomainsreferences1lukasz', 1);('kaiser mohammad babaeizadeh piotr milos blazej osinski roy h campbellkonrad czechowski dumitru erhan chelsea finn piotr kozakowski sergey levineet', 1);('modelbased', 1);('reinforcement learning atari arxiv preprint arxiv190300374 20192volodymyr', 1);('mnih koray kavukcuoglu david silver alex graves ioannis antonogloudaan wierstra martin riedmiller playing', 1);('reinforcement learningarxiv preprint arxiv13125602 20133alexandre', 1);('heuillet fabien couthouis natalia dazrodrguez explainability', 1);('indeep reinforcement learning', 1);('knowledgebased systems', 1);('alibekov ji kubalk robert babuka symbolic', 1);('55th conference', 1);('decision controlcdc', 1);('kubalk erik derner jan', 1);('robert babuka symbolic', 1);('regressionmethods reinforcement learning', 1);('ieee access', 1);('g wilson sylvain cussatblanc herv luga julian', 1);('miller evolvingsimple', 1);('atari games', 1);('proceedings genetic evolutionarycomputation', 1);('conference pages', 1);('junyent anders jonsson vicen gmez deep', 1);('pixel domains', 1);('automatedplanning scheduling', 1);('bandres blai bonet hector gener', 1);('pixels almostreal time', 1);('proceedings aaai', 1);('lyu fangkai yang bo liu steven gustafson sdrl', 1);('interpretable anddataecient', 1);('proceedingsof aaai', 1);('yuzheng zhuang paul weng hankz hankui zhuo dong li wulong liuand jianye hao learning', 1);('symbolic rules', 1);('interpretable deep reinforcement learningarxiv preprint arxiv210308228 202111matthew', 1);('daniel kudenko combining', 1);('adaptive agents multiagent systems iii adaptation multiagentlearning', 1);('springer', 1);('li haoyi xiong xingjian li xuanyu wu xiao zhang ji liu jiang bian', 1);('dou', 1);('interpretable deep learning', 1);('interpretation', 1);('interpretability trustworthinessand', 1);('arxiv preprint arxiv210310689 202113mikel', 1);('landajuela brenden k petersen sookyung kim claudio p santiago rubenglatt nathan mundhenk jacob', 1);('pettit daniel faissol discovering', 1);('machinelearning', 1);('verma vijayaraghavan murali rishabh singh pushmeet kohli swaratchaudhuri programmatically', 1);('interpretable reinforcement learning', 1);('internationalconference machine learning', 1);('garnelo kai arulkumaran murray shanahan towards', 1);('deep symbolicreinforcement learning arxiv preprint arxiv160905518 201616daiki', 1);('kimura masaki ono subhajit chaudhury ryosuke kohita akifumi wachidon joven agravante michiaki tatsubori asim munawar alexander grayneurosymbolic', 1);('reinforcement learning rstorder logic', 1);('the2021 conference', 1);('empirical methods', 1);('language processing', 1);('online punta cana dominican', 1);('november', 1);('associationfor computational linguistics', 1);('doi 1018653v12021emnlpmain283', 1);('url', 1);('httpsaclanthologyorg2021emnlpmain283 2017artur davila', 1);('garcez aimore resende riquetti dutra eduardo alonso towardssymbolic', 1);('common sense arxiv preprint arxiv180408597 201818osbert', 1);('bastani yewen pu armando solarlezama', 1);('policy extraction arxiv preprint arxiv180508328 201819julian', 1);('francis miller cartesian', 1);('geneticprogramming', 1);('machines', 1);('coppens kyriakos efthymiadis tom lenaerts ann tim miller rosinaweber daniele magazzeni distilling', 1);('deep reinforcement learning policies softdecision trees', 1);('proceedings ijcai', 1);('explainable articialintelligence pages', 1);('samek thomas wiegand klausrobert mller', 1);('explainable articialintelligence', 1);('understanding', 1);('deep learning models arxivpreprint arxiv170808296 201722miles', 1);('cranmer alvaro sanchezgonzalez peter battaglia rui xu kyle cranmerdavid spergel shirley ho discovering', 1);('symbolic models', 1);('learning withinductive biases arxiv preprint arxiv200611287', 1);('zheng tianlong chen tingkuei hu zhangyang wang symbolic', 1);('learningto optimize', 1);('towards', 1);('interpretability scalability arxiv preprint arxiv220306578 202224andrea', 1);('dittadi frederik k drachmann thomas bolander', 1);('pixelsin atari', 1);('symbolic representations arxiv preprint arxiv201209126 202025brenden', 1);('k petersen mikel landajuela larma nathan mundhenk claudio p santiagosoo k kim joanne kim deep', 1);('recovering', 1);('mathematicalexpressions data', 1);('policy gradients arxiv preprint arxiv191204871 201926thomasphiliprunarssonandmagnusthorjonsson', 1);('evolutionanddesignofdistributedlearningrules', 1);('ieee symposium combinations evolutionary computationand neural networks proceedings', 1);('ieee symposium combinations', 1);('computation neural networks cat', 1);('gustafson edmund k burke natalio krasnogor', 1);('ieee congress evolutionary computation', 1);('orchard lin wang', 1);('neural learning rule 2016international', 1);('joint', 1);('neural networks ijcnn', 1);('ieee201629esteban real chen liang david quoc', 1);('automlzero evolving', 1);('algorithms scratch', 1);('machine learning', 1);('lin yifu wu skand vishwanath peri weihao sun gautam singh feideng jindong jiang sungjin ahn', 1);('unsupervised', 1);('scenerepresentationviaspatialattentionanddecomposition arxiv preprint arxiv200102407 202031trevorstephens', 1);('gplearn2015 url', 1);('httpsgplearnreadthedocsioenstableindexhtml 201932john', 1);('schulman filip wolski prafulla dhariwal alec radford oleg klimovproximal', 1);('policy optimization algorithms arxiv preprint arxiv170706347 201733antonin', 1);('ran ashley', 1);('maximilian ernestus adam gleave anssi kanervisto', 1);('dormann', 1);('stable baselines3 201934yitao', 1);('liang marlos', 1);('machado erik talvitie michael bowling', 1);('state ofthe art control atari games', 1);('shallow reinforcement learning arxiv preprintarxiv151201563 201535volodymyr', 1);('mnih adria puigdomenech badia mehdi mirza alex graves timothylillicrap tim harley david silver koray kavukcuoglu asynchronous', 1);('international conference machine learning pages19281937', 1);