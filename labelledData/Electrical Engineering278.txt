('aoi', 34);('pr', 25);('lemma', 24);('drl', 22);('nn', 17);('mse', 15);('dqn', 15);('aoistate', 14);('seddpg', 12);('sedqn', 10);('if\x191', 10);('theorem', 9);('fig', 8);('sedrl', 8);('qvalue', 7);('ddpg', 7);('mdp', 5);('denition', 5);('denitions', 5);('bellman', 5);('online', 5);('automatica', 5);('remote estimation systems', 4);('mdps', 4);('dqn ddpg', 4);('hands0', 4);('hands\x0e', 4);('seaction', 4);('sensor scheduling', 4);('update', 4);('algorithm', 4);('initial value function', 4);('v1', 4);('1\x00pk1v0 00i', 4);('mar', 4);('furthermore', 3);('structural properties', 3);('wncss', 3);('liu li', 3);('markov', 3);('threshold property optimal policy', 3);('kalman', 3);('nsensors', 3);('problem', 3);('optimal scheduling policy', 3);('optimal value function', 3);('nand', 3);('nix iprh', 3);('xhx', 3);('remark', 3);('qvalues', 3);('initialize', 3);('td', 3);('sedqn seddpg', 3);('average', 3);('v0sis', 3);('v01', 3);('1\x00pi1v0 00i', 3);('pj1\x00pi1v0 00i', 3);('v0', 3);('shi optimal', 3);('oct', 3);('loss function', 2);('penalties actions', 2);('remote estimation', 2);('wncs', 2);('decision processes', 2);('reinforcement learning', 2);('different sensors', 2);('average sum', 2);('remote estimation system', 2);('threshold structure', 2);('training time', 2);('spectral radius', 2);('remote estimator', 2);('markovian', 2);('immediate reward', 2);('channelstate', 2);('sensor nat state', 2);('sensor ndenition', 2);('sensor ns', 2);('schedule', 2);('optimal policy', 2);('0i\x15 i', 2);('qs0a\x03\x15qs0a0', 2);('aoistates', 2);('xh0x', 2);('nix iprh0', 2);('nij nihnia\x03ni', 2);('h0withh', 2);('i j', 2);('based', 2);('2m 1given states', 2);('righthand side', 2);('qs0a\x03\x15qs0abased', 2);('optimal scheduling problem', 2);('se action selection', 2);('nfor', 2);('remote estimation system1initialize replay memory', 2);('dto', 2);('generate', 2);('loose se action selection method', 2);('compute', 2);('store', 2);('calculate tdiandadibased', 2);('repeat', 2);('algorithm line', 2);('direct output actor', 2);('execute', 2);('nn\x02moutput', 2);('training', 2);('performance comparison', 2);('value function', 2);('v0s', 2);('v0spropagates bellman', 2);('havev1 \x0e', 2);('j1\x00v0 i1 j1\x00v0', 2);('similar case', 2);('1\x00pi1v0 i', 2);('j 1\x00v0 i 11\x00v0 00i', 2);('b\x01to', 2);('if\x191\x10', 2);('k 1\x00 pj1pk1v0 00i', 2);('liu e quevedo li', 2);('proc ieee', 2);('control vol', 2);('jan', 2);('art', 2);('remote state estimation', 2);('june', 2);('ieee trans autom', 2);('drl optimaltransmission schedulingjiazheng chen wanchun liu member ieeedaniel e quevedo fellow ieee saeed r khosravirad member ieeeyonghui li fellow ieee branka vucetic', 1);('fellow ieeeabstractremote', 1);('state estimation largescale', 1);('dynamic processes plays', 1);('important role inindustry', 1);('applications paper focus transmission scheduling problem remoteestimation system', 1);('structural properties optimal sensor scheduling policyover', 1);('channels building theoretical guidelines', 1);('structureenhanceddeep reinforcement learning', 1);('framework optimal scheduling system', 1);('theminimum overall estimation meansquare error', 1);('structureenhancedaction selection method tends', 1);('select actions obey policy structure explores theaction space', 1);('enhances learning efciency', 1);('new loss function guides', 1);('converge optimal policy structure quicklyour numerical experiments', 1);('algorithms savethe training time', 1);('algorithms addition show', 1);('dynamic scheduling problems', 1);('remote state estimationindex', 1);('termsremote', 1);('state estimation', 1);('reinforcement learning sensor scheduling threshold structurei', 1);('ntroductionwireless', 1);('control systems', 1);('sensors actuatorscontrollers plants key component industry', 1);('inpart work', 1);('ieee', 1);('chen', 1);('vucetic', 1);('electrical information engineering', 1);('sydneysydney nsw', 1);('australia', 1);('email jiazhengchensydneyeduau wanchunliusydneyeduau yonghuilisydneyeduaubrankavuceticsydneyeduau', 1);('e quevedo', 1);('electrical engineering robotics queensland universityof', 1);('qut brisbane australia', 1);('email dquevedoieeeorg', 1);('r khosravirad nokia bell laboratoriesmurry', 1);('nj', 1);('usa', 1);('email saeedkhosraviradnokiabelllabscom w', 1);('liu', 1);('corresponding authorarxiv221212704v1 csit', 1);('dec', 1);('20222many areas', 1);('industrial automation vehicle', 1);('systems building automation andsmart grids', 1);('highquality realtime', 1);('remote estimation dynamicsystem plant states plays', 1);('important role', 1);('control performance stability ofwncss', 1);('transmission scheduling wireless sensors limitedbandwidth needs', 1);('remote estimation performancethere', 1);('works transmission scheduling', 1);('problem multiloop', 1);('communication resources', 1);('transmission power consumption', 1);('stability constraintin', 1);('optimal sensor scheduling problems', 1);('overall estimation meansquare error', 1);('classical methods policy value iterations', 1);('recent work', 1);('reduces thecomputation complexity', 1);('optimal scheduling problems approximate', 1);('problems curse dimensionality', 1);('highdimensional stateand action spaces1in', 1);('recent years', 1);('deep neural networks function approximators', 1);('qnetwork dqn', 1);('multisensormultichannel scheduling problems', 1);('different remote estimation scenarios', 1);('problems systems', 1);('signicantperformance gains heuristic methods terms estimation quality recentwork', 1);('algorithms actorcritic structure', 1);('specic scheduling problems', 1);('distinguish sensor scheduling problems', 1);('wenote drawback general', 1);('policy exploration effectivelyfor specic tasks', 1);('local minima', 1);('total failurethus', 1);('drlbased', 1);('optimala key', 1);('current work optimal transmission scheduling1in', 1);('optimal scheduling twosensor systems', 1);('threshold structures', 1);('state space', 1);('multiple regions fordifferent scheduling actions words optimal policy structure', 1);('boundaries state space', 1);('singlesensorsinglechannel system proves optimal policy athreshold terms sensors age information', 1);('sensor willbe', 1);('multisensormultichannel system whereall channels static sensor', 1);('constant packetdrop probability frequencychannels work', 1);('optimal scheduling policy multidimensionalthreshold structure terms sensor', 1);('different numbers packets carryingeach measurement', 1);('packet numbers sensor', 1);('limitations thechannel models', 1);('channel models', 1);('adoptedin practice channel states', 1);('wireless propagation', 1);('differentfrequency bandwidths', 1);('different properties', 1);('different channel qualities limitedto', 1);('threshold structure optimal sensor schedulingpolicy', 1);('singlechannel system transmission success failure determinedbefore scheduling action', 1);('thestructural properties optimal policies', 1);('ideal assumptions', 1);('challenge toprove existence structural results optimal scheduling policy general multisensormultichannel system', 1);('work inthe', 1);('open literature', 1);('optimal scheduling problemsin paper', 1);('optimal sensor scheduling problem general', 1);('nsensormchannel', 1);('main contributions work', 1);('optimal sensor scheduling policy threshold structure terms boththe', 1);('states sensors', 1);('corresponding channel states channel statesof', 1);('different frequencies', 1);('knowledgethis rst structural result optimal scheduling policies', 1);('channels the4fig', 1);('remote', 1);('state estimation system', 1);('nprocesses mchannelsliterature', 1);('addition show structural property exists', 1);('wide range ofdynamic schedulingresource allocation problems', 1);('remote state', 1);('counterexample show property exist\x0fwe rst formulate sensor scheduling problem', 1);('problem building', 1);('threshold properties optimal policy', 1);('particular design', 1);('actionselection method tends', 1);('select actions obey threshold structure', 1);('suchan', 1);('action selection method explore action space', 1);('new lossfunction guides', 1);('converge optimal policy structure', 1);('action selection method novel loss function redesign', 1);('frameworks scheduling ie', 1);('deep deterministic policy gradientddpg', 1);('algorithms \x0four', 1);('extensive numerical results', 1);('mseby', 1);('importantly', 1);('algorithms converge', 1);('system settings thatcannot', 1);('algorithmsoutline system model', 1);('remote state estimation system', 1);('iithe', 1);('sensor scheduling problem formulation introduction structural properties', 1);('iii', 1);('proofs structural properties optimal scheduling policy are5given section', 1);('iv', 1);('numerical results', 1);('vi', 1);('followedby conclusions section', 1);('viiii ystem modelwe', 1);('ndynamic', 1);('asensor preprocesses', 1);('raw measurements sends state estimates remoteestimator', 1);('mwireless', 1);('dynamic process model', 1);('local state', 1);('estimationeach', 1);('dynamic process nis', 1);('discretetime linear timeinvariant', 1);('lti', 1);('system as11', 1);('21xnt1anxntwntyntcnxntvntn2f1ngt2f1g1where xnt2rlnis processns state time ynt2renis state measurement thesensornan2rln\x02lnandcn2ren\x02lnare system matrix measurement', 1);('wnt2rlnandvnt2renare process disturbance measurement', 1);('iid zeromean', 1);('gaussian', 1);('random vectorsn0wnandn0vn', 1);('an8n', 1);('dynamic processes', 1);('remote estimationproblem', 1);('references thereindue presence noise', 1);('sensor nexecutes', 1);('lter preprocess', 1);('raw measurement generate state estimate xsntat time t20 asxsntjt\x001anxsnt\x001 2apsntjt\x001anpsnt\x001anwn 2bkntpsntjt\x001cncnpsntjt\x001cnvn\x0012cxsntxsntjt\x001kntynt\x00cxsntjt\x001 2dpsnt', 1);('in\x00kntcnpsntjt\x001', 1);('2ewhere xsntjt\x001andpsntjt\x001are', 1);('corresponding estimation errorcovariance sensor n', 1);('xsntandpsntare posterior state estimate', 1);('estimation error covariance sensor nat timet', 1);('particular sensornsends', 1);('remote estimator ynt packet', 1);('thelocal state estimation error covariance matrix', 1);('aspsnte\x02xsnt\x00xntxsnt\x00xnt\x03 3kntis', 1);('gain sensor n andinis identity matrix', 1);('2a 2b presentthe prediction steps 2c', 1);('steps note local', 1);('kalmanlters', 1);('steady state mode literature', 1);('20and references therein2', 1);('error covariance matrix', 1);('aconstant ie', 1);('psnt\x16pn8tnb wireless communications remote', 1);('estimationthere mwireless', 1);('channels eg subcarriers', 1);('transmissions wherem\x14n', 1);('iid block', 1);('channelswhere channel quality', 1);('packet transmission varies packet', 1);('n\x02mmatrix htdenote', 1);('channel state system time wherethe element nth row mth column', 1);('hnmt2h\x0812 \x16h', 1);('thechannel state sensor nand', 1);('remote estimator channel', 1);('channel states total distribution hnmt', 1);('asprhnmti qnmi8t 4wherep\x16hi1qnmi 18nm instantaneous channel state', 1);('htis', 1);('available remoteestimator', 1);('standard channel estimation methodsthe packet drop probability channel state i02h', 1);('loss generalitywe', 1);('p1\x15p2\x15\x01\x01\x01\x15 p\x16h', 1);('dene packet success rate channel statehnmt aspnmt2fp1p \x16hg wherepi01\x00pi0due', 1);('communication channels', 1);('mout', 1);('ateach time step', 1);('ant2f012mgrepresent channel allocation sensor nat timet whereant0if sensornis scheduledmif sensornis', 1);('channel m2thenth local', 1);('lter converges', 1);('steady state', 1);('ancnis', 1);('anpwnis', 1);('channel eachchannel', 1);('constraints antare', 1);('5where 1\x01is indicator', 1);('schedule actions packet dropouts sensor ns', 1);('time slot dene packet reception indicator as\x11nt81if sensorns packet', 1);('randomness \x11ntand', 1);('remote estimator performs stateestimation', 1);('time slot', 1);('remote state estimate minimizes theestimation', 1);('stochastic recursionxnt18anxsntif\x11nt 1anxntotherwise6where', 1);('anis', 1);('system matrix process', 1);('sensor ns packet receivedthen', 1);('remote estimator propagates', 1);('previous time slot estimate currentstate', 1);('estimation error covariance aspntehxnt\x00xnt xnt\x00xnti8an\x16pnanwnif\x11nt 1anpntanwnotherwise7where \x16pnis', 1);('local estimation error covariance sensor', 1);('3let nt2f12gdenote ageofinformation', 1);('sensor nat timet measuresthe', 1);('sensor packet', 1);('wehave nt181 if\x11nt', 1);('otherwise8if sensor', 1);('good channels', 1);('corresponding average', 1);('due scheduling constraint', 1);('to8estimator stability issues', 1);('eg 15from', 1);('error covariances', 1);('aspntf ntn\x16pn 9wherefnx', 1);('anxanwnandf', 1);('1n\x01 fnf n\x01', 1);('estimationmse ie', 1);('trpntmonotonically', 1);('state nt20iii', 1);('p roblem formulation threshold structurein', 1);('paper aim nd', 1);('dynamic scheduling policy \x19\x01that uses', 1);('states allsensors', 1);('channel states minimize', 1);('mseof', 1);('allnprocesses innite time horizonproblem 1max\x19limt1etxt1nxn1\x00 ttrpntwhere 201is discount factorproblem', 1);('problem instantaneous estimation', 1);('mse pnt', 1);('state ntin', 1);('8and channel states iid', 1);('mdp3a mdp formulation1', 1);('st tht2snn\x02hn\x02m t 1t 2t', 1);('nt2nnis aoi', 1);('state vector', 1);('andchannel states2 overall schedule action', 1);('a1ta2tant2af012mgnunder constraint', 1);('nn\x00mactions nchoosemproblem', 1);('total policy \x19is stateaction', 1);('ie at\x19st3 transition probability', 1);('prst1jstatis', 1);('state st1given thecurrent state stand action', 1);('state transition', 1);('independent time index3not', 1);('transmission scheduling', 1);('feasible solution', 1);('remoteestimation stability condition terms', 1);('dynamic process parameters channel statistics', 1);('hasa solution', 1);('remote estimation stability condition system', 1);('focus optimalsolution', 1);('rest paper', 1);('stability condition', 1);('previous work 159given action aand state drop subscript use sandsto representthe', 1);('channel states haveprsjsa', 1);('ha prh', 1);('prhcan', 1);('ha qnn1pr', 1);('nj nhanandpr nj nhan 8pnm', 1);('n 1anm1\x00pnmif n n 1anm1', 1);('n n 1an', 1);('time tis', 1);('negative sum estimationmsepnn1\x00trpnt', 1);('pntdened', 1);('function nt reward', 1);('state b threshold', 1);('structure optimal mdp solutionunlike', 1);('systems staticchannels aim', 1);('structural property optimal policy general multisensormultichannel system', 1);('channels belowdenition', 1);('policy', 1);('channelstate threshold scheduling policyif channelmis', 1);('state s0', 1);('h0nm', 1);('hexcept', 1);('sensor nchannelmstate withh0nmhnm channel mis', 1);('policy aoistate', 1);('threshold scheduling policy ifchannelmis', 1);('state s0 0nhwhere 0nis idential', 1);('0n\x15 n', 1);('channel mor betterchannel', 1);('states sensor nis', 1);('channel mat', 1);('certain state channelquality ofhnmimproves', 1);('channel states thethreshold policy', 1);('assigns channel mto sensorn', 1);('state sensor10a', 1);('states b', 1);('actions channel statesfig', 1);('structure', 1);('where\x0fand\x02represent schedule sensor 1and', 1);('schedule sensor nto achannel', 1);('previous oneto', 1);('threshold structure nd optimal policyof twosensorsinglechannel system', 1);('conventional value iterationalgorithm', 1);('aoiand', 1);('channel state spaces properties', 1);('inspired', 1);('theabove result', 1);('optimal policy structural propertiesin', 1);('hreshold structure optimal policieswe', 1);('structural properties optimal scheduling policy', 1);('value iterationconcepts', 1);('denition optimal value function', 1);('v\x03st sr', 1);('thestateaction value function', 1);('qstat s\x02a ras', 1);('current state st tht optimal value function', 1);('future cost ie', 1);('optimal policy \x19\x03\x01v\x03st max\x19e1xt0t t0\x00trst04we note', 1);('threshold structural results terms', 1);('static channels', 1);('moreoverthe', 1);('proofs limitations', 1);('threshold structure multidimensional state actionscenario', 1);('singledimensional scenarios', 1);('theproof optimal policys structural property', 1);('part action space', 1);('full action space mustbe', 1);('show optimally11the optimal value function satises', 1);('equationv\x03st rst maxat2axst1prst1jstatv\x03st1 11where optimal action a\x03is', 1);('optimal policy \x19\x03\x01 iea\x03t\x19\x03st arg maxat2axst1prst1jstatv\x03st1given', 1);('current stateaction pair standat stateaction value function', 1);('function measures', 1);('future cost optimalpolicy\x19\x03\x01asqstat rst', 1);('xst1prst1jstatv\x03st1', 1);('qsta\x03t\x15qstatfor', 1);('notation simplicity use assto', 1);('atstst1in followingwe', 1);('threshold properties optimal schedulingpolicy', 1);('classical value iteration algorithm', 1);('optimal solution', 1);('value iteration', 1);('initial value function tth iteration', 1);('v0s2v', 1);('vis', 1);('measurable function ie', 1);('vsr', 1);('thetth iteration', 1);('vt1bhvti b\x01 vv bellman', 1);('operatorbhvtis rs maxa2axsprsjsavts 13we', 1);('elucidate convergence optimality value iterationslemma', 1);('optimal policy exists operator', 1);('bhas', 1);('v02v', 1);('byvt1bvtconverges norm tov\x03 ielimt1vtv\x03before', 1);('technical lemma monotonicity12of optimal value functionlemma', 1);('monotonicity consider', 1);('0ih 0i', 1);('appendix alemma', 1);('comparison value functions', 1);('states theproof channelstate threshold', 1);('threshold propertiesa', 1);('propertyin', 1);('optimal policy general multisensormultichannel system channelstate threshold property', 1);('optimal policy \x19\x03\x01of multisensormultichannel system channelstate threshold property', 1);('state s', 1);('qsa\x03\x15qsa8a2', 1);('exists a\x03im states0', 1);('h0im h0imandhare', 1);('element h0im followinginequality', 1);('qs0a0\x03\x15qs0a', 1);('a0\x03is optimal action state s0witha0i\x03msinceqs0a0\x03\x15qs0a\x03 need', 1);('toproveqs0a\x03\x15qs0a0', 1);('qs0a\x03\x15qsa\x03and', 1);('qs0a0 inthe', 1);('h0to', 1);('h0imfor', 1);('notation simplicityfirst', 1);('havepr j', 1);('ha nyn1pr', 1);('nj nhnan', 1);('ij ihiai', 1);('nij nihniani 14where ni', 1);('i\x001 i1', 1);('nandani', 1);('actions sensors', 1);('hi hi1hi2himis vector channel states sensor iand', 1);('hni', 1);('that13qsa rs', 1);('xsprsjshav\x03srs xhx prh pr', 1);('hav\x03srs xhx', 1);('nij nihniani', 1);('haveqs0a\x03 rs0', 1);('ij ih0ia\x03iv\x03s0\x15rs', 1);('ij ihia\x03iv\x03sqsa\x03where inequality', 1);('rs0 rsh0imhima\x03im', 1);('inequalityp0imv\x031 nih1\x00p0imv\x03 i1 nih\x15pimv\x031 nih1\x00pimv\x03 i1', 1);('p0im\x15pimandv\x031 nih\x15v\x03 i1 nihfrom', 1);('thatqsa rs', 1);('nij nihnia0ni', 1);('ij ihia0iv\x03srs0', 1);('nij nihnia0nipr ij ih0ia0iv\x03s0qs0awhere', 1);('ij ihia0iv\x03s', 1);('ij ih0ia0iv\x03s0b', 1);('propertywe', 1);('network scenarios', 1);('different numbers sensors channels below1', 1);('twosensorsinglechannel', 1);('optimalpolicy twosensorsinglechannel system', 1);('threshold policy', 1);('inother', 1);('words optimal action state schedule sensor n optimal action isstill schedule sensor nat states increase sensor ns', 1);('aoi14theorem', 1);('optimal policy \x19\x03\x01of twosensorsinglechannel system', 1);('aoistatethreshold', 1);('suppose state', 1);('optimal action toschedule sensor iea\x03i', 1);('state s0 0ihwhere 0i\x15 i optimal action isstilla0\x03i 1remark', 1);('analytical challenges', 1);('simple proof highlynontrivialfirst', 1);('main difculty proof', 1);('different actions', 1);('multiple dimensions', 1);('comparison multidimensional value functionshowever', 1);('optimal value functions', 1);('state changes withinone dimension example', 1);('hs0', 1);('0h s\x0e \x0eh', 1);('0i j \x0e 0i 00j 0i\x15 i', 1);('00j\x14 j', 1);('vs\x15vs0but', 1);('vsandvs\x0esecond', 1);('sensor iresults', 1);('reward scheduling sensor ican', 1);('sensors misunderstanding', 1);('thatthe inequality', 1);('qs0a\x03\x15qs0acan', 1);('qsa\x03\x15qsa', 1);('wedrop', 1);('constant channel state', 1);('optimal value functions simplicity', 1);('equal topi1v\x031 j11\x00pi1v\x03', 1);('j1\x15pj1v\x03 0i111\x00pj1v\x03', 1);('equal topi1v\x031 j11\x00pi1v\x03 i1 j1\x15pj1v\x03 i111\x00pj1v\x03 i1 j117we', 1);('packetsuccess rates', 1);('different sensors ie pi1andpj1proof', 1);('technical lemma terms optimal value functions packet success rateswhich', 1);('channel statesdenition', 1);('joint', 1);('letx0x00', 1);('maxfx0x00gandx0x00 minfx0x00gdenote', 1);('real numbers', 1);('dene', 1);('x0x00 x01x001x0nx00n15andx0x00 x01x001x0nx00nas joint meet vectors x0andx002rnrespectivelylemma', 1);('probabilistic supermodularity n', 1);('\x0eh \x0e 00i 0j 00i\x14 i 0j\x15 j andi6j2f12g', 1);('holdspj1v\x03ss\x0e pj1\x00pi1v\x03ss\x0e\x15pj1v\x03s pj1\x00pi1v\x03s\x0e 18proof', 1);('appendix bin', 1);('state i j', 1);('transition channel statesis', 1);('constant proof', 1);('thatpi1v\x031 j1\x15pj1v\x03 i11\x00pj1\x00pi1v\x03 i1 j1 19by', 1);('havepi1v\x031 j 1\x15pj1v\x03', 1);('11\x00pj1\x00pi1v\x03 0i', 1);('j 1which', 1);('qs0a\x03\x15qs0a2 multisensorsinglechannel', 1);('optimal policy hasthe', 1);('threshold property case', 1);('similar twosensor case rst', 1);('provethe probabilistic supermodularity', 1);('value iterations', 1);('difcult nd', 1);('useful inequalities', 1);('thetarget inequality', 1);('construct inequalities terms optimal valuefunction packet success rates', 1);('threshold property theseinequalities', 1);('provean asymptotic structural property belowtheorem', 1);('optimal policy \x19\x03\x01of multisensorsinglechannel system', 1);('2hasan asymptotic', 1);('threshold property suppose state', 1);('optimal actionis transmit estimation sensor iea\x03i', 1);('states s0 0ihwhere 0i i optimal policy', 1);('a0\x03i 116theorem', 1);('shows threshold structure', 1);('exists optimal policy multisensorsinglechannel system', 1);('domainproof theorem', 1);('qsa\x03\x15qsa8a2a', 1);('exists a\x03i', 1);('state s0 0ihwhere 0i i haveqs0a\x03\x15qs0a', 1);('anasymptotic mannerlemma', 1);('0i i', 1);('v\x03sproof', 1);('appendix cbased lemma', 1);('asymptotic probabilistic supermodularity belowlemma', 1);('asymptotic probabilistic supermodularity n', 1);('00i 0j', 1);('nwith', 1);('i 00i j\x14 0j andi6j2f1ng probabilistic supermodularity', 1);('appendix din', 1);('onqsa\x03\x15qsaand similarto proof', 1);('state i j nij', 1);('qsa\x03\x15qsais', 1);('equal topi1v\x031 j', 1);('nij\x15pj1v\x03 i', 1);('nij\x00pj1\x00pi1v\x03 i', 1);('havepi1v\x031 j', 1);('j1 nijand thuspi1v\x031 j1 nij1\x00pi1v\x03', 1);('j1 nij\x15pj1v\x03', 1);('nij 1\x00pj1v\x03', 1);('j1 nijwhich', 1);('multisensormultichannel', 1);('systems case', 1);('complex singlechannelone', 1);('state dimension', 1);('multidimensional channel', 1);('thusthe aoistate', 1);('structural property optimal policyproposition', 1);('optimal action a\x03 a\x031a\x03ia\x03na\x03imand state s0 0ihwhere 0i\x15 i optimal channel assignments', 1);('constant ie a\x03na0n\x038a\x03n6 0andi6n optimal action sensor jatstates0is occupy channel sensorjis', 1);('state ieaj\x03', 1);('itschannel condition', 1);('sensor iehim\x15hjmproposition', 1);('channel wont', 1);('sensors withworse channel conditions sensor', 1);('increases optimal channel assignmentrules channels', 1);('sameproof proposition', 1);('equivalent state', 1);('qsa\x03\x15qsaexists', 1);('state s0 0ihwhere 0i\x15 i', 1);('qs0a\x03\x15qs0a0based', 1);('qsa\x03\x15qsa0', 1);('havex nijxhx ix jpr nijj nijhnija\x03nij', 1);('prh pr', 1);('ij ihia\x03i', 1);('jj jhja\x03jv\x03s\x15x nijxhx ix jpr nijj nijhnija0nijprhpr ij ihia0ipr jj', 1);('ona\x03nija0nij havepr nijj nijhnija\x03nij', 1);('nijj nijhnija0nij 22thus', 1);('inequality derivedx ix jpr\x00 ij ihia\x03i\x01pr\x00 jj jhja\x03j\x01v\x03\x00s\x01\x15x ix jpr\x00 ij ihia0i\x01pr\x00 jj jhja0j\x01v\x03\x00s\x01for notation simplicity state rewritten i j states', 1);('for18 iand jare', 1);('a\x03ima\x03j 0a0i 0anda0jm lefthand side theright hand side', 1);('equal to8pimv\x031 j11\x00pimv\x03 i1 j1pjmv\x03 i111\x00pjmv\x03 i1 j123then', 1);('inequality derivedpimv\x031 j 1\x15pjmv\x03 i', 1);('pim\x00pjmv\x03 i', 1);('j 1\x15pjmv\x03 0i', 1);('pim\x00pjmv\x03 0i', 1);('24where rst inequality', 1);('inequality usinglemma', 1);('pim\x15pjm inequality', 1);('rewritten aspimv\x031 j11\x00pimv\x03', 1);('j1\x15pjmv\x03 0i111\x00pjmv\x03', 1);('qs0a\x03\x15qs0a0from', 1);('generality', 1);('results note theoretical results', 1);('inthis section', 1);('thereward function user', 1);('function terms', 1);('state and2 total reward function problem sum', 1);('individual rewards words forother scheduling problems', 1);('different systems', 1);('example scheduling problem', 1);('overall expectedsum', 1);('ie instantaneous reward rst', 1);('pnn1\x00', 1);('nt sum estimation', 1);('msethen', 1);('optimal policy structural propertiesremark', 1);('counterexample', 1);('optimal scheduling problemof multisensor', 1);('remote estimation system reward function', 1);('negative product ofthe', 1);('individual estimation', 1);('ie rs \x00qni1trpit', 1);('policy twosensorsinglechannel', 1);('remote estimation system constructedreward function', 1);('optimal policy threshold policy', 1);('optimal policy twosensorsinglechannel scheduling problem multiplicative reward function \x0fand\x02represent schedule sensor', 1);('tructure enhanced drlin', 1);('optimal scheduling problems', 1);('references therein', 1);('signicant performance improvements heuristic policies', 1);('structural propertiesin', 1);('performance improvement se algorithms', 1);('via structureenhanced dqnbased', 1);('equation terms', 1);('optimalpolicyqstat rst', 1);('est1\x14', 1);('maxat1qst1at1\x15 26a', 1);('uses neural network', 1);('\x12to approximateqstatbyqstat\x12and use nd optimal action ie a\x03t', 1);('action space', 1);('iiia dqn nn\x00mqvalueoutputs', 1);('different stateaction pairs train', 1);('needs sample data consistingof states actions rewards', 1);('states dene loss function', 1);('collecteddata minimize loss function nd', 1);('dqntraining', 1);('structures optimal policies beforeto utilize knowledge threshold policy structure', 1);('se action selection method', 1);('select reasonable actions', 1);('enhance data', 1);('se20loss', 1);('function denition', 1);('training algorithm', 1);('loose se action selectionstage utilizes part structural property', 1);('tight se actionselection stage utilizes', 1);('full structural property', 1);('stage rsttwo stages', 1);('se action selection schemes se loss function train', 1);('reasonable threshold policy', 1);('stage policy explorationin', 1);('present loose tight se action selection schemes se lossfunction1', 1);('loose', 1);('select action a\x0ffrom', 1);('entire action spacewith probability \x0ffor action exploration probability 1\x00\x0f generate', 1);('aas simplicity drop time index', 1);('action selectionsthe threshold structure suggests actions sand state', 1);('orchannel state', 1);('infer action', 1);('action state witha', 1);('threshold propertiesin', 1);('threshold property loosese action selection difcult nd actions', 1);('training utilize', 1);('tight se action selection stagegiven state', 1);('dene state', 1);('sn nh', 1);('eachn calculate', 1);('corresponding action', 1);('asanan1 ann ann arg maxaqsna\x12recall ann2f01mgis channel index', 1);('sensor nat state snifann0', 1);('threshold property implies channel annor', 1);('sensor nat state dene', 1);('quality asmnfm0jhnm0hnannm0 1mgthen se action sensor n', 1);('mnwith', 1);('probability \x18and', 1);('equal annwith probability 1\x00\x1821ifann', 1);('threshold property', 1);('actionthen dene action', 1);('greedy policy ie', 1);('methodat state sasaa1a2 arg maxaqsa\x12thus', 1);('se action sensor nidentical', 1);('dqnmethod', 1);('ie annow dene se action', 1);('a1a2 actionmeets constraintmxm11anm\x141nxn11anm\x141which', 1);('select action aasaand', 1);('identical conventional methodasa2', 1);('tight', 1);('loose se action selection rst infer', 1);('action sensor nat state', 1);('action state smalleraoi sn check', 1);('loose se action satises channelstate thresholdproperty belowfor notation simplicity use 0to', 1);('se channel selection sensor ngiven state dene state', 1);('channel ms state sensor nas\x7fsnm \x7fhnm \x7fhnmandhare', 1);('element \x7fhnmnm hnm\x001 wecalculate', 1);('corresponding action\x7fanm\x7fanm1\x7fanmn \x7fanmnarg max\x7faq\x7fsnm\x7fa\x12from channelstate', 1);('threshold properties scheduling action \x7fanmn shouldbe', 1);('\x7fanmn se action sensor nisanm otherwiseanis', 1);('action se action', 1);('satisfying thresholdproperties a1a2 action', 1);('select greedy action aa3 se loss function training transition statatatrtst1is', 1);('areplay memory rtrstdenotes', 1);('different', 1);('capacityn2initialize policy network random weights \x123initialize target network weights \x12\x124forepisode 12e 1do5', 1);('state s06 fort 12t do7', 1);('select action atby', 1);('sedqn8 execute', 1);('action atand observe rtandst19', 1);('action arg maxaqsta\x1210', 1);('transition statatatrtst1ind11 sample random batch transitions siaiaiairisi112', 1);('everyt0steps', 1);('\x12\x12t15 end for16end for17forepisode', 1);('e1e', 1);('tight se action selection method19end for20forepisode', 1);('e2e', 1);('execute dqn', 1);('algorithm 1122end fordqn', 1);('se action aand greedy action addition executedaction alettisiaiaiatrisi1denote theith transition', 1);('batch replaymemory', 1);('dene temporaldifference', 1);('tiastdiyi\x00qsiai\x12', 1);('27whereyiri max ai1qsi1ai1\x12is estimation', 1);('step isto measure gap', 1);('right sides', 1);('different dqnwe', 1);('introduce actiondifference', 1);('ad', 1);('error measure difference', 1);('qvaluebetween', 1);('se strategy greedy strategyadiqsiai\x12\x00qsiai\x12 28since optimal policy threshold structure', 1);('action ashould', 1);('identical tothe optimal action', 1);('\x12should lead', 1);('small difference', 1);('dene loss function', 1);('tias23li\x12', 1);('1td2i 1\x00 1ad2iifaiaitd2i otherwisewhere 1is hyperparameter balance importance', 1);('tdiandadi', 1);('words ifthe se action', 1);('td ad', 1);('tderrorbased', 1);('loss function adoptedgiven batch size b overall loss function isl\x12 1bbxi1li\x12 29to optimize \x12', 1);('wellknown gradient descent method calculate gradient asbelowr\x12l\x12 1bbxi1r\x12li\x12 30wherer\x12li\x12is', 1);('asr\x12li\x12 8\x0021\x00 1adir\x12qsiai\x12\x00qsiai\x12 1tdir\x12qsiai\x12ifaiai\x002tdir\x12qsiai\x12 otherwisethe details', 1);('structureenhanced ddpgdifferent dqn', 1);('qvalue ddpg', 1);('agent twonns', 1);('parameter \x12and actor', 1);('parameter \x16', 1);('particular theactor', 1);('approximates optimal policy \x19\x03sby\x16s\x16 critic', 1);('qsabyqsa\x12', 1);('general critic', 1);('judges whetherthe', 1);('action actor', 1);('dqn theactorcritic', 1);('continuous large action spacewhich', 1);('dqnto', 1);('scheduling problem discrete action space', 1);('action mappingscheme', 1);('nn ncontinuous', 1);('corresponding sensors 1ton', 1);('recall24that dqn nn\x00moutputs nvalues', 1);('order sensorswith', 1);('mranking', 1);('channels 1tom', 1);('\x0011as output nal outputs theactor', 1);('virtual action v', 1);('virtual action vand', 1);('action acan', 1);('virtualaction v', 1);('real action', 1);('seddpgstage', 1);('stage ith', 1);('present se action selection method se loss function sequel1 se action selection general action selection approach', 1);('viv\x0fivi vitoaia\x0fiai ai respectivelydifferent', 1);('\x0fgreedy actions action', 1);('thecurrent state isvi\x16si\x16 31and random action v\x0fiwas', 1);('original continuous output ofthe actor', 1);('nn2', 1);('se loss function', 1);('different sedqn seddpg', 1);('different loss functionsfor', 1);('loss function asin', 1);('gradient critic', 1);('thenext virtual action vi1is', 1);('state si1 ie vi1\x16si1\x16', 1);('qsi1\x16si1\x16\x12for', 1);('introduce difference actions', 1);('se strategy viand actor', 1);('vi viis', 1);('ie vivi se action', 1);('thenthe loss function', 1);('stateaction pair', 1);('identical conventionalddpg', 1);('loss function transition', 1);('tiis', 1);('2qsivi\x12 1\x00', 1);('vi\x00vi2ifviviqsivi\x12 otherwise32and', 1);('overall loss function', 1);('batch is25algorithm', 1);('capacityn2initialize actor network critic network random weights \x16and\x123initialize target network weight \x16\x16\x12\x124forepisode 12e 1do5', 1);('random noise', 1);('action exploration6', 1);('state s07 fort 12t do8', 1);('select action vtby', 1);('seddpg9 mapping', 1);('virtual action vtto', 1);('real action at10', 1);('action atand observe rtandst111', 1);('action vt\x16st\x1612', 1);('transition stvtvtvtrtst1ind13 sample random batch transitions sivivivirisi114', 1);('virtual actions vivivi15', 1);('target network\x16 \x0e\x16 1\x00\x0e\x16\x12 \x0e\x12 1\x00\x0e\x1218 end for19end for20forepisodee1 12e 2do21', 1);('tight se action selection method22end for23forepisodee2 12e do24', 1);('algorithm 2625end forl\x16 1bbxi1li\x16 33by', 1);('chain rule', 1);('gradient ofthe overall loss function terms \x16asr\x16l\x16 1bbxi1r\x16li\x16 34wherer\x16li\x16is', 1);('2rviqsivi\x12r\x16\x16si\x16\x0021\x00 2vi\x00\x16si\x16r\x16\x16si\x16ifvivirviqsivi\x12r\x16\x16si\x16 otherwisethe details', 1);('summary hyperparametershyperparameters sedqn seddpg valueinitial', 1);('values \x0fand\x18 1decay rates \x0fand\x18 0999minimum\x0fand\x18 001minibatch size b 128experience replay memory size', 1);('20000discount factor 095hyperparameters', 1);('sedqnlearning', 1);('rate 00001decay rate learning rate 0001target network update frequency 100weight', 1);('05input dimension network', 1);('dimension network', 1);('nn\x00mhyperparameters seddpglearning', 1);('rate actor network', 1);('rate critic network 0001decay rate learning rate 0001soft parameter target update \x0e 0005weight critic network loss function', 1);('05weight actor network loss function', 1);('09input dimension actor network', 1);('dimension actor network', 1);('ninput', 1);('dimension critic network 2nn\x02moutput dimension critic network 1fig', 1);('processes trainingwithn 6m 3fig', 1);('processes trainingwithn 10m 5vi', 1);('n umerical experimentsin', 1);('dqn ddpga experiment setupsour', 1);('numerical experiments', 1);('intel core', 1);('i5 9400f', 1);('cpu', 1);('ghz', 1);('ram nvidia rtx', 1);('gpu', 1);('remote estimation system weset dimensions process state measurement ln 2anden', 1);('respectivelythe system matrices fangare', 1);('range of114 entries', 1);('cnare', 1);('range 01wnandvnare identitymatrices', 1);('channel state', 1);('\x16h 5levels', 1);('corresponding packetdrop probabilities 0201501005and001 distributions channel states ofeach sensorchannel pair nm ieqnm1qnm\x16hare', 1);('training use', 1);('adam', 1);('gradient reset theenvironment episode', 1);('steps episode numbers', 1);('se actionthe', 1);('tight se action', 1);('thesettings', 1);('i27table ii performance comparison sedrl benchmarks', 1);('average estimation msesystem scalenm paramsetupdqn sedqn ddpg seddpg63', 1);('processes trainingwithn 6m 3b', 1);('performance comparisonfig', 1);('processes training achievedby', 1);('algorithms benchmarks system settings', 1);('shows thatthe', 1);('training episodes convergence', 1);('decreases theaverage sum', 1);('dqn fig', 1);('training episodes reduces average sum', 1);('stages ie last150 episodes', 1);('training performance impliesthat se stages', 1);('optimal policiesin', 1);('ii', 1);('test performance', 1);('different numbers sensors channels', 1);('different settings system parameters ie', 1);('ancn', 1);('10000step simulations', 1);('6sensor3channel systemsthe', 1);('reduces average', 1);('dqn ddpg seddpg', 1);('similar performance', 1);('ddpg seddpg', 1);('converge experiments', 1);('10sensor5channel systems', 1);('dqn sedqn', 1);('ddpg inparticular seddpg', 1);('experiment', 1);('20sensor10channel systems', 1);('channelstate thresholdproperty', 1);('10m 5fig', 1);('selection stagewheren 10m 5therefore performance improvement', 1);('large systemsin addition', 1);('multisensor scheduling problem', 1);('dqnwe', 1);('test effectiveness channelstate threshold property', 1);('loose se actionselection stage', 1);('channelthreshold property', 1);('selection process', 1);('convergence speed', 1);('training method converge nearoptimal policy', 1);('seddpg fig', 1);('selection stage training convergence time doubles comparedwith', 1);('similar performancethus', 1);('action selection', 1);('loose action selection stageare', 1);('algorithmsvii c', 1);('onclusionin', 1);('remote estimationsystem satises number threshold properties', 1);('theoretical guidelineswe', 1);('novel actionselection method', 1);('new loss function', 1);('training efciency numerical results29have', 1);('estimationmeansquare error', 1);('inaddition', 1);('range optimal schedulingproblems', 1);('remote state estimation systems future work investigateother structural properties optimal scheduling resource allocation policies wirelesscommunications', 1);('drlalgorithmsappendix aproof lemma', 1);('function dene', 1);('wfunction', 1);('wsavt s\x02a\x02v rwsavt', 1);('xsprsjsavts', 1);('converge optimal value function', 1);('v\x03s', 1);('independent ofthe', 1);('monotonic iev0s0\x14v0s 36to', 1);('sufcient show value functionv1sis monotonic iev1s0\x14v1s 37and monotonicity value function', 1);('b\x01fromv0stov\x03s', 1);('value iteration dene optimal action optimalpolicy tth iteration asat\x19ts rs arg maxa2axsprsjsavt\x001s 38from', 1);('relationship value function thewfunctionsvt1s', 1);('wsatvt\x15wsavt', 1);('optimal action a1\x191s0to', 1);('followinginequalityv1s0\x00v1s\x14ws0a1v0\x00wsa1v0 40by', 1);('thatws0a1v0\x00wsa1v0 r 0\x00r', 1);('nix 0iprh', 1);('nij nihnia1ni', 1);('0ij 0ihia1iv0s0\x00xhx nix ipr nij nihnia1ni', 1);('ij ihia1iv0s 41which implies states', 1);('ass iands0 0ifor notation simplicity', 1);('0i 0i 1j 0ihia1i', 1);('i i 1j ihia1i havepr 0i 0i 1j 0ihia1iv0 0i 1\x14pr i i 1j ihia1iv0 i', 1);('0i 1j 0ihia1i', 1);('i 1j ihia1ito', 1);('thatpr 0i 1j 0ihia1iv01', 1);('i 1j ihia1iv01', 1);('r 0ir i', 1);('41r 0i\x00r i \x14x 0ipr 0ij 0ihia1iv0 0i\x00x ipr ij ihia1iv0 i\x15\x14044from', 1);('v1s0\x14v1sthus', 1);('monotonicity value function', 1);('operatorb\x01to optimal value function', 1);('v\x03sappendix bproof lemma', 1);('3in proof', 1);('need submodularity value function', 1);('similar tothe proof', 1);('submodular and31probabilistic supermodular', 1);('\x0ehwhere \x0e 00i 0jwith 00i\x14 iand 0j\x15 j', 1);('v0ss\x0e\x14v0s v0s\x0e', 1);('45apj1v0ss\x0e pj1\x00pi1v0ss\x0e\x15pj1v0s pj1\x00pi1v0s\x0e 45bto', 1);('45a 45b', 1);('sufcient show valuefunctionv1sis submodular probabilistic supermodular iev1ss\x0e', 1);('v1ss\x0e\x14v1s v1s\x0e', 1);('46apj1v1ss\x0e pj1\x00pi1v1ss\x0e\x15pj1v1s pj1\x00pi1v1s\x0e 46band properties value function', 1);('b\x01fromv0stov\x03ssimilar', 1);('states s ands\x0e \x0e channelstates', 1);('constant \x0e i 0jand \x0e 00i j terms policysince', 1);('system single channel proof', 1);('optimalpolicy as\x19t ito', 1);('sensor iis', 1);('state ieati 1in', 1);('46a cases aandb 46b cases aandb withdifferent packet success ratesaifpj1\x14pi1', 1);('different optimal actions statesa1\x191 \x0e \x191 \x0e a2\x191 \x0e \x191 \x0e j a3\x191 \x0e iand\x191 \x0e j a4\x191 \x0e jand\x191 \x0e ia1', 1);('\x0e \x191 \x0e', 1);('\x0e\x00v1 \x00v1 \x0e\x14w \x0eiv0 w \x0eiv0\x00w iv0\x00w \x0eiv0', 1);('r \x0e r \x0e r r \x0e', 1);('pi1\x02v01 0j', 1);('j 1\x00v01 j 1\x00v01 0j 1\x03 1\x00pi1\x02v0 i1 0j1v0', 1);('0j1\x03\x140 4832where inequality', 1);('\x0e \x191 \x0e j proof', 1);('similar case a1', 1);('w \x0ejv0 w \x0ejv0\x00w jv0\x00w \x0ejv0\x140a3', 1);('\x0e iand\x191 \x0e j thenv1 \x0e', 1);('\x0e\x00v1 \x00v1 \x0e\x14w \x0eiv0 w \x0ejv0\x00w iv0\x00w \x0ejv0 \x02pi1v01 0j11\x00pi1v0 i1 0j1pj1v0 00i111\x00pj1v0', 1);('j1\x00pi1v01 j1\x001\x00pi1v0 i1 j1\x00pj1v0 00i11\x001\x00pj1v0', 1);('0j1\x03 \x02pi1v01 0j1pi1\x00pj1v0', 1);('j1\x00pi1v01 j1\x00pi1\x00pj1v0', 1);('0j1\x03 1\x00pi1\x02v0\x00 i1 0j1v0', 1);('45a 45ba4', 1);('\x0e jand\x191 \x0e thenv1 \x0e', 1);('\x0e\x00v1 \x00v1 \x0e\x14w \x0ejv0 w \x0eiv0\x00w iv0\x00w \x0ejv0 pj1v0 i11\x00v0', 1);('1\x00pj1v0 i1 0j1\x00v0', 1);('j 1\x00v0 i', 1);('j 1\x14 1\x00pi1v0 i1 0j1v0 i1 j1\x00v0', 1);('0j1\x00v0 i1 j149where', 1);('pj1\x14pi1v0 i11\x14v0 00i11andv0 i1 0j1\x14v0', 1);('\x0e\x00v1 \x00v1 \x0e\x140which', 1);('46abifpj1\x15pi1 proof', 1);('different cases', 1);('different packet success rates33a', 1);('ifpj1\x14pi1', 1);('thenpj1v1 pj1\x00pi1v1 \x0e\x00pj1v1 \x0e\x00pj1\x00pi1v1 \x0epj1\x02v1 i1 j1\x00v1', 1);('j1\x03pi1\x00pj1\x02v1 i1 0j1\x00v1', 1);('ifpj1\x15pi1', 1);('different optimal actions statesb1\x191 \x191 \x0e b2\x191 \x191 \x0e j b3\x191 iand\x191 \x0e j b4\x191 jand\x191 \x0eib1', 1);('\x191 \x0e thenpj1v1 pj1\x00pi1v1 \x0e\x00pj1v1 \x0e\x00pj1\x00pi1v1 \x0e\x14pj1w iv0pj1\x00pi1w \x0eiv0\x00pj1w \x0eiv0\x00pj1\x00pi1w \x0eiv0pi1\x02r i 0j\x00r 00i 0j\x03 pj1\x02pi1v01 j11\x00pi1v0 i1 j1\x03 pj1\x00pi1\x02pi1v01 0j', 1);('0j 1\x03\x00 pj1\x02pi1v01 j', 1);('j 1\x03\x00 pj1\x00pi1\x02pi1v01 0j', 1);('0j 1\x03 50from 45b havepj1v01 j1 pj1\x00pi1v01 0j 1\x00v01 j 1\x00pi1v01 0j 1\x14051andpj1v0 i', 1);('52\x00pj1v0 00i', 1);('j 1\x00pj1\x00pi1v0 i', 1);('onr i 0j\x14r 00i 0j', 1);('thatv1 pj1\x00pi1v1 \x0e\x00pj1v1 \x0e\x00pj1\x00pi1v1 \x0e\x140which', 1);('\x191 \x0e j proof', 1);('similar case b1', 1);('pj1w jv034pj1\x00pi1w \x0ejv0\x00pj1w \x0ejv0\x00pj1\x00pi1w \x0ejv0\x140b3', 1);('iand\x191 \x0e j thenpj1v1 pj1\x00pi1v1 \x0e\x00pj1v1 \x0e\x00pj1\x00pi1v1 \x0e\x14pj1w iv0pj1\x00pi1w \x0ejv0\x00pj1w \x0eiv0\x00pj1\x00pi1w \x0ejv0\x14 pj1\x02pj1v0 00i11pj1\x00pi1v0 i1 0j1\x00pj1v0 i11\x00pj1\x00pi1v0', 1);('0j1\x03\x00 pj1\x00pi1v0 i', 1);('0j 1\x00pj1\x00pi1v0 00i', 1);('pj1v0 i', 1);('j 1\x00pj1v0 00i', 1);('j 1\x00 pi1pj1\x02v0 00i', 1);('j 1\x0353where inequality', 1);('r i 0j\x14r 00i 0j 45b havepj1v0 00i11pj1\x00pi1v0 i1 0j1\x15pj1v0 i11pj1\x00pi1v0', 1);('equation pj1\x00pj1pi1\x02v0 00i', 1);('j 1\x0355which', 1);('45a pj1pj1pi1', 1);('thatpj1v1 pj1\x00pi1v1 \x0e\x00pj1v1 \x0e\x00pj1\x00pi1v1 \x0e\x140which', 1);('jand\x191 \x0e', 1);('case aandpj1\x15pi1 havepi1v1 pi1\x00pj1v1 \x0e\x14pi1v1 \x0e pi1\x00pj1v1 \x0ein addition', 1);('probabilistic supermodularity optimal policyofv1scan', 1);('threshold property terms jin thiscase', 1);('\x191 \x0e jfrom\x191 j implies thatw\x00\x00 i 0j\x01jv0\x01\x15w\x00\x00 i 0j\x01iv0\x01', 1);('topj1v0 i111\x00pj1v0 i1 0j1\x15pi1v01 0j11\x00pi1v0 i1 0j135which exactlypj1v0 i', 1);('pi1\x00pj1v0 i', 1);('0j 1\x15pi1v01 0j', 1);('\x191 \x0e ipi1v01 0j 1\x15pj1v0 00i', 1);('pi1\x00pj1v0 00i', 1);('havepj1v0 i11pj1\x00pi1v0', 1);('0j1\x15pj1v0 00i11pj1\x00pi1v0 i1 0j1which', 1);('1st iteration', 1);('asthe', 1);('iteration probabilistic supermodular case', 1);('nonexistence method iterations value iterationthus submodularity probabilistic supermodularity value function', 1);('v\x03sappendix cproof lemma', 1);('4similar proof', 1);('v0shas', 1);('0ihwhere 0i iv0s0', 1);('sufcient show value functionv1shas property iev1s0', 1);('v1s', 1);('60similar proof', 1);('states s ands0 0i action as\x19ts ito', 1);('ati 1for state', 1);('states differentoptimal value functions', 1);('different optimal actions states36a', 1);('0i\x11i thenv1 0\x00v1 \x14w\x00 0iv0\x01\x00w\x00 iv0\x01pi1r 0i j\x00r i j \x02pi1v01 j11\x00pi1v0', 1);('j1\x00pi1v01 j1\x001\x00pi1v0 i1 j1\x03 0where', 1);('inequality base r 0i j r i jand 59b', 1);('0i\x11j proof', 1);('w 0jv0\x00w jv0 0appendix', 1);('dproof lemma', 1);('5similar proof', 1);('\x0eh \x0e 00i 0jwith 00i iand 0j\x15 jpj1v0ss\x0e pj1\x00pi1v0ss\x0e\x15pj1v0s pj1\x00pi1v0s\x0e 61to', 1);('sufcient show value functionv1sis probabilistic supermodular iepj1v1ss\x0e pj1\x00pi1v1ss\x0e\x15pj1v1s pj1\x00pi1v1s\x0e 62similar proof', 1);('states s i jands\x0e \x0e 00i 0jand action \x19ts ito', 1);('\x0e i 0jand \x0e 00i j', 1);('inthe', 1);('case aandbwith', 1);('different packet success ratesaifpj1\x14pi1 thenpj1v1 pj1\x00pi1v1 \x0e\x00pj1v1 \x0e\x00pj1\x00pi1v1 \x0epj1v1 \x00v1 \x0e pi1\x00pj1v1 \x0e\x00v1 \x0e\x14037where', 1);('different optimal actions statesb1\x191 \x191 \x0e b2\x191 \x191 \x0e j b3\x191 iand\x191 \x0e jb4\x191 jand\x191 \x0e b5\x191 \x191 \x0e kk6ijb6\x191 iand\x191 \x0e k b7\x191 kand\x191 \x0e b8\x191 jand\x191 \x0e k b9\x191 kand\x191 \x0e j b10\x191 k1and\x191 \x0e k2for cases b1 b2 b3 b4 proof proof', 1);('3because states', 1);('sensor iand sensorj', 1);('constant cases', 1);('thereforein', 1);('cases addition cases \x191 kor\x191 \x0e k states', 1);('i j k', 1);('state kis constantb5', 1);('\x191 \x0e kk6ij thenpj1v1 pj1\x00pi1v1 \x0e\x00pj1v1 \x0e\x00pj1\x00pi1v1 \x0e\x14pj1w kv0pj1\x00pi1w \x0ekv0\x00pj1w \x0ekv0\x00pj1\x00pi1w \x0ekv0\x14 pj1\x02pk1v0 i1 j111\x00pk1v0 i1 j1 k1\x03 pj1\x00pi1pk1v0 00i', 1);('k 1\x00 pj1\x00pi1pk1v0 i', 1);('1\x00pk1v0 i', 1);('r \x0e r', 1);('derivedthat pk1pj1v0 i', 1);('0j 11\x00pj1v0 00i', 1);('j 11\x00pj1\x00pi1v0 i', 1);('64and 1\x00pk1pj1v0 i', 1);('k 1\x00pj1v0 00i', 1);('k 1\x00pj1\x00pi1v0 i', 1);('havepj1v1 pj1\x00pi1v1 \x0e\x00pj1v1 \x0e\x00pj1\x00pi1v1 \x0e\x140which', 1);('iand\x191 \x0e k thenpj1v1 pj1\x00pi1v1 \x0e\x00pj1v1 \x0e\x00pj1\x00pi1v1 \x0e\x14pj1w iv0pj1\x00pi1w \x0ekv0\x00pj1w \x0ekv0\x00pj1\x00pi1w \x0eiv0\x14 pj1\x02pi1v01 j1 k11\x00pi1v0 i1 j1 k1\x03 pj1\x00pi1pk1v0 00i', 1);('k 1\x00 pj1\x00pi1pi1v01 0j', 1);('i 00i havev0 i1 j1 k1', 1);('j1 k1\x00v0', 1);('j1 k167andv0 i1', 1);('vs0', 1);('i1 j1 k1andv0 i1', 1);('inequality1\x00pi1pj1v0 i1 j1 k1 pj1\x00pi1v0 00i', 1);('k 1\x00pi1v0 00i', 1);('pj1\x00pi1v0 i', 1);('k 1\x140which', 1);('directlyfor cases b7 b8 b9 b10 proof', 1);('similar case b6thus probabilistic supermodularity value function', 1);('v0spropagates', 1);('thebellman operator', 1);('v\x03sreferences1 j chen', 1);('vucetic structureenhanced', 1);('deep reinforcement learning optimaltransmission scheduling', 1);('available httpsdoiorg1048550arxiv2205118612', 1);('coleri ergen', 1);('fischione', 1);('lu k h johansson wireless', 1);('network design control systems', 1);('asurvey ieee commun surv tutor', 1);('may', 1);('k huang', 1);('vucetic savkin optimal', 1);('downlinkuplink scheduling wireless', 1);('iot ieee internet', 1);('schenato', 1);('sinopoli franceschetti k poolla sastry foundations', 1);('control estimation lossynetworks', 1);('k gatsis pajic ribeiro g j pappas opportunistic', 1);('wireless channels', 1);('ieee transautom', 1);('han j wu h zhang', 1);('multiple linear dynamical systems', 1);('wu x ren dey', 1);('multiple sensors', 1);('channels packet transmissionconstraint', 1);('forootani r iervolino tipaldi dey transmission', 1);('scheduling multiprocess multisensor remoteestimation', 1);('feb', 1);('r sutton g barto reinforcement', 1);('learning introduction', 1);('mit', 1);('z zhao', 1);('vucetic deep', 1);('learning wireless', 1);('systems joint', 1);('approach arxiv preprint', 1);('available httpsdoiorg1048550arxiv22100067311', 1);('leong ramaswamy e quevedo h karl', 1);('shi deep', 1);('reinforcement learning wireless', 1);('cyberphysical systems', 1);('liu k huang e quevedo', 1);('vucetic li deep', 1);('reinforcement learning wireless scheduling', 1);('available httpsdoiorg1048550arxiv21091256213 b', 1);('demirel ramaswamy e quevedo h karl deepcas', 1);('deep reinforcement learning algorithm forcontrolaware scheduling', 1);('ieee contr syst lett', 1);('yang h rao lin xu p shi optimal', 1);('reinforcement learning approach', 1);('inf sci', 1);('apr', 1);('g pang', 1);('vucetic drlbased', 1);('resource allocation', 1);('ieeetrans wirel commun', 1);('available httpsdoiorg1048550arxiv22051226716', 1);('z guo e brunskill directed', 1);('exploration reinforcement learning arxiv preprint', 1);('jun', 1);('onlineavailable httpsdoiorg1048550arxiv19060780517', 1);('wu x ren qs jia k h johansson', 1);('shi learning', 1);('remote state estimationunder', 1);('uncertain channel condition', 1);('ieee trans', 1);('netw syst', 1);('wu k ding p cheng', 1);('multiple sensors lossy bandwidth', 1);('trans netw syst', 1);('p hsu e modiano', 1);('duan age', 1);('information design analysis optimal scheduling algorithms', 1);('procieee int symp inf theory june', 1);('liu e quevedo li k h johansson', 1);('vucetic remote', 1);('state estimation smart sensors', 1);('markovfading', 1);('liu e quevedo k h johansson', 1);('vucetic li stability', 1);('remote state estimation multiplesystems', 1);('multiple markov', 1);('early access', 1);('aug', 1);('puterman markov', 1);('decision processes discrete stochastic', 1);('john wiley', 1);('handbooks', 1);('operations research management science vol', 1);('hern', 1);('lasserre', 1);('topics discretetime', 1);('control processes', 1);('berlin germanyspringer', 1);('topkis supermodularity', 1);('princeton', 1);('university press', 1);('p lillicrap j j hunt pritzel n heess erez tassa silver wierstra continuous', 1);('control withdeep reinforcement learning arxiv preprint', 1);('sep', 1);('available httpsdoiorg1048550arxiv150902971', 1);