('mongolian', 30);('mntts2', 28);('tts', 22);('fastspeech2', 15);('mntts', 14);('mongolian tts', 13);('f1', 12);('hifi-gan', 11);('rui liu', 10);('fastspeech2+hifi-gan', 8);('yifan hu', 7);('f3', 7);('f2', 7);('international conference', 7);('guanglai gao', 7);('feilong bao', 6);('kailin liang', 6);('bin liu', 6);('ss-mos', 6);('multi-speaker mongolian text-to-speech synthesis dataset', 6);('ieee', 6);('english', 5);('n-mos', 5);('100k steps', 5);('baseline system', 4);('experimental results', 4);('text', 4);('fig', 4);('speech', 4);('proceedings', 4);('interspeech', 4);('china', 3);('furthermore', 3);('speech synthesis', 3);('tacotron2', 3);('fastspeech', 3);('mandarin', 3);('speaker similarity', 3);('learning [', 3);('mean', 3);('opinion score', 3);('ron j weiss', 3);('zhifeng chen', 3);('sheng zhao', 3);('articial intelligence', 3);('neural information processing systems', 3);('corr', 3);('language processing', 3);('haizhou li', 3);('robust multi- speaker', 2);('tacotron', 2);('inner mongolia', 2);('wavenet', 2);('melgan', 2);('low-resource language', 2);('industry practitioners', 2);('naturalness mean opinion score', 2);('similarity mean opinion score', 2);('satisfactory performance', 2);('corpus structure', 2);('mainstream languages', 2);('speaker diversity', 2);('libritts', 2);('speech recognition', 2);('professional female native', 2);('wide range', 2);('latin sequences', 2);('audio', 2);('specically', 2);('min', 2);('max', 2);('id', 2);('word numbers', 2);('mel-spectrogram', 2);('speaker encoder module', 2);('conv', 2);('dropout rate', 2);('generative adversarial network', 2);('fastspeech2+grin-lim', 2);('ground truth audio', 2);('ground truth', 2);('condence', 2);('emotional speech synthesis', 2);('emotion category', 2);('future work', 2);('navdeep jaitly', 2);('zongheng yang', 2);('yu zhang', 2);('yuxuan wang', 2);('icassp', 2);('acoustics', 2);('processing', 2);('neural machine translation', 2);('yonghui wu', 2);('proc', 2);('neural', 2);('icml', 2);('yi ren', 2);('xu tan', 2);('tao qin', 2);('zhou zhao', 2);('fast', 2);('advances', 2);('annual', 2);('aron', 2);('van den', 2);('oord', 2);('sander dieleman', 2);('heiga zen', 2);('karen simonyan', 2);('nal kalchbrenner', 2);('koray kavukcuoglu', 2);('september', 2);('isca', 2);('machine learning', 2);('generative', 2);('adversarial networks', 2);('ialp', 2);('yonghe wang', 2);('berrak sisman', 2);('open-source multi-speaker mongolian text-to-speech synthesis dataset kailin liangy', 1);('bin liuy', 1);('yifan huy', 1);('guanglai gao inner mongolia', 1);('hohhot', 1);('liangkailin98 @ foxmail.com', 1);('iframe_liu @ 163.com', 1);('hyfwalker @ 163.com liurui_imu @ 163.com', 1);('{ csfeilong', 1);('csggl } @ imu.edu.cn', 1);('abstract', 1);('text-to-speech', 1);('low-resource languages', 1);('attractive research issue', 1);('industry nowadays', 1);('ocial language', 1);('inner mongolia autonomous region', 1);('representative low-resource language', 1);('people worldwide', 1);('relative lack', 1);('open-source datasets', 1);('open-source multi-speaker', 1);('various topics', 1);('announcer records', 1);('real-world applications', 1);('training recipe', 1);('keywords', 1);('mongolian text-to-speech', 1);('open-source dataset multi-speaker', 1);('introduction text-to-speech', 1);('input text', 1);('human-like speech [', 1);('standard technology', 1);('human-computer interaction', 1);('cell phone voice assistants', 1);('car navigation', 1);('smart speakers', 1);('recent years', 1);('dierent', 1);('traditional methods', 1);('concatenation [', 1);('synthesize speech', 1);('neural end-to-end', 1);('remarkable performance', 1);('encoder-decoder', 1);('architecture [', 1);('typical models', 1);('transformer tts', 1);('deep voice', 1);('corresponding author', 1);('rui liu.y', 1);('high-level talents introduction project', 1);('scientists', 1);('national natural science foundation', 1);('.arxiv:2301.00657v1 [ eess.as ]', 1);('dec', 1);('inference speed', 1);('fastspech2', 1);('mainstream methods', 1);('neural network', 1);('wavernn', 1);('synthesize speech sounds', 1);('human sounds', 1);('important factor', 1);('rapid development', 1);('large scale corpus resources', 1);('slow progress', 1);('corpus collection', 1);('single-speaker dataset', 1);('young female native', 1);('speech synthesis datasets', 1);('baseline models source code', 1);('motivated', 1);('data size', 1);('textual content', 1);('state-of- the-art', 1);('] model', 1);('] vocoder', 1);('accompaniedbaselinemodelformntts2.weconductlisteningexperimentsand report', 1);('main contributions', 1);('multi- speaker', 1);('total audio duration', 1);('various domains', 1);('weusedthestate-of-the-artnon-autoregressive fastspeech2modeltobuildthebaselinemodelandvalidateourmntts2.3', 1);('source code', 1);('statistical information', 1);('experimental setup', 1);('future research directions', 1);('related', 1);('ljspeech', 1);('single- speaker dataset', 1);('aishell', 1);('available resources', 1);('low resource data', 1);('] methods', 1);('large-scale training data', 1);('practical scenarios', 1);('various models', 1);('good results', 1);('huang', 1);('emotional embeddings', 1);('new method', 1);('phrase prediction system [', 1);('immediately', 1);('dnn-based mongolian', 1);('speech synthesis system', 1);('hmm', 1);('bidirectional', 1);('term memory', 1);('bilstm', 1);('prediction step', 1);('traditional speech synthesis system', 1);('above works', 1);('m2asr-mongo', 1);('speech recognition corpus', 1);('environment noise', 1);('improper speaking style issues etc', 1);('dataset [', 1);('total duration', 1);('high-quality multi-speaker', 1);('mntts2 dataset', 1);('rst revisit', 1);('dataset briey', 1);('preliminary work', 1);('high-quality single-speaker', 1);('alignment errors', 1);('ambient noise', 1);('overall quality', 1);('mongolian text-', 1);('challenge', 1);('low-resource scenario', 1);('ncmmsc20221', 1);('intelligent information processing', 1);('minority languages', 1);('construction pipeline', 1);('audio-text alignment', 1);('rst step', 1);('large amount', 1);('natural idea', 1);('text materials', 1);('crawl text information', 1);('electronic books', 1);('text topics', 1);('usage scenarios', 1);('following', 1);('unsuitable content', 1);('sensitive political issues', 1);('religious issues', 1);('pornographic content', 1);('positive contribution', 1);('original intention', 1);('compared', 1);('performs agglutinative characteristic [', 1);('letters express dierent styles', 1);('dierent contexts', 1);('serious harmonic phenomenon [', 1);('latin alphabet', 1);('entire pipeline', 1);('latin conversion', 1);('text regularization', 1);('previous work', 1);('mntts2 f1', 1);('spkid_1_uttid.txt spkid_1_uttid.wavf2', 1);('spkid_1_uttid.txt spkid_1_uttid.wav spkid_1_uttid.txt spkid_1_uttid.wav', 1);('fig.1', 1);('folder structure', 1);('audiorecordingandaudio-textalignment dierentwiththemntts', 1);('weinvitedthreenativemongolian-speakingannouncerstorecordtheaudio.each announcer', 1);('consent form', 1);('data collection', 1);('adobe audition2as', 1);('duringtherecordingprocess', 1);('audio segment', 1);('constant distance', 1);('slight pause', 1);('comma position', 1);('appropriate pitch boost', 1);('question mark position', 1);('corresponding natural audio', 1);('latin sequence', 1);('latin word', 1);('letter thatmakesupthewordiscalledacharacter.charactersalsoincludepunctuation marks', 1);('speech data', 1);('corpus', 1);('corresponding text collection', 1);('statistics results', 1);('statistical unitspeaker idf1 f2 f3 charactertotal', 1);('wordtotal', 1);('f3 fig.2', 1);('word number distributions', 1);('sentence duration distributions', 1);('wav', 1);('format les', 1);('txt', 1);('utf-8', 1);('corresponding text', 1);('statistical results', 1);('entire corpus', 1);('average number', 1);('shortest sentencemntts2', 1);('longest sentence', 1);('statistical unit', 1);('total number', 1);('minimum value', 1);('maximum value', 1);('sentence duration', 1);('obvious concentration', 1);('normal distribution', 1);('speech synthesis experiments toverifythevalidityofourmntts2', 1);('mean opinion score', 1);('mos', 1);('experimental setup weusethetensorflowttstoolkit3tobuildanend-to-endttsmodelbasedon', 1);('model converts', 1);('state-of-the-art non-autoregressive [', 1);('] speech synthesis model', 1);('extracts duration', 1);('speech waveform', 1);('input conditions', 1);('fast training speed', 1);('variance information', 1);('pitch prediction', 1);('wavelet transform', 1);('controllable speech synthesis', 1);('main reason', 1);('speaker encoder', 1);('softplus layers3', 1);('network architecture setting', 1);('text encoder', 1);('layer size', 1);('variance predictors', 1);('initial learning rate', 1);('vocoder builds', 1);('high-quality audio', 1);('one- dimensional', 1);('fusion module', 1);('character embedding mongolian scriptstext encoder', 1);('encoder speaker_id positional encodingvariance adaptor positional encodingmel-spectrogram decoder hifi-gan mrf', 1);('hifi-ganspeaker embedding fig.3', 1);('multi- scaleandmulti-perioddiscriminators.thegeneragorkernelsizeofhifi-ganis7', 1);('cycle scale', 1);('periodic discriminator', 1);('melgan discriminator', 1);('averagepooling1d', 1);('kernel size', 1);('activation function', 1);('leakyrelu', 1);('stft loss', 1);('corresponding vocoder', 1);('extract duration', 1);('attention contrast', 1);('model training', 1);('fastspeech2modelwastrainedwith200kstepstodothenalspeechgeneration', 1);('hifi-gans', 1);('above models', 1);('tesla v100 gpus', 1);('naturalness evaluation', 1);('full comparison', 1);('truth', 1);('baseline model', 1);('grin-lim', 1);('phase information', 1);('additional training', 1);('assess naturalness', 1);('evaluation process', 1);('audio speeches', 1);('quiet environment', 1);('ground', 1);('truth speech', 1);('high-quality speech generation', 1);('naturalness', 1);('systemspeaker idf1 f2 f3 fastspeech2+grin-lim', 1);('similarity evaluation', 1);('speaker similarity performance', 1);('fastspeech+hifi-gan', 1);('ten', 1);('mongolian-speaking', 1);('system performs', 1);('good performance', 1);('auditioning', 1);('f1s', 1);('signicant characteristics', 1);('speakers voice information', 1);('thisexperimentshowsthatthemntts2datasetcanbeusedforspeechsynthesis work', 1);('multi-speaker scenarios', 1);('similarity mean', 1);('systemspeaker idf1 f2 f3 fastspeech2+hifi-gan', 1);('challenges', 1);('empathy ai', 1);('attention [', 1);('conversational scenarios', 1);('hot research topics nowadays [', 1);('emotional intensity', 1);('speech generation', 1);('interesting direction [', 1);('emotion intensity', 1);('in-depth expansion', 1);('conclusion wepresentedalarge-scale', 1);('releasing', 1);('knowledge attribution', 1);('license', 1);('commercial use', 1);('robust multi-speaker', 1);('introduce emotional', 1);('model hyperparameters', 1);('subsequent analyses.mntts2', 1);('references', 1);('jonathan shen', 1);('ruoming pang', 1);('mike schuster', 1);('rj skerrv-ryan', 1);('natural tts synthesis', 1);('mel spectrogram predictions', 1);('in2018 ieee', 1);('signal processing', 1);('f.charpentierandm.stella', 1);('diphonesynthesisusinganoverlap-addtechniquefor', 1);('speech waveforms concatenation', 1);('lawrencerrabiner', 1);('atutorialonhiddenmarkovmodelsandselectedapplications', 1);('kyunghyun cho', 1);('bart', 1);('merrinboer', 1);('dzmitry bahdanau', 1);('yoshua bengio', 1);('encoderdecoder', 1);('syntax', 1);('semantics', 1);('structure', 1);('statistical translation', 1);('rj skerry-ryan', 1);('daisy stanton', 1);('ying xiao', 1);('samy bengio', 1);('towards', 1);('end-to-end speech synthesis', 1);('naihan li', 1);('shujie liu', 1);('yanqing liu', 1);('ming liu', 1);('transformer network', 1);('thirty-third aaai', 1);('thirty-first innovative applications', 1);('aaai symposium', 1);('educational advances', 1);('sercan', 1);('arik', 1);('mike chrzanowski', 1);('adam coates', 1);('gregory frederick diamos', 1);('andrew gibiansky', 1);('yongguo kang', 1);('xian li', 1);('john miller', 1);('andrew y ng', 1);('jonathan raiman', 1);('deep', 1);('real-time', 1);('neural text-to-speech', 1);('yangjun ruan', 1);('tie- yan liu', 1);('controllable text', 1);('hanna m. wallach', 1);('hugo larochelle', 1);('alina beygelzimer', 1);('florence', 1);('emily', 1);('fox', 1);('garnett', 1);('neurips', 1);('december', 1);('vancouver', 1);('bc', 1);('canada', 1);('chenxu hu', 1);('tie-yan liu', 1);('high-quality end-to-end text', 1);('learning representations', 1);('iclr', 1);('virtual event', 1);('austria', 1);('may', 1);('openreview.net', 1);('oriol vinyals', 1);('alex graves', 1);('andrew w. senior', 1);('generative model', 1);('raw audio', 1);('isca speech synthesis', 1);('sunnyvale', 1);('ca', 1);('usa', 1);('erich elsen', 1);('seb noury', 1);('norman casagrande', 1);('edward lockhart', 1);('florian stimberg', 1);('ecient', 1);('neural audio synthesis', 1);('jennifer g. dy', 1);('andreas krause', 1);('stockholmsmssan', 1);('stockholm', 1);('sweden', 1);('july', 1);('pmlr', 1);('kundan kumar', 1);('rithesh kumar', 1);('thibault', 1);('boissiere', 1);('lucas gestin', 1);('wei zhen teoh', 1);('josesotelo', 1);('alexandredebrbisson', 1);('yoshuabengio', 1);('conditional waveform synthesis', 1);('jungil kong', 1);('jaehyeon kim', 1);('jaekyoung bae', 1);('hi-gan', 1);('high delity speech synthesis', 1);('uradyn e bulag', 1);('linguistic anxiety', 1);('anthropologist', 1);('pengkai yin', 1);('open-source mongolian text-to-speech synthesis dataset', 1);('keith ito', 1);('linda johnson', 1);('lj speech dataset', 1);('viet dang', 1);('rob clark', 1);('ye jia', 1);('yao shi', 1);('hui bu', 1);('xin xu', 1);('shaoji zhang', 1);('ming li', 1);('aishell-3', 1);('multi-speaker mandarin tts corpus', 1);('horace', 1);('barlow', 1);('unsupervised', 1);('xiaojin jerry zhu', 1);('semi-supervised', 1);('learning literature survey', 1);('karl weiss', 1);('taghi', 1);('khoshgoftaar', 1);('dingding wang', 1);('big data', 1);('aihong huang', 1);('yu shan', 1);('in2021', 1);('ruiliu', 1);('feilongbao', 1);('guanglaigao', 1);('mongolianprosodicphrase', 1);('sux segmentation', 1);('text-to-speech system', 1);('deep neural network', 1);('national conference', 1);('man-machine speech communication', 1);('springer', 1);('hui zhang', 1);('improving', 1);('bilstm model', 1);('tiankai zhi', 1);('ying shi', 1);('wenqiang', 1);('guanyu li', 1);('dong wang', 1);('m2asr- mongo', 1);('free mongolian speech database', 1);('cocosda', 1);('international committee', 1);('co-', 1);('standardisation', 1);('speech databases', 1);('assessment techniques', 1);('o-cocosda', 1);('singapore', 1);('november', 1);('xueliang yan', 1);('weihua wang', 1);('segmentation-based', 1);('mongolian lvcsr approach', 1);('jiatao gu', 1);('james bradbury', 1);('caiming xiong', 1);('victor o. k. li', 1);('richard socher', 1);('non-autoregressive', 1);('robert', 1);('streijl', 1);('stefan winkler', 1);('david', 1);('hands', 1);('multimedia systems', 1);('decoding', 1);('neural text-to-speech training', 1);('ieee/acm transactions', 1);('reinforcement', 1);('emotional text-to-speech synthesis', 1);('emotion discriminability', 1);('hynek hermansky', 1);('honza cernock', 1);('luks burget', 1);('lori lamel', 1);('odette scharenborg', 1);('petr motlcek', 1);('speech communication', 1);('brno', 1);('czechia', 1);('august', 1);('haolin zuo', 1);('de hu', 1);('explicit', 1);('intensity control', 1);