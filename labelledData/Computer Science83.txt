('wang', 30);('s.', 21);('c.', 19);('mead', 18);('z.', 17);('zhou', 15);('h.', 15);('x.', 15);('style codes', 14);('j.', 14);('m.', 13);('chen', 12);('li', 12);('l.', 12);('zhang', 11);('ji', 11);('liu', 11);('wu', 11);('xu', 10);('facial expressions', 9);('figure', 9);('hdtf', 9);('a.', 9);('head videos', 8);('style encoder', 8);('style code', 8);('style clip', 8);('wav2lip', 8);('yu', 8);('cvpr', 8);('d.', 8);('k.', 8);('w.', 8);('r.', 8);('specically', 7);('liang', 7);('zisserman', 7);('f-lmd', 7);('eamm', 7);('style', 6);('cpbd', 6);('m-lmd', 6);('gc-a vt', 6);('k=', 6);('dsync', 6);('style discriminator', 6);('g.', 6);('eccv', 6);('b.', 6);('f.', 6);('head generation', 5);('speech content', 5);('sinha', 5);('expression parameters', 5);('style-controllable dynamic decoder', 5);('audio-driven', 5);('guo', 5);('chung', 5);('syncconf', 5);('ssim', 5);('yang', 5);('accurate lip-sync', 5);('proceedings', 5);('springer', 5);('e.', 5);('ieee', 5);('ding', 5);('fan', 5);('styletalk', 4);('style reference video', 4);('feed-forward layers', 4);('prajwal', 4);('different speaking styles', 4);('audio encoder', 4);('qian', 4);('tu', 4);('zhu', 4);('ground truth', 4);('lip-sync discriminator', 4);('syncnet', 4);('porikli', 4);('style space', 4);('visualization', 4);('dyffn', 4);('t.', 4);('graphics', 4);('advances', 4);('p.', 4);('q.', 4);('face', 4);('loy', 4);('c. c.', 4);('aaai', 4);('n.', 4);('extensive', 3);('accurate lip synchronization', 3);('target speaker', 3);('speaker speaks', 3);('one-shot setting', 3);('dynamic decoder', 3);('image renderer', 3);('3d facial animations', 3);('ren', 3);('facial motion patterns', 3);('safari', 3);('india', 3);('hernando', 3);('universal style extractor', 3);('inspired', 3);('facial ex- pressions', 3);('petridis', 3);('pantic', 3);('reference image', 3);('style reference', 3);('deng', 3);('style vectors', 3);('pc-a vs', 3);('vct', 3);('upper face', 3);('pca', 3);('temporal discriminator', 3);('dstyle', 3);('qi', 3);('video clips', 3);('condence score', 3);('mouth area', 3);('zheng', 3);('duan', 3);('computer vision', 3);('acm transactions', 3);('tog', 3);('cao', 3);('han', 3);('iccv', 3);('one-shot', 2);('computer', 2);('tsinghua', 2);('lip sync', 2);('generation framework', 2);('style-aware adaptive transformer', 2);('thanks', 2);('audio clip', 2);('virtual human creation', 2);('yin', 2);('identity reference', 2);('audio content', 2);('dynamic facial motion patterns', 2);('discriminator', 2);('ea', 2);('a0 t\x00w', 2);('arbitrary style clips', 2);('vaswani', 2);('ex- tract', 2);('vetter', 2);('expression param- eters', 2);('triplet constraint', 2);('unseen style clips', 2);('pro- pose', 2);('style clips', 2);('suwajanakorn', 2);('seitz', 2);('fried', 2);('yi', 2);('thies', 2);('lahiri', 2);('das', 2);('wiles', 2);('koepke', 2);('sadoughi', 2);('busso', 2);('tfrom', 2);('transformer encoder', 2);('attention weights', 2);('style tokens', 2);('makeittalk', 2);('karras', 2);('=kx k=1\x19k', 2);('upper face group', 2);('meaningful style space', 2);('pointnet', 2);('qualitative', 2);('style reference videos', 2);('demo video', 2);('patchgan', 2);('goodfellow', 2);('triplet loss', 2);('dong', 2);('shen', 2);('structural similarity', 2);('ablation study', 2);('audio-visual dataset', 2);('adam', 2);('kingma', 2);('ba', 2);('snyder', 2);('rtx', 2);('gpu', 2);('learning rate', 2);('quantitative evaluation', 2);('landmark distance', 2);('blur detection', 2);('narvekar', 2);('karam', 2);('full model', 2);('van', 2);('maaten', 2);('hinton', 2);('neutral style', 2);('cui', 2);('kou', 2);('ineccv', 2);('maddox', 2);('r. k.', 2);('j. s.', 2);('biswas', 2);('bhowmick', 2);('speech-driven', 2);('jia', 2);('o.', 2);('tewari', 2);('theobalt', 2);('bao', 2);('l. j', 2);('acm', 2);('international conference', 2);('multimedia', 2);('deep', 2);('ieee transactions', 2);('speaker recognition', 2);('neural information processing systems', 2);('international journal', 2);('ni', 2);('zeng', 2);('budagavi', 2);('luo', 2);('styles yifeng ma1', 1);('suzhen wang2', 1);('zhipeng hu2,3', 1);('changjie fan2', 1);('tangjie lv2', 1);('yu ding2,3', 1);('zhidong deng1', 1);('xin yu4', 1);('bnrist', 1);('thuai', 1);('key laboratory', 1);('intelligent', 1);('systems', 1);('university 2virtual', 1);('human group', 1);('netease fuxi ai lab,3zhejiang', 1);('university 4university', 1);('sydney', 1);('dingyu01 g @ corp.netease.com mayf18 @ mails.tsinghua.edu.cn', 1);('michael @ tsinghua.edu.cn', 1);('xin.yu @ uts.edu.au', 1);('abstract different', 1);('head methods', 1);('signicant progress', 1);('natural facial expres- sions', 1);('stable head motions', 1);('generate diverse', 1);('arbitrary reference speak-', 1);('one-shot portrait', 1);('extract dy- namic facial motion patterns', 1);('intro- duce', 1);('style-controllable decoder', 1);('fa- cial animations', 1);('aware adaptation mechanism', 1);('portrait image', 1);('authentic visual', 1);('project page', 1);('introduction audio-driven', 1);('broad applications', 1);('video creation', 1);('tremendous progress', 1);('head pose generation', 1);('high-delity video generation', 1);('various styles', 1);('real-world scenarios', 1);('different people', 1);('* work', 1);('netease', 1);('corresponding', 1);('audiostyle referencevideo', 1);('aoutput', 1);('bstyle', 1);('bfigure', 1);('illustration', 1);('one-shot image', 1);('additional style reference videos', 1);('different situations', 1);('such signicant diversities', 1);('great challenge', 1);('previous work', 1);('discrete emotion classes', 1);('recent methods', 1);('control facial expressions', 1);('additional emo- tional source video', 1);('facial motion characteristics', 1);('frame-by-frame fashion', 1);('temporal dynamics', 1);('universal spatio-temporal representation', 1);('one-shot speaker image', 1);('head generation framework', 1);('framework rst encodes', 1);('input audio', 1);('corresponding latent', 1);('asarxiv:2301.01081v1 [ cs.cv ]', 1);('jan', 1);('encoder self-attention pooling text', 1);('hello ph', 1);('hh', 1);('audio encoder image renderer', 1);('style reference video expression parameters1', 1);('phoneme labels audio', 1);('+style code reference image', 1);('multi', 1);('attention fc relu fc', 1);('softmax 1,12,2 ,1', 1);('feed', 1);('layers', 1);('query key valuecalculation', 1);('attention attention expert weights', 1);('adaptive layer style', 1);('mouth discriminator', 1);('+style code audio featureaudio feature3dmm', 1);('styletalk framework', 1);('dynamic decoder figure', 1);('framework rst extracts sequential 3dmm expression parameters \x0e1', 1);('nfrom', 1);('vand', 1);('esto', 1);('style code s.', 1);('encodes phoneme labels', 1);('edgenerates', 1);('expression parameters ^\x0ewithsanda0', 1);('ertakes', 1);('expression parameters ^\x0eand', 1);('identity reference image', 1);('iras', 1);('output video', 1);('ref- erence image', 1);('primary goal', 1);('universal style encoder', 1);('latent style code', 1);('sequential 3d', 1);('model', 1);('blanz', 1);('style code space', 1);('fur-', 1);('meaningful space', 1);('driving', 1);('challenging one-to-many', 1);('trans- form', 1);('ambiguous one-to-many', 1);('conditional one-to-one', 1);('observe un-', 1);('satisfactory lip-sync', 1);('visual artifacts', 1);('faces exhibit', 1);('large facial motions', 1);('style-controllable dynamic transformer', 1);('de- coder', 1);('multi-head attention', 1);('great importance', 1);('style manipulation', 1);('hence', 1);('kernel weights', 1);('attention mechanism', 1);('kkernels', 1);('adaptive mechanism', 1);('style-controllable one-to-one', 1);('different styles', 1);('generate photo-realistic', 1);('diverse speak-', 1);('novel one-shot', 1);('style-controllable audio-', 1);('creates authentic', 1);('diverse styles', 1);('speaker image', 1);('beneting', 1);('style-controllable dynamic transformer decoder', 1);('related', 1);('considerable attention', 1);('person- agnostic methods', 1);('person-specic', 1);('kemelmacher-shlizerman', 1);('person- specic methods', 1);('synthesize photo-realistic', 1);('recently', 1);('introduce neural radiance elds', 1);('head generation.person-agnostic methods aim', 1);('early methods', 1);('ja-', 1);('accurate mouth movements', 1);('deep learning', 1);('pra-', 1);('aforementioned methods', 1);('generate videos', 1);('arbitrary speakers', 1);('stylized', 1);('expres- sive facial expressions', 1);('head gen- eration', 1);('emotion informa- tion', 1);('explicit emotion labels', 1);('generate expressive', 1);('additional emotional source video', 1);('target speaker frame-by-frame', 1);('pre- vious works captures', 1);('temporal co-activations', 1);('proposed method', 1);('novel framework', 1);('irof', 1);('aof', 1);('v=is', 1);('nof', 1);('y=^i1', 1);('tin', 1);('target speaker speaks', 1);('eawhich', 1);('phoneme labels a1', 1);('esthat', 1);('compact style codes', 1);('ed', 1);('3dmm expression parameters ^\x0e1', 1);('im- age renderererwhich generates', 1);('pirenderer', 1);('ren- derer', 1);('input fir', 1);('vgin', 1);('window length', 1);('eais', 1);('audio contains', 1);('articulation-irrelevant information', 1);('such information', 1);('phoneme labels', 1);('mel frequency cepstrum coefcients', 1);('mfcc', 1);('audio signals', 1);('phoneme labels at\x00w', 1);('t+ware con-', 1);('word embeddings', 1);('a0 t2r256', 1);('speech recognition tool', 1);('esextracts', 1);('dynamic fa- cial motion patterns', 1);('such informa- tion', 1);('style video clip', 1);('sequential expression parame- ters\x0e1', 1);('n2rn\x0264', 1);('previous methods', 1);('static expressions', 1);('static images', 1);('sequen- tial 3dmm expression parameters', 1);('input tokens', 1);('af-', 1);('temporal correlation', 1);('encoder outputs', 1);('n. intu-', 1);('video clip', 1);('typical frames', 1);('self-attention pool-', 1);('style information', 1);('layer adopts', 1);('token-level attention weights', 1);('token-level attention weights repre-', 1);('frame-level contributions', 1);('video-level style code', 1);('nal style code s2rds', 1);('s= softmax', 1);('wsh', 1);('ht', 1);('trainable parameter', 1);('h=', 1);('[ s1', 1);('sn ] 2rds\x02nis', 1);('fea- tures', 1);('style vector', 1);('early stage', 1);('vanilla transformer de- coder', 1);('articulation representations a0 t\x00w', 1);('style code sas input', 1);('style code 2w+1times', 1);('positional encod- ings', 1);('transformer decoder', 1);('latent articulation representations', 1);('middle out-', 1);('feed-forward network', 1);('output expression parameters', 1);('aforementioned decoder', 1);('defective lip movements', 1);('facial expressions whenmead', 1);('hdtf method ssim', 1);('ours', 1);('quantitative results', 1);('large facial move- ments', 1);('static kernel weights', 1);('network weights', 1);('important role', 1);('novel style-aware adaptive feed-forward layers', 1);('aware adaptive layer utilizes', 1);('weights ~wk', 1);('such parallel weights', 1);('ex- perts', 1);('distinct facial motion patterns', 1);('additional layers', 1);('softmax', 1);('atten- tion weights', 1);('feed-forward layer weights', 1);('kx', 1);('attention weight', 1);('kthfeed-forward layer weights ~wk', 1);('style-controllable dynamic feed-forward layers', 1);('y=g\x10 ~wt', 1);('activation function', 1);('accu- rate', 1);('lip movements', 1);('disentanglement', 1);('upper', 1);('lower', 1);('different motion patterns', 1);('low frequency', 1);('high frequency', 1);('motion patterns', 1);('separate networks.we rst', 1);('style-controllable dynamic decoders', 1);('upper face decoder', 1);('expression parame- ters', 1);('mouth movements', 1);('expression bases', 1);('supplementary materials', 1);('objective function', 1);('framework generates', 1);('sequential training strategy', 1);('temporal consistency', 1);('l=', 1);('lat', 1);('dtem', 1);('mouth shape varies', 1);('audio window', 1);('video win- dow', 1);('con- trol', 1);('facial movements', 1);('pure mouth shape representation', 1);('mouth vertices', 1);('mesh vertex coordinates', 1);('mouth encoder', 1);('phoneme encoder', 1);('phoneme win- dow', 1);('cosine similarity', 1);('probability thatemandeaare synchronous', 1);('psync=em\x01ea', 1);('mouth gt wav2lip pc-avs avct gc-avteamm oursaudio', 1);('mouth gt wav2lip pc-avs avct gc-avteamm oursaudiospeaker style', 1);('person agnostic methods', 1);('please', 1);('dmis', 1);('framework maximize synchronous probability', 1);('sync losslsyncon', 1);('lsync=1 llx', 1);('pi', 1);('input sequen- tial 3dmm expression parameters \x0e1', 1);('l. specically', 1);('ps2rcthat', 1);('cdenotes', 1);('style dis- criminator', 1);('isola', 1);('.the style discriminator', 1);('cross- entropy loss', 1);('style discriminator guides', 1);('vivid speaking styles', 1);('style losslstyle', 1);('lstyle=\x00log', 1);('ps', 1);('dtemlearns', 1);('input se- quences', 1);('3dmm expression parameters \x0e1', 1);('l.dtemfol-', 1);('gan', 1);('hinge loss', 1);('ltem.triplet constraint intuitively', 1);('similar speaking styles', 1);('style triplet constraint', 1);('vcwith', 1);('style c', 1);('style clipsvp c', 1);('vn', 1);('corresponding style codessc', 1);('sp candsn c.', 1);('ltrip=', 1);('maxfksc\x00sp ck2\x00ksc\x00sn ck2+', 1);('margin parameter', 1);('loss', 1);('self-driven setting', 1);('l1', 1);('lrec=\x16ll1', 1);('lssim', 1);('land^\x0e1', 1);('lare', 1);('ratio coefcient', 1);('total loss', 1);('aforementioned loss terms', 1);('l=\x15reclrec+\x15tripltrip+\x15synclsync+', 1);('\x15tem= 1and \x15style= 1.neutralintensity', 1);('emotional style codes', 1);('w011', 1);('darker', 1);('emotion intensity', 1);('w/o trip w/o style source image mouth', 1);('gtstyle', 1);('full =16 w/o sync=4', 1);('interpolation', 1);('experiments datasets', 1);('con- struct', 1);('in-the-lab talking-face corpus', 1);('different intensity levels', 1);('emo- tions', 1);('high-resolution in-the-wild', 1);('in- tensity level', 1);('speaker share', 1);('original videos', 1);('fomm', 1);('siarohin', 1);('fps', 1);('details', 1);('pytorch', 1);('eris', 1);('celeb', 1);('er', 1);('dstyleis', 1);('es', 1);('edanddtemare', 1);('quantitive', 1);('conduct quantitative evaluations', 1);('lip synchronization', 1);('m- lmd', 1);('whole face', 1);('cumu-', 1);('probability', 1);('state-of-the-art methods in-', 1);('self-driven set-', 1);('rst image', 1);('corresponding audio clip', 1);('audio input', 1);('generates movements', 1);('head poses', 1);('ground truth videos', 1);('gen- erates mouth movements', 1);('reference images', 1);('qualitative evaluation', 1);('iden- tity reference', 1);('unseen dur-', 1);('gener- ate', 1);('speaker identity bet- ter', 1);('style control', 1);('mouth shape', 1);('furthermore', 1);('gc-', 1);('vt', 1);('plausible background', 1);('neutral speaking style', 1);('entire face', 1);('accurate lip- sync', 1);('satisfactory identity preservation', 1);('plau- sible backgrounds', 1);('user study', 1);('effective- ness', 1);('supplemen- tary materials', 1);('ablation', 1);('conduct ablation studies', 1);('adaptive feedforward layer', 1);('vanilla feedforward layer', 1);('setk= 4=16in', 1);('dynamic feedforward layer', 1);('variants utilize', 1);('variant w/o', 1);('facial motions', 1);('dstyleandltrip', 1);('triplet constraint compelour framework', 1);('bad lip synchronization', 1);('inspection style', 1);('2d space', 1);('stochastic neighbour embed-', 1);('emotions \x02', 1);('extract style codes', 1);('distinct color', 1);('speaker cluster', 1);('emo- tion', 1);('color cor-', 1);('emotion rst gathers', 1);('low inten- sity', 1);('neutral emotion', 1);('notably', 1);('similar facial motion patterns', 1);('anger vs disgust', 1);('surprise vs', 1);('aforementioned observations', 1);('mean- ingful style space', 1);('style manipulation thanks', 1);('videos transi- tion', 1);('style intensity', 1);('new speaking styles', 1);('conclusion', 1);('head genera- tion framework', 1);('generates one-shot audio-', 1);('arbi- trary style reference video', 1);('facial animations', 1);('style-controllable modules', 1);('previous works', 1);('method captures', 1);('spatio-temporal co-activations', 1);('method creates photo-', 1);('acknowledgments', 1);('hangzhou key', 1);('innovation program', 1);('key', 1);('program', 1);('zhe-', 1);('province', 1);('arc-discovery', 1);('dp220100800', 1);('arc-decra', 1);('de230100477', 1);('national science foundation', 1);('china', 1);('nsfc', 1);('grant', 1);('guo qiang', 1);('ai', 1);('smart mo- bility', 1);('saic', 1);('xinya ji', 1);('borong liang', 1);('yan pan', 1);('generous help', 1);('lincheng li', 1);('zhimeng zhang', 1);('helpful discussions', 1);('references blanz', 1);('morphable model', 1);('26th an- nual conference', 1);('interactive tech- niques', 1);('good talking-head video generation', 1);('arxiv preprint arxiv:2005.03201', 1);('talking-head', 1);('rhythmic head motion', 1);('lip', 1);('movements generation', 1);('hier-', 1);('archical cross-modal', 1);('dynamic pixel-wise loss', 1);('jamaludin', 1);('arxiv preprint arxiv:1705.02966', 1);('asian conference', 1);('facial animation', 1);('tong', 1);('accurate', 1);('cvprw', 1);('triplet', 1);('siamese network', 1);('zollh', 1);('finkelstein', 1);('shecht-', 1);('goldman', 1);('d. b.', 1);('genova', 1);('jin', 1);('agrawala', 1);('text-based', 1);('talking-head video', 1);('i.', 1);('pouget-abadie', 1);('mirza', 1);('warde-farley', 1);('ozair', 1);('courville', 1);('bengio', 1);('generative', 1);('adversarial nets', 1);('neural in- formation processing systems', 1);('ad-nerf', 1);('audio driven neural radiance', 1);('head synthesis', 1);('arxiv preprint arxiv:2103.11078 .isola', 1);('j.-y', 1);('efros', 1);('image-', 1);('to-image translation', 1);('conditional adversarial networks', 1);('inproceedings', 1);('pattern recognition', 1);('one-shot emotional', 1);('audio-based emotion-aware motion model', 1);('arxiv preprint arxiv:2205.15278', 1);('emotional video portraits', 1);('laine', 1);('aittala', 1);('hellsten', 1);('lehtinen', 1);('aila', 1);('analyzing', 1);('image qual- ity', 1);('d. p.', 1);('stochastic optimization', 1);('arxiv preprint arxiv:1412.6980', 1);('kwatra', 1);('frueh', 1);('lewis', 1);('bregler', 1);('lipsync3d', 1);('data-efcient learning', 1);('personalized', 1);('faces', 1);('video', 1);('pose', 1);('lighting nor-', 1);('write-a-speaker', 1);('text-based emotional', 1);('rhythmic talking-head generation', 1);('pan', 1);('hong', 1);('expressive', 1);('audio-visual control', 1);('incvpr', 1);('semantic-aware', 1);('implicit neural audio-driven video portrait generation', 1);('arxiv preprint arxiv:2201.07786', 1);('n. d.', 1);('no-reference per- ceptual image sharpness metric', 1);('cumulative prob- ability', 1);('international workshop', 1);('quality', 1);('multimedia experience', 1);('mukhopadhyay', 1);('namboodiri', 1);('jawahar', 1);('lip sync expert', 1);('lip generation', 1);('c. r.', 1);('su', 1);('mo', 1);('guibas', 1);('3d classication', 1);('segmen- tation', 1);('zhi', 1);('gao', 1);('speech', 1);('co-speech', 1);('gesture synthesis', 1);('t. h.', 1);('piren-', 1);('controllable portrait image generation', 1);('semantic neural', 1);('conditional sequential generative adversar- ial networks', 1);('affective computing', 1);('self-attention', 1);('arxiv preprint arxiv:2008.01077 .siarohin', 1);('lathuili', 1);('` ere', 1);('tulyakov', 1);('ricci', 1);('sebe', 1);('order motion model', 1);('image anima- tion', 1);('yadav', 1);('generalized', 1);('face genera-', 1);('arxiv preprint arxiv:2205.01155', 1);('garcia-romero', 1);('sell', 1);('povey', 1);('khudanpur', 1);('x-vectors', 1);('robust', 1);('dnn embeddings', 1);('international confer- ence', 1);('signal processing', 1);('icassp', 1);('everybodys', 1);('arxiv preprint arxiv:2001.05201', 1);('talk-', 1);('conditional recurrent adversarial net- work', 1);('arxiv preprint arxiv:1804.04786', 1);('s. m.', 1);('kemelmacher- shlizerman', 1);('synthesizing', 1);('learning lip sync', 1);('elgharib', 1);('niener', 1);('neural', 1);('voice puppetry', 1);('fa- cial reenactment', 1);('visualizing', 1);('machine learning research', 1);('shazeer', 1);('parmar', 1);('uszkoreit', 1);('jones', 1);('gomez', 1);('a. n.', 1);('kaiser', 1);('polosukhin', 1);('at-', 1);('realis-', 1);('tic speech-driven facial animation', 1);('qiao', 1);('emotional talking-face generation', 1);('au-', 1);('audio-driven one-shot talking-head generation', 1);('head motion', 1);('ijcai', 1);('audio-visual corre- lation learning', 1);('articial intelligence', 1);('rethinking', 1);('inter-', 1);('national conference', 1);('computational linguistics', 1);('bovik', 1);('a. c.', 1);('sheikh', 1);('h. r.', 1);('simoncelli', 1);('e. p.', 1);('image', 1);('quality assessment', 1);('error visibility', 1);('image process-', 1);('x2face', 1);('pose codes', 1);('dou', 1);('imitating', 1);('realistic audio-', 1);('bender', 1);('q. v', 1);('ngiam', 1);('cond-', 1);('conditionally', 1);('efcient inference', 1);('neural information processing sys-', 1);('ye', 1);('video generation', 1);('head pose', 1);('arxiv preprint arxiv:2002.10137', 1);('cun', 1);('bai', 1);('styleheat', 1);('one-', 1);('shot high-resolution', 1);('arxiv preprint arxiv:2203.04036', 1);('fernando', 1);('ghanem', 1);('hartley', 1);('facial component heatmaps', 1);('transformative discriminative neural networks', 1);('hallucinating', 1);('low- resolution', 1);('transforma- tive discriminative autoencoders', 1);('pose dynamics', 1);('com-', 1);('zhao', 1);('huang', 1);('facial', 1);('synthesizing dynamic', 1);('implicit attribute learning', 1);('flow-guided one-shot', 1);('face generation', 1);('high-resolution audio-visual dataset', 1);('audio- visual representation', 1);('sun', 1);('audio-visual representation', 1);('shechtman', 1);('echevarria', 1);('kaloger-', 1);('makelttalk', 1);('speaker-aware talking-head animation', 1);('m.-d.', 1);('a.-h.', 1);('audio-visual learning', 1);('automation', 1);('computing', 1);