('lipschitz', 33);('lemma', 26);('mfgne', 25);('mfg', 15);('theorem', 13);('nash', 12);('anahtarci', 11);('assume', 10);('international conference', 10);('zaman', 8);('guo', 8);('markov', 7);('tdlearning', 7);('hence', 7);('nagents', 6);('kotsalis', 6);('sags', 6);('oas', 6);('nplayer', 5);('o2', 5);('mfgs', 5);('assumption', 5);('proposition', 5);('ctd', 5);('algorithm', 5);('rl', 4);('fenchel', 4);('mdp', 4);('jensens', 4);('pmlr', 4);('machine learning', 4);('mao', 3);('independent learning', 3);('nplayer sags', 3);('denition', 3);('pma', 3);('bregman', 3);('lan', 3);('large class', 3);('firstly', 3);('eld games', 3);('nashequilibrium', 2);('o1n', 2);('nagent', 2);('marl', 2);('eld game', 2);('perrin', 2);('theresults', 2);('exact case', 2);('lauri', 2);('fg', 2);('nothis', 2);('algorithms literature', 2);('meaneld reward populationpolicy pair', 2);('mdps', 2);('mirror ascent operator', 2);('population', 2);('qvalues', 2);('policy', 2);('fixed', 2);('regularizer h', 2);('l1', 2);('conversely', 2);('learning rate', 2);('th e', 2);('convergence', 2);('td', 2);('fo', 2);('conditional tdlearning', 2);('kmtdmpgmrequire', 2);('pg1do5 formtditerations do6', 2);('compute ctd', 2);('assume ea', 2);('k1', 2);('vhqhqhthe', 2);('vand qfunctionsqqare', 2);('proofof lemma', 2);('eq', 2);('br', 2);('sbe', 2);('dene', 2);('response operator', 2);('pe', 2);('tmix', 2);('denote', 2);('quantifying', 2);('corollary', 2);('learning', 2);('advances', 2);('mathematics operations', 2);('siam', 2);('optimization', 2);('systems', 2);('matthie', 2);('neural information processing systems', 2);('arxiv221214449v1 mathoc', 1);('dec', 1);('mirror ascent ecient', 1);('learningin mean', 1);('gamesbatuhan yardim', 1);('computer scienceeth zurichsemih cayci', 1);('mathematics information processingdepartment mathematicsrwth aachen universitymatthieu geist', 1);('mfgeistgooglecomgoogle research brain teamniao niaoheinfethzchdepartment', 1);('computer scienceeth zurichabstractmeaneld', 1);('theoretical tool', 1);('games literature', 1);('theoretical results', 1);('variations population generativemodel', 1);('arbitrary modications population distribu tion learningalgorithm', 1);('policy mirror ascent converge thenash equilibrium', 1);('sample trajectory', 1);('population generative model', 1);('themean eld', 1);('divergent approach literature', 1);('bestresponse map rst show policy mirror ascent map', 1);('construct contractive operator', 1);('tdlearning nagent', 1);('value functions', 1);('time stepsthese results', 1);('sample complexity guarantees orac lefree setting', 1);('sample path', 1);('furthermore', 1);('nagentswithnitesampleguaranteeskeywords', 1);('eld games policy mirror ascent reinforcement learning1', 1);('introductionmultiagent', 1);('reinforcement learning', 1);('challenging problem despitethis', 1);('wide spectrum applications instance n ance', 1);('shavandi khedmati2022', 1);('civil engineering', 1);('wiering', 1);('multiplayer ga mes', 1);('samvelyan', 1);('energy markets', 1);('rashedi', 1);('robotic control', 1);('matig', 1);('cloud resource management', 1);('framework rst', 1);('lasry lions', 1);('useful theoretical tool analyzi ng specic class', 1);('nis', 1);('large main insight', 1);('analyzes limitinggame', 1);('condition rewards transition dynami cs the1game symmetric agent', 1);('distribution agents statesie agents', 1);('conceptually mfg', 1);('formulates representative', 1);('distribution agents', 1);('agentsthis abstract framework', 1);('tractable theoreticall characterize approximatenash equilibrium', 1);('nagent marl', 1);('ngrows anahtarci', 1);('saldi', 1);('plays instance se e', 1);('themfg', 1);('nancial systems uctions city planningwhile', 1);('similar ideas principle va rious mathematical formalizations', 1);('nite horizon case', 1);('lasryli', 1);('ons conditions', 1);('equilibria timedependent pol icies', 1);('2021in innite horizon setting time', 1);('dependent evolutio n population becomesa challenge', 1);('population distributioncarmona', 1);('stationary population distributions', 1);('xie', 1);('paper studiesthis stationary', 1);('ta', 1);('ble 1in parallel', 1);('literature exists plethora res ults', 1);('policy gradient', 1);('pg', 1);('rl agarwal', 1);('cen', 1);('li', 1);('tomar', 1);('qlearning', 1);('employe stationary', 1);('literature instance', 1);('recent result singleagent', 1);('mirrordescent style operato r achieveo1 samplecomplexity', 1);('mdps lan', 1);('algorithm b uilds conditional', 1);('tdlearning kotsalis', 1);('establishes v ariational inequality equivalentto value estimation problem', 1);('samples fr om single trajectory fromthe', 1);('chain context linear quadratic', 1);('mfgs lq mfg', 1);('policy gradientmethod entropy regularization', 1);('ac celerate learning', 1);('actorcritic', 1);('cont ext', 1);('lqmfg fu', 1);('al2019 general', 1);('wi th', 1);('qva', 1);('asimilar', 1);('mirror descent operator', 1);('nite horizon', 1);('continuous time analysis', 1);('heuristic extensionto', 1);('insights f rom', 1);('mfg pg', 1);('literature toobtain', 1);('timestep complexity innite horizon stationary', 1);('strong acle assumptions relatedto population distribution instance', 1);('generativemodel oracle', 1);('samples game', 1);('dynamic population distribution', 1);('similarly nplayer', 1);('weak simulator oracle', 1);('samples state transitions rewards anypopulation distribution', 1);('theoraclefree sandbox learning', 1);('natural actorcritic method', 1);('slow twotimescal e update populationdistribution', 1);('estimates state dy namics', 1);('population distribution tim e step', 1);('thealgorithm realworld problems', 1);('the2no populationmanipulationsinglepathnagentsimulatorindependentlearningguo', 1);('noanahtarci', 1);('nosubramanian', 1);('noxie', 1);('nozaman', 1);('summaryof', 1);('discretestateaction spaces requirementsalgorithm', 1);('modify control population agents example', 1);('control transportation infrastructure', 1);('large number drivers', 1);('costly force drivers particula r conguration simulatingthe system', 1);('impossible run simulations star', 1);('arbitrary populationcongurations asit', 1);('thecaseinstrategic vide ogames withunknowncomplicateddynamics', 1);('oneofourmaingoals', 1);('istoproposeanalgorithmtha tinteracts', 1);('policiesof agents', 1);('typical single agent', 1);('inthispaperwetacklethequestionofindependent learning', 1);('existingalgorithmsfor mfgs', 1);('controller making simulation', 1);('policies ofagents', 1);('show policy mirror ascent', 1);('algorithm extendedto', 1);('independent learning algorithm run', 1);('additional knowledge thepopulation environment observations ir actions state transitionsand rewards', 1);('similar idea', 1);('independent lea', 1);('thetwo agent setting', 1);('daskalakis', 1);('sayin', 1);('learning ongraphs', 1);('gu', 1);('ana lyze independentlearningwhereagent states', 1);('nodes undir', 1);('independentlearning', 1);('large scale', 1);('potential games', 1);('player zerosum games independe nt learning algorithms havebeen', 1);('various settings', 1);('ozdaglar', 1);('contributionspolicy', 1);('mirror ascent operators', 1);('results inthestationary', 1);('anahtarc', 1);('response map', 1);('lipschitzcontinuous', 1);('blanket assumption', 1);('op erator', 1);('dierent approach provethat policy mirror ascent yield acontraction', 1);('comparable conditions', 1);('strong conc avity regularizer', 1);('thisdierence', 1);('algorithm needa subroutine compute bestresponse step3no population manipulation work', 1);('simulate singleepisode', 1);('anonymous game algorith interact environment', 1);('policies agents', 1);('thisisincontrastwithpa', 1);('stworkwheretheempiricalstatedistribution agents', 1);('population show', 1);('singleagent conditionaltd learning results', 1);('innplayer games', 1);('generative model absence population oracle', 1);('complexities includinga population bias nonhomogeneous dynamics', 1);('due ev', 1);('weestablish tdlearning', 1);('pat h simulations', 1);('o2samples', 1);('errorsample eciency algorithms dier', 1);('response need tobe', 1);('population distribution', 1);('value function estimati', 1);('necessary policy mirrorascent iteration approach yields sample comp lexity', 1);('opposedto exampleo4', 1);('o4a anahtarci', 1);('2022independent learning', 1);('fundamental question', 1);('competitive multiagent learnin g iswhether', 1);('controller question', 1);('signi cant meaneld games wherethe', 1);('large number agents ight', 1);('knowledge establ ish rst', 1);('nagents2 mean', 1);('game formalizationfirstly', 1);('problem assu mesis anitestate spaceandais niteaction space denotethe', 1);('probability meas ureson niteset', 1);('xby xwe', 1);('sa leth ar0be', 1);('dene symmetric', 1);('anonymous game states', 1);('sags nplayerswhich', 1);('main object interest work', 1);('state reward dynamicsdepend empirical distribution states', 1);('ag ents', 1);('anonymity andare agent', 1);('symmetric', 1);('anonymous games', 1);('anplayer', 1);('anonymous gameis tuple', 1);('nsaprfor', 1);('initial states players si0ni1snand policiesini1ninduce random sequence states rewards sititritit eachtimestep t0sit1psitaitt aitisit ritrsitaitti 1nwherepsa', 1);('ssandrsa s01map', 1);('state action andpopulation distribution tuples transition probabiliti es', 1);('rewards t41nsummationtextni1summationtextssbdsitsessis', 1);('thefsitimeasurable random vector empirical statedistribution time tof agentswith', 1);('initial states n', 1);('initial distribution0sand policies 1nn formalize', 1);('discountedreturns agentdenition', 1);('reward thenplayer', 1);('sags nsaprwe', 1);('return player ifor', 1);('initial state distribution 0sand', 1);('nasjih0 ebracketleftbiggsummationdisplayt0tparenleftbigrsitaitthisitparenrightbigvextendsinglevextendsinglevextendsinglevextendsinglevextendsinglesj00ajtjsjtsjt1psjtajttt0j1nbracketrightbiggbased', 1);('returns intr oduce notion anash equilibrium', 1);('nplayer sagsdenition', 1);('anntuple policies 1nnand', 1);('initial distribution 0sconstitute', 1);('equilibrium 0if alli 1nwe havejih0maxjihparenleftbigi0parenrightbigwherei 1i1i1nnas', 1);('useful theoretical analysis tool', 1);('approximate th enplayer game theinnite agent limit', 1);('main insight', 1);('observesa single representative agents policy', 1);('po pulation', 1);('innitesimal opponents', 1);('distributi ons', 1);('reward representative agent st population use thelettervto distinguish', 1);('inniteagent reward th e', 1);('rewards ofthenagent', 1);('jdenition', 1);('meaneld', 1);('reward dene', 1);('sasvh ebracketleftbiggsummationdisplayt0trstathstvextendsinglevextendsinglevextendsinglevextendsinglevextendsingles0atstst1pstatbracketrightbiggwith', 1);('equilibrium concept', 1);('mfglimit', 1);('wi th optimality conditionand stability condition stability condition', 1);('ensures thatthe population distribution', 1);('policy population distribution', 1);('spairis', 1);('conditions satisedstability summationdisplaysasaspssaoptimality', 1);('vh', 1);('maxvhif optimality condition', 1);('vhmaxvh', 1);('callamfgnewhile optimality condition corresponds obj ective singleagent', 1);('rlincorporating', 1);('decision pr ocesses yields evolvingmdpdependingonthepopulationdistributionie aninn itefamilyofmdps', 1);('conceptually', 1);('mirrors case multi agent', 1);('comment th e eect regularizationdue toh', 1);('c8themain', 1);('motivation tostudytheabstract', 1);('concept hat correspondsto anapproximate', 1);('wellknown resultproposition', 1);('assumethat', 1);('technical conditions', 1);('existsannnn0such anash equilibrium', 1);('nplayer sagsin', 1);('standard techniques', 1);('paper aoparenleftbig1nparenrightbignash equilibrium', 1);('main objectivefor rest paper', 1);('section willanalyze', 1);('exact case3', 1);('relevant operators exact casein', 1);('section show', 1);('innite agent limit', 1);('point iteration', 1);('show convergence', 1);('exact settingwhere value functions', 1);('stochastic case state majorresults ideas postpone proofs appen dices section', 1);('2022denitions equip setssawith discrete metric dxy', 1);('bdxneationslashy', 1);('theset stateaction value functions', 1);('qqsa r', 1);('equip spacessrsand', 1);('arawith', 1);('forqqqwe', 1);('normsbadblbadbl1 supssbadblssbadbl1badblqqbadbl', 1);('ar0is', 1);('concave function', 1);('awith', 1);('respectto normbadblbadbl1see', 1);('appendix dene umax arg maxuahuhmaxhumaxmax maxs umaxfor allssandqmax1hmax1we', 1);('dierentiable simplicity', 1);('results yield straightforwardgeneralization tothecase', 1);('hr0we dene convex setsuhuahuhmaxhand h suhss1as', 1);('standard previous work', 1);('smoot hness assumptions', 1);('prassumption', 1);('prthere', 1);('exists constants', 1);('kkskallslar0such', 1);('thatsssaaasbadblpsapsabadbl1kbadblbadbl1ksdsskadaarsarsalbadblbadbl1lsdssladaawithout loss generality', 1);('kska2', 1);('thatbadblpsapsabadbl12 andrsarsa131', 1);('population update operatorsone', 1);('critical tool analysis', 1);('framework underst', 1);('evolution thepopulation dene operator characterize single step', 1);('update population update operator', 1);('ssis', 1);('aspops summationdisplaysssummationdisplayaasaspssasswe', 1);('introduce shorthand notation npop poppoppopbracehtipupleft bracehtipdownrightbracehtipdownleft', 1);('results instance', 1);('2019in notation denition constants', 1);('lipschitzlemma', 1);('population updates population update operator popis', 1);('continuous withbadblpoppopbadbl1lpopbadblbadbl1ka2badblbadbl1wherelpop', 1);('ks2kak', 1);('framework wewouldlike tocomputeuni quepopulationdistributions', 1);('stable respect apolicy', 1);('thisrequires', 1);('pas work', 1);('stable population', 1);('stable ie', 1);('lpop1we', 1);('formalize operator maps policies', 1);('stable distributionsdenition', 1);('stable population operator popunder', 1);('stable population operator', 1);('sis', 1);('unique population distribution thatpoppop popthat', 1);('ssitis', 1);('limnnpopand popis', 1);('operators popand popare', 1);('population oracles32', 1);('policy mirror ascent operatorfor', 1);('policy updates', 1);('approach litera ture', 1);('apolicy mirror ascent', 1);('response operatorwe dene qhandqhfunctions stateaction pair asqhsa', 1);('ebracketleftbiggsummationdisplayt0trstathstvextendsinglevextendsinglevextendsinglevextendsinglevextendsingles0sa0ast1pstatat1st1t0bracketrightbiggqhsa rsasummationdisplaysapssaasqhsawealsodene vhs', 1);('notethat qhsa', 1);('qhsahs denitions place analyze operator hat maps populationpolicy pairs qfunctions operator', 1);('crucial online learning algorithm itcan', 1);('current policydenition', 1);('qoperator dene operator q', 1);('sqasq', 1);('qhqsthe key result qis', 1);('subset policies', 1);('heq 1lemma', 1);('continuity qleth 0be arbitrary exists', 1);('lqlqdepending', 1);('hsuch handsbadblqqbadbllqbadblbadbl1lqbadblbadbl1lemma', 1);('subclass policieshowever', 1);('result shows', 1);('policy c', 1);('subset hfor', 1);('h 0lemma', 1);('letsarbitrary', 1);('optimal response ssvhs max', 1);('vhs thenlhwherelhlalska2ks8we', 1);('lhobtained', 1);('kh1ofanahtarci', 1);('results generalize', 1);('continuityof optimal', 1);('respect pol icywithout introducingstringent assumptions', 1);('main policy improvement operatordenition', 1);('mirror ascent operator learning rate', 1);('dene thepolicy mirror ascent update operator mdqasmdqs arg maxuulhanbacketletuqsanbacketihthu12badblusbadbl22ssqqwe', 1);('continuity mdas result', 1);('duality strongmonotonicity gradient operator', 1);('convex fu nctionslemma', 1);('continuity mdmdis', 1);('continuous arguments qqq', 1);('thatbadblmdqmdqbadbl1lmdbadblbadbl1lmdqbadblqqbadblwherelmdq2a12aandlmd1a1we dene', 1);('main operator interest learning rat e', 1);('step respect', 1);('thestationary distribution', 1);('xedpoint iterationprocess nd', 1);('main justication followinglemma demonstrates', 1);('policies 0lemma', 1);('0be arbitrary pair', 1);('operator islipschitz', 1);('lonbadblbadbl1', 1);('lqdened lqlpoplqlqwe', 1);('lqonly', 1);('llalskkska ldepends', 1);('onthe dimensionality problem namelyradicalbiga dependence disappear whenthe learning rate', 1);('llqmoreover sags', 1);('large 09it', 1);('holdthat l q', 1);('whenever l qholds contraction', 1);('large learning rate instance', 1);('lq1', 1);('results mirror theconditions contraction', 1);('resp onse operator', 1);('response eac h stageit', 1);('natural alternative', 1);('form solutions mdin', 1);('certain cases eg entropy regularizer', 1);('con tinuity general divergences withrespect postpone consideration', 1);('future work', 1);('computin', 1);('g mdwill', 1);('conc ave problem computationalaspects mdare', 1);('learning exact caseconcluding', 1);('present theorem charac terizes linear convergenceto', 1);('exact caseproposition', 1);('learning mfgne', 1);('assume mfgne l1for', 1);('updatest1 tfor allt0', 1);('t1', 1);('havebadbltbadbl1ltbadbl0badbl12ltproofthe result', 1);('point contractionproperty', 1);('6in case access generativemodel', 1);('literature ieat time twecan samplefrom', 1);('pstat', 1);('andrstat forarbitrary', 1);('implies asamplecomplexity bypluggingin amethodt estimate valuefunctions andthe', 1);('section show generative model assu mption isnot', 1);('necessary single sample path empirical', 1);('ulation evolution sucientremark', 1);('contraction property', 1);('case bestresponse opera tors', 1);('anatural', 1);('question learning stationary', 1);('po ssible', 1);('universal proof intractability appendices', 1);('c7that', 1);('response operators', 1);('lit erature', 1);('trivial ie bestresponse poli cy result similarto', 1);('cui koeppl', 1);('dierent characterizationremark', 1);('n e', 1);('c8see', 1);('geist', 1);('idea regularization btain', 1);('true ne', 1);('new infact', 1);('perolat', 1);('playerimperfect information game', 1);('reference policy', 1);('open questionif', 1);('mfgnecan', 1);('simi lar scheme instance usinga', 1);('divergence regularizer', 1);('dhsoldswhereoldis', 1);('samplebased learning n agentsafter', 1);('deterministic operator comput', 1);('main goal show', 1);('simulation single path', 1);('map qcan', 1);('simulation steps singletrajectory accumulation error past iter ations controlleddenitions', 1);('probabilistic setup singlepath learnin g withnagentsletsi0ni1snbe arbitrary', 1);('initial states timestep', 1);('agent iasitfori 1n', 1);('transitionsaititsit sit1psitaittfori 1n tis', 1);('thefsitni1measurable empirical state distribution asbefore', 1);('ftthe', 1);('sigma algebra', 1);('ftfsitaitritnti1t0we', 1);('note learning algorithm context sp ecifyitat timetfor anyagenti', 1);('observations itmust', 1);('beft1measurable denezsa 01sa', 1);('itto random transition observations agentiat timet', 1);('sitaitritsit1ait1 setz', 1);('nssthenite', 1);('possible empirical state distributions', 1);('nagentsin', 1);('persistence excit ation condition', 1);('withtechnical conditions probability transition functi', 1);('blanket assumption formalize persistence excitation sucient', 1);('assumptionthe rst assumption', 1);('entire training process takeeach action state probability', 1);('persistence', 1);('exists pinf0such that1maxaspinffor anyssaa2', 1);('aspinf0qsaqmaxsasa itholds mdqaspinfsasaequivalent conditions hthe persistence excitation assumption', 1);('concave h instance entropy regularization', 1);('infact', 1);('possible characterize', 1);('technical conditions n gradienthat boundaryof', 1);('ato', 1);('d1assumption', 1);('sucient', 1);('satisfying aspinf0ssaa', 1);('initial states si0isn exists', 1);('tmix0mix0such', 1);('uniform policy', 1);('persistence excitation', 1);('sincewe employ', 1);('concave regularizer h', 1);('certain conditions h', 1);('aperiodicityand irreducability', 1);('van roy', 1);('mixing empirical population boundsnext', 1);('useful results', 1);('deviation empiricalpopulation distribution', 1);('characte rize', 1);('terms thestate visitation probabilities agent', 1);('empirical populationdistribution tapproximates', 1);('o1nlemma', 1);('empirical', 1);('time t0 agentifollows', 1);('arbitrary policy iso thataitisit sit1psitaittt0i 1nletbe arbitrary policy 1nsummationtextibadblibadbl1 t0', 1);('thatebracketleftbigvextenddoublevextenddoubletpoptvextenddoublevextenddouble1vextendsinglevextendsinglevextendsingleftbracketrightbig1lpop1lpopparenleftbiggradicalbig2snka2parenrightbiggthe dependence', 1);('policies agents deviate e achother', 1);('additional bias', 1);('empirical p opulation distribution', 1);('learning case', 1);('whereas term signicant forthe', 1);('independent learning case', 1);('variance agents policy updates usefulcorollary quantify', 1);('deviation table population respectto policy timecorollary', 1);('stable distribution conditions', 1);('7for anyt0 haveebracketleftbigvextenddoublevextenddoubletpopvextenddoublevextenddouble1vextendsinglevextendsinglevextendsingleftbracketrightbig11lpopparenleftbiggradicalbig2snka2parenrightbigg2lpopas', 1);('representative agen ts state visitation probabilities', 1);('eld population bia term result generalizessimilar', 1);('chains case time dependence dueto population dynamicsproposition', 1);('mean', 1);('mc', 1);('letsi0i snbe', 1);('arbitrary initialstates agent', 1);('polic yi isaitisit sit1psitaittt0i 1nletbe arbitrary policy 1nsummationtextibadblibadbl1', 1);('t0and', 1);('anyi 1n', 1);('thatbadblpsitpopbadbl1cmixtmix2ktmix2mixradicalbig2sntmixkak2mix1lpoptmixkamixbadblibadbl1wheremix maxbraceleftbiglpop1mix1tmixbracerightbigandcmix4tmixmaxk1mixtmixmaxlpop1mix1tmix12anintuitivedetail aboveisthatthemixingconstant mixisequal tothemaximumofthemarkov chain', 1);('mix1tmixand population', 1);('lpop hence', 1);('chainandthe population', 1);('essential use convergence resul ts', 1);('nite sample boundsfor', 1);('tdlearning nplayer sags', 1);('tdlearning population dynamicsone', 1);('main results paper variant', 1);('tdle', 1);('sags specically', 1);('results co nditional', 1);('ctdkotsalis', 1);('learner waits', 1);('tim e steps', 1);('updates ensure sucient convergence', 1);('steady state', 1);('variational inequality approach', 1);('introduce rele vant operatorsdenition', 1);('operators ctd letand', 1);('dene thebellman operator', 1);('tqqastqsa rsahssummationdisplaysapparenleftbigssaparenrightbigparenleftbigasparenrightbigqparenleftbigsaparenrightbigfor q q', 1);('fq mqtq', 1);('diagsassarsasais stateactiondistribution matrix', 1);('eld distribution', 1);('denethe stochastic', 1);('fqzq', 1);('withfq parenleftbigqsarhsqsaparenrightbigesaqq sarsaznote', 1);('clean omit dependence ftffon theregularizer h', 1);('fwith', 1);('respect abstract', 1);('stable distribution', 1);('time twe observe', 1);('fparenleftbigqit2parenrightbigfori1nfromsimulationswith nagents themainchallenge', 1);('fcanestimate fwell', 1);('population populationd ependent', 1);('markovchain', 1);('mix evaluation f', 1);('waits certainnumber samples', 1);('step utilize', 1);('algorithm performs', 1);('tdlearnin', 1);('g rst agent 1it', 1);('tis', 1);('contractive andfis', 1);('lipschitz lf', 1);('withrespect thebadblbadbl2norm onqfor anyfis', 1);('modulus f 1mixpinf isanbacketletfqqqhanbacketihtfbadblqqhbadbl22qqwhereqh', 1);('true value function policy meaneld popas', 1);('section 3ouranalysisconsistsoftwosteps 1weprovethatthectda lgorithmofkotsalis', 1);('andstate theex plicit boundsee', 1);('4in appendices', 1);('quantify bias convergenc e rate', 1);('distribution case', 1);('main result', 1);('purp ose13algorithm', 1);('mmtd', 1);('learning ratesmm policiesii', 1);('initial statessi0isett0 andq0sa', 1);('qmaxssaaform01m1doformtditerations', 1);('dotake simulation step iaitisitritrsitaittsit1psitaitttt1end forctd', 1);('update qm1qmmf1parenleftbigqm1t2', 1);('s1t2a1t2r1t2s1t1a1t1parenrightbigend forreturnqmof clearer presentation', 1);('depende nce problem parameters wedene', 1);('lhsuch lhmtdlog1flog40cmixlog1mix', 1);('ctd2161lh12mixpinfctdpop120kl1lh12mixpinf ctdpop2109kl1lhtmixradicalbig2s1lpop123mixpinfctdpol15tmix1lhkl8kak1lpop123mixpinf ctdpol2101lh4katmix2122mixpinftheorem', 1);('learning population', 1);('assume assumption', 1);('iaspinffor alli', 1);('assume algorithm', 1);('run policiesiiarbitrary', 1);('initial agent states si0i learning rates m21t0m1m0andanym 0mtdmtd', 1);('ifis', 1);('arbitrary policy 1nsummationtextibadblibadbl1andqqh random output', 1);('qmof algorithm', 1);('satisesebadblqmqbadblctd1radicalbigmt0mt01ctd2mradicalbigmt0mt01ctdpop1lmtdpopctdpop2nctdpol1ctdpol2badbl1badbl1if policies agents ie alli', 1);('eldpop ino2log1 timesteps bias term', 1);('oparenleftbig1nparenrightbigas', 1);('learning mfgne samplesin', 1);('section analyze', 1);('mfgn e', 1);('sample pathusingnagents rst', 1);('policies synchron', 1);('policy yi', 1);('byeach agent', 1);('local observations alg orithms', 1);('simple iteration1estimate values', 1);('eachagent', 1);('keepingtheirpoliciesconstant performsctd learningformpg0 steps time', 1);('mtdsamples td', 1);('updates2policy update', 1);('onepolicy mirror ascentupdate', 1);('qvalue', 1);('estimates update', 1);('caseboth algorithms', 1);('form sit1psitaitt andritrsitaitt tis theempirical state distribution', 1);('time twhich thealgorithm control', 1);('singlepath ge nerativemodelfree algorithmsalgorithm', 1);('centralized mfgpmarequire', 1);('initial statessi0i1set0maxandt02fork0k1do3q0sa', 1);('qmaxsa4form0m', 1);('stepiaitsitritrsitaittsit1psitaitt7 tt18 end for9', 1);('updateqm1qmmfkparenleftbigqm1t2parenrightbig10end for11pma step k1 mdqmpgk12end for13return policy', 1);('kalgorithm', 1);('mfgpmarequire', 1);('initial statessi0i1seti0maxiandt02fork0k1do3saiqi0sa', 1);('qmax4form0m', 1);('stepiaitisitritrsitaittsit1psitaitt7 tt18 end for9', 1);('update iqim1qimmfikparenleftbigqimit2parenrightbig10end for11pma stepiik1 mdqimpgik12end for13return policiesikiwe rst', 1);('present main theorem', 1);('algorithmuses samples path single agent', 1);('nfor', 1);('updatesthe policies agents', 1);('simplicity theorem algorithm', 1);('number iterations', 1);('beforehand canbe', 1);('budget inputtheorem', 1);('centralized', 1);('0an arbitrary learning rate whichsatisesl1', 1);('assumptions', 1);('015be arbitrary learning rates mare', 1);('log81logl1andmpgmaxbraceleftbig4ctd1lmdq1l1116ctd22l2mdq1l22bracerightbigthen random output', 1);('kof algorithm', 1);('total number time steps', 1);('kmpgmtd', 1);('o2log21sample', 1);('complexityin fact', 1);('mfgne oparenleftbig1nparenrightbigne nagent sags theorem', 1);('shows aalgorithm', 1);('oparenleftbig1nparenrightbigne sags o2time', 1);('case agent learns separat e policies observingtheir stateaction transitions', 1);('algorith', 1);('uses path ie theobservations agent ito update policy', 1);('thealgorithm employ', 1);('multiple timescales', 1);('desirable symmetry propertythat learner', 1);('protocol meaning hand', 1);('procedure betweenlearners', 1);('starts agr eement total number ofepochsk proof utilizes bounds', 1);('terms polic deviations ofagents', 1);('case learning rate wi', 1);('important achievethe', 1);('l1 assumptions', 1);('0be arbitrary', 1);('letthe', 1);('learning rates mfor', 1);('forclmdqctdpol1ctdpol2lmdo11 ifc1', 1);('ifc', 1);('ifc1', 1);('letmtdmaxlog41l1lmdqctdpop11ctdpol1lmdqc1ck1logl1popmtdandmpg41l2lmdqmaxbraceleftbigctd114ctd2lmdq22bracerightbig1ctdpol1lmdqc1ck216then random output ikiof', 1);('satises agents 1nebracketleftbigbadblikbadbl1bracketrightbigoparenleftbig1nparenrightbigremark', 1);('constant cin', 1);('oparenleftbig1parenrightbig', 1);('large enoughstrong concavity modulus ccan', 1);('theorem implies sample complexity', 1);('o2for sags', 1);('smooth dynamics eg', 1);('large enoug h', 1);('kal', 1);('smoothness conditions foro2sample complexity', 1);('restrictive general ca se', 1);('complexity polynomial', 1);('o2cwherecscales', 1);('with2logclogl1 learning rateis', 1);('aect sample comp lexity clvary', 1);('note logcintroduces logarithmic dependency cin general case tothe parameters', 1);('tmix1mix', 1);('discussionoverall', 1);('policy mirror ascent', 1);('sample complexityfor', 1);('anonymous game approach', 1);('literature thatwe', 1);('mirror ascent type operators', 1);('res ponse exhibit similarcontraction properties', 1);('strong oracle assumptio ns', 1);('game dynamics', 1);('futureextensions', 1);('nerfor instance potential basedanalysis', 1);('results policy mirror descent', 1);('emp hasize', 1);('3proves nite sample guarantees', 1);('sa mple complexity bemuch', 1);('case hypothesize', 1);('sample complexitywould', 1);('achievable independent learning case', 1);('ner analysis empiricalpopulation distributionacknowledgments', 1);('disclosure fundingthe', 1);('nccr automation eth', 1);('research gr ant', 1);('swiss national science foundation', 1);('project funding', 1);('clarication notation constantsthe', 1);('interior boundary', 1);('xrdare', 1);('dene pointset distance dxx inf xxbadblxxbadbl2xmndenotes setof matrices elements', 1);('x foraxmn', 1);('theith row vector', 1);('asaiand jth column vector aj', 1);('stochastic', 1);('matrices withpositive entries row sums', 1);('exxforxxis vector onlythe entry', 1);('corresponding xset 1badblbadbldenotes dual norm badblbadbla note notation functions', 1);('certain cases', 1);('useful alternatebetween', 1);('equivalent denitions', 1);('certain functions', 1);('aandasee anahtarci', 1);('al2022 instance', 1);('dene state transition probabi lities functions', 1);('psasstaking', 1);('stateaction pairs input', 1);('stateprobability distribution actions relation', 1);('pssu', 1);('convention functio n onfona', 1);('equivalent function', 1);('awithfnotation', 1);('constants analysis', 1);('rsimplicity use convention function mult iple variables', 1);('fxy thelipschitz', 1);('constant fwith respect xyare', 1);('lfxlfylfrespectivelyb', 1);('lemmasin', 1);('general lemmas', 1);('hroughout paperlemma', 1);('georgii', 1);('fera', 1);('real valuedfunction', 1);('probability measures', 1);('e thenvextendsinglevextendsinglevextendsinglevextendsinglevextendsinglesummationdisplayefeesummationdisplayefeevextendsinglevextendsinglevextendsinglevextendsinglevextendsinglesupefeinfefe2badblbadbl1weprovidetwogeneralizationsofthislemma', 1);('therstonead aptedfromkontorovich', 1);('ramanan2008 lemma a2lemma', 1);('gerpa vector value function twoprobability measures', 1);('e thenvextenddoublevextenddoublevextenddoublevextenddoublevextenddoublesummationdisplayegeesummationdisplayegeevextenddoublevextenddoublevextenddoublevextenddoublevextenddouble1g2badblbadbl1whereg', 1);('supeebadblgegebadbl1proofwe use fact badblubadbl1 supbardblvbardbl1uvby duality norms', 1);('applying', 1);('thison vectorsummationtextegeee obtainvextenddoublevextenddoublevextenddoublevextenddoublevextenddoublesummationdisplayegeeevextenddoublevextenddoublevextenddoublevextenddoublevextenddouble1 supbardblvbardbl1summationdisplayeeegev18take vrdsuch thatbadblvbadbl1', 1);('eis', 1);('nite eeegevgevbadblgegebadbl1badblvbadblbadblgegebadbl1by application', 1);('olders inequality', 1);('hand sidein', 1);('result followswe generalize lemma belowlemma', 1);('assumev1v2danda1akb1bkrdd0are', 1);('stochastic matrices', 1);('induction k', 1);('fork', 1);('sincebadblv1a1a2ak1v2b1b2bk1badbl1badblv1a1akv2b1bkbadbl1supjbadblak1jbk1jbadbl1hence general statement', 1);('duality establis h', 1);('strong co nvexity', 1);('strong concavity withrespect arbitrary normsdenition', 1);('strong', 1);('letfrdrbe', 1);('convex function withdomainsxrdfxrand letbadblbadblrdr0be arbitrary norm', 1);('rd iffor', 1);('convex function modulus respect norm badblbadbl19as', 1);('convex function fis', 1);('concave functionwith respect norm badblbadbl', 1);('standard concept', 1);('fenche', 1);('frdris convex functionwith domain', 1);('srd fenchel', 1);('conjugate frdris', 1);('asfy supxsanbacketletxyanbacketihtfxfor details', 1);('ne', 1);('fenchelconjugate', 1);('wellknown duality resultlemma', 1);('convex withrespect norm badblbadbland domain', 1);('srd then1fis', 1);('rd2fy', 1);('arg maxssanbacketletxyanbacketihtfx3fis1smooth respect badblbadbl iebadblfyfybadbl1badblyybadblyyrdproofseelemma15ofshalevshwartz', 1);('singer2007orlemma61', 1);('operators lipschitz continuityin', 1);('lipschi', 1);('tz continuity operators mentionedinthemaintext', 1);('someoftheseresultsareorganizeda', 1);('al2022 setting others relevant mirror ascen', 1);('unique casec1', 1);('lipschitz continuity abadblbadbl1ofprfirstly', 1);('rsas01', 1);('andpsassas rewards action probabilities probability distributio ns actions', 1);('rsu', 1);('summationtextaauarsa andpsu summationtextaauapsa allssuasthese', 1);('alternative denitions', 1);('practical establ', 1);('rpare lipschitz', 1);('prfor', 1);('allsssuuasrsursulbadblbadbl1lsdssla2badbluubadbl1badblpsupsubadbl1kbadblbadbl1ksdsska2badbluubadbl120proofthe lemma', 1);('inequalit iesrsursuvextendsinglevextendsinglevextendsinglevextendsinglevextendsinglesummationdisplayaauarsasummationdisplayaauarsavextendsinglevextendsinglevextendsinglevextendsinglevextendsinglevextendsinglevextendsinglevextendsinglevextendsinglevextendsinglesummationdisplayaauarsasummationdisplayaauarsavextendsinglevextendsinglevextendsinglevextendsinglevextendsinglesummationdisplayaauarsarsavextendsinglevextendsinglevextendsinglevextendsinglevextendsinglesummationdisplayaauarsasummationdisplayaauarsavextendsinglevextendsinglevextendsinglevextendsinglevextendsinglewherethe', 1);('linefollows fromjensens inequality', 1);('lemma8', 1);('inequality rst term', 1);('value functionsin', 1);('continuity value fu nctions policy optimalvaluefunctionswith respect toapopulation', 1);('weden ethethreevaluefunctionbelowdenition', 1);('value function', 1);('ebracketleftbiggsummationdisplayt1trstathstvextendsinglevextendsinglevextendsinglevextendsinglevextendsingles0sst1pstitstatstbracketrightbiggqhsa ebracketleftbiggsummationdisplayt1trstathstvextendsinglevextendsinglevextendsinglevextendsinglevextendsingles0sa0ast1pstitstatstbracketrightbiggqhsa rsasummationdisplaysssummationdisplayaapssaasqhsasimilarly q', 1);('functions argument usin', 1);('ssqhsu summationtextaqhsauaandqhsu summationtextaqhsaualikewise dene', 1);('standard optimal value functions', 1);('optimal value function', 1);('asvhs maxvhs', 1);('qhsa', 1);('functions argument usare', 1);('ssqhsu summationtextaqhsauaandqhsu', 1);('useful characterization value functions', 1);('bellman', 1);('lemma states sta ndard result', 1);('value', 1);('points value', 1);('satisfyvhs summationdisplayaasparenleftbiggrsahssummationdisplaysspssavhsparenrightbiggqhsa', 1);('rsahssummationdisplaysssummationdisplayaapssaasqhsaqhsa rsasummationdisplaysssummationdisplayaapssaasparenleftbigqhsahsparenrightbiglikewise', 1);('optimal value functions', 1);('points satisfyingvhs maxuabracketleftbiggsummationdisplayauaparenleftbiggrsahusummationdisplaysspssavhsparenrightbiggbracketrightbiggqhsu', 1);('rsahusummationdisplaysssummationdisplayamaxuabracketleftbiguapssavhsbracketrightbigqhsa rsasummationdisplaysssummationdisplayamaxuabracketleftbiguapssavhsbracketrightbigc3 lipschitz continuity policy mirror ascentas', 1);('work focus policy mirror ascentscheme', 1);('response operator section cludes relevant proofs thecontinuity policy mirror descent response', 1);('continuity mirror ascent', 1);('u0aqra 0kaa convex', 1);('dene graakasgqu0 arg maxukquhu1badbluu0badbl2222thengis', 1);('qu0 isbadblgqu0gqu0badbl1lgqbadblqqbadbllgubadblu0u0badbl1wherelgq2a12algu1a1proofthe', 1);('continuity respect', 1);('convex uwith respect normbadblbadbl1 continuity respect u0 rst dene functionfu', 1);('gradient obtainfu qhu1uu0by', 1);('strong concavity hhis', 1);('monotone operator parameter', 1);('denebynkthe', 1);('normal cone operator', 1);('corresponding set', 1);('k nkis', 1);('kis', 1);('convex rst order optimality conditions w e haveugqu00funku0qhu1uu0nku0q1u0parenleftbigg1ihnkparenrightbigguuparenleftbigg1ihnkparenrightbigg1q1u0where', 1);('resolventparenleftbig1ihnkparenrightbig1is function', 1);('sincenkis', 1);('monotone inbadblbadbl1gparenleftbig1ihnkparenrightbigis', 1);('monotone operator parameter1a', 1);('theng1is lipschitz', 1);('function constanta1between', 1);('g11u0', 1);('continuous u0 followsthatgis', 1);('continuous u0with respect norm badblbadbl1with', 1);('lgudenedin', 1);('main text', 1);('state ss mdqs gqss thesetkulhin', 1);('continuity gshown', 1);('supremum sides respect sand', 1);('denition normson obtainbadblmdqmdqbadbl1lgqbadblqqbadbllgubadblbadbl1so', 1);('lmdqlgqlmdlguc4 lipschitz continuity value functionsin', 1);('continuous main diculty', 1);('adependenceonthelipschitzcontinuity h', 1);('werstprovetwotechnical', 1);('point denition', 1);('vh lemma', 1);('denition hin', 1);('1vhs1vhs2rs1s1rs2s2hs1hs2summationdisplaysparenleftbigpss1s1pss2s2parenrightbigvhslslahsupssvhsvhs2badblpsss1pss2s2badbl1lslahmin2kska2supssvhsvhswhich yields lemma', 1);('hand side s1s2lemma', 1);('vhassume', 1);('h 0arbitrary handsbadblvhvhbadbllvbadblbadbl1lvbadblbadbl1forlv4lakalvs41lv2lklvs21andlvsis', 1);('previous computationsvhsvhsrssrssvextendsinglevextendsinglevextendsinglevextendsinglevextendsinglesummationdisplaysparenleftbigpsssvhspsssvhsparenrightbigvextendsinglevextendsinglevextendsinglevextendsinglevextendsinglelabadblbadbl1lbadblbadbl1lvs2kbadblbadbl1ka2badblbadbl1supsvhsvhs24where', 1);('application triang', 1);('inequality previouslemmanext', 1);('main body paperproofof', 1);('denition qh terms', 1);('vhsince', 1);('thelipschitz continuity', 1);('vhhas', 1);('specically', 1);('lqlvlvskac5 lipschitz', 1);('continuity population updateproofof', 1);('proof relies', 1);('9vextenddoublevextenddoublepoppopvextenddoublevextenddouble1vextenddoublevextenddoublevextenddoublevextenddoublevextenddoublesummationdisplaysspsssummationdisplaysspssvextenddoublevextenddoublevextenddoublevextenddoublevextenddouble1vextenddoublevextenddoublevextenddoublevextenddoublevextenddoublesummationdisplaysspsssummationdisplaysspssvextenddoublevextenddoublevextenddoublevextenddoublevextenddouble1bracehtipupleft bracehtipdownrightbracehtipdownleft bracehtipuprightavextenddoublevextenddoublevextenddoublevextenddoublevextenddoublesummationdisplaysspsssummationdisplaysspssvextenddoublevextenddoublevextenddoublevextenddoublevextenddouble1bracehtipupleft bracehtipdownrightbracehtipdownleft bracehtipuprightbthe rst term', 1);('asummationdisplayssvextenddoublevextenddoublepsspssvextenddoublevextenddouble1ka2vextenddoublevextenddoublevextenddoublevextenddouble1kbadblbadbl1for', 1);('supremum use', 1);('pto', 1);('sssbadblpsspssbadbl1ksdsskabadblssbadbl1ks2kadssfrom lemma', 1);('continuity popas', 1);('main text25lemma', 1);('continuity popassume', 1);('sis lipschitz', 1);('lpopka21lpopproofletl', 1);('denitionbadblpoppopbadbl1badblpoppoppoppopbadbl1lpopbadblpoppopbadbl1ka2badblbadbl1hence result followsc6', 1);('point denition op timal value functionwe s1s2svhs1vhs2vextendsinglevextendsinglevextendsinglevextendsinglevextendsinglesupuaparenleftbiggrs1uhusummationdisplayspss1uvhsparenrightbiggsupuaparenleftbiggrs2uhusummationdisplayspss2uvhsparenrightbiggvextendsinglevextendsinglevextendsinglevextendsinglevextendsinglesupuavextendsinglevextendsinglevextendsinglevextendsinglevextendsinglers1urs2usummationdisplayspss1upss2uvhsvextendsinglevextendsinglevextendsinglevextendsinglevextendsinglelsks2supssvhsvhshencevhs1vhs2ls1ks2', 1);('sinceqhsa rsasummationtextspssavhswe', 1);('optimality conditions policy sswe havemaxuaanbacketletqhsuanbacketihthu anbacketletqhssanbacketihthsanbacketletqhsumaxanbacketihthmaxthis implies hmaxhsanbacketletqhssumaxanbacketihtproofof', 1);('fr ommdp optimality stability', 1);('notethat', 1);('implies mdqh optimality conditions', 1);('followsthatis optimal respect', 1);('continuity', 1);('responsein', 1);('present short proof tha', 1);('continuous nontrivial nite', 1);('sa firstly', 1);('optimal action values', 1);('qsa sasqsa', 1);('maxebracketleftbiggsummationdisplayt0trstatvextendsinglevextendsinglevextendsinglevextendsinglevextendsingles0sa0ast1pstatat1st1t0bracketrightbiggfor anyss', 1);('optimal action map', 1);('s2aasas', 1);('aaqsaqsa allaa2afor', 1);('map br bestresponse operator', 1);('ifsupp brsasssswe', 1);('optimal policies population distribution hencea', 1);('br bris', 1);('unique generalsince', 1);('nonzero action probabilities arbitrar', 1);('brthat assigns probabilities optimal actions iscontinuous', 1);('sbadblbadbl1', 1);('equivalent norm', 1);('fo r', 1);('general subclass operators', 1);('sand denition', 1);('optimal', 1);('stable policy map', 1);('ssss1ssis', 1);('optimal action', 1);('subsets actions', 1);('a1a sa', 1);('forma1as intersectiondisplay1isparenleftbigasiparenrightbig1aisthat nonempty', 1);('equivalently a1as', 1);('a1asis', 1);('asingle policy parenleftbiga1asparenrightbiga1aswhile', 1);('technical condition', 1);('literature clarify exa mplesexample', 1);('pure', 1);('fix', 1);('detsso thatdetaks braceleftbigg1ifa1a2ak1asakas0otherwisedetassigns probability 1to rst optimal action', 1);('thethe action probabilities', 1);('whic h actions optimal27example', 1);('uniform', 1);('map optimal actions', 1);('deneunif sso', 1);('thatunifas 1asifaas', 1);('equal probability optimal actions', 1);('state sunifis', 1);('boltzman', 1);('policy ofguo', 1);('limit', 1);('br takehstrongly', 1);('concave considerthe regularization function hfor', 1);('consider', 1);('h arg maxvh', 1);('whilefor', 1);('c takethe limit 0and show thath0 lim0h arg maxebracketleftbiggsummationdisplayt0thstvextendsinglevextendsinglevextendsinglevextendsinglevextendsingles0atstst1pstatt0bracketrightbiggwhich implies h0is', 1);('theunique optimal policy maximizes regularizer term n', 1);('submdpwhere state sthe', 1);('available actions h0depends onlyon optimal action', 1);('asfinally', 1);('denition place', 1);('present main sult subsectionlemma', 1);('continuous oas', 1);('oasmap', 1);('brfor alls', 1);('constant ie some0 0sproofsis', 1);('subs ets actionsa1a2an', 1);('subsets form', 1);('a1a2an', 1);('form partition domain', 1);('hence oas', 1);('mageim discrete', 1);('singl etonthe lemma shows blanket continuity assumptions f', 1);('responsemight betoostronginmfg reducingthemfg problemtothelea rningofaconstant policythe', 1);('bes response singleagent', 1);('problem lead operators continuo', 1);('approximate discontinuousbest response case h0c8', 1);('regularization biasin', 1);('section dene', 1);('equilibriu quantify relationshipbetween', 1);('equilibriu mlemma', 1);('unregularized value mfgne', 1);('sasv ebracketleftbiggsummationdisplayt0trstatvextendsinglevextendsinglevextendsinglevextendsinglevextendsingles0atstst1pstatbracketrightbigg28a', 1);('holdstability summationdisplaysasaspssaoptimality', 1);('maxvif optimality condition', 1);('vmaxv', 1);('mfgnein', 1);('unique e', 1);('regularizedmfgne pair', 1);('quantify bias astraightforward manner', 1);('regularization bias', 1);('leth a0hmaxfor', 1);('hmax0and letbe', 1);('hmax1mfgneproofthestability conditionsforregularizedandunregularize dmfgne areidentical', 1);('forthe', 1);('optimality condition note pair', 1);('svvhhmax1', 1);('smaxvmaxvhhmax1for', 1);('entropy regularizer hentu summationtextaualogua 0the bias', 1);('byloga1d sample', 1);('based learningd1 conditions', 1);('persistence excitationwe', 1);('choices h persistence excitation', 1);('conditions hassume', 1);('concave modulus', 1);('aumaxa', 1);('uuadua furtherassume', 1);('qmax4then', 1);('exists pinf0such qq0qqmax', 1);('thatmdqas pinffor allssaaproofulhis', 1);('ulha', 1);('assumeulhaneationslashdenoteqqq', 1);('functions fulhrandg 01rasfu anbacketletuqsanbacketihthu12badblusbadbl22 gt futumaxu29here utumaxuulhfor allt01 convexity', 1);('ulhandthefact', 1);('umaxulhwe show fu', 1);('ulhby', 1);('g0 direction ofincrease', 1);('meanvalue theorem exists', 1);('g gg0 sucient show thatfor', 1);('computing', 1);('fuumaxuumaxuqsumaxuhuumaxuumaxu1uumaxusumaxunote magnitudes rst', 1);('terms boun', 1);('hu umaxuumaxu', 1);('qmax4for', 1);('shows g g0', 1);('ucan maximizer f', 1);('thisimplies', 1);('mdqas0 sasasince domain', 1);('compact mdis', 1);('compact sasa exists pinf0 suchthat mdqaspinffor allqq andsasathe lemma', 1);('dierentiability hat boundary instance fortheentropyregularizer hentu summationtextaualoguatheassumptionoflemma21is', 1);('lim 0infuuhuumaxu', 1);('d2 generalized monotone variational inequalities biasin', 1);('section generalize results', 1);('bias estimates operator', 1);('nonmarkovian sequences', 1);('reader work th e', 1);('full presentation theirideas', 1);('proof incorporati ng biastheorem', 1);('letfx rdbe', 1);('letttzbe', 1);('random process space', 1);('fpwheretrnwithprobability', 1);('ft1tandfxrd letbadblbadblbe', 1);('norm onxwithdualbadblbadbl andvbe', 1);('vxy12badblxybadbl2', 1);('holda1 exists', 1);('unique point xxsuch thatanbacketletfxxxanbacketiht0xxa2fis', 1);('0anbacketletfxxxanbacketiht2vxxxx 4a3fis', 1);('constant l thatbadblfx1fx2badbllbadblx1x2badblx1x2x30a4 exists 0such tzebracketleftbiggvextenddoublevextenddoublevextenddoublefxttebracketleftbigfxttft1bracketrightbigvextenddoublevextenddoublevextenddouble2ft1bracketrightbigg2222badblxtxbadbl25a5 exists 0c 001such thattzalmost surelyvextenddoublevextenddoublevextenddoublefxebracketleftbigfxtft1bracketrightbigvextenddoublevextenddoublevextenddoublecbadblxxbadblxx 6then log1log20clog1t0 maxbraceleftbig8l221622bracerightbigk2t0k1 iterations', 1);('byxk1 arg minxxkangbracketleftbigfxkkxangbracketrightbigvxkx', 1);('note triangle inequality appl ication', 1);('youngsinequalitybadblfxtfxttbadbl22badblfxtefxttft1badbl22badblefxttft1fxttbadbl2by', 1);('equations', 1);('young inequality concludesimilar', 1);('bias term thatebadblfxtfxttbadbl22424c222ebadblxtxbadbl2 8as', 1);('op timality conditions', 1);('equation', 1);('monotonicity condition', 1);('pr operty divergence', 1);('v12k22kl2evxk1xevxkx18c222k222k2ck2k242kebadblxkxbadbl31applying youngs', 1);('inequality term kebadblxkxbadbl obtain12k22kl2evxk1xevxkx18c222k222k2ck2k242k102k10ebadblxkxbadbl2hence conclude12k22kl2evxk1xevxkx18c222k222k2ckk102k242k102', 1);('setting k kt0kt01 note assumptions', 1);('k 1k obtaink12k22kl2evxk1x118c222122212c1vx1xksummationdisplayk1k2k242ksummationdisplayk1kk102the result', 1);('sums andsummationtextkk1kk10kt0kt01d3', 1);('stochastic population update boundsproofof lemma', 1);('bias term', 1);('ebracketleftbigg1nsummationdisplayssnsummationdisplayi1bdsit1sesvextendsinglevextendsinglevextendsinglevextendsinglevextendsingleftbracketrightbiggsummationdisplayssesnsummationdisplayi11npssitisittwhere', 1);('ftmeasurability', 1);('tandsit obtainbadblpoptet1ftbadbl1vextenddoublevextenddoublevextenddoublevextenddoublevextenddoublesummationdisplayssesnsummationdisplayi11npssitisittpssitsittvextenddoublevextenddoublevextenddoublevextenddoublevextenddouble11nnsummationdisplayi1vextenddoublevextenddoublepsitisittpsitsittvextenddoublevextenddouble1ka2we', 1);('compute variance timestep', 1);('independence candecompose 2varianceebadblt1et1ftbadbl22ft 1n2nsummationdisplayi1ebadblesit1eesit1ftbadbl22ft2n32where use fact badblesit1eesit1ftbadbl222', 1);('particular obtainebadblt1et1ftbadbl1ft', 1);('inequality fact badblvbadbl1dbadblvbadbl2for anyvrd', 1);('generalize result law', 1);('e xpectations andthe fact thatftft deriveebracketleftbigvextenddoublevextenddoublet11poptvextenddoublevextenddouble1vextendsinglevextendsinglevextendsingleftbracketrightbigebracketleftbigbadblt1poptbadbl1vextendsinglevextendsingleftbracketrightbigebracketleftbigvextenddoublevextenddoublepopt1poptvextenddoublevextenddouble1vextendsinglevextendsinglevextendsingleftbracketrightbigebracketleftbigebracketleftbigbadblt1poptbadbl1vextendsinglevextendsingleftbracketrightbigvextendsinglevextendsingleftbracketrightbigebracketleftbiglpopvextenddoublevextenddoubletpoptvextenddoublevextenddouble1vextendsinglevextendsinglevextendsingleftbracketrightbigradicalbig2snka2lpopebracketleftbigvextenddoublevextenddoubletpoptvextenddoublevextenddouble1vextendsinglevextendsinglevextendsingleftbracketrightbigwhere', 1);('thelipschitz continuity operator pophence', 1);('useful corollary', 1);('empirical popul ation', 1);('onthe state single agentcorollary', 1);('haveebracketleftbigvextenddoublevextenddoubletpoptvextenddoublevextenddouble1vextendsinglevextendsinglevextendsinglesjt sftbracketrightbig1lpop1lpopmixparenleftbiggradicalbig2snka2parenrightbiggproofthe corollary', 1);('law total expectation fact mixingconditions', 1);('psjt', 1);('sft mix33next', 1);('full proof', 1);('proof loo', 1);('levin peres', 1);('complication thetransitions homogeneous', 1);('due population eectsproofof', 1);('transition matrix inducedby', 1);('psspsss', 1);('note denitionis', 1);('ransition probabilitiespss irreducability aperiodicity', 1);('assumpt', 1);('fact theunique stationary distribution stochastic matrix', 1);('mdenote', 1);('thestochastic matrix rows', 1);('xmmforanystochastic', 1);('andmpm sequence matrices', 1);('aii', 1);('proofwe denoteproducttextii1aia1a2aiwe', 1);('result rst agent', 1);('ptdenotethestochastic', 1);('transitionmatrix attime tgiven', 1);('ptssps1tss1t1s assumption', 1);('tmix0', 1);('mix0 thematrix', 1);('pjproducttextjtmixtj1tmix1ptsatises', 1);('pjss', 1);('j dene stochastic matrices', 1);('qjimplicitly', 1);('equationpj 1mqj 1mix proof', 1);('induction', 1);('jby induction', 1);('followingholdsjproductdisplayj1pj 1jmjjproductdisplayj1qjjsummationdisplayl21l1jlmparenleftbigpjptmixparenrightbigjproductdisplayll1ql10the identity', 1);('straightforward manner', 1);('j1 denoting', 1);('obtain1jmpj1jjproductdisplayj1qjparenleftbig1mqj1parenrightbigparenleftbiggjsummationdisplayl21l1jjmparenleftbigpjpparenrightbigjproductdisplayll1qlparenrightbiggparenleftbig1mqj1parenrightbig34we observe thatproducttextjj1qjis stochastic matrix', 1);('mpmto', 1);('obtain1jmparenleftbigpj1ptmixparenrightbig1jmptmixj1mj1jproductdisplayj1qjparenleftbiggjsummationdisplayl21l1jjmparenleftbigpjptmixparenrightbigjproductdisplayll1qlparenrightbiggparenleftbig1mqj1parenrightbig1jmparenleftbigpj1ptmixparenrightbig1j1mj1jproductdisplayj1qjjsummationdisplayj21l1jjmparenleftbigpjptmixparenrightbigjproductdisplayll1qlparenleftbig1mqj1parenrightbigthe result', 1);('identityjsummationdisplayj21l1jjmparenleftbigpjptmixparenrightbigjproductdisplayll1ql1m 0since', 1);('pjproducttextjlj1qlandpproducttextjlj1qlare', 1);('stochastic matricesstep', 1);('pjproducttextjtmixtj1tmix1ptwecan', 1);('erproducttextrt1pjtmixtfrom', 1);('right r', 1);('obtainjtmixrproductdisplayt1ptmjjproductdisplayj1qjermermparenleftbigerprparenrightbigjsummationdisplayj21l1jjmparenleftbigpjpparenrightbigjproductdisplayll1qlerletusbe arbitrary column vector', 1);('multiplying', 1);('side uon', 1);('1norm squarevextenddoublevextenddoublevextenddoubleuproducttextjtmixrt1ptvextenddoublevextenddoublevextenddouble1square2jjsummationdisplayj2jjvextenddoublevextenddoublevextenddoubleuparenleftbigpjpparenrightbigqjvextenddoublevextenddoublevextenddouble1vextenddoublevextenddoublevextenddoubleuparenleftbigerprparenrightbigvextenddoublevextenddoublevextenddouble12jjsummationdisplayj2jjjtmixsummationdisplaytj1tmix1supjbadblptjpjbadbl1jtmixrsummationdisplaytjtmix1supjbadblptjpjbadbl1for usand stochastic matrices', 1);('qj', 1);('population bias', 1);('quantify error', 1);('observe', 1);('law total probability', 1);('denitionassumettmix concludebadblptspsbadbl1summationdisplaynsvextenddoublevextenddoublep1sspssvextenddoublevextenddouble1pts1tssummationdisplaynsparenleftbiggkbadblbadbl1ka2badbl1badbl1parenrightbiggpts1tska2badbl1badbl1kmixebadbltbadbl1where', 1);('line consequence', 1);('lem', 1);('obt resultd4', 1);('ctd populationproofof theorem', 1);('th e divergencevqq 12badblqqbadbl22', 1);('assumptions a1a3have', 1);('strong monotonicity modu lusf 1mixpinfandlipschitz', 1);('lf', 1);('a4', 1);('fby', 1);('qmstay', 1);('qmaxat', 1);('popfor anyqq haveef1qtft', 1);('eqsarsahsqsaesaftersarsa1esaft36by lemma', 1);('bybadblersarsa1esaftbadbl2l1lpopparenleftbiggradicalbig2snka2parenrightbigg2llpopfor rst term', 1);('vsasa1as1asqsarsahsqsaesaandvsasa asasqsarsahsqsaesaeqsarsahsqsaesaftsummationdisplayssaanspstsftptstsftpssavsasasummationdisplayssaanspstsftparenleftbigptstsftpssapssaparenrightbigvsasabracehtipupleft bracehtipdownrightbracehtipdownleft bracehtipuprightsummationdisplayssaapstsftpssavsasavsasabracehtipupleft bracehtipdownrightbracehtipdownleft bracehtipuprightsquaresummationdisplayssaapstsftpssavsasabracehtipupleft bracehtipdownrightbracehtipdownleft bracehtipuprightwe analyze', 1);('observe inequality', 1);('term square', 1);('analyze term', 1);('note thatfq', 1);('mmipqqwherem', 1);('qqas', 1);('vectors inrsa', 1);('utilizing proposition', 1);('maxmm concludebadblfqbadbl2parenleftbigg2cmixtmix4ktmix2mixradicalbig2sn2tmixkak2mix1lpop2tmixkamixbadblibadbl1parenrightbiggbadblqqbadbl237since havebadblqqbadbl221lh1andbadblbadbl2badblbadbl', 1);('theorem followsd5', 1);('resultsproofof theorem', 1);('firstly theorem', 1);('combinations states siisnthat probability 1ebadblqkqkbadblsikmtdmpg sikctd1radicalbigmpgt0mpgt01ctd2radicalbigmpgradicalbigmpgt0mpgt01ctdpop1lmtdpopctdpop2nsince policies', 1);('mpgmtd', 1);('probability 1ebadblqkqkbadblk31l4lmdqctdpop2nmoreover probability 1ebadblk1badbl1k', 1);('ebadblmdqkkbadbl1kebadblmdqkkbadbl1kebadblmdqkkmdqkkbadbl1kebadblkbadbl1kebadblmdqkkmdqkkbadbl1klbadblkbadbl1lmdqebadblqkqkbadblkwhich', 1);('inequality k', 1);('implies statement theorem notingbadbl0badbl12proofof', 1);('theorem rst agent 1we use reference policy k1kat iteration k k 01kdenethe random', 1);('variable ksummationtextni1badblik1kbadbl1', 1);('clearly', 1);('forkthroughout training', 1);('iterations f agenti obtainebadblqikq1kbadblikictd1radicalbigmpgt0mpgt01ctd2radicalbigmpgradicalbigmpgt0mpgt01ctdpop1lmtdpopctdpop2nctdpol1kctdpol2badblik1kbadbl1for simplicity', 1);('summands independen ofkastd', 1);('iterative expectations', 1);('withprobability 1ebadblik11k1badbl1iki', 1);('ebadblmdqikikmdq1k1kbadbl1ikilmdqebadblqikq1kbadblikilmdbadblik1kbadbl1lmdqtdlmdqctdpol1klmdqctdpol2lmdbadblik1kbadbl1in', 1);('expectation sides obtainek1lmdqctdpol1ctdpol2eklmdeklmdqtd 11hence', 1);('denition cand', 1);('ekklmdqtd applying theorem', 1);('agent 1ebadbl1k1badbl1lebadblkbadbl1lmdqtdlmdqctdpol1ekand', 1);('clmdqctdpol1ctdpol2lmdebadbl1kbadbl12lklmdqtd1llmdqctdpol1k1summationdisplayk1lkk1ekthe result', 1);('constants theorem39referencesalekh', 1);('agarwal sham kakade jason lee gaurav mahaj', 1);('theory ofpolicy gradient methods', 1);('optimality', 1);('approximation istribution', 1);('journal ofmachine', 1);('anahtarci deha kariksiz naci saldi qlear', 1);('applications', 1);('2022ren e', 1);('carmona mathieu lauri', 1);('zongjun tan modelf', 1);('ree meaneld', 1);('meaneld mdp meaneld', 1);('arxiv preprint arxiv191012802 2019shicongcenchenchengyuxinchenyutingwei andyuejiec hi', 1);('fastglobalconvergenceof', 1);('natural policy gradient methods entropy regulariza tionoperations', 1);('resesarch', 1);('cuiand heinz koeppl approximately', 1);('eld ga mes', 1);('entropyregularizeddeep reinforcement learning', 1);('articial intelligence', 1);('andstatistics pages', 1);('daskalakis dylan j foster noah golowich', 1);('independent policy gradient methods', 1);('competitive reinforcement learning', 1);('ding chenyu wei kaiqing zhang mihailo jov', 1);('independent policygradient largescale markov potential games', 1);('sharper', 1);('r ates function approximationand gameagnostic convergence', 1);('fu zhuoran yang yongxin chen zhaoran wang acto', 1);('ndsnash equilibria linearquadratic meaneld games', 1);('representations', 1);('geist bruno scherrer olivier pietquin', 1);('markovdecision processes', 1);('pages 21602169pmlr 2019hansotto', 1);('georgii gibbs', 1);('measures phase transitions ngibbs', 1);('measures phasetransitions', 1);('gruyter', 1);('gu xin guo xiaoli wei renyuan xu meaneld', 1);('mu ltiagent', 1);('network approach arxiv preprint arxiv210802731 2021xin', 1);('guo anran hu renyuan xu junzi zhang learning', 1);('advancesin neural information processing systems', 1);('guo anran hu renyuan xu junzi zhang', 1);('general fram ework learningmeaneld games', 1);('research 2022a40xin', 1);('guo renyuan xu thaleia zariphopoulou entropy', 1);('reg ularization', 1);('eldgames learning', 1);('research 2022bminyihuang', 1);('rolandpmalham', 1);('e andpeter', 1);('ecaines largepop', 1);('ulationstochasticdynamicgames closedloop mckeanvlasov systems nash cert ainty equivalence principlecommunications', 1);('information systems', 1);('aryeh kontorovich kavita ramanan concentratio', 1);('n inequalities dependentrandom variables', 1);('themartingale method', 1);('annals probability', 1);('kotsalis guanghui lan', 1);('simplean', 1);('doptimal methodsfor stochastic variational inequalities ii', 1);('markovian', 1);('noise poli cy evaluation', 1);('lan policy', 1);('mirror descent reinforcement lear', 1);('linear', 1);('problem classes', 1);('mathematical', 1);('pages148 2022jeanmichel', 1);('lasry pierrelouis lions mean', 1);('japanese journal mathematics', 1);('sarah perrin sertan girgin paul mulle', 1);('ayush jain th', 1);('cabannes georgios piliouras julienperolat romualdelie olivier pietquin', 1);('scalable deep reinforcement learning algorithms fo r', 1);('levin yuval peres markov', 1);('times volume', 1);('americanmathematical soc', 1);('li tuo zhao guanghui lan homotopic', 1);('policy mirror escent', 1);('implicit regularization', 1);('sample co mplexity arxiv preprintarxiv220109457 2022weichao', 1);('mao haoran qiu chen wang hubertus franke zbigni', 1);('kalbarczyk ravi iyerand tamer basar', 1);('meaneld game approach cloud resource management withfunction approximation', 1);('advances neural information processing systems', 1);('2022la etitia', 1);('matignon guillaume j laurent nadine', 1);('piat hysteretic', 1);('qlearningan algorithm', 1);('reinforcement learning c ooperative multiagent teamsin2007', 1);('ieeersj', 1);('intelligent robots', 1);('ieee', 1);('mei chenjun xiao csaba szepesvari dale schu', 1);('urmans globalconvergence rates softmax policy gradient methods', 1);('international conference onmachine', 1);('nesterov', 1);('lectures', 1);('convex optimization volume', 1);('springer', 1);('ozdaglar muhammed sayin kaiqing zhang indep', 1);('endent learning instochastic games arxiv preprint arxiv211111743 2021julien', 1);('perolat remi munos jeanbaptiste lespiau shayeg omidshaei mark rowlandpedroortega neil burch thomasanthony david balduzzi', 1);('b art', 1);('devylder', 1);('frompoincar', 1);('e recurrence convergence imperfect informat ion games', 1);('finding', 1);('equilibriumvia regularization', 1);('pages 85258535pmlr 2021julien', 1);('sarah perrin romuald elie mathieu lauri', 1);('georgios piliouras matthieugeist karl tuyls olivier pietquin scaling', 1);('eld g ames online mirrordescent', 1);('proceedings', 1);('autonomo', 1);('agents', 1);('perrin julien p', 1);('mathieu lauri', 1);('geist romuald elie olivierpietquin fictitious', 1);('continuoust', 1);('imeanalysis andapplicationsadvances', 1);('perrin mathieu lauri', 1);('julien p', 1);('geist romuald elie olivierpietquin mean', 1);('eld games ock reinforcement learning way', 1);('inijcai', 1);('rashedi mohammad amin tajeddini hamed kebriaei markov', 1);('game approachfor multiagent', 1);('strategies electri city market', 1);('iet generationtransmission distribution', 1);('saldi tamer basar', 1);('raginsky markovnash', 1);('eq uilibriainmeaneld gameswithdiscountedcost', 1);('journal control', 1);('samvelyan tabishrashid christianschroederdew', 1);('gregory farquhar nantasnardelli tim g j rudner chiaman hung philip h torr jakob foerster', 1);('whiteson', 1);('starcraft multiagent challenge', 1);('proc', 1);('internationalconference autonomous agents multiagent systems aam as2019 aamas19page', 1);('richland sc', 1);('foundat', 1);('autonomous agentsand multiagent systemsmuhammed sayin kaiqing zhang david leslie tamer basar', 1);('asuman ozdaglardecentralized', 1);('zerosum markov games', 1);('advances neural informationprocessing systems', 1);('shalevshwartz yoram singer online', 1);('theory', 1);('algorithms applications', 1);('phd', 1);('hebrew', 1);('university 2007alishavandiandmajidkhedmati', 1);('amultiagent', 1);('deepreinfor cementlearningframeworkforalgorithmic', 1);('nancial markets', 1);('expert systems applications', 1);('liorshani yonathanefroniandmohammadghav', 1);('mirrordescentpolicy', 1);('learning representations', 1);('tsitsiklis benjamin van roy analysis', 1);('temporal diference learning function approximation', 1);('wiering multiagent', 1);('reinforcement learning rac', 1);('light control', 1);('machinelearning proceedings seventeenth', 1);('conf', 1);('icml2000', 1);('pages11511158 2000qiaomin', 1);('xie zhuoran yang zhaoran wang andreea minca learning', 1);('inmeaneld games', 1);('machinelearning', 1);('aneeq', 1);('zaman alec koppel sujay bhatt tamer ba', 1);('oraclefreereinforcement', 1);('learning meaneld games', 1);('sa mple path arxiv preprintarxiv220811639', 1);