('ieee', 15);('icassp', 12);('phoneme alignment', 11);('voice conversion', 10);('tts', 10);('asr', 10);('ground truth', 10);('per', 10);('yourtts', 8);('speech', 8);('styletts-vc', 7);('processing', 7);('vc', 6);('baseline models', 6);('text aligner', 6);('text encoder', 6);('international conference', 6);('acoustics', 6);('target speaker', 5);('styletts', 5);('again-vc', 5);('vqmivc', 5);('pitch extractor', 5);('llatent', 5);('pitch curve', 4);('speech decoder', 4);('mos', 4);('acc', 4);('rtf', 4);('hung-yi lee', 4);('one-shot', 4);('nima mesgarani', 3);('source speaker', 3);('arbitrary target speaker', 3);('one-shot voice conversion', 3);('any-to-any voice conversion', 3);('tts-based', 3);('real-time inference', 3);('latent representation', 3);('text representation', 3);('cycle consistency loss function', 3);('mel- spectr', 3);('training', 3);('mi', 3);('adain', 3);('style encoder', 3);('figure', 3);('style code', 3);('f0', 3);('attention alignment', 3);('english', 3);('different models', 3);('evaluation results', 3);('phoneme error rate', 3);('objective evaluations', 3);('eis', 3);('data augmentation', 3);('cong han', 2);('reference speech', 2);('speech content', 2);('speech representation', 2);('novel data augmentation scheme', 2);('input text', 2);('oice conversion', 2);('instance normalization [', 2);('mellotron', 2);('cotatron', 2);('zhang', 2);('encoder training', 2);('target speakers', 2);('adversarial objectives', 2);('decoder', 2);('encoder', 2);('step', 2);('mel-spectrogram encoder', 2);('mutual information', 2);('text', 2);('tacotron', 2);('style', 2);('style code s=s', 2);('starganv2-vc', 2);('residual block', 2);('libritts', 2);('speaker information', 2);('reconstruction loss', 2);('speech ^xtrg', 2);('adversarial training', 2);('full objectives', 2);('full objective functions', 2);('vctk', 2);('unseen speakers', 2);('% condence intervals', 2);('phoneme sequences', 2);('torchaudio', 2);('objective', 2);('method acc', 2);('subjective evaluations', 2);('opinion score', 2);('speaker similarity', 2);('espnet', 2);('different training objectives', 2);('pmlr', 2);('da-yi wu', 2);('vector quantization', 2);('adaptive instance normalization', 2);('huaizhen tang', 2);('xulong zhang', 2);('jianzong wang', 2);('ning cheng', 2);('jing xiao', 2);('voice con- version', 2);('con-', 2);('chung-ming chien', 2);('yinghao aaron li', 2);('computer vision', 2);('ron j weiss', 2);('zhifeng chen', 2);('yu zhang', 2);('interspeech', 2);('text-to- speech', 2);('jaehyeon kim', 2);('jungil kong', 2);('advances', 2);('one-shot voice conversion by knowledge transfer from style-based tts models yinghao aaron li', 1);('electrical engineering', 1);('columbia', 1);('usa abstract one-shot', 1);('speakers identity', 1);('novel approach', 1);('cycle consistent', 1);('ad- versarial training', 1);('high delity', 1);('sim- ilarity', 1);('additional mel-spectrogram encoder', 1);('teacher-student knowledge', 1);('approach results', 1);('subjective evaluation shows', 1);('previous state-of-the-art one-shot voice conversion models', 1);('index terms v', 1);('represen- tations', 1);('introduction v', 1);('speakers voice', 1);('anothers voice', 1);('linguis- tic', 1);('prosodic information', 1);('recent', 1);('deep learning', 1);('particular type', 1);('reference audio', 1);('unseen speakers voice', 1);('speakers voice unseen', 1);('model needs', 1);('potential sources', 1);('target speakers [', 1);('speaker identity', 1);('successful one-shot voice conversion', 1);('vector quantization [', 1);('models [', 1);('source speaker information', 1);('source speech', 1);('source audio', 1);('source- specic information', 1);('essential problems', 1);('major drawback', 1);('addi- tional mel-spectrogram encoder', 1);('auto- matic speech recognition', 1);('generalization problem', 1);('reconstruct speech', 1);('input pitch', 1);('different speaker', 1);('non-parallel one-shot voice conversion framework', 1);('text-to-speech model', 1);('aforementioned generalization problems', 1);('rst training', 1);('mel- spectrogram encoder', 1);('recon- struct', 1);('decoder output', 1);('real speech', 1);('previous method [', 1);('phoneme alignment representations', 1);('subjective human evaluation shows', 1);('model outperforms', 1);('previous state-of-the- art one-shot voice conversion model', 1);('978-1-6654-7189-3/22/ $', 1);('ieeearxiv:2212.14227v1', 1);('[ eess.as ]', 1);('dec', 1);('encoderphonemesinput mel- spectr', 1);('text aligner pitch extractor re-synthesized mel-spectr', 1);('encoder pitch extractorrefer', 1);('style encoder adainrefer', 1);('style encoder adain resblock decoderadain resblock decoder converted mel-spectr', 1);('inference schemes', 1);('synthesize target speech', 1);('reference mel-spectrogram', 1);('input mel-spectrogram', 1);('inference procedures', 1);('unseen source', 1);('convolutional layers', 1);('non- causal', 1);('rnn', 1);('faster-than-real-time vocoder', 1);('multiple contributions', 1);('cycle consistency', 1);('adversarial objective', 1);('introduce novel data augmentation', 1);('voice conversion results', 1);('loss function', 1);('voice conversion applications', 1);('pro- pose', 1);('alternative solution', 1);('maximization objective', 1);('audio samples', 1);('methods', 1);('styletts styletts', 1);('non-autoregressive text-to- speech model', 1);('integrates style information', 1);('adap- tive instance normalization', 1);('duration predictor', 1);('prosody predictor', 1);('de- scribe', 1);('prosody predic-tors', 1);('decoder training', 1);('input phonemes t', 1);('encodes tinto latent representation htext=t', 1);('input mel-spectrogram x', 1);('en- coder extracts', 1);('voice conver- sion application', 1);('speaker em-', 1);('domain-specic linear projection layers', 1);('gsynthesizes', 1);('mel-spectrogram ^x=g', 1);('input audio x', 1);('pxis pitch contour', 1);('log norm', 1);('xper frame', 1);('residual blocks', 1);('style code sis', 1);('g. thepx', 1);('feature maps', 1);('style vector', 1);('standard deviation', 1);('l\x1bandl\x16are', 1);('linear projections', 1);('adaptive gain', 1);('style vector s.', 1);('discriminator', 1);('das', 1);('instarganv2-vc [', 1);('domain-specic linear projection layer', 1);('domain-specic layer', 1);('ais', 1);('automatic speech recognition', 1);('corpus [', 1);('fis', 1);('jdc', 1);('network [', 1);('harvest [', 1);('text input', 1);('additional encoderethat encodes', 1);('mel-spectrogram xintohensuch thatg', 1);('encoder learns', 1);('1-d residual blocks', 1);('unnatural speech', 1);('hen= htext\x01dalignis', 1);('input xin', 1);('energy nint= logs', 1);('np', 1);('nthe', 1);('num- ber', 1);('speech content hen=e', 1);('com- pute', 1);('synthesize ^xtrgfrom', 1);('pinandninare 1-dimensional nor-', 1);('possible speech', 1);('training objectives', 1);('rst train', 1);('aforementioned objective', 1);('mel-spectrogram x2xysrc', 1);('refer- encexref2xytrg', 1);('source speaker ysrc2y', 1);('target speakerytrg2y', 1);('loss functions', 1);('mel', 1);('mel-spectrogram x2xand', 1);('corresponding text t2t', 1);('lrec=ex', 1);('t\x02 kx\x00g', 1);('phoneme representation', 1);('monotonic version', 1);('dynamic pro-', 1);('algorithm [', 1);('at- tention alignments', 1);('meaningful style code', 1);('speaker embeddings', 1);('style reconstruction', 1);('lsty=ex', 1);('xref\x02 ks', 1);('energy information', 1);('en- coder loss', 1);('len=ex', 1);('xref [ k^x\x00g', 1);('k1 ]', 1);('x=g\x10 ^htext\x01^dalign', 1);('^htextand ^dalignare text', 1);('speech sample ^xand^xrefis', 1);('ref- erence audio', 1);('xis synthe-', 1);('speech sample', 1);('novel data augmentation', 1);('target space', 1);('ro- bust models', 1);('phoneme', 1);('orig- inal phoneme content', 1);('phoneme loss function', 1);('phonetic content', 1);('linear projection', 1);('pfor', 1);('lmi=ex', 1);('ttx', 1);('p\x01e', 1);('i\x01 #', 1);('ce', 1);('cross- entropy loss function.cycle consistency loss', 1);('decoder gener- alizes', 1);('different input style codes', 1);('lcycle', 1);('t\x02 x\x00g\x00 h', 1);('n^xtrg\x01 1\x03', 1);('h=htext\x01 ^dalignandxis', 1);('andx=^xin equation', 1);('adversarial', 1);('original cross-entropy loss function', 1);('loss fol-', 1);('ladv=ex', 1);('ysrc [ logd', 1);('] +', 1);('ex', 1);('ytrg [ log', 1);('lfm=ex', 1);('lx', 1);('nl dl', 1);('lis', 1);('total number', 1);('danddldenotes', 1);('lthlayer withnlfeatures', 1);('xand hare', 1);('dis', 1);('smax dlrec+\x15stylsty+\x15cyclelcycle', 1);('pmax dlen+\x15cyclelcycle', 1);('+\x15milmi +\x15advladv+\x15fmlfm', 1);('experiments', 1);('datasets', 1);('] corpus', 1);('various accents', 1);('sen- tences', 1);('speak- ers', 1);('% /10 %', 1);('comparison', 1);('method mos-n mos-p ground truth', 1);('text sequences', 1);('open-source tool1', 1);('fft', 1);('hop size', 1);('window length', 1);('mel bins', 1);('hi-gan', 1);('training details', 1);('\x15cycle =', 1);('\x15adv= 1and\x15fm=', 1);('mod- els', 1);('adamw', 1);('optimizer [', 1);('weight decay \x15=', 1);('learning rate = 10\x004and batch size', 1);('shortest length', 1);('test accuracy', 1);('real time factor', 1);('nvidia geforce rtx', 1);('ti gpu', 1);('evaluations', 1);('mos-n', 1);('mos-p', 1);('u.s.', 1);('eval- uations', 1);('online survey', 1);('amazon mechanical', 1);('example', 1);('attention alignment dalignand', 1);('\x01h\x001 textwhere h\x001 textis', 1);('pseudoinverse ofhtext', 1);('monotonic alignment', 1);('eacts', 1);('turk', 1);('recent baseline mod- els', 1);('state-of-the- art model', 1);('ofcial implementation234', 1);('test speaker', 1);('fair compar- ison', 1);('higan', 1);('sam- ples', 1);('model labels', 1);('different speakers reading', 1);('source input', 1);('different samples', 1);('different lengths', 1);('multiple stim- uli', 1);('mushra', 1);('subtle difference', 1);('atten- tion check', 1);('speaker classication', 1);('speech intelligibility [', 1);('speaker classi- cation model', 1);('resnet-18', 1);('speaker label', 1);('test speakers', 1);('classication accuracy', 1);('speech waveforms', 1);('//github.com/kimythanly/again-vc 3https', 1);('//github.com/wendison/vqmivc 4https', 1);('subjective', 1);('opinion scores', 1);('ci', 1);('method mos-n mos-p proposed', 1);('w/o augmentation', 1);('ablation', 1);('abla- tion study', 1);('lmiandlcycle', 1);('addi- tion', 1);('latent loss', 1);('] hurts', 1);('t\x02 k', 1);('encoder loss', 1);('xin equation', 1);('results', 1);('dif- ferent models', 1);('subjective evaluation experiment', 1);('ad- dition', 1);('flow-based', 1);('speaker-classication test accuracy', 1);('baseline', 1);('% w/o augmentation', 1);('% -lmi', 1);('% -lcycle', 1);('% +llatent', 1);('% compute', 1);('jacobian', 1);('matrix inversion', 1);('flow-', 1);('ablation study results', 1);('lmior lcycle', 1);('baseline model', 1);('outperforms models', 1);('lmiorlcycle', 1);('classi- cation accuracy', 1);('objective metrics', 1);('similar- ity drop', 1);('htextwhich con- sists', 1);('token embeddings', 1);('monotonic alignment dalignwith', 1);('shows thate', 1);('phoneme rep- resentations', 1);('pseudoin- verse', 1);('with- outltextfails', 1);('dalignwithh\x001 text', 1);('different representation', 1);('natural speech', 1);('monotonic align- ment', 1);('speech unclear', 1);('pro- duce incorrect phonetic content', 1);('conclusions', 1);('novel cycle consistency', 1);('maximization objectives', 1);('latent re- construction objective', 1);('state-of-the-art performance', 1);('natu- ralness', 1);('various metrics', 1);('previous models', 1);('latent reconstruction loss', 1);('speech similarity', 1);('potential explanation', 1);('more-', 1);('one-shot voice conversion systems', 1);('faster- than-real-time vocoder', 1);('text labels', 1);('large-scale speech cor- pora', 1);('text la- bels', 1);('speaker representation', 1);('acknowledgments', 1);('national institute', 1);('nih- nidcd', 1);('marie-josee', 1);('henry r. kravis', 1);('references', 1);('kaizhi qian', 1);('yang zhang', 1);('shiyu chang', 1);('xuesong yang', 1);('mark hasegawa-johnson', 1);('autovc', 1);('zero-shot', 1);('voice style', 1);('autoencoder loss', 1);('interna-', 1);('tional conference', 1);('machine learning', 1);('ju-chieh chou', 1);('cheng-chieh yeh', 1);('content representations', 1);('instance normalization', 1);('arxiv preprint arxiv:1904.05742', 1);('voice conver- sion', 1);('yen-hao chen', 1);('tsung-han wu', 1);('ac- tivation guidance', 1);('benjamin', 1);('niekerk', 1);('leanne nortje', 1);('herman kamper', 1);('vector-quantized', 1);('neural networks', 1);('acoustic unit discovery', 1);('arxiv preprint arxiv:2005.09409', 1);('disong wang', 1);('liqun deng', 1);('yu ting yeung', 1);('xiao chen', 1);('xunying liu', 1);('helen meng', 1);('vector', 1);('quan- tization', 1);('speech representation disentanglement', 1);('one-shotvoice conversion', 1);('arxiv preprint arxiv:2106.10132', 1);('avqvc', 1);('contrastive learning', 1);('arxiv preprint arxiv:2202.10020', 1);('zhonghao li', 1);('benlai tang', 1);('xiang yin', 1);('yuan wan', 1);('ling xu', 1);('chen shen', 1);('zejun ma', 1);('ppg-based', 1);('adversarial representation learn-', 1);('jheng-hao lin', 1);('yist y lin', 1);('s2vc', 1);('representa- tions', 1);('arxiv preprint arxiv:2104.02901', 1);('mingyang zhang', 1);('yi zhou', 1);('li zhao', 1);('haizhou li', 1);('transfer', 1);('speech synthesis', 1);('non-parallel training data', 1);('ieee/acm transactions', 1);('audio', 1);('language process-', 1);('edresson casanova', 1);('julian weber', 1);('christopher d shulby', 1);('arnaldo candido', 1);('eren g', 1);('moacir', 1);('ponti', 1);('towards', 1);('zero-shot multi- speaker tts', 1);('zero-shot voice conversion', 1);('machine learn-', 1);('adam gabry', 1);('goeric huybrechts', 1);('manuel sam ribeiro', 1);('julian roth', 1);('giulia co-', 1);('roberto barra-chicote', 1);('bartek perz', 1);('jaime lorenzo-trueba', 1);('oice lter', 1);('few-shot', 1);('text-to-speech speaker adaptation', 1);('post- processing module', 1);('ieee in-', 1);('ternational conference', 1);('ruobai wang', 1);('yu ding', 1);('lincheng li', 1);('changjie fan', 1);('acous-', 1);('zhen zeng', 1);('edward xiao', 1);('tgavc', 1);('improving', 1);('autoencoder voice conversion', 1);('ieee au-', 1);('speech recognition', 1);('understanding work-', 1);('asru', 1);('rafael valle', 1);('jason li', 1);('ryan prenger', 1);('bryan catan-', 1);('multispeaker', 1);('expressive voice syn- thesis', 1);('global styletokens', 1);('seung-won', 1);('doo-young kim', 1);('myun-chul joe', 1);('transcription-guided', 1);('speech encoder', 1);('any-to-many voice conversion', 1);('parallel data', 1);('arxiv preprint arxiv:2005.03295', 1);('generative model', 1);('diverse text-to-speech synthesis', 1);('arxiv preprint arxiv:2205.15439', 1);('xun huang', 1);('serge belongie', 1);('arbitrary', 1);('style trans- fer', 1);('jonathan shen', 1);('ruoming pang', 1);('mike schuster', 1);('navdeep jaitly', 1);('zongheng yang', 1);('yuxuan wang', 1);('rj skerrv-ryan', 1);('natural tts synthesis', 1);('mel spectrogram predictions', 1);('ali zare', 1);('non-parallel framework', 1);('heiga zen', 1);('viet dang', 1);('rob clark', 1);('ye jia', 1);('yonghui wu', 1);('lib-', 1);('arxiv preprint arxiv:1904.02882', 1);('sangeun kum', 1);('juhan nam', 1);('joint', 1);('voice melody', 1);('convolu- tional recurrent neural networks', 1);('applied', 1);('masanori morise', 1);('fundamental frequency estimator', 1);('speech signals.', 1);('dmitry ulyanov', 1);('andrea vedaldi', 1);('victor lempitsky', 1);('instance', 1);('fast stylization', 1);('arxiv preprint arxiv:1607.08022', 1);('sungwon kim', 1);('sun-', 1);('yoon', 1);('glow-tts', 1);('generative ow', 1);('monotonic alignment search', 1);('neural information processing systems', 1);('malik boudiaf', 1);('rony', 1);('imtiaz masud ziko', 1);('eric granger', 1);('marco pedersoli', 1);('pablo piantanida', 1);('is-', 1);('ben ayed', 1);('mutual information viewof metric learning', 1);('pairwise losses', 1);('ineuropean conference', 1);('springer', 1);('jaekyoung bae', 1);('hi-', 1);('generative', 1);('adversarial networks', 1);('high delity speech synthesis', 1);('neural in-', 1);('processing systems', 1);('junichi yamagishi', 1);('christophe veaux', 1);('kirsten macdon-', 1);('cstr', 1);('vctk corpus', 1);('multi-speaker cor- pus', 1);('cstr voice', 1);('yao-yuan yang', 1);('moto hira', 1);('zhaoheng ni', 1);('anjali chourdia', 1);('artyom astafurov', 1);('caroline chen', 1);('ching- feng yeh', 1);('puhrsch', 1);('david pollack', 1);('dmitriy genzel', 1);('donny greenberg', 1);('edward z. yang', 1);('jason lian', 1);('jay mahadeokar', 1);('jeff hwang', 1);('ji chen', 1);('peter goldsbor-', 1);('prabhat roy', 1);('sean narenthiran', 1);('shinji watan-', 1);('soumith chintala', 1);('vincent quenneville-b', 1);('yangyang shi', 1);('building blocks', 1);('speech processing', 1);('arxiv preprint arxiv:2110.15018', 1);('ilya loshchilov', 1);('frank hutter', 1);('fixing', 1);('weight decay regularization', 1);('shinji watanabe', 1);('takaaki hori', 1);('shigeki karita', 1);('tomoki hayashi', 1);('jiro nishitoba', 1);('yuya unno', 1);('nelson enrique yalta soplin', 1);('jahn heymann', 1);('matthew wiesner', 1);('nanxin chen', 1);('adithya renduchintala', 1);('tsubasa ochiai', 1);('end-to-end', 1);('speech processing toolkit', 1);('pro-', 1);