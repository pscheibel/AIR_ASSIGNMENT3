('resgrad', 59);('tts', 43);('fastspeech', 25);('ddpms', 17);('sample quality', 17);('libritts', 14);('inference speed', 13);('vctk', 12);('figure', 12);('resgrad-4', 12);('iclr', 11);('inference steps', 10);('diffusion models', 10);('diffgan-tts', 10);('prodiff', 10);('mos', 9);('ground-truth mel-spectrogram', 8);('liu', 8);('ren', 8);('rtf', 8);('diffspeech', 8);('ljspeech', 7);('popov', 7);('gradtts-50', 7);('resunet', 7);('neurips', 7);('ddpm', 6);('kong', 6);('huang', 6);('gaussian', 6);('batch size', 6);('resgrad-50', 6);('oord', 5);('chen', 5);('model size', 5);('compared', 5);('grad-tts', 5);('cmos', 5);('heiga zen', 5);('icml', 5);('diffusion probabilistic models', 4);('lee', 4);('lam', 4);('small real-time factor', 4);('different methods', 4);('residual', 4);('ground-truth pitch', 4);('english', 4);('gt', 4);('model cmos', 4);('step', 4);('sander dieleman', 4);('karen simonyan', 4);('xu tan', 4);('yi ren', 4);('zhou zhao', 4);('slow inference speed', 3);('lightweight model', 3);('real-time factor', 3);('inference process', 3);('van den', 3);('kingma', 3);('dhariwal', 3);('ho', 3);('diffusion model', 3);('residual space', 3);('lj-speech', 3);('inference stage', 3);('speech quality', 3);('mel-spectrogram f', 3);('gradtts', 3);('multi-speaker dataset', 3);('fft', 3);('hz', 3);('frequency cutoffs', 3);('hop length', 3);('hifi-gan', 3);('adam', 3);('160k steps', 3);('nvidia v100 gpu', 3);('teacher model', 3);('quality speech', 3);('voice quality', 3);('ground truth', 3);('resgrad-l', 3);('erich elsen', 3);('norman casagrande', 3);('adversarial networks', 3);('prafulla dhariwal', 3);('jonathan ho', 3);('dan su', 3);('dong yu', 3);('neural vocoder', 3);('interspeech', 3);('koray kavukcuoglu', 3);('sungroh yoon', 3);('diederik p. kingma', 3);('generative', 3);('qiuqiang kong', 3);('haohe liu', 3);('wei ping', 3);('tao qin', 3);('tie-yan liu', 3);('deliang wang', 3);('ddpm-based tts', 2);('high sample quality', 2);('plug-and-play way', 2);('single-speaker dataset', 2);('multiple speakers', 2);('similar speech quality', 2);('large number', 2);('high-dimensional data space', 2);('lei', 2);('previous methods', 2);('data space', 2);('specically', 2);('whole speech', 2);('nichol', 2);('speech synthesis', 2);('previous works', 2);('otherwise', 2);('mel- spectrogram', 2);('diffusion probabilistic model', 2);('ground-truth mel-spectrograms', 2);('clean residual data', 2);('lj-', 2);('test dataset', 2);('yamagishi', 2);('kim', 2);('extract 80-band mel-spectrogram', 2);('high-frequency details', 2);('resid- ual', 2);('transformer', 2);('1d convolution', 2);('kernel size', 2);('input/output size', 2);('duration predictor', 2);('variance adaptor', 2);('u-net', 2);('nvidia v100 gpus', 2);('gpu', 2);('learning rate', 2);('different models', 2);('baseline models', 2);('following liu', 2);('high-quality text-to-speech', 2);('diffusion steps', 2);('ddpm-based', 2);('text-to-speech model', 2);('shallow diffusion mechanism', 2);('fast diffusion model', 2);('iteration steps', 2);('background noise', 2);('synthesis task', 2);('residual samples', 2);('pitch information', 2);('residual sample', 2);('size', 2);('decoder blocks', 2);('wang', 2);('res- grad', 2);('image synthesis', 2);('luis c. cobo', 2);('high delity speech synthesis', 2);('nanxin chen', 2);('yu zhang', 2);('ron j. weiss', 2);('sheng zhao', 2);('improving', 2);('denoising', 2);('rongjie huang', 2);('max w. y', 2);('jun wang', 2);('jinglin liu', 2);('progressive', 2);('nal kalchbrenner', 2);('seb noury', 2);('florian stimberg', 2);('jaehyeon kim', 2);('jungil kong', 2);('tim salimans', 2);('ben poole', 2);('variational', 2);('yuxuan wang', 2);('kexin zhao', 2);('bryan catanzaro', 2);('kundan kumar', 2);('rithesh kumar', 2);('jose sotelo', 2);('yoshua bengio', 2);('aaron c. courville', 2);('qiao tian', 2);('diffusion gans', 2);('oriol vinyals', 2);('alex graves', 2);('fast', 2);('stefano ermon', 2);('score-based', 2);('arash vahdat', 2);('karsten kreis', 2);('high quality acoustics', 2);('sounds', 2);('r esidual denoising diffusion prob- abilistic models for text to speech zehua chen\x031y', 1);('yihan wu4', 1);('yichong leng5', 1);('jiawei chen6', 1);('haohe liu7', 1);('xu tan3 yang cui2', 1);('ke wang2', 1);('lei he2', 1);('sheng zhao2', 1);('jiang bian3', 1);('danilo mandic1', 1);('d.mandic g @ imperial.ac.uk fxuta', 1);('heleig @ microsoft.com', 1);('abstract denoising diffusion probabilistic', 1);('text-to- speech', 1);('strong capability', 1);('high- delity samples', 1);('iterative renement process', 1);('high-dimensional data space results', 1);('real- time systems', 1);('previous', 1);('num- ber', 1);('lightweight diffusion model', 1);('re- ne', 1);('output spectrogram', 1);('model output', 1);('corresponding ground- truth speech', 1);('acceleration methods', 1);('synthesize speech', 1);('re- duces', 1);('generation target', 1);('experimental', 1);('speed-up methods', 1);('synthesizes speech', 1);('baseline methods', 1);('audio', 1);('ntroduction', 1);('recent years', 1);('tan', 1);('great progress', 1);('deep generative models', 1);('auto-regressive models', 1);('mehri', 1);('kalchbrenner', 1);('rezende', 1);('mohamed', 1);('variational autoencoders', 1);('peng', 1);('gen- erative adversarial networks', 1);('kumar', 1);('binkowski', 1);('based', 1);('iterative renement', 1);('state-of-the-art methods', 1);('acoustic models', 1);('major disadvantage', 1);('\x031imperial college', 1);('london,2microsoft azure speech,3msra,4renmin', 1);('china,5university', 1);('china,6south china', 1);('technology,7university', 1);('surrey', 1);('microsoft', 1);('corresponding', 1);('1arxiv:2212.14518v1 [ eess.as ]', 1);('dec', 1);('text stop gradientaddition tts resgrad estimated mel generated residual refined melfigure', 1);('illustration', 1);('rst predicts', 1);('pre- dicts', 1);('corresponding ground-truth mel-spectrogram', 1);('rst utilize', 1);('generate speech', 1);('generate high quality speech', 1);('complex learning space', 1);('plug-and-play model', 1);('main contributions', 1);('novel method', 1);('synthe- sis', 1);('high quality speech', 1);('speed-up baselines', 1);('r elated work denoising', 1);('state-of-the-art generation results', 1);('various tasks', 1);('ramesh', 1);('various methods', 1);('vahdat', 1);('zheng', 1);('xiao', 1);('salimans', 1);('prior/noise distribution', 1);('condition information', 1);('koizumi', 1);('2prior/noise distribution', 1);('latent representation', 1);('additional input', 1);('additional', 1);('training techniques', 1);('gan-based', 1);('frame- work', 1);('above methods', 1);('different perspectives', 1);('sufcient number', 1);('corresponding ground-truth speech', 1);('origin data space', 1);('small number', 1);('comparable sample quality', 1);('relevant work', 1);('whang', 1);('initial image', 1);('residual estimation', 1);('image quality', 1);('objective distances', 1);('generate residual information', 1);('new training congurations', 1);('ethod', 1);('verview of resgrad', 1);('mel-spectrogram esti-', 1);('residual con- tains high-frequency details', 1);('ormulation of resgrad', 1);('residual xis', 1);('text input', 1);('noise \x0f\x18n', 1);('noise schedule', 1);('< \x01\x01\x01 <', 1);('time step t2 [', 1);('p\x16 tx0', 1);('1\x00\x16 t', 1);('= 1\x00 t', 1);('\x16 t', 1);('=qt s=1 sdenotes', 1);('corresponding noise level', 1);('time step t.', 1);('previous work', 1);('score 3functions\x12which', 1);('data log-density logq', 1);('data pointxt', 1);('=\x00\x0fp1\x00\x16 t.', 1);('training objective', 1);('t s\x12', 1);('+\x0fp1\x00\x16 t', 1);('rst estimate', 1);('text input ywith', 1);('model f', 1);('noise p', 1);('data sample', 1);('residual x0by', 1);('=ty t=1p\x12', 1);('residual ^x0andf', 1);('nal output melref= ^x0+f', 1);('dvantages of resgrad', 1);('non-iterative method', 1);('full use', 1);('non-iterative model', 1);('rough sample', 1);('iterative model', 1);('standing', 1);('non- iterative', 1);('just predicts', 1);('iterative way', 1);('large room', 1);('plug-and-play', 1);('just acts', 1);('e xperimental setup', 1);('d ataset and preprocessing dataset', 1);('open-source benchmark datasets', 1);('ito', 1);('johnson', 1);('100english speech recordings', 1);('female speaker', 1);('following kong', 1);('model evaluation', 1);('model training', 1);('zen', 1);('clean subset', 1);('training dataset', 1);('extract 108speakers', 1);('rst 5recordings', 1);('//keithito.com/lj-speech-dataset/ 2https', 1);('//research.google/tools/datasets/libritts/ 3https', 1);('preprocessing', 1);('open-source tools', 1);('graphme sequence', 1);('phoneme sequence', 1);('raw waveform', 1);('common practice', 1);('128-band mel-spectrogram', 1);('jang', 1);('calculation', 1);('utilizes ground-truth pitch/duration', 1);('con- sider', 1);('ground-truth duration', 1);('residual calculation', 1);('length mismatch', 1);('samples quality', 1);('ground-truth samples', 1);('odel configuration', 1);('multi-head self-attention', 1);('phoneme encoder', 1);('mel-spectrogram decoder', 1);('multi-head attention', 1);('two-layer convolution network', 1);('fastspeech2', 1);('convolution networks', 1);('nal linear projection layer', 1);('convolution layers', 1);('dropout rate', 1);('ofcial open-source', 1);('model parameters', 1);('generate 48khz waveform', 1);('open-source code5', 1);('//github.com/kan-bayashi/parallelwavegan 5https', 1);('t raining and evaluation', 1);('adamw', 1);('weight delay \x15=', 1);('learning rate plan schedule', 1);('vaswani', 1);('100k steps', 1);('moderate quality speech', 1);('diffusion time steps', 1);('max length', 1);('strong sample quality improvement', 1);('500k training steps', 1);('human subjective evaluation', 1);('mean opinion score', 1);('comparison mean opinion score', 1);('invite 14native', 1);('overall sample quality', 1);('5-point scale', 1);('objective measurement real-time factor', 1);('nvidia v', 1);('100gpu device', 1);('t raining of baseline models', 1);('reproducible comparison', 1);('speaker dataset', 1);('acoustic model', 1);('different data', 1);('warmup stage', 1);('main stage', 1);('warmup', 1);('stage trains', 1);('auxiliary decoder', 1);('fast- speech', 1);('main stage trains', 1);('t=1', 1);('1= 0:5and 2=', 1);('train models', 1);('300k steps', 1);('loss converge', 1);('fast diffsion model', 1);('n-step', 1);('following huang', 1);('rst train', 1);('following popov', 1);('train model', 1);('original mel- spectrograms', 1);('1,700k steps', 1);('nvidia v100gpu', 1);('following ren', 1);('r esults', 1);('peech quality and inference speed', 1);('recording', 1);('ground-truth recordings', 1);('mel +', 1);('synthesize waveform', 1);('popular non- autoregressive', 1);('fast speed', 1);('gans', 1);('//github.com/keonlee9420/diffgan-tts 7https', 1);('//github.com/moonintheriver/diffsinger 8https', 1);('//github.com/rongjiehuang/prodiff 9https', 1);('main reasons', 1);('firstly', 1);('speech samples', 1);('secondly', 1);('residual part', 1);('ljspeech libritts vctk', 1);('rtf mos', 1);('rtf recordings', 1);('4:91\x060:02\x00 4:4\x060:05\x00 4:75\x060:02\x00', 1);('mel +v ocoder 4:28\x060:06\x00 4:5\x060:04\x00 4:61\x060:04\x00', 1);('2:14\x060:09 0:009\x00 \x00 \x00 \x00', 1);('synthesis high-quality speech', 1);('experimental settings', 1);('interference factors', 1);('open-source datasets', 1);('oice quality', 1);('multi- speaker', 1);('rate datasets', 1);('inference', 1);('diff- speech', 1);('inference time', 1);('4-step iteration', 1);('speech-up method', 1);('iteration step', 1);('challenging datasets', 1);('appendix', 1);('blation study residual calculation', 1);('ground-truth residual samples', 1);('ground-truth data samples', 1);('quality comparison', 1);('gt residual', 1);('generated residual', 1);('generated residual samplefigure', 1);('left column shows', 1);('ground-truth residual sample', 1);('right column shows', 1);('corresponding ground-truth residual sample', 1);('pred', 1);('comparison', 1);('resunet\x000:18', 1);('resgrad figure', 1);('generates mel-spectrogram', 1);('frequency information', 1);('ground truth mel-spectrogram', 1);('residual prediction', 1);('resid- ual information', 1);('unet', 1);('ronneberger', 1);('residual convolutions', 1);('convolutional layer', 1);('resconv', 1);('batch normalization', 1);('leakyrelu activation', 1);('linear convolutional operation', 1);('residual unet', 1);('residual information', 1);('promising results', 1);('high-frequency informa- tion', 1);('speech super-resolution task', 1);('large diffusion model', 1);('decoder model', 1);('comparison results', 1);('ase study', 1);('conduct case study', 1);('generate mel-spectrograms', 1);('generates mel-spectrograms', 1);('clean residual samples', 1);('onclusion', 1);('corresponding ground-truth', 1);('different', 1);('whole data space', 1);('re- sult', 1);('iterative steps', 1);('synthesize high-quality speech', 1);('low real-time factor', 1);('experiments', 1);('syn- thesize', 1);('future work', 1);('model architectures', 1);('mikolaj binkowski', 1);('jeff donahue', 1);('aidan clark', 1);('mohammad norouzi', 1);('william chan', 1);('wave-', 1);('estimating', 1);('waveform generation', 1);('zehua chen', 1);('ke wang', 1);('shifeng pan', 1);('danilo p. mandic', 1);('infergrad', 1);('icassp', 1);('alexander quinn nichol', 1);('diffusion', 1);('ajay jain', 1);('pieter abbeel', 1);('fastdiff', 1);('fast conditional diffusion model', 1);('high-quality speech synthesis', 1);('ijcai', 1);('huadai liu', 1);('chenye cui', 1);('acm multimedia', 1);('keith ito', 1);('linda johnson', 1);('lj', 1);('speech dataset', 1);('lj-speech-dataset/', 1);('won jang', 1);('dan lim', 1);('jaesam yoon', 1);('bongwan kim', 1);('juntae kim', 1);('univnet', 1);('multi-resolution spectrogram discriminators', 1);('high-delity waveform generation', 1);('edward lock-', 1);('aron van den', 1);('efcient', 1);('neural audio synthesis', 1);('sungwon kim', 1);('glow-tts', 1);('generative ow', 1);('monotonic alignment search', 1);('advances', 1);('information processing systems', 1);('glow', 1);('inneurips', 1);('yuma koizumi', 1);('kohei yatabe', 1);('michiel bacchiani', 1);('specgrad', 1);('diffu-', 1);('sion probabilistic model', 1);('adaptive noise spectral', 1);('arxiv preprint arxiv:2203.16749', 1);('jaekyoung bae', 1);('hi-gan', 1);('yin cao', 1);('keunwoo choi', 1);('decoupling', 1);('phase estimation', 1);('deep resunet', 1);('music source separation', 1);('ismir', 1);('zhifeng kong', 1);('jiaji huang', 1);('diffwave', 1);('versatile diffusion model', 1);('audio synthesis', 1);('thibault', 1);('boissiere', 1);('lucas gestin', 1);('wei zhen teoh', 1);('alexandre', 1);('br', 1);('melgan', 1);('conditional waveform synthesis', 1);('bilateral', 1);('heeseung kim', 1);('chaehun shin', 1);('chang liu', 1);('qi meng', 1);('wei chen', 1);('priorgrad', 1);('data-driven adaptive', 1);('sang-gil lee', 1);('boris ginsburg', 1);('bigvgan', 1);('universal neural vocoder', 1);('large-scale training', 1);('arxiv preprint arxiv:2206.04658', 1);('yan zhao', 1);('chuanzeng huang', 1);('toward', 1);('general speech restoration', 1);('corr', 1);('woosung choi', 1);('xubo liu', 1);('neural', 1);('speech super-resolution', 1);('chengxi li', 1);('feiyang chen', 1);('diffsinger', 1);('singing', 1);('voice synthesis', 1);('aaai', 1);('songxiang liu', 1);('high-delity', 1);('efcient text-to-speech', 1);('arxiv preprint arxiv:2201.11972', 1);('soroush mehri', 1);('ishaan gulrajani', 1);('shubham jain', 1);('samplernn', 1);('unconditional end-to-end neural au- dio generation model', 1);('aaron', 1);('andrew senior', 1);('wavenet', 1);('generative model', 1);('raw audio', 1);('arxiv preprint arxiv:1609.03499', 1);('kyubyong', 1);('jongseok kim', 1);('kainan peng', 1);('zhao', 1);('non-autoregressive', 1);('neural text-to-speech', 1);('vadim popov', 1);('ivan v', 1);('vladimir gogoryan', 1);('tasnima sadekova', 1);('mikhail a. kudinov', 1);('grad-', 1);('aditya ramesh', 1);('alex nichol', 1);('casey chu', 1);('mark chen', 1);('hierarchical', 1);('text- conditional image generation', 1);('clip latents', 1);('arxiv preprint arxiv:2204.06125', 1);('chenxu hu', 1);('high-quality end-to-end text', 1);('danilo jimenez rezende', 1);('shakir mohamed', 1);('olaf ronneberger', 1);('philipp fischer', 1);('thomas brox', 1);('convolutional', 1);('biomed- ical image segmentation', 1);('miccai', 1);('lecture notes', 1);('incomputer science', 1);('springer', 1);('jiaming', 1);('chenlin meng', 1);('implicit models', 1);('yang', 1);('jascha sohl-dickstein', 1);('abhishek kumar', 1);('stochastic differential equations', 1);('frank soong', 1);('neural speech synthesis', 1);('arxiv preprint arxiv:2106.15561', 1);('jan kautz', 1);('latent space', 1);('11aaron van den', 1);('yazhe li', 1);('igor babuschkin', 1);('george', 1);('driessche', 1);('edward lockhart', 1);('dominik grewe', 1);('nal kalch-', 1);('helen king', 1);('tom walters', 1);('dan belov', 1);('demis hassabis', 1);('parallel', 1);('high-delity speech synthesis', 1);('ashish vaswani', 1);('noam shazeer', 1);('niki parmar', 1);('jakob uszkoreit', 1);('llion jones', 1);('aidan n gomez', 1);('kaiser', 1);('illia polosukhin', 1);('attention', 1);('heming wang', 1);('towards', 1);('robust speech super-resolution', 1);('ieee/acm transactions', 1);('speech', 1);('processing', 1);('jay whang', 1);('mauricio delbracio', 1);('hossein talebi', 1);('chitwan saharia', 1);('alexandros g. dimakis', 1);('peyman milanfar', 1);('deblurring', 1);('stochastic renement', 1);('cvpr', 1);('zhisheng xiao', 1);('tackling', 1);('generative learning trilemma', 1);('junichi', 1);('multi-speaker corpus', 1);('cstr voice', 1);('// datashare.ed.ac.uk/handle/10283/3443', 1);('rob clark', 1);('viet dang', 1);('ye jia', 1);('yonghui wu', 1);('zhifeng chen', 1);('huangjie zheng', 1);('pengcheng', 1);('weizhu chen', 1);('mingyuan zhou', 1);('truncated', 1);('diffusion proba- bilistic models', 1);('arxiv preprint arxiv:2202.09671', 1);('h uman evaluations and comments', 1);('perceptual quality', 1);('resgrad-50 words', 1);('hiss level', 1);('noticeable hiss levels', 1);('pitch', 1);('volume drop', 1);('similar comments', 1);('good articulation', 1);('audible buzz', 1);('good acoustic quality', 1);('quiet hiss', 1);('realistic', 1);('sound realistic ..', 1);('prodiff slight', 1);('human voice', 1);('robotic voice', 1);('articulation high level', 1);('gt-mel', 1);('lacks', 1);('realistic changes', 1);('slight', 1);('high frequency artefact', 1);('realis- tic audio', 1);('realistic tonal changes', 1);('audio samples', 1);('audio books', 1);