('vblrl', 19);('international conference', 12);('proceedings', 11);('lrl', 11);('theorem', 10);('rl', 10);('pblrl', 9);('qlearning', 9);('cartsafe', 8);('algorithm', 8);('momdp', 7);('figure', 6);('qfunctions', 6);('lexicographic', 6);('lemma', 5);('neural information processing systems', 5);('ldqn', 5);('lagrange', 5);('lexicographic qlearning', 5);('similarly', 4);('ehrn1t', 4);('tolerance function', 4);('intersection', 4);('gridnav', 4);('apropo', 4);('safe region', 4);('riis', 4);('proposition', 4);('ais', 4);('kiis', 3);('optimal policy', 3);('inproceedings', 3);('machine learning', 3);('rcpo', 3);('momdps', 3);('kl', 3);('furthermore', 3);('la2c', 3);('tsteps', 3);('a2c', 3);('barto', 3);('suppose', 3);('momdpm vblrl', 3);('qvalue', 3);('qfunction', 3);('gabor', 3);('therandom seed', 2);('environment plot corresponds policy', 2);('environment cansee', 2);('costfigure', 2);('reward00', 2);('experiment section', 2);('ldqn cartsafe', 2);('environment use networks', 2);('gridnav intersection', 2);('apropos', 2);('\x0ftgreedy \x0ft', 2);('liuet', 2);('bhatnagar', 2);('ode', 2);('lj\x12\x15forj', 2);('ppo', 2);('jiinstead', 2);('convergence respect', 2);('vblrls', 2);('qare', 2);('qdecrease', 2);('terminal state', 2);('maximum norm', 2);('qln1 sopn', 2);('qn1converges', 2);('sarsa', 2);('update rule', 2);('pn', 2);('bsitq', 2);('qvalues', 2);('bsm', 2);('cambridge', 2);('articial intelligence', 2);('policy optimization', 2);('agent reward', 2);('la2c lppo', 2);('la2cand lppo', 2);('ifthe coefcient', 2);('converges policy \x02\x0fm', 2);('lppo', 2);('e\x19\x021jzsaj\x14\x10\x03\x14c\x10kzk2for', 2);('any\x192\x054 exists c 0such z2rdand\x10 0we', 2);('initial output weights', 2);('0and variance1dand', 2);('normal distribution', 2);('initial input weights drawnfrom', 2);('relu', 2);('bounded2 neural networks widths', 2);('somenited0 reward function', 2);('s\x02a\x12rdfor', 2);('steady state distribution', 2);('qi', 2);('td0', 2);('atemperature parameter semigradient', 2);('twolayer neural networks', 2);('let\x19ajs\x12', 2);('limitaverage objectives', 2);('independent c\x14jsjand w2wsuch \x08w 1then', 2);('full rank ie', 2);('\x08be thejsj\x02cmatrix rows s \x08has', 2);('letting', 2);('sisirreducible3', 2);('markov', 2);('riisbounded2', 2);('that1sandaare nite reward function', 2);('td0rule', 2);('wi sfor choice ofstate', 2);('vis', 2);('representational power ofthe parametrisations \x19andviorqi', 2);('\x02\x0fm \x0fi\x150is', 2);('\x02\x0f1 conditionscwhenm', 2);('conm\x19', 2);('viorqi', 2);('eachkiand reward function', 2);('differentiable parameters \x12 assumethat form objective function', 2);('\x19a policy', 2);('letmbe momdp', 2);('borkar', 2);('td', 2);('kiand', 2);('monte carlo', 2);('slaters', 2);('t limt1\x11it', 2);('optimisation problem', 2);('lagrangian', 2);('momdp vblrl', 2);('vblrl lexicographic', 2);('tolerance parameter', 2);('terminal state probability one4 learning rates tsa201satisfy conditionspt tsa 1andpt tsa21withprobability', 2);('either', 2);('optimal if1sandaare nite2 reward functions bounded3', 2);('bandit algorithm', 2);('sutton', 2);('s\x02arm\x02s\x02n', 2);('t2na time parameter', 2);('aa', 2);('aset states', 2);('lexicographic bandit algorithm letsbe', 2);('natural policy gradient', 2);('ac', 2);('seijen', 2);('methods', 2);('thomas', 2);('chow', 2);('zilberstein', 2);('liu', 2);('times drawing path thealgorithm environment', 1);('run experiment', 1);('gure displays visualisations policies learnedby algorithms', 1);('apropo gridnav', 1);('21522policy visualisations', 1);('rcpo gridnav', 1);('82687policy visualisations', 1);('var ac gridnav', 1);('68138policy visualisations', 1);('lppo gridnav', 1);('74696policy visualisations', 1);('la2c gridnav', 1);('73066policy visualisations', 1);('ldqn gridnav', 1);('71943policy visualisations', 1);('a2c gridnav', 1);('50548policy visualisations', 1);('dqn gridnav', 1);('65287policy visualisations', 1);('times drawing trajectory cartbody green tip pole orange36123', 1);('run experimentthey generatedby', 1);('gure displays visualisations policies learnedby algorithmsin', 1);('apropo cartsafe', 1);('73893policy visualisations', 1);('rcpo cartsafe', 1);('60446policy visualisations', 1);('var ac cartsafe', 1);('74344policy visualisations', 1);('lppo cartsafe', 1);('60441policy visualisations', 1);('la2c cartsafe', 1);('98053policy visualisations', 1);('99713policy visualisations', 1);('a2c cartsafe', 1);('99956policy visualisations', 1);('dqn cartsafe', 1);('94373policy visualisations', 1);('direction sometimesreaches goal09231', 1);('board lot', 1);('reachthe goal', 1);('la2c lppo var ac', 1);('environment allowsfor policies', 1);('reward sometimesgo', 1);('cautious andsometimes', 1);('learns policies generate rewardbut', 1);('lotof reward', 1);('a2c var ac', 1);('difcult display policy static2d', 1);('visualisations policies', 1);('thegridnav environment', 1);('5displays visualisations policies', 1);('present additional experimental data', 1);('policy visualisationshere', 1);('secondorder terms sequential optimisation impact performance butthat impact', 1);('lppomoreover', 1);('similar terms', 1);('comparison result thisexperiment', 1);('simultaneous method', 1);('sequential optimisation objectives', 1);('secondorder derivatives loss andwith', 1);('pblrlin cartsafe', 1);('different versions', 1);('experiment compares', 1);('10environment interacts 1e601234costldqnla2clppolppo2ndseqlpposeqlppo2ndla2c2ndseqla2cseqla2c2nd b', 1);('10environment interacts 1e600010203040506rewardldqnla2clppolppo2ndseqlpposeqlppo2ndla2c2ndseqla2cseqla2c2nda', 1);('ldqns', 1);('experiment studies impact', 1);('10environment interacts 1e6000510152025costldqn0ldqn02ldqn04ldqn06ldqn08ldqn1 b', 1);('10environment interacts 1e6000102030405rewardldqn0ldqn02ldqn04ldqn06ldqn08ldqn1a', 1);('cartsafe11available httpsgithubcomsawcordwellpymdptoolbox00', 1);('lppo la2c', 1);('specically', 1);('different settings ofpblrl', 1);('present data compares', 1);('comparison different settings', 1);('choice slack parameter impact performance algorithmbut impact', 1);('result experiment isshown', 1);('si \x18sitfrom', 1);('particular run', 1);('slack parameter', 1);('vllrls', 1);('present additional data investigates thepractical signicance', 1);('vblrlsslackhere', 1);('hyperparameter', 1);('layers 512neurons each11', 1);('layers bias terms', 1);('environment theagents', 1);('gridnav cartsafe', 1);('cartsafe intersection', 1);('optimiser learning rate', 1);('adam', 1);('performs update', 1);('replaybuffer size', 1);('dqn ldqn', 1);('stepsand batch size', 1);('32as batch size', 1);('thealgorithms update networks', 1);('dqn', 1);('bestresponse oracle', 1);('prediction control', 1);('separate networks', 1);('networks algorithms', 1);('cartsafeall', 1);('0001\x01maxa2\x01 siqisain', 1);('gridnavand intersection', 1);('slack si \x18sit 005\x01maxa2\x01 siqisain', 1);('times environmentin section', 1);('minimisecost subject maximise reward algorithmwas', 1);('probability least095 lexicographic algorithms', 1);('var acs', 1);('j2\x19\x141 apropos', 1);('environmentrcpos constraint', 1);('acs', 1);('j2\x19\x145 aproposconstraints j1\x19290100j2\x19205', 1);('gridnavenvironment rcpos', 1);('j1\x19290350j2\x192010 var acs', 1);('constraint wasj2\x19\x1410', 1);('rcpos', 1);('noise \x1b', 1);('mdptoolbox11but gaussian', 1);('layers bias terms code', 1);('neural networks', 1);('vectorand use', 1);('state onehot', 1);('slack si \x18sit 001\x01maxa2\x01 siqisa', 1);('algorithms tabular witha learning rate t', 1);('noteworthy implementation detailsin section', 1);('section 4as', 1);('section list hyperparameters', 1);('hyperparametersin', 1);('global lexicographic optimum10', 1);('3where note \x14k1then iwe have\x16qi\x16f1\x0fwi\x0f\x12c2rqi\x16qic2rf\x16f0\x0fk0\x0fi0which', 1);('precise denitionsletting\x0f1 limtk1\x0ftk1allows', 1);('density ratios seeequation', 1);('0k\x001with probability\x14\x001kand havem 2\x00e\x12\x03maxa2aqsaw02 r2f\x01\x0fk \x001k1\x0fft \x03k\x14\x001k\x0fq1t\x03kjaj \x001k1\x0fft2for constants \x03kand\x03kthat', 1);('boundj1\x12\x03\x00ekj1\x12k\x14\x0ftk1logjajmpk\x001k0\x14\x002k\x0fk1\x00 1pk\x001k0\x14\x001kwherekis', 1);('ktimes', 1);('ppoupdate', 1);('divergence penalty term', 1);('recall \x14isthe coefcient', 1);('c2rq1\x16q1andlimt1\x0fft c2rf\x16frespectively supposingthat\x19and\x14are', 1);('1then limt1\x0fq1t', 1);('error bounds cases', 1);('instantiatethe update rules w1and\x12', 1);('corollary secondinequality use fact k0\x00\x03k22\x14ru usingdifferent instantiations ghru and\x13', 1);('iwhereiis', 1);('0t\x001with probability\x13t1\x00h\x004\x132t0is', 1);('pt\x001t0c1ru\x132t\x11pt\x001t0\x00\x13t1\x00h\x004\x132t\x01 c2ru\x16u\x14\x10rupt\x001t0c1ru\x132t\x11pt\x001t0\x00\x13t1\x00h\x004\x132t\x01c2ru\x16uwheretis', 1);('bypt\x001t0\x00\x13t1\x00h\x004\x132t\x01we havee\x17ith\x00usat\x00u0sa\x03\x012i\x14\x10eik0\x00\x03k22', 1);('tand', 1);('thentelescoping', 1);('local linearisation uat 0\x03is thate\x17h\x00u0sa\x03\x00gsa\x00h\x01u0s0a0\x03\x01r\x00u0sa\x03i\x00\x03\x150for any2b0ru andc1ruandc2ru\x16uare constantsof', 1);('error bounde\x17h\x00usat\x00u0sa\x03\x012jti\x141\x13t1\x00h\x004\x132t\x10kt\x00\x03k22\x00e\x17\x02kt1\x00\x03k22jt\x03c1ru\x132t\x11c2ru\x16ufor choice distribution \x17overs\x02a u0is', 1);('\x13with series learning rates f\x13tgt2nsuch that\x13t201p1t0\x13t1 andp1t0\x13t21then time twe havethe', 1);('initial parameters', 1);('function gand constanth \x000 arg min2b0ruk\x000k2projects0to ballb0rufjk0\x00k2\x14rugof radiusruabout', 1);('learning rate \x13', 1);('byt1 \x00ht\x00\x13\x00usat\x00gsa\x00h\x01us0a0t\x01rusatifor', 1);('error fora generic network u\x01\x01of form representingfand', 1);('aforementioned result relies', 1);('full expositionthe', 1);('readerto paper', 1);('previous proof', 1);('constant learning rates b focus asymptotic convergence c assumption thatsamples', 1);('minor adaptions accommodate useof', 1);('liuetal', 1);('ourarguments', 1);('theory multiplicative weights', 1);('hsu', 1);('result wemake use', 1);('recent results globaloptimality', 1);('divergence penalty \x14 1thenlim\x16f\x16qi1\x0f 0proof proof', 1);('discussion3 q\x19i\x01\x012\x08qi\x01\x01wijwi2ry', 1);('unif\x001110said dependence', 1);('kppo', 1);('thatqireplacesaiin standardppo loss', 1);('fand actionvalue', 1);('facts hand', 1);('steadystate distribution policy \x12sutton', 1);('initial distribution', 1);('j1isbased', 1);('various gradient updatesin order reect d\x121', 1);('limitaverage setting proof canbe', 1);('standarda2c algorithm', 1);('particularlyclear concise proof', 1);('limitaverage case', 1);('recent exposition section', 1);('original proof', 1);('tsitsiklisand van roy', 1);('bellman', 1);('toconverge probability', 1);('conditions algorithm', 1);('i10given byd\x12is limt11ptt0 titxt0 tiprstsjs0\x12is0under', 1);('state distribution d\x12ithat', 1);('linear semigradient td0', 1);('critic updates', 1);('converges policy \x02\x0fmproof', 1);('limt1et\x122\x02\x0fmor\x02\x0fias requiredcorollary', 1);('optimisation problem forthemthobjective', 1);('procedure concludes', 1);('overall argument readilybe', 1);('proof thisinductive step', 1);('j1ji', 1);('global lexicographic optimum', 1);('\x0fi\x0f0\x12\x0fwi limt1 t\x0f0\x12\x0fwiof local', 1);('vibywi', 1);('\x0f0\x12\x0f\x12\x01\x12maxsarisa k\x15\x18k1maxjisarjsa1\x00 j\x13where\x15\x18 arg min\x150max\x12li\x12\x15\x00\x18ixj1\x15jand\x18 maxjisa\x0f\x12rjsa1\x00 jgiven approximation', 1);('optimal solution primal formulation', 1);('condition theduality gap solution', 1);('solution dual formulation', 1);('li', 1);('local global saddle point thelagrangian', 1);('convex \x15given\x12\x03thus \x12\x03\x15\x03forms', 1);('liis', 1);('global minimum', 1);('\x12t\x15t0\x12\x03 asimilar argument \x15t0j\x12\x03\x15\x03j where\x15t0j\x12is thelimit \x15t0jrecursion static parameters \x12and thusthatli\x12\x03\x15\x03is', 1);('values \x15jover time\x12\x15t0isthe limit \x12recursion static parameters \x15t0 \x00\x15ensures \x150', 1);('ode\x15t0j\x00r\x15jli\x12\x15t0\x15t0wheret0indexes', 1);('forms discrete approximation', 1);('thate\x12kj\x00 t\x00kj\x12 kj\x00 t\x00kj\x12 r\x15jli\x12\x15for eachj2f1m\x001g gradient update in2 uses', 1);('timescale mayagain view', 1);('actor critics', 1);('asthe', 1);('lirespectivelywhat', 1);('multipliers \x15is thatli\x12\x03\x15\x15is', 1);('stationary point \x12\x03\x15ofthe', 1);('hence', 1);('additionof single scalar term', 1);('k1ki\x001with', 1);('linear combination', 1);('liin\x12', 1);('form assumption', 1);('craven', 1);('generalisation local invexity beingstraightforward', 1);('invex ifand exists function grn\x02rnrnsuchthatfx1\x00fx2\x15gx1x2rfx2for allx1x22rnhanson', 1);('differentiable function frnris', 1);('recall thata', 1);('benisrael mond', 1);('k1is', 1);('itmust case', 1);('optimal stationary point', 1);('k1converges', 1);('compact setgiven gradient descent', 1);('\x00\x12\x00r\x12li\x12t\x15\x01wheretindexes values \x12over time projectionoperator \x00\x12ensures iterates', 1);('ode\x12t', 1);('respect \x12and canthus', 1);('lagrangian liwith', 1);('estimateof gradient', 1);('eachi2f1mgand soe\x12hr\x12ki\x12 i\x001xj0\x15jr\x12kj\x12ir\x12ki\x12 i\x001xj0\x15jr\x12kj\x12 r\x12li\x12\x15in words update rule', 1);('e\x12ki\x12 ki\x12for', 1);('estimates generatedfrom trajectories', 1);('view static', 1);('critic parameters', 1);('lj\x12\x15as lagrange', 1);('updates updates j ioccur', 1);('view \x12as staticwith respect', 1);('updates \x12with respect', 1);('current learningrate', 1);('eachj2f1i\x001gand that\x11\x11iis', 1);('thenwe', 1);('conditions c policy \x02\x0fi\x001', 1);('choice learning rates', 1);('k1ki\x001as', 1);('respect objectives', 1);('inductive hypothesis onlythat\x12has', 1);('thusan\x0foptimum \x0f1\x0fw1 ofji conditions c asrequiredlet', 1);('k1and', 1);('local global optimum', 1);('assumption \x12converges', 1);('reduces thecase withm', 1);('k1\x12', 1);('multipliers \x15as notfeature inl1\x12\x15', 1);('view \x12as static respect tothese', 1);('asthe updates \x12with respect', 1);('purpose analysis theabove argument wiw\x03i\x12', 1);('inductive argument convergence \x12with respect thelagrangianl1\x12\x15objective critic parametersare', 1);('base case', 1);('case latterlet', 1);('actionvalue criticbut analogous arguments', 1);('case value', 1);('exposition wefocus', 1);('remarks proof', 1);('w\x03i\x12for eachwi9in', 1);('assumptionunder conditionscwe process', 1);('updates vector withe parameters\x12and\x15may', 1);('borkar2008', 1);('tothe learning rates', 1);('multitimescale approach', 1);('sufcient lexicographic optimum withrespect latterproof proof proceeds induction i2f1mgfor', 1);('safein knowledge lexicographic optimum respectto', 1);('j1 jm', 1);('k1kmas', 1);('fact focus convergence accordingto objectives', 1);('simple inductive argument general setting', 1);('true policygradient', 1);('seethatr\x12k1\x12serves faithful surrogate', 1);('k1', 1);('estimate gradient', 1);('local global ofj1by', 1);('assumptionpblrl converges optimum', 1);('lppo1', 1);('choice objective function', 1);('main body paper whenm', 1);('max\x12se\x12\x02jvi\x12s\x00v\x03isj\x03\x14\x0fwiwe max\x122\x02\x0fi\x001ji\x12\x00ji\x12\x03\x14\x0fwiwhere\x12\x032arg max\x122\x02\x0fi\x001ji\x12 majority proofwe', 1);('ji\x12 psisv\x03is', 1);('small neighbourhood lexicographic optima9in', 1);('onlyprove convergence', 1);('vi', 1);('byw\x03i\x12will zero error', 1);('v\x03iof', 1);('paternain', 1);('neural network \x0f\x12', 1);('\x0fiis function \x0f\x12 thetabular setting \x19is', 1);('criticin proof use \x0f\x12\x150to quantify representational power policy parametrisation followsmin\x122\x02max\x19sxaj\x19ajs\x00\x19ajs\x12j\x14\x0f\x12where', 1);('existssome limit point w\x03i\x12 limt1etwifor eachwiwhen\x12is', 1);('differentiable wifori2 f1mgand suppose', 1);('actionvalue functions thatare', 1);('minor observations beginningthe prooftheorem', 1);('main theorem', 1);('limt1 it\x11i\x001t 0our', 1);('m\x110\x11mg i2f1mgwe have\x13t2011xt0\x13t11xt0\x13t21andlimt1', 1);('restate conditions learning rates \x132f', 1);('\x00\x12\x14\x12 it\x10r\x12ki\x12 i\x001xj0\x15jr\x12kj\x12\x11\x151\x15j \x00\x15h\x15j\x11t\x00kj\x00 t\x00kj\x12\x01i8j 2we', 1);('lagrangian li\x12', 1);('reference update rules', 1);('full exposition beginby', 1);('standard multitimescale stochastic approximation argument seechapter', 1);('convergence proceeds', 1);('pblrls', 1);('pblrl convergence proofthe', 1);('relative lexicographicallyoptimal actions9', 1);('haveany arbitrary values', 1);('thenql3saqlmsacould', 1);('action awherea62\x01s1 withql1sa\x19maxaql1saandql2samaxa2\x01s1ql2sa', 1);('howeverthis', 1);('means thattheq2values increase stay', 1);('restrictions actions', 1);('q2', 1);('q1values', 1);('increase tolerance', 1);('qtsi', 1);('relationship \x01', 1);('qts1\x12\x01s1', 1);('qts0', 1);('j3\x19jm\x19without', 1);('tomake claims', 1);('1qma1 arg maxaqisa andqi1sa1 qi1sa2 forsomesa1a2 andinote', 1);('qsitq', 1);('happenifqisa1 qisa2\x00limt1', 1);('certain conditions limit value of\x19 doesnot rule possibility \x19itself', 1);('policy \x19will converge establishes', 1);('doesnot state', 1);('qthat', 1);('additional choices band', 1);('necessary theremight', 1);('andthe properties lexicographic bandit algorithms implies thesecond point propositionnote conditions', 1);('q lemma', 1);('conditions band', 1);('thistogether', 1);('full derivation sake brevity', 1);('introducebut omit', 1);('extra computationaland theoretical overhead', 1);('optimal policy high probability workwe', 1);('rate whichq1q mchange conjecture scheme', 1);('therate band', 1);('design scheme', 1);('some\x11twhere limt1\x11t', 1);('q2tsa\x15ql2sa\x00\x11tfor', 1);('high corresponding ql2valueswe', 1);('direction ofvalues', 1);('q2twill', 1);('allsand allt00\x15t0 meansthat', 1);('qt00s1\x12\x01s1for', 1);('properties lexicographic bandit algorithms implies rst point propositionsinceq1tconverges ql1 time t0such \x01', 1);('q1tconverges', 1);('q1tis', 1);('optimal policy max ibismaxsq 1qmlimt1', 1);('that1j1\x19\x03\x00j1\x19t\x14max 1b1\x00 \x00\x15t sequencef\x15tgt2nsuch limt1\x15t 02j2\x19\x03\x00j2\x19t\x14max 2b1\x00 \x00\x11t sequencef\x11tgt2nsuch limt1\x11t 0where\x19\x03is', 1);('1q1qmfor allt0\x15t conditions', 1);('qsit0q1qm\x14 bsit0', 1);('exists t2nsuchthat', 1);('moreoversuppose', 1);('qand', 1);('andlexicographic \x0fgreedy tolerance', 1);('lexicographic bandit algorithm withtolerance b uses', 1);('sarsa expected sarsa', 1);('suppose vblrl', 1);('ts\x01', 1);('iwherersa1s \x0ersa2s', 1);('mdpmhfsgfa1a2gtir', 1);('j\x190\x15j\x19\x00\x0e1\x00', 1);('j\x190andlimn1j\x19\x00pni1', 1);('n\x0e alln2nthen', 1);('j\x19n\x15j\x19\x00pni1', 1);('\x190 straightforward inductive argumentwe', 1);('nonstationary policy', 1);('let\x19nbe', 1);('thisbound tightproof', 1);('thenj\x190\x15j\x19\x00\x0e1\x00', 1);('i letq\x19s\x190s\x15maxaq\x19sa\x00\x0efor alls2s', 1);('mdp mhsatir', 1);('let\x19and\x190be', 1);('evidence', 1);('long limit values band', 1);('likely converge policy', 1);('convergencein practice', 1);('itshould', 1);('theqivalues converge', 1);('ofqi1qmif tolerance parameters', 1);('riduring', 1);('thanqitsa1\x00qitsa2 case a1ora2might', 1);('haveqlisa1 qlisa2and tolerance shrinks', 1);('small certain time', 1);('\x01 tsi', 1);('0faster thanq1qmapproach theirlimit values', 1);('qapproach', 1);('reason insufciency', 1);('long tolerances approach', 1);('optimal policy policyshould', 1);('qto0is', 1);('convergence band', 1);('tolerance parameterswe', 1);('commenton problem', 1);('priorknowledge reward functions', 1);('reward signals', 1);('vblrl tolerancein', 1);('lexicographic doubleqlearning', 1);('assumption', 1);('q1tqmtconverge', 1);('rn1inm pnpn', 1);('optimal values', 1);('forrn1inm\x00 values correspond', 1);('qan1tandqbn1tconverge', 1);('particular initialisation', 1);('rn1running', 1);('qan1tandqbn1tlike', 1);('time t0the agent update', 1);('mthe', 1);('time t0such t00\x15t0we that\x01 t00sn1 \x01sn1letm\x00be', 1);('theinductive assumption', 1);('true inductivesteppnpn', 1);('toql1qlnast1 base case', 1);('qa1tqantandqb1tqbntconverge', 1);('induction reward signalsforn2f0mg', 1);('converge lexicographicallyoptimal policyproof', 1);('seehasselt', 1);('converges optimalpolicyproof', 1);('mdp', 1);('qlearninglemma', 1);('separate proof', 1);('optimal policywe', 1);('lrlmust', 1);('optimal values properties lexicographicbandit algorithms assumption', 1);('q1qmmust', 1);('holdsthis exhausts cases means inductive stepholds', 1);('fact w1\x01\x01\x01w\x11we obtain\x14k tkw1\x01maxsaxs02s1ds\x001ptsa s0\x12wds0\x001wds0\x13xs02sds\x11ptsa s0\x12w\x11wds0\x13ctk tkw1\x01maxsaxs02s1ds\x001ptsa s0\x12wds0\x001wds0\x00w\x11wds0\x13xs02sptsa s0\x12w\x11wds0\x13ct\x14k tkw1\x01maxsa\x0f\x12wds0\x001wds0\x00w\x11wds0\x13\x12w\x11wds0\x13ct\x14k tkw1\x01\x14ctthis means thatkefftjptgk1\x14\x14k tkw1ctwhere\x14201andct0ast1', 1);('inequality n1maxsaxs02sptsa s0 maxa0 s0a0 ct\x14maxsaxs02sptsa s0 maxa0 s0a0 \x01wds0\x12wds0wds0\x13ct\x14maxsa\x10wds sa \x11\x01maxsaxs02sptsa s0\x12wds0wds0\x13ctby', 1);('maximum normby straightforward induction w1\x01\x01\x01w\x11and that\x12w\x11wi\x131\x00\x0f \x12wi\x001wi\x13\x0f1let\x14 maxi2f1\x11g\x10w\x11wi\x111\x00\x0f \x10wi\x001wi\x11\x0f cannow', 1);('anaction andw1w\x11are weights', 1);('constant \x0fgives', 1);('terminal state probability onethe', 1);('maximal distance', 1);('theconstant\x11gives', 1);('positive probabilityafteri\x001steps regardless actions', 1);('different distances terminal state example s2siand agent sthenthe agent', 1);('theweight ofhsaiiswdsintuitivelys1s\x11represent', 1);('s\x02awhere', 1);('andwi105\x10wi minj2f1dg\x10wj\x0f1\x00\x0fwj\x00wj\x001\x11\x11k\x01kw1be', 1);('thenwii elsew1', 1);('if\x0f', 1);('dsnbe function ds iifs2si\x0f min\x1a2f2\x11gmina2amins2sdptsa2s1\x01\x01\x01s\x1a\x001', 1);('s\x116sabsasa1\x01\x01\x01sbwhereab2nwithab', 1);('msi1fs62s1\x01\x01\x01sij8a2aptsa2s1\x01\x01\x01si0g\x11be', 1);('terminal states', 1);('lets1be', 1);('case needto use specic', 1);('aterminal state probability', 1);('holdsnow suppose n1', 1);('n11we furthermore have\x14 n1maxsaj sajct n1k tk1ctthis means thatkefftjptgk1\x14 n1k tk1ctwhere n1201andct0ast', 1);('bydts max\x000\x1bslack lex maxaqn1tsa\x00lex maxaqln1sa\x00maxajqn1tsa\x00qln1saj\x01note thatdtsis0aftert0 soct0ast1', 1);('dts0i kt\x14 n1maxsaes0\x18tsahmaxa0 s0a0 ictwherect maxsdts kt anddtsis error correctionterm', 1);('es0\x18tsahmaxa0 qn1ts0a0\x00qln1s0a0', 1);('follows\x14 n1maxsa', 1);('implies \x1bslack lex maxaqn1t00sa\x00lex maxaqln1sa\x14maxajqn1t00sa\x00qln1sajfor allt00\x15t0', 1);('assumption 5implies time t0such \x01sn1 \x01\x1bsn1forallt00\x15t0', 1);('q1tqntconvergetoql1qlnast1', 1);('lex maxa0qn1ts0a0\x00lex maxa0qln1s0a0i \x01 n1ktrecall thatlex maxaqln1sa maxa2\x01sn1qln1sa\x1bslack lex maxaqn1tsa maxa2\x01\x1bsn1qn1tsathe inductive assumption states', 1);('es0\x18tsah\x1bslack', 1);('n1\x1bslack lex maxa0qn1ts0a0\x00rn1sas0\x00 n1lex maxa0qln1s0a0i kt maxsa', 1);('es0\x18tsahrn1sas0', 1);('n1\x1bslack lex maxa0qn1tst1a0\x00qln1sai kt maxsa', 1);('exhausts cases canconclude kt0ast1', 1);('sarsa expected sarsathenkt0ast1', 1);('means thatlimt01e\x14vn1t0s\x00maxa2\x01\x1bsn1qn1t1sa\x15 0in words', 1);('means thatlimt01prvn1t0s maxa2\x01\x1bsn1qn1t1sa 1since action space', 1);('bsn1', 1);('\x01\x1bsn1 \x01', 1);('case \x1b limt1 b', 1);('tolerance ofb properties lexicographic bandit algorithms thatlimt1prbq1qmst2\x01', 1);('bis', 1);('bsn', 1);('and\x1bsn 1q1t1qmt1 limt01', 1);('thatevn1t1sea\x18bq1t1qmt1st1qn1t1sawherebis bandit algorithm', 1);('hence vblrl', 1);('means kt00', 1);('qt00sn1for', 1);('inductive assumption holdsthen implies time t0such that\x01\x1bsn1 \x01', 1);('1t0q1t1qmt1if assumption', 1);('qsn', 1);('recall that\x1bsn 1q1t1qmt1 limt01', 1);('qt1sn1qn1t1saand', 1);('vn1t1s maxa2\x01', 1);('lexicographic qlearning q', 1);('recall \x1bslack lex maxaqn1t1sa maxa2\x01\x1bsn1qn1t1saif', 1);('letktdenote', 1);('lex maxaqn1t1sai', 1);('ehvn1t1s\x00\x1bslack', 1);('n1vn1t1st1\x00 n1\x1bslack lex maxa0qn1t1st1a0i', 1);('eh', 1);('n1\x1bslack lex maxa0qn1t1st1a0\x00qln1sai maxsa', 1);('n1\x1bslack lex maxa0qn1t1st1a0\x00qln1sa n1vn1t1st1\x00 n1\x1bslack lex maxa0qn1t1st1a0i \x14maxsa', 1);('n1vn1t1st1\x00qln1sai maxsa', 1);('followskefftjptgk1 maxsa', 1);('q1tqmtare', 1);('b words nota typo', 1);('qor', 1);('to1 limit notappliedto arguments', 1);('qor btakes', 1);('time parameter', 1);('qor bwhen', 1);('asthe limit', 1);('important note \x1bis', 1);('lexicographicbandit algorithm', 1);('bsit0q1tqmtwhere bis', 1);('let\x1bsiq 1tqmt limt01', 1);('otherwise sarsa expectedsarsa', 1);('qsit0q1tqmtwhere qis', 1);('\x1bsiq 1tqmt limt01', 1);('lexicographicqlearning', 1);('show condition', 1);('theorem implyconditions', 1);('updaterule assumptions', 1);('rn1t n1vn1t1st1\x00qln1sanote vn1t1st1comes', 1);('qn1tsa\x00qln1saftsa', 1);('letxs\x02a\x10tsa tsa tsa', 1);('pnpn', 1);('inductive step', 1);('p0', 1);('ql1qlnast1 base case', 1);('q1tqntconverge', 1);('forn2f0mg', 1);('proofproof proof proceeds induction reward signals', 1);('agentsestimate ofqliat timet', 1);('qitis', 1);('thatqliis trueqfunction ith reward whereas', 1);('ith reward time', 1);('qitto', 1);('update rules', 1);('qlearning sarsa expected sarsaall td', 1);('formqistat 1\x00 tstat\x01qisa tstat\x01rit ivntst1where vntsestimates value sunderrn', 1);('tdupdate', 1);('update rule temporal difference', 1);('aqvalue', 1);('time whereas tolerancefunctions', 1);('notethat\x1bdoes', 1);('tolerance \x1b', 1);('q1qi\x001with', 1);('maximiseql1qli\x001 and\x1bslack lex max aqisais maximalvalue ofqiatsifais', 1);('maxa2\x01\x1bsiqisawhere \x01\x1bs0aand\x01\x1bsi1fa2\x01\x1bsijqisa\x15maxa02\x01\x1bsiqisa0\x00\x1bsiq 1qmg wordslex maxaqlisais maximal value qliatsifaischosen actions', 1);('qisa', 1);('returns nonnegative realvalue dene \x1bslack lexmax as\x1bslack lex max', 1);('input state reward priorityand sequence', 1);('function \x1bs\x02f1mg\x02s\x02armr0that', 1);('dene lexmax aslex maxaqlisa maxa2\x01siqlisawith \x01s1aand\x01si1 arg maxa2\x01siqi1samoreover', 1);('optimal policies', 1);('optimalif correspond qvalues', 1);('theproof need', 1);('everystateaction pair', 1);('conditionnote condition', 1);('qmust', 1);('itstolerance function', 1);('1qmmina6a0jqisa\x00qisa0j lexicographic', 1);('lexicographicbandit algorithm satises condition 0limt1', 1);('bused', 1);('s2sa2a5 tolerance function', 1);('willconverge policy \x19that', 1);('sarsaexpected sarsa lexicographic qlearning', 1);('theoremtheorem', 1);('general statement', 1);('seesingh', 1);('maximum normproof', 1);('t1 wherek\x01k1is', 1);('3pt\x10txt 1andpt\x10txt21with probabilityone4varfftxtjptg\x14k1 \x14k tk12for somek2rand\x142015kefftjptgk1\x14\x14k tk1ct wherect0withprobability', 1);('if1xis nite2\x10txt201and8x6xtwe have\x10tx', 1);('tconverges 0with probability', 1);('tandft\x001areptmeasurable t\x151', 1);('0arep0measurable \x10t', 1);('sequence increasing\x1belds \x100and', 1);('letptbe', 1);('tftibe stochastic process where\x10t tftxrsatisfy t1x \x001\x00\x10txt\x01\x01 txt \x10txt\x01ftxtwithxt2xandt2n', 1);('leth\x10t', 1);('converges lexicographicallyoptimal policy making use', 1);('vblrl convergence proofwe', 1);('similar kinds setups7', 1);('error correspond tothe tolerance benet general tolerance functions dene', 1);('tolowprecision oats', 1);('time case tolerance', 1);('si andq1qm mightalso wish\x1bto', 1);('constant setting sitq 1qm\x1b\x01jmaxa2\x01qisaj where\x1b201 thenthe tolerance', 1);('scale reward mightbe benecial express tolerance proportion ratherthan', 1);('exible way example', 1);('usset tolerance', 1);('need samethese generalisations', 1);('qmay', 1);('notethat', 1);('1tq 1qmg', 1);('qts0aand\x01 qtsi1fa2\x01 qtsijqisa\x15maxa02\x01 qtsiqisa0\x00 qsi', 1);('qts0iqis0a\x01where', 1);('update ruleqisa \x001\x00 tsa\x01\x01qisa tsa\x01\x00risas0 imaxa2\x01', 1);('qis', 1);('wesay lexicographic', 1);('similar way', 1);('generalise denition lexicographic', 1);('bs0aand\x01 bsi1fa2\x01 bsijqisa\x15maxa02\x01 bsiqisa0\x00limt1 bsi1tq', 1);('1where \x01', 1);('bs\x02f1mg\x02n\x02s\x02armr0is', 1);('lexicographic bandit algorithm tolerance function', 1);('thena', 1);('q1qms\x02arasequence qfunctions', 1);('general denition lexicographic banditalgorithmsdenition', 1);('requirethatlimt1 sitq 1qmexists revisitthe specication', 1);('action statesat timet', 1);('positive variable corresponding toa tolerance words sitq 1qmis tolerance ithreward', 1);('asinput state reward priority time step sequence', 1);('s\x02f1mg\x02n\x02s\x02armr0be', 1);('general formof', 1);('everywhere andthat', 1);('main text paper assumethat tolerance parameter', 1);('form vblrlthere', 1);('24thinternational conference', 1);('pomdps lexicographic reward preferences', 1);('zilberstein multiobjective', 1);('kyle hollins wray', 1);('thesis university', 1);('chris watkins learning delayed rewards phd', 1);('ieee symposiumon adaptive dynamic programming reinforcementlearning', 1);('theoretical andempirical analysis', 1);('hasseltwhiteson shimon marco wiering', 1);('seijen hado', 1);('harm', 1);('ieee transactions automaticcontrol', 1);('analysis temporaldifference learning withfunction approximation', 1);('vanroy', 1);('jn tsitsiklis', 1);('van roy', 1);('natural actorcritic algorithms', 1);('philip thomas bias', 1);('internationalconference neural information processing systems', 1);('natural actorcritic', 1);('philip thomas william dabneysridhar mahadevan stephen giguere projected', 1);('learning representations', 1);('mannor reward', 1);('chen tessler daniel j mankowitz', 1);('pages10571063 1999tessler', 1);('gradientmethods reinforcement learning function approximation', 1);('richard sutton david mcallestersatinder singh yishay mansour policy', 1);('press 2018sutton', 1);('richard sutton andrew gbarto reinforcement learning introduction themit', 1);('discussion paper', 1);('multipliers revisitedcowles commission', 1);('morton slater lagrange', 1);('singlestep onpolicy reinforcementlearningalgorithms', 1);('convergenceresults', 1);('littman csaba szepesv', 1);('satinder singh tommi jaakkolamichael', 1);('optimization algorithms arxiv170706347 2017singh', 1);('john schulman filip wolski prafulla dhariwal alec radford oleg klimov proximalpolicy', 1);('report university', 1);('technical', 1);('connectionist systems', 1);('gavin rummery mahesan niranjan online', 1);('niranjan', 1);('survey multiobjective sequential', 1);('roijers p vamplew whiteson r dazeley', 1);('engineering complex computersystems', 1);('ieee', 1);('theory lexicographic multicriteriaoptimization', 1);('mj rentmeesters wk tsaiand kweijay lin', 1);('autonomous agents multiagent systems', 1);('gaussian processmodels', 1);('policy search', 1);('roberts safe', 1);('k polymenakos abate', 1);('pages 755375632019polymenakos', 1);('reinforcement learning zero duality gap', 1);('chamon miguel calvofullana alejandro ribeiro constrained', 1);('santiago paternain luiz', 1);('management', 1);('g mitten preference', 1);('information processing systems', 1);('international conference onneural', 1);('learning convex constraintsinproceedings 33rd', 1);('robert eschapire reinforcement', 1);('iii miroslav dud', 1);('brantley hal daum', 1);('sobhan miryoose kiant', 1);('proceedingsof', 1);('trust regionproximal policy optimization attains', 1);('wang neural', 1);('boyi liu qi cai zhuoran yang', 1);('cybernetics systems', 1);('ieeetransactions systems', 1);('comprehensive overview', 1);('liu x xu hu multiobjectivereinforcement', 1);('systems', 1);('ieeetransactions', 1);('optimal control safety priority', 1);('k lesser abate multiobjective', 1);('abate', 1);('vijay r konda john ntsitsiklis actorcritic', 1);('tsitsiklis', 1);('pages 153115382001konda', 1);('sham kakade', 1);('design choices inproximal policy optimization arxiv200910897 2020kakade', 1);('chloe chingyun hsu celestine mendlerdunner moritz hardt revisiting', 1);('neuralinformation processing systems', 1);('hado v hasselt', 1);('mathematical analysisand applications', 1);('sufciency thekuhntucker conditions journal', 1);('morgan hanson', 1);('machinelearning', 1);('reinforcement learning', 1);('multicriteria', 1);('csabaszepesv', 1);('zsolt kalm', 1);('zoltan g', 1);('bulletin australian mathematical', 1);('local minima', 1);('bd craven invex', 1);('conferenceon neural information processing systems', 1);('safe reinforcement learning', 1);('yinlam chow nachum edgarduenezguzman mohammad ghavamzadeh alyapunovbased', 1);('reinforcement learning percentilerisk criteria journal', 1);('yinlam chow mohammadghavamzadeh lucas janson marco pavoneriskconstrained', 1);('book agency 2008chow', 1);('vivek borkar stochastic approximation hindustan', 1);('automatica', 1);('shalabh bhatnagar richard sutton mohammad ghavamzadeh mark lee naturalactorcritic', 1);('dimitri bertsekas nonlinear programming athena scientic', 1);('applied mathematics', 1);('society series b', 1);('australian mathematical', 1);('invexity journal', 1);('mondwhat', 1);('benisrael', 1);('mond', 1);('10225127619987available httpsgithubcomeleurenthighwayenvbenisrael', 1);('neural computation', 1);('natural gradient', 1);('shunichi amari', 1);('conferenceon machine learning', 1);('avivtamar pieter abbeel constrained', 1);('joshua achiam david', 1);('reference', 1);('epsrc doctoraltraining partnership', 1);('possible directions future workacknowledgmentshammond acknowledges support', 1);('optimality respectto primary objectives applications', 1);('extra rewardsignals', 1);('eg toguide learning', 1);('knowledge level safety canbe', 1);('safe aspossible lack', 1);('different kinds problems example', 1);('art interms learning speed nal performancewe conclude', 1);('impose safety constraints', 1);('furtherwhen', 1);('result scalability', 1);('reward functions', 1);('learning time algorithmsgrows', 1);('\x0foptimal policies case', 1);('inherits convergence guarantees afunction objectives', 1);('optimal policy tabular setting', 1);('rl vblrlconverges', 1);('empirical performance algorithms', 1);('favourabletheoretical guarantees', 1);('lexicographic multiobjective problems generalthan', 1);('discussion conclusionswe', 1);('rst objective difcult thenthe', 1);('dqn a2c thisshows', 1);('reward constrainedalgorithms', 1);('la2cand rcpo', 1);('manage increase reward', 1);('cost random agent', 1);('challenging algorithmsincur', 1);('original rewardstructure task', 1);('different environments', 1);('collision occurs whichis', 1);('destination cost', 1);('car intersection densetrafc', 1);('environment highwayenv7the agent', 1);('low safety andlow', 1);('low reward', 1);('high reward andrcpo', 1);('var ac', 1);('la2c lppoobtain', 1);('similar terms safety', 1);('la2c lppo rcpo', 1);('safest algorithm italso', 1);('random direction withprobability', 1);('eachtime step agent', 1);('enters unsafe square', 1);('large gridworld goal region number unsafe squaresthe agent', 1);('environment gymsafety basedon environment', 1);('reward algorithm run tentimes environmentthe', 1);('costand subject', 1);('probability cost', 1);('rewardsubject constraint cost bounds reward andcost', 1);('apropo var ac', 1);('plot average reward cost yaxis thenumber environment interactions xaxis environmentrcpo', 1);('ldqn la2c lppo apropo rcpo varacfigure', 1);('intersectionenv costdqn a2c', 1);('intersectionenv reward0', 1);('10k 20k00010203040506e', 1);('gridnav cost0', 1);('gridnav reward0', 1);('2m 4m00051015c', 1);('cartsafe cost0', 1);('cartsafe reward00', 1);('reward00 05m 10m0001020304050607a', 1);('dqn a2c', 1);('safer policies', 1);('var acstruggles', 1);('safe policies', 1);('cost thecart', 1);('pole whilst', 1);('environment agent', 1);('cartpole', 1);('environment gymsafety6is version ofthe', 1);('neural network replay buffer callldqn', 1);('number environments performance metricand safety constraint algorithms synthesise slightlydifferent kinds policies nonetheless sufcientlysimilar relevant comparison', 1);('varconstraints chow', 1);('actorcritic algorithm', 1);('apropo miryoose', 1);('lrl rcpo tessler', 1);('experimentcompares performance', 1);('terms performance metric safety constraint', 1);('lexicographic rl safety constraintsmany', 1);('large numbers objectives42', 1);('meaning algorithms canbe', 1);('additionalreward functions', 1);('scales number rewards datasuggest learning time', 1);('shows learning time', 1);('rst experiment', 1);('scaling number rewardsour', 1);('supplementary material', 1);('additional experiments', 1);('experimental details', 1);('vblrl pblrl', 1);('scales number reward functions', 1);('show learning time', 1);('werst', 1);('experimentsin', 1);('red andlexicographic', 1);('lexicographic ppo', 1);('lexicographic expected sarsa', 1);('stable value algorithms', 1);('episode toconverge', 1);('agents longrun average reward', 1);('record number episodesit', 1);('number rewards eachtrial generate', 1);('or512 states', 1);('number ofepisodes convergence yaxis number rewardsignals xaxis', 1);('plot learning time', 1);('161k2k3k4k b', 1);('divergence penalty \x14 1thenlim\x16f\x16qi1\x0f', 1);('unif\x00113 q\x19i\x01\x012\x08qi\x01\x01wijwi2ry', 1);('thatqireplacesaiin standardppo losskppo updates', 1);('exp\x00 \x001fsa\x12\x01and suppose fand actionvalue', 1);('converges policy \x02\x0fmcorollary', 1);('corollaries proofs corollariescontain discussion references', 1);('a2c ppo', 1);('certain conditionscthat sufcient local globalconvergence', 1);('conclude section', 1);('lexicographic ppo lppowe', 1);('lexicographica2c la2c', 1);('ka2ciandkppoirespectively', 1);('criticin remainder paper', 1);('exists limitpointw\x03i\x12 limt1etwifor eachwiwhen\x12is', 1);('differentiable wifori2f1mgand suppose', 1);('actionvalue functions', 1);('usinga critic', 1);('pblrltheorem', 1);('local global \x0foptimum correspondingobjective function', 1);('algorithm obtains localor global\x0foptimum', 1);('standard stochastic approximation argument', 1);('kppo1', 1);('ka2c1or ppo', 1);('thechoice objective function', 1);('1pblrl reduces', 1);('nonlexicographic algorithm corresponds objective function', 1);('inherits convergenceguarantees', 1);('update rules wibelow show', 1);('range ofobjective functions', 1);('lrl pblrl', 1);('ageneral', 1);('viat', 1);('forvigiven bywi wi t\x00\x0eitrwivi\x01 where\x0eitis', 1);('td0update', 1);('learning rate instance', 1);('update theparameterswiof critic', 1);('criticviorqi estimate', 1);('small number coefcients citat timestepand', 1);('simple formulate lexicographic optimisation problem collection objective functions', 1);('thatpmjm1 jt', 1);('simpleupdate\x12 \x00\x12\x12r\x12k\x12 wherek\x12mxi1citki\x12andcit it\x15imxji1 jtand', 1);('updates \x12for eachi timetwe', 1);('estimate nextnote', 1);('critic \x15i9 update\x1210 ifs0is terminal thens\x18ielses s011return\x12where \x00\x15\x01 max\x010\x00\x12projects\x12to', 1);('else\x11 \x11i8 updatewiif', 1);('ki\x127', 1);('do4t 1a\x18\x19ss0\x18tsa5 fori2f1mgdo6 ifki\x12has', 1);('i1initialise\x12w1wm\x151\x15m2t 0\x11 \x110s\x18i3while\x12has', 1);('policybased lexicographic rlinputmhsati r', 1);('updates\x12 \x00\x12\x14\x12 it\x10r\x12ki\x12 i\x001xj1\x15jr\x12kj\x12\x11\x15\x15j \x00\x15h\x15j\x11t\x00kj\x00 t\x00kj\x12\x01i8j2f1i\x001galgorithm', 1);('lagrangian livia', 1);('learning rates iand\x11\x11iwe maycompute saddle point solution', 1);('limit respect', 1);('mtin order', 1);('limt1 it\x11i\x001t 0we', 1);('m\x110\x11mgand alli2f1mgwe have\x13t2011xt0\x13t11xt0\x13t21andlimt1', 1);('multiplier \x11iafter convergence respect ithobjective assumethat learning rates \x132f', 1);('rate \x11of', 1);('optimal solution', 1);('\x12at differenttimescales', 1);('ourkey', 1);('slow sampleinefcient', 1);('large diverse thisprocess', 1);('solution space lexicographicoptima objective function', 1);('optimisation problem forli wherei2f1mg', 1);('natural approach', 1);('whereli\x12\x15ki\x12 i\x001xj1\x15j\x00kj\x12\x00kj \x01a', 1);('saddle point min\x150max\x12li\x12\x15of', 1);('dual problem', 1);('slater', 1);('constraint qualicationslaters condition', 1);('tto decay', 1);('satisfying theabove constraints practice learning', 1);('small constant tolerance parameter', 1);('8j2f1i\x001gwhere 0is', 1);('ki\x12subject kj\x12\x15kj\x00', 1);('ki letkjkj\x120for', 1);('optimise \x12with respect', 1);('k1ki\x001and', 1);('bertsekas', 1);('current value solvethese problems', 1);('k1remains', 1);('condition loss respect', 1);('timescale optimise \x12usingk2while', 1);('rst optimise \x12usingk1', 1);('reward function update parameters \x19\x01\x12with multitimescale approach', 1);('kifor', 1);('objective function', 1);('introduce family lexicographic policy gradientalgorithms algorithms', 1);('policybased algorithmswe', 1);('applicable tohighdimensional state spaces32', 1);('function approximators', 1);('tabular straightforward', 1);('rewards discussthis issue supplementary material', 1);('knowledge ofthe', 1);('guarantees aboutthe limit behaviour', 1);('then1j1\x19\x03\x00j1\x19t\x14 1\x00 \x00\x15t sequence f\x15tgt2nsuch limt1\x15t 02j2\x19\x03\x00j2\x19t\x14 1\x00 \x00\x11t sequence f\x11tgt2nsuch limt1\x11t 0where\x19tis policy time tand\x19\x03is lexicographicallyoptimal policyproposition', 1);('sarsa expected sarsa lexicographic qlearning', 1);('small motivate intuition formalguarantee behaviour', 1);('range general determineda priori', 1);('strong theupper', 1);('condition', 1);('action inevery state', 1);('converges lexicographicallyoptimal policy \x19if conditions', 1);('lexicographicdoubleqlearning', 1);('optimal policytheorem', 1);('fori2f1mgdo6 updateqi7 ifs0is terminal thens\x18ielses s08t 19return\x19s7limt1bq1qmstwe', 1);('tsa5', 1);('bq1qmst algorithm', 1);('i1initialiseq1qmt 0s\x18i2whileq1qmhave', 1);('valuebased lexicographic rlinputmhsati r', 1);('fori2f1mgdo5x maxa02\x01qisa06 \x01 fa2\x01jqisa\x15x\x00 g7a\x18unif\x018returnaalgorithm', 1);('a4', 1);('\x0fgreedyinputq1qmst1with probability \x0fstdoa\x18unifa2else3 \x01', 1);('04available httpsgithubcomlrhammondlmorlalgorithm', 1);('constant supplementary material relax assumptions3the', 1);('thesame tolerance parameter', 1);('main text paper', 1);('s2sa2a5 tolerance satises condition', 1);('convergeto policy\x19that', 1);('sarsa expectedsarsa lexicographic qlearning', 1);('uses lexicographic bandit algorithm', 1);('supplementary material4theorem', 1);('allour', 1);('core result', 1);('different properties3we', 1);('family algorithms', 1);('update rule invblrl', 1);('varying', 1);('qbisa\x01in', 1);('letqisa 05\x00qaisa', 1);('qbi', 1);('analogous update', 1);('reward update theqvalues probability 05we setqaisa \x001\x00 tsa\x01\x01qaisa tsa\x01\x10risas0 i\x01qbi\x00s0arg maxa02\x01 siqais0a0\x01\x11and', 1);('qfunctionsqaiqbifor', 1);('qlearninghasselt', 1);('sarsa expectedsarsa alternatively', 1);('maximise rewardsof', 1);('range actionsthat', 1);('wherethe maxoperator', 1);('2r0is tolerance parameter2this rule analogous', 1);('\x001\x00 tsa\x01\x01qisa tsa\x01\x00risas0 imaxa2\x01 siqis0a\x01where \x01 s0a\x01 si1fa2\x01 sijqisa\x15maxa02\x01 siqisa0\x00 g', 1);('lexicographic qlearning qisa', 1);('followingupdate rule', 1);('lexicographic bandit algorithm rule updatingtheqvalues line', 1);('rl bis', 1);('algorithm lexicographic multiobjective', 1);('limt1\x0fst 0andp1t0\x0fst1for alls2swe introduce', 1);('theexploration probabilities \x0fstshould', 1);('actions action aalso maximisesq2with tolerance example lexicographic bandit algorithm', 1);('action asuch thatamaximisesq1with tolerance', 1);('lexicographic bandit algorithm thelimit', 1);('advantage actorcritic a2c', 1);('thatlimt1prbq1qmst2\x01 sm 1where \x01 s0aand\x01 si1fa2\x01 sijqisa\x15maxa02\x01 siqisa0\x00 g1due choice baseline', 1);('bandit algorithm tolerance 2r0is afunctionb', 1);('alexicographic', 1);('q1qms\x02ara', 1);('optimal actionsdenition', 1);('bandit algorithms', 1);('valuebased algorithmswe', 1);('optimal policies simpliciter31', 1);('wedrop notation', 1);('when\x0f', 1);('compact localneighbourhood \x12 \x02\x0f0\x02\x0f\x001 \x02', 1);('\x02\x0fi1f\x122\x02\x0fijmax\x1202ni\x12ji\x120\x00ji\x12\x14\x0figto dene local lexicographic\x0foptima whereni\x12\x12\x02\x0fiis', 1);('global lexicographic \x0foptima', 1);('\x02\x0fi1to dene', 1);('m\x05\x0fi1f\x192\x05\x0fijmax\x1902\x05\x0fiji\x190\x00ji\x19\x14\x0fig', 1);('\x0foptimal if\x192\x05\x0fmwhere \x05\x0f0 \x05', 1);('momdp mwithmrewards', 1);('optimal policygiven', 1);('lexicographic multiobjectiveproblems learning', 1);('present family', 1);('lexicographic reinforcement learningin', 1);('7rsas i3', 1);('riassas', 1);('vector mrewards 201mdenesmdiscount rates dene', 1);('mdp rs\x02a\x02s rmreturns', 1);('tuple hsati', 1);('mdpmomdp', 1);('multiple objectivesthis setting', 1);('policy synthesis', 1);('multiobjective reinforcement learning morl', 1);('strong convergence guaranteeshsuet', 1);('stateof art performance', 1);('tokppo\x12eth\x19atjst\x12\x19atjst\x12olda\x12stat\x00\x14\x01dkl\x19st\x12k\x19st\x12oldiwhere\x14is scalar weight algorithms', 1);('proximal policy optimisationppo schulman', 1);('new old policiesas', 1);('kullbackleibler kl', 1);('common penalty', 1);('amari1998 kakade', 1);('large steps policy space', 1);('policy gradient algorithms usedsurrogate objective functions increase stability', 1);('konda tsitsiklis', 1);('temporal difference error \x0etrt v\x12st1\x00v\x12storrtv\x12st1\x00v\x12st\x00j\x12when', 1);('vparameterised', 1);('v\x12with function', 1);('a\x12byapproximating', 1);('estimate usingka2c\x12et\x02log\x19atjst\x12\x01a\x12stat\x03wherea\x12is', 1);('k\x12', 1);('differentiable respect \x122\x02\x1arxand\x12is', 1);('methods policy\x19\x01\x12is', 1);('expectedsarsa', 1);('rummery niranjan', 1);('current behaviour obtainthesarsa', 1);('term maxaqst1ain rule abovewithqst1at1orea\x18\x19sqst1awhere\x19is thepolicy', 1);('\x001\x00 tstat\x01\x01qstat tstat\x01\x00rt maxaqst1a\x01wheretis time step tstatis learning rate', 1);('qstat', 1);('watkins', 1);('popular rule', 1);('ways update', 1);('lookup table case agent istabular function approximatorthere', 1);('select actions state', 1);('particular state bandit algorithmthat', 1);('particular action', 1);('reward conditional takinga', 1);('qfunctionqs\x02arthat', 1);('v\x19s0and advantage function a\x19sa', 1);('es0\x18tsarsas0', 1);('policy \x19we', 1);('psisv\x19s', 1);('limitaverage expectationthe objective', 1);('e\x19p1t0', 1);('\x19froms iev\x19s', 1);('cumulative reward', 1);('distribution agents actions statethe value function v\x19sof\x19is', 1);('athatspecies', 1);('allaa stationary policy', 1);('probabilisticmappingffromxtoy state terminal iftsa sandrsas', 1);('herefx ydenotes', 1);('201isadiscount factor', 1);('agentmoves state stos0by', 1);('wherersas0is reward', 1);('srs\x02a\x02s rarewardfunction', 1);('aninitial state distribution', 1);('iis', 1);('atransition function', 1);('sis', 1);('setof actionsts\x02a', 1);('atuplehsatir iwheresis', 1);('markov decision process mdp', 1);('backgroundreinforcement learning rl', 1);('safe limit policy2', 1);('safety learning', 1);('incontrast', 1);('polymenakos', 1);('dec', 1);('etarxiv221213769v1 cslg', 1);('achiam', 1);('safe learning', 1);('algorithmsabove general methods alsoemphasise', 1);('limit policies', 1);('reward subject constraint thesecond', 1);('lexicographic optimisation', 1);('knowledge level safetythat', 1);('encode safety constraints', 1);('convex setour contributions', 1);('reward vector', 1);('algorithm accepts arbitrary number reward signals learns policy', 1);('miryoose', 1);('penalty signal', 1);('cvar', 1);('var', 1);('introduce techniques maximise reward subjectto constraints valueatrisk', 1);('certain threshold', 1);('introduce algorithm maximises reward subject constraint expectation additionalpenalty signal stay', 1);('tessler', 1);('lesser abate', 1);('satisfying safety constraint setup', 1);('learna policy maximises performance metric subject', 1);('rl lrl', 1);('natural application lexicographic', 1);('rentmeesters', 1);('mitten', 1);('roijers', 1);('different waysfor overview', 1);('trade rewards', 1);('morl', 1);('complex environments', 1);('benchmarks stateoftheart algorithms', 1);('alsoprove algorithms converge', 1);('work general stateoftheart', 1);('3our contribution', 1);('footnote', 1);('special case', 1);('multiobjective rl morlhas', 1);('related worklexicographic', 1);('convergence algorithms benchmark stateoftheart methods constrainedreinforcement learning number environments11', 1);('wide range', 1);('discountedr2reward techniques easilybe', 1);('policies \x19also maximises', 1);('authoramong', 1);('r1reward', 1);('policy \x19such that\x19maximises', 1);('r1rm', 1);('multiple reward functions', 1);('policy gradient algorithms', 1);('present family actionvalue algorithms', 1);('techniques solvinglexicographic multiobjective problems preciselywe', 1);('algorithmsin work introduce', 1);('agent prioritise lessimportant objectives cases difcult', 1);('scalar reward function encodes sucha task', 1);('general straightforward way', 1);('moreimportant others', 1);('task followingthese laws involves', 1);('laws robotics', 1);('asimovs', 1);('consider', 1);('tasks difcult eveninfeasible', 1);('scalar reward function', 1);('unknown environments process trial errorwhere task', 1);('introductionreinforcement', 1);('reinforcement learning algorithms1', 1);('impose safety constraints behaviour agent comparetheir performance context', 1);('practical applicability specic application show howour algorithms', 1);('scalability performance algorithms', 1);('converge policies', 1);('actionvalue policy gradient algorithms', 1);('reward signal', 1);('policy maximises rst rewardsignal subject constraint', 1);('multiple reward signals goalis', 1);('lexicographic multiobjective problems problems', 1);('lewishammond charliegrifn aabate gcsoxacukabstractin work introduce reinforcement learning techniques', 1);('oxfordfjoarskalse', 1);('science university', 1);('lexicographic multiobjective reinforcement learningjoar skalse\x03lewis hammond charlie grifn alessandro abatedepartment computer', 1);