('flops', 38);('emo', 34);('irmb', 21);('mhsa', 21);('sec', 20);('transformer', 15);('dw-conv', 15);('fig', 15);('emo-5m', 13);('cvpr', 13);('params', 12);('iclr', 11);('top-1', 10);('tab', 10);('iccv', 10);('stem2816321x1 conv', 9);('mobilenetv2', 9);('sota', 9);('imagenet-1k', 8);('ew-mhsa', 8);('ln', 8);('efcient model', 7);('irb', 7);('metaformer', 7);('mlp', 7);('conv1x1 conv', 6);('conv', 6);('conv3x3 dw-convattn mat', 6);('inverted residual block', 6);('ade20k', 6);('meta mobile block', 6);('rmb', 6);('] [', 6);('emo-1m', 6);('mobile applications', 5);('resnet-like', 5);('emo-1m/2m/5m', 5);('mobilenet', 5);('vision transformer', 5);('batch size', 5);('rethinking', 5);('left', 4);('cnn', 4);('cnn-based', 4);('model performance', 4);('furthermore', 4);('dense applications', 4);('coco2017', 4);('transformer-based', 4);('ssdlite', 4);('deeplabv3', 4);('edgenext', 4);('res2net', 4);('mobilevit', 4);('series [', 4);('m2block', 4);('w-mhsa', 4);('efcient operator', 4);('dense prediction', 4);('core modules', 4);('comparatively', 4);('edgenext-s', 4);('different stages', 4);('hugo touvron', 4);('neurips', 4);('vision transformers', 4);('pmlr', 4);('eccv', 4);('sachin mehta', 4);('msanorm ffn norm1x1 convidentity multi-headself-attention1x1 conv', 3);('conv3x3dw-conv1x1 conv', 3);('convfeed-forwardnetworkinvertedresidual blockmeta mobile moduleefficienttoken mixerattn matexpansionratio1x1 conv', 3);('convinverted residualmobile block3x3 dw-convattn map4', 3);('conv3x3 dw-convattn', 3);('map x', 3);('irmb1x1 conv identity multi-headself-attention1x1 conv', 3);('convdw-conv1x1 conv', 3);('conv1x1 convfeed-forwardnetworkinvertedresidual blockmetamobile blockattn map=1=4', 3);('top-1accuracyinimagenet-1k emo-1memo-2memo-5memo-6m mvitv3arxiv22mvitv2arxiv22mvitv1iclr22deiticml21vitaeneurips21edgeviteccv22mpvitcvpr22coatarxiv22 pvtv2cvm22xcitneurips21', 3);('irmbefficientoperator q k v emo', 3);('multi-head self-attention', 3);('right', 3);('efcient models', 3);('vit', 3);('inverted residual mobile block', 3);('model short-distance dependency', 3);('long-distance interactions', 3);('jiangning zhang', 3);('yabiao wang', 3);('chengjie wang', 3);('sota cnn-/transformer-based', 3);('model accuracy', 3);('standard convolution', 3);('trans-', 3);('computer vision', 3);('cnn-', 3);('based', 3);('ffn', 3);('specically', 3);('channel attention [', 3);('xe', 3);('receptive eld', 3);('bn', 3);('adamw', 3);('randaugment', 3);('cutmix', 3);('pytorch', 3);('object', 3);('retinanet', 3);('ms-coco', 3);('comparison', 3);('emo-2m', 3);('semantic', 3);('pspnet', 3);('ablation', 3);('cpu', 3);('gpu', 3);('dw-convmhsamlp', 3);('grad-cam', 3);('zheng zhang', 3);('liang-chieh chen', 3);('xiangyu zhang', 3);('jian sun', 3);('deep', 3);('andrew howard', 3);('mark sandler', 3);('bo chen', 3);('mingxing tan', 3);('sergey ioffe', 3);('szegedy', 3);('icml', 3);('mohammad rastegari', 3);('x irmb x irmb x irmb x irmb', 2);('q k v emo', 2);('conv1x1 convmetamobile block1x1 conv1x1 conv metaformer figure', 2);('performance', 2);('dense predictions', 2);('essential unity', 2);('general concept', 2);('specic instantiation', 2);('cnn-like', 2);('transformer-like', 2);('recent years', 2);('re- searchers', 2);('inceptionv3', 2);('separable convolution', 2);('subsequent works [', 2);('depth-wise convolution', 2);('signicant improvements', 2);('transformer-', 2);('satisfactory results', 2);('recently', 2);('mhsa/ffn', 2);('arguments expansion ratio \x15andefcient operator', 2);('fto', 2);('different modules', 2);('basic ability', 2);('different structural instantiations', 2);('identity', 2);('mobile- vitv2', 2);('neural networks', 2);('squeezenet', 2);('deit', 2);('how-', 2);('chen', 2);('mobile', 2);('complex modules', 2);('block', 2);('channel dimension', 2);('mpl', 2);('c+', 2);('lc', 2);('expansion speed', 2);('1m5m7m i', 2);('model', 2);('deit-tiny', 2);('% #', 2);('pvt-tiny', 2);('pvt', 2);('usability', 2);('uniformity', 2);('effectiveness', 2);('current efcient models', 2);('edgevit', 2);('position embeddings', 2);('source code', 2);('dropout', 2);('mixup', 2);('erasing', 2);('multi-scale', 2);('surprisingly', 2);('emo-6m', 2);('setting', 2);('] dataset', 2);('batch', 2);('drop', 2);('edgenext-xxs', 2);('pvtv2-b0', 2);('mvitv1-s', 2);('mmdetection', 2);('details', 2);('backbone', 2);('mnetv2', 2);('mvitv2-0.5', 2);('mvitv2-0.75', 2);('mobilevitv2', 2);('object detection', 2);('mobilevitv2emo', 2);('semantic segmentation', 2);('emo-5mmpvit-tinyresnet-50 mobilevitv2emofigure', 2);('params flops', 2);('paramflopsnones-4s-34s-234s-1234', 2);('nones-4s-34s-234s-1234 droppathratetop-1 batchsizetop-1mhsastagestop-1 lnstagesthroughput', 2);('paramflops', 2);('figure', 2);('effect', 2);('drop path rate', 2);('transformer-based mpvit', 2);('attention', 2);('matthijs douze', 2);('gabriel synnaeve', 2);('image transformers', 2);('han', 2);('efcient deployment', 2);('ziwei liu', 2);('jianping shi', 2);('hartwig adam', 2);('xiyang dai', 2);('mengchen liu', 2);('lu yuan', 2);('lukasz kaiser', 2);('/ / github', 2);('quoc v', 2);('practical', 2);('data augmentation', 2);('image recognition', 2);('matthew', 2);('chang xu', 2);('kaiming', 2);('weijun wang', 2);('ruoming pang', 2);('vijay vasudevan', 2);('menglong zhu', 2);('mobile vision applications', 2);('springer', 2);('jiashi feng', 2);('jiashi li', 2);('xin xia', 2);('xing wang', 2);('xuefeng xiao', 2);('min zheng', 2);('hongsheng li', 2);('tsung-yi lin', 2);('piotr dollr', 2);('ze liu', 2);('han hu', 2);('yutong lin', 2);('yixuan wei', 2);('yue cao', 2);('swin', 2);('ilya loshchilov', 2);('frank hutter', 2);('shufenet', 2);('hannaneh hajishirzi', 2);('mobile devices', 2);('francisco massa', 2);('vincent vanhoucke', 2);('jon shlens', 2);('zbigniew wojna', 2);('inception ar- chitecture', 2);('matthieu cord', 2);('alexandre sablayrolles', 2);('herv jgou', 2);('wenhai wang', 2);('enze xie', 2);('xiang li', 2);('deng-ping fan', 2);('kaitao', 2);('ding liang', 2);('tong lu', 2);('ping luo', 2);('ling shao', 2);('dacheng tao', 2);('yong liu', 2);('evolutionary algorithm', 2);('rethinking mobile block', 1);('efcient neural', 1);('jiangning zhang1,2xiangtai li3jian li1liang liu1zhucun xue4 boshen zhang1zhengkai jiang1tianxin huang2yabiao wang1ychengjie wang1y', 1);('lab', 1);('tencent2zhejiang university3peking university4wuhan', 1);('code', 1);('//github.com/zhangzjn/emo irmb', 1);('1m5m7m irmb', 1);('abstracted', 1);('meta-mobile block', 1);('network', 1);('mobilenet-v2', 1);('inductive block', 1);('specic modules', 1);('different expansion ratio\x15andefcient operator', 1);('f. absorbing', 1);('vs .flops comparisons', 1);('sota transformer-based', 1);('abstract', 1);('paper focuses', 1);('low parameters', 1);('lightweight methods', 1);('trading-off model accuracy', 1);('work rethinks', 1);('meta- mobile block', 1);('motivated', 1);('@ zju.edu.cn', 1);('casey- wang @ tencent.com', 1);('jasoncjwang @ tencent.com', 1);('corresponding author.on', 1);('massive', 1);('state-of-the-art methods', 1);('introduction', 1);('mobile applications lim-', 1);('lightweight models', 1);('parameters andlow', 1);('signicant attention', 1);('efcient model dates', 1);('] era', 1);('uses asym- metric convolutions', 1);('pa- rameters', 1);('com- ponent', 1);('remarkably', 1);('1arxiv:2301.01146v1 [ cs.cv ]', 1);('jan', 1);('standard efcient module', 1);('new ideas', 1);('new lightweight modules', 1);('in-', 1);('natural induction bias', 1);('cnn-pure', 1);('low level', 1);('essentially', 1);('extreme core', 1);('basic block', 1);('alexey', 1);('extensive dataset', 1);('massive resource consumptions', 1);('challenging problem', 1);('researchers focus', 1);('lin- ear complexity [', 1);('spatial resolution', 1);('channel ra- tio [', 1);('mhsa-pure', 1);('improve- ments', 1);('efcient hybrid models', 1);('cnns', 1);('[ 39,40,54 ]', 1);('current methods', 1);('complex struc- tures [', 1);('multiple hybrid modules [', 1);('hybrid efcient block', 1);('cnn-based irb', 1);('efcient block', 1);('mobile application', 1);('structural point', 1);('feed-forward network', 1);('meta block', 1);('model performances es-', 1);('ablation study', 1);('effective meta block', 1);('intuitive com- parisons', 1);('efcient methods', 1);('efcient model design', 1);('current efcient modules', 1);('infer- ring', 1);('con- crete', 1);('inductive meta block', 1);('inverted resid-', 1);('mobile block', 1);('resnet-like efcient model', 1);('contains naive convolution', 1);('complex structural design', 1);('competitive results', 1);('22.0/25.2/27.9 map', 1);('2.3m/3.3m/6.0m parame- ters', 1);('mo-', 1);('% # /-50 % # /-62 % #', 1);('33.5/35.3/37.98 miou', 1);('5.6m/6.9m/10.3m parameters', 1);('related', 1);('efcient cnn', 1);('mo- bile vision applications', 1);('extensive attention', 1);('] replaces', 1);('de- creases channel numbers', 1);('model parameters', 1);('] factorizes', 1);('asymmetric convolutions', 1);('] introduces depth-wise', 1);('large amount', 1);('subsequent light- weight models [', 1);('above hand- craft methods', 1);('automatic architecture design', 1);('search space [', 1);('considerable results', 1);('nas-based', 1);('paper rethinks', 1);('residual block', 1);('mutual strengths', 1);('] rst introduces', 1);('structure [', 1);('visual tasks', 1);('massive improvements', 1);('ef- cient transformer training', 1);('] employ', 1);('] pyramid structure', 1);('dense prediction tasks', 1);('2d convolution', 1);('optimization difculty', 1);('local inductive bias', 1);('works [', 1);('hybrid models', 1);('mod- els', 1);('low computational power', 1);('tao', 1);('] intro- duce', 1);('additional learnable tokens', 1);('global depen- dencies', 1);('] design', 1);('parallel structure', 1);('two-way bridge', 1);('borrowing convolution operation', 1);('] absorbs', 1);('] fuse', 1);('blocks [', 1);('vision tasks', 1);('current efcient approaches', 1);('strong train-', 1);('balance parameters', 1);('easy-to-use model', 1);('sim- ple', 1);('methodology', 1);('meta mobile', 1);('m2', 1);('consistent essence expression', 1);('different instantiations', 1);('basic motivation', 1);('image input', 1);('ex- pansion', 1);('ewith output/input ratio', 1);('xe=mlp', 1);('complexity', 1);('maximum path length', 1);('input/output', 1);('rc\x02w\x02w', 1);('l=w2', 1);('wandware', 1);('map size', 1);('window size', 1);('kandgare kernel size', 1);('group number', 1);('module', 1);('params flops mpl mhsa', 1);('c 8c2l+4cl2+3l2o', 1);('c 8c2l+ 4cll + 3llo', 1);('inf', 1);('ck2=g+1', 1);('intermediate operator', 1);('fenhance', 1);('fur- ther', 1);('identity operator', 1);('static convolution', 1);('ef- cient network design', 1);('fas', 1);('xf=f', 1);('input/output ratio equaling\x15to', 1);('xs=mlp', 1);('xf', 1);('residual connection', 1);('nal output', 1);('y=x+xs', 1);('notice', 1);('activation functions', 1);('inverted residual mobile block based', 1);('model local', 1);('fin', 1);('andconvolution operations', 1);('naive implementation', 1);('unaffordable expenses', 1);('main reasons', 1);('intermediate dimen- sion', 1);('input dimension', 1);('total image pixels', 1);('specic inuences', 1);('window-mhsa', 1);('trade-off cost', 1);('no-', 1);('attention matrix', 1);('xfor', 1);('q=k=x', 1);('v=xe', 1);('expanded window mhsa', 1);('ew- mhsa', 1);('skip', 1);('model 3i', 1);('conv1x1 convmetamobile block1x1 conv1x1 conv metaformerfigure', 1);('diagram', 1);('irmb .right', 1);('structure', 1);('resnet-like emo', 1);('toy experiment', 1);('] 5.7m', 1);('w/irmb 4.9m', 1);('-156m #', 1);('+2:1 %', 1);('] 13.2m', 1);('w/irmb 11.7m', 1);('-98m #', 1);('+0:3 %', 1);('k\x001 +', 1);('efcient equivalent', 1);('channel-consistent projection', 1);('attention matrix times', 1);('\x15 >', 1);('fortunately', 1);('informa- tion ow', 1);('xto', 1);('only linear operations', 1);('equivalent proposition', 1);('head number', 1);('multiplication result', 1);('matrix multiplication', 1);('boosting existing', 1);('assess irmb performance', 1);('training setting', 1);('columnar structure', 1);('efcient visual models', 1);('mobile ap- plications', 1);('simple', 1);('complex operators', 1);('pos- sible', 1);('model complexity', 1);('good performance', 1);('ef-', 1);('fewer', 1);('accuracy trade-off', 1);('criterion', 1);('efciency', 1);('satised', 1);('partially', 1);('method', 1);('criterion mobilenet', 1);('ours', 1);('bilenet series [', 1);('recent mobilevit', 1);('signicant per- formances', 1);('pretty results', 1);('basic blocks', 1);('above criteria', 1);('4- phase', 1);('overall framework', 1);('re- cent efcient methods [ 37,39,40 ]', 1);('specic module', 1);('com- plex operators', 1);('inductive bias', 1);('variant settings', 1);('ex- pansion rates', 1);('channel numbers', 1);('congu- rations', 1);('] +silu [', 1);('] +gelu [', 1);('basic classi- cation', 1);('core', 1);('items emo-1m emo-2m emo-5m depth', 1);('emb', 1);('dim', 1);('exp', 1);('ratio', 1);('light-weight methods', 1);('experiments', 1);('image classication setting', 1);('various training recipes', 1);('meth- ods [', 1);('unfair comparisons', 1);('training recipe', 1);('model persuasion', 1);('subsequent fair comparisons', 1);('supp', 1);('dataset [', 1);('extra datasets', 1);('] optimizer', 1);('weight decay 5e\x002', 1);('learning rate 6e\x003', 1);('cosine', 1);('scheduler [', 1);('warmup epochs', 1);('label smoothing', 1);('stochastic depth [', 1);('layerscale', 1);('token labeling', 1);('training [', 1);('timm', 1);('gpus', 1);('sotas', 1);('small scales', 1);('quantitative results', 1);('method obtains', 1);('com- plex modules', 1);('mobilevitv2-like', 1);('strong training recipe', 1);('top-', 1);('cnn-based mobilenetv3-l-0.50', 1);('transformer-based mobilevitv2-0.5', 1);('larger meo-2m', 1);('mobilevit-xs', 1);('edgevit-xxx', 1);('] obtains', 1);('+78 %', 1);('+27 %', 1);('con-', 1);('superior trade-off', 1);('/ #', 1);('contemporary counterparts', 1);('sur-', 1);('training recipes', 1);('contemporary methods', 1);('please', 1);('clearer comparisons', 1);('super-params.mnetv3', 1);('iccv19vit', 1);('iclr21deit', 1);('icml21mvitv1', 1);('iclr22mvitv2', 1);('] arxiv22edgenext [', 1);('] arxiv22emo', 1);('ours epochs', 1);('optimizer rmsprop adamw adamw adamw adamw adamw adamw learning', 1);('rate 6.4e\x0023e\x0031e\x0032e\x0032e\x0036e\x0036e\x003', 1);('learning', 1);('rate decay 1e\x0053e\x0015e\x0021e\x0025e\x0025e\x0025e\x002', 1);('warmup', 1);('label', 1);('path rate', 1);('9/0.5 9/0.5 9/0.5', 1);('position', 1);('classication', 1);('blue backgrounds', 1);('dis- play', 1);('subsequent experiments', 1);('reso', 1);('pub mnetv1-0.50', 1);('mnetv3-l-0.50', 1);('iccv19 mvitv1-xxs', 1);('iclr22 mvitv2-0.5', 1);('eccvw22 emo-1m', 1);('mnetv2-1.40', 1);('cvpr18 mnetv3-l-0.75', 1);('iccv19 mocovit-1.0', 1);('cvm22 mvitv1-xs', 1);('iclr22 mformer-96m', 1);('cvpr22 edgenext-xs', 1);('eccvw22 edgevit-xxs', 1);('eccv22 emo-2m', 1);('mnetv3-l-1.25', 1);('iccv19 efcientnet-b0', 1);('icml19 deit-ti', 1);('icml21 xcit-t12', 1);('nips21 lightvit-t', 1);('iclr22 mvitv2-1.0', 1);('eccvw22 poolformer-s12', 1);('cvpr22 mformer-294m', 1);('cvpr22 mpvit-t', 1);('cvpr22 edgevit-xs', 1);('eccv22 emo-5m', 1);('com- parison', 1);('library [', 1);('re- place', 1);('corresponding coun- terparts', 1);('obvious advantages', 1);('2.3m parameters', 1);('boosts +2.1', 1);('detection performance', 1);('# map', 1);('mnetv1', 1);('mnetv3', 1);('mvitv1-xxs', 1);('resnet50', 1);('mvitv2-1.25', 1);('detection results', 1);('apsapm apl resnet-50', 1);('pvtv1-tiny', 1);('edgevit-xxs', 1);('sota mobilevit', 1);('consis-', 1);('mobilevit-', 1);('s [', 1);('fur-', 1);('qualitative visualizations', 1);('indi- cate', 1);('accurate information', 1);('mmseg-', 1);('mentation library [', 1);('opti- mizer', 1);('sota transformer-based mobilevitv2', 1);('vari- ous scales', 1);('segmentation frameworks', 1);('obtains 33.5/35.3/37.8 miou', 1);('qualitative', 1);('zoom', 1);('segmentation performance', 1);('backbonedeeplabv3', 1);('] #', 1);('miou #', 1);('mvitv2-1.0', 1);('efcient irmb', 1);('consistent conclusions', 1);('back- bone network', 1);('qualitative segmen- tation results', 1);('emo-based', 1);('stable results', 1);('comparison ap- proach', 1);('consistent bathtub', 1);('baseball eld segmentation results', 1);('throughput comparison', 1);('present through-', 1);('evaluation results', 1);('sota edgenext', 1);('test platforms', 1);('amd epyc', 1);('v100 gpu', 1);('speed boosts', 1);('+20 %', 1);('+116 %', 1);('simple irmb', 1);('complex structures', 1);('module [', 1);('normalization type', 1);('throughput', 1);('method flops cpu gpu edgenext-xxs', 1);('edgenext-xs', 1);('tremendous negative impact', 1);('vision models', 1);('optimiza- tion', 1);('4a shows', 1);('s-34', 1);('s-1234', 1);('throughput decreases sig-', 1);('accidentally', 1);('nans', 1);('neces- sary', 1);('mhsa-included emo', 1);('4b illustrates', 1);('affects model accuracy', 1);('efcient model obtains', 1);('4c explores', 1);('training parameter', 1);('range [', 1);('uctuates accuracy', 1);('4d explores', 1);('low', 1);('performance degradation', 1);('high batch size', 1);('sufferfrom performance saturation', 1);('compo- nents', 1);('ew-mhsa dw-conv top-1', 1);('components', 1);('fcontains', 1);('ablation experiment', 1);('rst row', 1);('fdegenerates', 1);('respectable result', 1);('component contributes', 1);('experiment proves', 1);('explanatory analysis dw-convmhsamlp', 1);('distri- butions', 1);('expansion/shrinkage operations', 1);('mhsa.distributions', 1);('displays distribu- tions', 1);('low proportion', 1);('% /4.1 %', 1);('% /14.6 %', 1);('attention visualizations', 1);('illus- trate', 1);('different models', 1);('cnn-based resnet', 1);('specic objects', 1);('global features', 1);('salient objects', 1);('global regions', 1);('various tasks', 1);('feature similarity visualizations', 1);('convolution', 1);('andmhsa operations', 1);('resnet', 1);('imagedw-convw-mhsaboth figure', 1);('diagonal', 1);('s-3', 1);('different components', 1);('diagonal pixels', 1);('stage-3', 1);('different compositions', 1);('short-distance correlations', 1);('long-distance correlations', 1);('distant locations', 1);('high similarities', 1);('comparative analysis', 1);('irmb q k v emo', 1);('paradigm', 1);('metaformer.yuet', 1);('] point', 1);('transformer/mlp-like', 1);('general architec- ture', 1);('similar con- clusions', 1);('frame- work', 1);('meta block pro- vides', 1);('illustrates paradigms', 1);('meta mo-', 1);('motiva- tion', 1);('high-performancetransformer/mlp-like models', 1);('to- ken mixer', 1);('efcient modules', 1);('model per- formances', 1);('different efcient opera- tor.2', 1);('sub- modules', 1);('w/ 5.1m #', 1);('poolformer-s12', 1);('w/ 11.9m #', 1);('conclusion', 1);('paper explores efcient architecture design', 1);('mo- bile applications', 1);('modern irmb', 1);('basic module contains', 1);('massive experiments', 1);('limitation', 1);('basic studies', 1);('cnn/mhsa-combined', 1);('cur- rent', 1);('complex op- erators', 1);('performer', 1);('upper limits', 1);('efcient model structure', 1);('resolution input', 1);('neural architec-', 1);('search', 1);('nas', 1);('heavy models', 1);('imagenet-21k', 1);('training aug- mentations/strategies [', 1);('limited', 1);('current computational power', 1);('near future', 1);('corresponding results', 1);('code repository', 1);('8references [', 1);('alaaeldin ali', 1);('mathilde caron', 1);('piotr bo-', 1);('armand joulin', 1);('ivan laptev', 1);('na-', 1);('neverova', 1);('jakob verbeek', 1);('xcit', 1);('cross-covariance', 1);('jimmy lei ba', 1);('jamie ryan kiros', 1);('geoffrey e hinton', 1);('layer', 1);('arxiv preprint arxiv:1607.06450', 1);('han cai', 1);('chuang gan', 1);('tianzhe wang', 1);('zhekai zhang', 1);('once-for-all', 1);('train', 1);('kai chen', 1);('jiaqi wang', 1);('jiangmiao pang', 1);('yuhang cao', 1);('yu xiong', 1);('xiaoxiao li', 1);('shuyang sun', 1);('wansen feng', 1);('jiarui xu', 1);('dazhi cheng', 1);('chenchen zhu', 1);('tian-', 1);('cheng', 1);('qijie zhao', 1);('buyu li', 1);('xin lu', 1);('rui zhu', 1);('yue wu', 1);('jifeng dai', 1);('jingdong wang', 1);('wanli ouyang', 1);('loy', 1);('dahua lin', 1);('open mmlab detection toolbox', 1);('arxiv preprint arxiv:1906.07155', 1);('george papandreou', 1);('florian schroff', 1);('atrous convolution', 1);('semantic image segmentation', 1);('arxiv preprint arxiv:1706.05587', 1);('yinpeng chen', 1);('dongdong chen', 1);('xiaoyi dong', 1);('zicheng liu', 1);('mobile-former', 1);('bridging', 1);('krzysztof marcin choromanski', 1);('valerii likhosherstov', 1);('david dohan', 1);('xingyou', 1);('andreea gane', 1);('tamas sarlos', 1);('peter hawkins', 1);('jared quincy davis', 1);('afroz mohiuddin', 1);('david benjamin belanger', 1);('lucy j colwell', 1);('adrian weller', 1);('mmsegmentation contributors', 1);('mmsegmenta-', 1);('openmmlab', 1);('semantic segmentation toolbox', 1);('com /', 1);('ekin d cubuk', 1);('barret zoph', 1);('jonathon shlens', 1);('search space', 1);('cvprw', 1);('jia deng', 1);('wei dong', 1);('richard socher', 1);('li-jia li', 1);('kai li', 1);('li fei-fei', 1);('imagenet', 1);('large-scale hierarchical image database', 1);('incvpr', 1);('ieee', 1);('alexey dosovitskiy', 1);('lucas beyer', 1);('alexander kolesnikov', 1);('dirk weissenborn', 1);('xiaohua zhai', 1);('thomas unterthiner', 1);('mostafa dehghani', 1);('matthias minderer', 1);('georg heigold', 1);('syl-', 1);('gelly', 1);('jakob uszkoreit', 1);('neil houlsby', 1);('transformers', 1);('stphane', 1);('leavitt', 1);('ari', 1);('morcos', 1);('giulio biroli', 1);('levent sagun', 1);('convit', 1);('improving', 1);('soft convolutional inductive biases', 1);('inicml', 1);('shang-hua gao', 1);('ming-ming cheng', 1);('kai zhao', 1);('xin-yu zhang', 1);('ming-hsuan yang', 1);('philip torr', 1);('newmulti-scale backbone architecture', 1);('ieee tpami', 1);('kai han', 1);('yunhe wang', 1);('qi tian', 1);('jianyuan guo', 1);('chunjing xu', 1);('ghostnet', 1);('cheap operations', 1);('shaoqing ren', 1);('residual learning', 1);('dan hendrycks', 1);('kevin gimpel', 1);('gaussian', 1);('error linear units', 1);('arxiv preprint arxiv:1606.08415', 1);('grace chu', 1);('yukun zhu', 1);('searching', 1);('mo- bilenetv3', 1);('andrew g howard', 1);('dmitry kalenichenko', 1);('tobias weyand', 1);('marco an-', 1);('mobilenets', 1);('efcient', 1);('convolu- tional neural networks', 1);('arxiv preprint arxiv:1704.04861', 1);('gao huang', 1);('yu sun', 1);('zhuang liu', 1);('daniel sedra', 1);('kilian q weinberger', 1);('stochastic depth', 1);('tao huang', 1);('lang huang', 1);('shan', 1);('fei wang', 1);('chen qian', 1);('lightvit', 1);('towards', 1);('light-weight convolution-', 1);('free vision transformers', 1);('arxiv preprint arxiv:2207.05557', 1);('forrest n iandola', 1);('moskewicz', 1);('khalid ashraf', 1);('william j dally', 1);('kurt keutzer', 1);('alexnet-level', 1);('mb model size', 1);('arxiv preprint arxiv:1602.07360', 1);('accelerating', 1);('deep network training', 1);('internal co- variate', 1);('zi-hang jiang', 1);('qibin hou', 1);('li yuan', 1);('daquan zhou', 1);('yujun shi', 1);('xiaojie jin', 1);('anran wang', 1);('tokens matter', 1);('token', 1);('inneurips', 1);('nikita kitaev', 1);('anselm levskaya', 1);('re-', 1);('efcient transformer', 1);('youngwan lee', 1);('jonghee kim', 1);('jeffrey willette', 1);('sung ju hwang', 1);('mpvit', 1);('multi-path', 1);('wei li', 1);('huixia li', 1);('rui wang', 1);('xin pan', 1);('next-vit', 1);('generation vision transformer', 1);('real- istic', 1);('industrial scenarios', 1);('arxiv preprint arxiv:2207.05501', 1);('kunchang li', 1);('yali wang', 1);('gao peng', 1);('guanglu', 1);('yu liu', 1);('yu qiao', 1);('uniformer', 1);('unied', 1);('efcient spatial-temporal representation learning', 1);('priya goyal', 1);('ross girshick', 1);('focal', 1);('dense object detection', 1);('michael maire', 1);('serge belongie', 1);('james hays', 1);('pietro perona', 1);('deva ramanan', 1);('lawrence zitnick', 1);('microsoft', 1);('common objects', 1);('hanxiao liu', 1);('karen simonyan', 1);('yiming yang', 1);('darts', 1);('differentiable architecture search', 1);('zhuliang yao', 1);('zhenda xie', 1);('jia ning', 1);('li dong', 1);('transformer v2', 1);('scaling', 1);('stephen lin', 1);('baining guo', 1);('hierarchical', 1);('sgdr', 1);('stochastic', 1);('gradient descent', 1);('warm restarts', 1);('decoupled', 1);('weight decay regularization', 1);('hailong ma', 1);('mocovit', 1);('convolutional vision transformer', 1);('arxiv preprint arxiv:2205.12635', 1);('ningning ma', 1);('hai-tao zheng', 1);('efcient cnn architec- ture design', 1);('muhammad maaz', 1);('abdelrahman shaker', 1);('hisham cholakkal', 1);('salman khan', 1);('syed waqas zamir', 1);('rao muhammad anwer', 1);('fahad shahbaz khan', 1);('cnn-transformer architecture', 1);('ineccvw', 1);('marjan ghazvininejad', 1);('srinivasan iyer', 1);('luke zettlemoyer', 1);('delight', 1);('light-weight transformer', 1);('light-', 1);('vision trans-', 1);('separable self- attention', 1);('mobile vision transformers', 1);('arxiv preprint arxiv:2206.02680', 1);('linda shapiro', 1);('espnetv2', 1);('power ef- cient', 1);('general purpose convolutional neural network', 1);('junting pan', 1);('adrian bulat', 1);('fuwen tan', 1);('xiatian zhu', 1);('lukasz dudziak', 1);('georgios tzimiropoulos', 1);('brais martinez', 1);('edgevits', 1);('competing', 1);('light-weight cnns', 1);('adam paszke', 1);('sam gross', 1);('adam lerer', 1);('james bradbury', 1);('gregory chanan', 1);('trevor killeen', 1);('zeming lin', 1);('natalia gimelshein', 1);('luca antiga', 1);('imperative style', 1);('learning library', 1);('andrey zh-', 1);('inverted', 1);('linear bottlenecks', 1);('ramprasaath r selvaraju', 1);('michael cogswell', 1);('abhishek das', 1);('ramakrishna vedantam', 1);('devi parikh', 1);('dhruv batra', 1);('grad-', 1);('visual', 1);('deep networks', 1);('nitish srivastava', 1);('geoffrey hinton', 1);('alex krizhevsky', 1);('ilya sutskever', 1);('ruslan salakhutdinov', 1);('simple way', 1);('mnasnet', 1);('platform-aware', 1);('neural architecture search', 1);('quoc', 1);('efcientnet', 1);('convolutional neural networks', 1);('training', 1);('data-efcient image transformers', 1);('atten- tion', 1);('going', 1);('ashish vaswani', 1);('noam shazeer', 1);('niki parmar', 1);('jakob uszko-', 1);('llion jones', 1);('aidan n gomez', 1);('kaiser', 1);('illia polosukhin', 1);('shakti n wadekar', 1);('abhishek chaurasia', 1);('mobilevitv3', 1);('mobile-friendly', 1);('effective fusion', 1);('arxiv preprint arxiv:2209.15159', 1);('pyra-', 1);('mid vision transformer', 1);('versatile backbone', 1);('dense pre- diction', 1);('improved', 1);('pyramid vision transformer', 1);('compu-', 1);('visual media', 1);('ross wightman', 1);('image models', 1);('com / rwightman / pytorch', 1);('haiping wu', 1);('bin xiao', 1);('noel codella', 1);('lei zhang', 1);('cvt', 1);('introducing', 1);('weijian xu', 1);('yifan xu', 1);('tyler chang', 1);('zhuowen tu', 1);('co-', 1);('scale conv-attentional image transformers', 1);('yufei xu', 1);('qiming zhang', 1);('jing zhang', 1);('vitae', 1);('vision', 1);('intrinsic inductive bias', 1);('weihao yu', 1);('mi luo', 1);('pan zhou', 1);('chenyang si', 1);('yichen zhou', 1);('xinchao wang', 1);('shuicheng yan', 1);('kun yuan', 1);('shaopeng guo', 1);('aojun zhou', 1);('fengwei yu', 1);('wei wu', 1);('incorporating', 1);('convolution designs', 1);('visual transformers', 1);('sangdoo yun', 1);('dongyoon han', 1);('seong joon oh', 1);('sanghyuk chun', 1);('junsuk choe', 1);('youngjoon yoo', 1);('regu-', 1);('larization strategy', 1);('strong classiers', 1);('localizable features', 1);('hongyi zhang', 1);('moustapha cisse', 1);('yann n. dauphin', 1);('david lopez-paz', 1);('empirical risk minimiza- tion', 1);('xiangtai li', 1);('yibo yang', 1);('eatformer', 1);('arxiv preprint arxiv:2206.09325', 1);('chao xu', 1);('jian li', 1);('wenzhou chen', 1);('ying tai', 1);('shuo chen', 1);('feiyue huang', 1);('analogous', 1);('design-', 1);('sequence model', 1);('advances', 1);('neural informa-', 1);('processing systems', 1);('qinglong zhang', 1);('yu-bin yang', 1);('rest', 1);('efcient trans-', 1);('visual recognition', 1);('xinyu zhou', 1);('mengxiao lin', 1);('efcient convolutional neural net- work', 1);('hengshuang zhao', 1);('xiaojuan qi', 1);('xiaogang wang', 1);('jiaya jia', 1);('pyramid', 1);('zhun zhong', 1);('liang zheng', 1);('guoliang kang', 1);('shaozi li', 1);('yi yang', 1);('aaai', 1);('vol- ume', 1);('bolei zhou', 1);('hang zhao', 1);('xavier puig', 1);('tete xiao', 1);('sanja fidler', 1);('adela barriuso', 1);('antonio torralba', 1);('ade20k dataset', 1);('ijcv', 1);