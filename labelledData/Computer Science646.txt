('adamw', 18);('cosine decay', 17);('proceedings', 7);('figure', 6);('vtm', 6);('international conference', 6);('hitea', 5);('comparison', 5);('moment information', 5);('mtre', 5);('vlp', 5);('ice cream licks fingers pleasure', 4);('specically', 4);('mlm', 4);('details', 4);('video question', 4);('ssv2template', 4);('msrvtt', 4);('cme', 3);('nextqa', 3);('following', 3);('singularity', 3);('gray methods', 3);('gap', 3);('retrieval task', 3);('proceedings ieeecvfconference computer vision pattern recognition', 3);('vision', 3);('proceedings ieeecvf', 3);('computer vision', 3);('coco', 3);('videolanguage tasks', 2);('furthermore', 2);('temporalorienteddatasets eg', 2);('ssv2template ssv2label', 2);('frame order', 2);('multimodal temporal relation exploration', 2);('lavender', 2);('fom', 2);('encodertext encodercls caucasian', 2);('multimodal pairs', 2);('positive words', 2);('alpro', 2);('bridgeformer', 2);('r1r5r10', 2);('vtc', 2);('prexlm', 2);('didemo', 2);('lsmdcfib', 2);('msvd', 2);('kis', 2);('data fair comparisonmethod', 2);('hiteaand', 2);('loss terms', 2);('retrieval dataset', 2);('evaluation', 2);('caption', 2);('dependency', 2);('shued', 2);('ssv2 template', 2);('videoqa', 2);('large performance drop', 2);('vqa', 2);('activitynet caption', 2);('crossmodal moment exploration', 2);('ieeecvf', 2);('computer vision patternrecognition', 2);('machine learning', 2);('advances', 2);('pmlr', 2);('inproceedings ieee', 2);('computer visionand pattern recognition', 2);('proceedings ieee', 2);('computer vision pattern recognition', 2);('advancesinneuralinformationprocessingsystems', 2);('learning', 2);('retrieval', 2);('imagenet21k', 2);('variations', 2);('ssv2label', 2);('object information', 2);('hiteaon', 2);('video captioning', 2);('youtube', 2);('gifs', 2);('qa', 2);('hitea hierarchical temporalaware videolanguage pretrainingqinghao ye guohai xu ming yanhaiyang xuqi qian ji zhang fei huangdamo', 1);('alibaba groupabstractvideolanguage', 1);('howevermostpreviousmethodsdirectlyinheritoradapttypicalimagelanguage', 1);('unique characteristic video ie temporal paper', 1);('temporal aware', 1);('videolanguage pretrainingframework', 1);('tasks formodelingcrossmodalalignmentbetweenmomentsandtextsaswellasthetemporalrelationsofvideotextpairs', 1);('specicallyweproposeacrossmodalmomentexplorationtasktoexplore', 1);('moments videos results', 1);('besidestheinherenttemporalrelationsarecapturedbyaligningvideotextpairsasawholeindierenttimeresolutionswithmultimodaltemporalrelationexploration', 1);('introduce shuingtesttoevaluatethetemporalrelianceofdatasetsandvideolanguage', 1);('videolanguage understanding generation tasks', 1);('hiteaalso', 1);('strong generalization ability', 1);('tasks zeroshot manner', 1);('modelsand', 1);('modelscope1 introductionvision', 1);('primary signals constitute realworld perception humanity', 1);('largescale', 1);('videolanguage pretraininghelps model', 1);('eective multimodal representation', 1);('signicant improvement variety videolanguage', 1);('tasks videotext retrieval video question', 1);('authoralongviewshortviewa caucasian', 1);('videoboy', 1);('licks fingersbalignmenttexteventtextalignmentmomenttextalignmentmultimodalalignmenta', 1);('caucasian', 1);('global perspective pretext b introducehiteabyvaryingvideoindierenttemporalviewsandmodelingcrossmodal alignment moments texts', 1);('thetemporal relations multimodal', 1);('success imagelanguage', 1);('clipbert26 singularity', 1);('representationsfrom image encoders aggregate', 1);('score aggregation function temporal encoder', 1);('furthermore milnce44andfrozen2switchimageencodertovideoencoderforspatiotemporalvideorepresentationlearningandalign', 1);('corresponding text addition', 1);('promising performance', 1);('temporal information relations areessential videolanguage pretraining1arxiv221214546v1 cscv', 1);('dec', 1);('less', 1);('atomic actions moments', 1);('time resolutions generatetwoviewslongshortfortheinputvideo', 1);('asaresulttheshotviewvideocliptendstorepresentthemomentinformation', 1);('longview video', 1);('express eventlevelinformation example shortview video clip', 1);('moment lick ngers ratherthan', 1);('ice cream', 1);('longview video underglobaleventperspective2ignoringthetemporalrelationsimplicitlyexistedinthevideo', 1);('knowingtheeventexpressedby', 1);('text moment', 1);('ice cream inferredfromthemomentlickngersshownbyshortviewvideohowever', 1);('implicit temporal relations moments', 1);('previous worksto address problems', 1);('hierarchicaltemporalaware', 1);('hiteaintroducestwonoveltemporalawarevideolanguagepretrainingtasksnamedcrossmodal', 1);('moment exploration', 1);('moments partial crossmodalalignmentbutalsocapturetemporalrelationsbetweenmultimodal pairs', 1);('rst generate longview shortview videos dierenttimeresolutionstobuildhierarchyoftheinputvideo', 1);('thenbased', 1);('similarities words shortview videowe', 1);('select relevant words', 1);('therest words', 1);('hard negatives', 1);('positive words shotviewvideo representations', 1);('moreovertocaptureassociationbetweenmomentsandtheeventwe', 1);('howeverdirectly', 1);('noisy dueto background similarity', 1);('multimodal alignment videotext pairs viathe', 1);('empowered', 1);('moment information andtemporal relations dierent', 1);('videoin spite good performance', 1);('recent studies 425revealmostvideolanguagedownstreamdatasetsarebiasedtowardsstillobjectsscenes etcwhilethetemporaldynamics negligible', 1);('temporal performance ofthevideolanguagepretrainingmodelandtemporalrelianceof', 1);('datasets introduce temporal shuingtest datasets', 1);('comprehensiveevaluation temporal', 1);('capability videolanguage', 1);('method achievessignicantimprovementonthedatasetswithheavytemporalreliancein summary key contributions followingsweproposeanovelhierarchicaltemporalawarevideolanguage', 1);('framework videolanguage understanding generation capabilitieswe introduce', 1);('additional temporalaware pretrainingtasks', 1);('crossmodal multimodalalignment', 1);('model moment information', 1);('semantics alsocapturetemporalrelationsbetweenmomentsandeventextensive experiments', 1);('videotext retrieval video question answeringand video', 1);('improvement respectively2', 1);('related workvideolanguage pretraining beneting', 1);('largenumber imagevideotext pairs videolanguage', 1);('superior capabilities variousvideotext benchmarks method', 1);('traditional', 1);('dense video', 1);('expensive computation overhead contrast', 1);('clipbert', 1);('suggests sparse', 1);('canenable aordable endtoend learning', 1);('recent', 1);('newmodel architectures', 1);('frozen', 1);('image video datasets', 1);('videotextcontrastive learning', 1);('vtc alpro', 1);('vtcvideotext', 1);('regionentity alignment', 1);('anddownstream tasks', 1);('videotext tasks', 1);('andmaskedvideomodelingmvm11aredesignedforvlphowever temporal characteristic video', 1);('end introduce novel hierarchical temporalaware', 1);('framework onlymodels', 1);('modeling', 1);('temporal characteristic acts asa', 1);('vital role', 1);('model the2longviewshortviewvideo', 1);('crossmodal moment explorationlongview featurestextmultimodal encodersharedmultimodal encodersharedlicksfingersboyhewithmultimodaltemporal relation explorationshortview featuresrankingtext featureslatexit', 1);('sha1base64vqmqpkrkhprxtjoaq9uodw1euqaaac3xicjvhlssnafd2n73fvjeamwarxjzwilotfckggykvqljkztju0l5kjwelduro3obbr3xdqvvdom4aprcunonhvpmbn32qhrxnkyxnlgyojymtk1ptm7nz8qn5xqr4hscrfjqduej3zlbau44uadkqrzsjimm92xandq6r46awiyifwt2qfc2pnfto1fmetxorzq9ji84c9odqtttsnel0rh3mdqzhesoqwxroumlbato6cdoa6caarwipaj4kyrcmmt0nlgahjk6flliikkpjagnmkzahleezjngefc9p18hynbkm9zqtqe49eaknlfomodyislqnfphe2s2n8u2p7tanv515ecrkxbd7l26yvdqkwiix1dg0m1hzpr1fhmjdfdutc3p1ulysektueoxspcxcuhfta1jta1q94yhxvmypve57ljnhtt6qbl76p8yeobxzlw8xycblq2c1gpylvrggd5rmncvzxhbp5xmbj3gy2sancwvcfaqauuyzjcluh8h4giz5alatexitlcmelatexit sha1base64syzebbqdr1dgfaeolqpdkbmwwkeaaac3nicjvhlssraeczja31hpymx4cj4whir9sik4efhfvcfv9bjoo4g8ykzibkcn2i1rwqp8johht1jbbitkhsu91vm93txr6xstt7jk6e3r7isdg0pdi6nj5vjexhplcrcnhvlrcucyvphekbrsk744ibpbatcx7zmorvn4sk9ajwv17g4ihg7da79titrlxmqwbaziczp98swnltiguzb3urbdfy6zanvsv6ydwslbfueqryqmthcbi0magrcssaglj5dolare3eenliekkfjagugsztrlqamruwzfdu0oyzzkpbkm9vqtqf49caktdblmojyeslqnevhm2s2n8c2p7nzjf7f0coiv6bd7l4j8786vyvekzz1dr7vfgtgvcdll0x3rd3clsvjieyoivpkj4q5lr50wdla1jduot0exnalytedlbozxdusaspn9ndb3nznwawtbc9uv1bluvcwjrnm0tyxsiin1neg7yvc4wgpxrfxbdwytprlepmcsxzdy9af0imlelatexitlmtrelatexit sha1base64oslaorrfxc9mt3fyzimaw3vz2eaaacxnicjvhlssnafd2nr1pfvzdugkvwvrip6rlopgsxfe0dteoyndaharime6uuwr9wq58mohhxfgfnqioihjmxpvotp3xj8orkic5zvnzczozskfwtlyyura8x1jwyspzlxboucslz9lgbchldcrxwdiy5nqd3vihxzreuuuyevf4rkyx7wy9fih6gnmkqlpm1cl1sesuhbpsaebmoirs1apicy7rrqsgfenwhfcea3hi6lmacwcxcr2mizoehilz3kna2psyogv4xa7o26fdrcagtneeivezoiwgv5lsxg5pisqthpvptomnxlmzv3mpjae24jfuy1jfbhhtidjpmp0lqo9hjoabnuug0zxxzkx1hrf39zupuih5g4jbsul4szuu76bbtnymrxvfvmm1kalbvwzab4l3fkgbshznngjuld39cuw0uqoezapoywvb2kv5hqckgupokhcfj3jcs1wzqiu17j5trvym2cs3zt18ap2lkcilatexitvllatexit sha1base64xe7hddovrhjrrax8sezzrlv78ueaaacxnicjvhlssnafd2nr1pfvzdugkvwvrirdvl002wl9gg1sjkd1qfpeiytprtbh3crnybgf6fd8yu1ci6icmzc85mdepw5eohznnwfnzs8sluwxcyurasbxc2tzhklkvegi4jitn0v4yeieumjffb2llk38gpe8odnot665tiruxihxjhvjrxbkpqceyqoevoqfl0sowxhlhswubkoivu1qpicsqqgshfcbwhfoeahhj6ondhicauiwlxkpawcy57feibuhandiyix0htotkbeh77zkynantanolkw3sksaipelyn2abegqcnfub98r46run6e9nxinifw6ius3zfyvttei0mejqufqtbfhdhusc0lnvtn7s9vkxkiido4r3fjmbnltm20ssmdt1bz8tftkzm9z5lusne9s1pwo7pcc6c5khzpsofnhwkqfzqppyws72az7hqkckghrkpcajnvbsva3qsq27z1qrl2m28w1zdx8ovjaplatexitvslatexit sha1base64yquoltw7ylyocszrcrynxktomsmaaadoxicnvllssnafd2nwtakq7cbivgqkoqrv0w3bhutcrywplptibmrtipltcpcauf4je4dkduqhvjcp4wadoshlm3hvojgfgitw3ezz1lzngrsfy4xotu9mzs3pzhelccrkmmen1fnphforycffcgnefkzxgsxc9h2pnzi9xvk6fm4ccpgsawi3vttbub2xgylolqfpybjz1hk2sifiky5ixd4flhq1cyypya5ldq0aaepfbdyi6pbtoiwzdcb0caqdidjysum1rgiskuiyy4mjcr6hxdtje2ps5ohtaxpxp2axam2ydm0jnrakarehthpdsxspqqmlccjvt1vpllnnvvdplkb9tqg9heneclwq5vurfounva0h3znd5u7teqgg22vkeujryqr2thtkqrmzs7mu8weoutesdymekyykexblppkk6hk5c7zqv6goiur50z3pniux0nnpl5thwfxxvlyma5elat1xb0szramlawrqdlczxsyr918s5whwvcglfgvffgpl22gjmtwcshytyalsdrbqlatexitvsclslatexit sha1base64onwx4yrupgwysmhmlcgytuikzy8aaadoxicnvllssnafd2nwtakq7cbivgqkoqrv0w3bhwowbvsluk02knzytkuiyhhnwp8evcelo3fod3hlh8iepnjdkzln3nenojbn5biis6y5njiyo5ccnjqemz2bn5gvfhemktgpg6yz0wvjusrpuuqgvc1d4dskue07hj9xeruyftlncekgwzeyrlzp293a7bjmfks1cksnx86w1bwepxszmxlhspzvahzjutncyvokjbcxochmvchg20eyihhqoaikwbxsjxweowejexbmzctehv9u5hpgibupdndpsynv07nlstlmbzavnotsmvvhojklpypu0ifxfhovqpqqnylmy33lnyln24dejvbyirw4ipy33vvnf3xrtkbw79nuv43d5muqafbkigxeosui7nj2ivvmctczhezcxkiijo4tfwymfpkt100lszrycqds1x9qxvkvs6z7k3xklszkl82n5co43ypxncvwwwqrt6jm0gwwsyi1oyxzq2mmb6usd4qrxudfujxvjwxh6btvywroid8n4fggn5kwnlatexitvlclslatexit sha1base64ho6jeonrwfslgws1s9uahbo771caaadtnicnvllsurafd2d9terltn6sbycdjik4ios9gng0hrvsfok1sxgsylvkwxcf0pfo1bqs38wf6a5eb5ulocoogxwsunxupacqp26qragqjvo7yluhboegayojymjibru9p7ii1yxlssjdl8mpafj8ket2qoi36y5dypg4gfbbcbknq5bki02rp9jjhptnsxgaml8s1k78spuuqkv5qli0sf7k7yhfbrpo81kmlumutk12v3643nkajh0xce3qgbnb6vrlcb46smfqiazhaklxbbcnio4cjardoyssjyiuoc5hglbkfvncp8qioe0ari4mmtfaaqrmz7rlrmxptxhxxuqrlkva72tpfagwffqzdak11th7ngdgkczu4jqr3lvld3kltgf8j38vtf71jvteqdy1q6f5fimeeudmyqf9lz5yrztjjcrpiko5tpkwaaxaltuyi7ay6ovn73wlqtwamdocdqu1du393ymdhfblrlzawdpcbauumkgmywi3nqlhwsyrpbajh2fa5xg1vrznq0nqzn11kryjg8ceo1l4ajqwywwlatexitvsclsvs1vsmlatexit sha1base644l3hju2elrn5ifgzlpndifzmyaaadtnicnvlltttafd0xbzqmlq10ycyiqlqhfnkoalmisumcslrqhhkgyj4mwypf8owjkov6newpzqlx9af4bd1d4zjlihaqqozcdc85mz5zgywkhxscy5q19orpvnjefpfy1flr1srq32rfjnjpzzgat4mfmgjmoe9gcqid7oc3eq8uew3vx5wyznikytris44exp0nc45d5kqbra8mrz6pskxuliwsvxw0t2nprq6apdzopvdlz0d7xjvqtz2oo4e9glgmamomxslvoehmviwfijbkubshmghoocalhxkhb2ijcynknr5jgpn4hzuxancj3rk3wmtdgya0fppcs1mteteb05mg2jk1jdtrhazdb5qisr9d7tumuqs53rhbitmfcje0if480r5e3sxvgdx7aflpq6ckjjgtnyojmcyjsjvmfeptofkfsvzyqpzispeez5ngkmmfnbtdvhagfvzfk6f6urfarwznqwklosf3j3u2wxac1xhfd7pfuu2dj6atgljdot5rt3zadj5hhz3sozzxoc79co6tn5zv29krzrhvmgtsdt4a1pnskylatexitvlclsvl1vlmlatexit sha1base64fqw5o58bqll2cdepdpr0i5wqeqaaadsxicnvllbtnafd1xoa0b2qqs2vinkrgiiruk2i4j2hsfuom0keiqsietynuvecynkzuv4gvy0kofahdnldxyo7w0qqjqqiswzfe49zwzn8iiuejhvaxqkebmzwntwfv3i5td1o7pyktmgzh7a0svnh4asehqkfyfbgfjjl3idij8ff9ueys5yjmkw9ykfhz2j8l4trkviro3njzyvm49ctjeswiewybchbtv22csvqixvvwv93gg5hucvez1wtdccwf20wdmahwlsmbsiwzfauhzbh6brbbcomslourkwuxtqoscsdeiw1mwpwyf0gp4zykygtshxmkkzge0s0z0t08yecvlqyylwu9m6xmhlht6mxwpn9w0legdgkyzu4iohfotovx16y94z8e2nq6aunjky4kg7fjjjmuaud8yofnpz5yt9zznjchlhkp5qpaeyaebqfg3nedpzdxkrnxnqpvoto9bw7uv9l8uanzt043e4b53usbfve2smqybx2mubmpzd9hcmpgakrlf8bvx1rv1a91zp361whxdeyxfvrx6ejrscqlatexitwclsw1wnvideolatexit sha1base64fqw5o58bqll2cdepdpr0i5wqeqaaadsxicnvllbtnafd1xoa0b2qqs2vinkrgiiruk2i4j2hsfuom0keiqsietynuvecynkzuv4gvy0kofahdnldxyo7w0qqjqqiswzfe49zwzn8iiuejhvaxqkebmzwntwfv3i5td1o7pyktmgzh7a0svnh4asehqkfyfbgfjjl3idij8ff9ueys5yjmkw9ykfhz2j8l4trkviro3njzyvm49ctjeswiewybchbtv22csvqixvvwv93gg5hucvez1wtdccwf20wdmahwlsmbsiwzfauhzbh6brbbcomslourkwuxtqoscsdeiw1mwpwyf0gp4zykygtshxmkkzge0s0z0t08yecvlqyylwu9m6xmhlht6mxwpn9w0legdgkyzu4iohfotovx16y94z8e2nq6aunjky4kg7fjjjmuaud8yofnpz5yt9zznjchlhkp5qpaeyaebqfg3nedpzdxkrnxnqpvoto9bw7uv9l8uanzt043e4b53usbfve2smqybx2mubmpzd9hcmpgakrlf8bvx1rv1a91zp361whxdeyxfvrx6ejrscqlatexitwclsw1wnlatexit sha1base64igmeu3h3yzwydxplqok4ubuuhaiaaadvhicnvllssqwfd0z9f0cdemmoaguhqgvqv2kbgq3co4kvoy0e7xyf22qsolvdvu9rme0dwyu3myipfgbk25nz7zlptknyzblx3mo1a2bwahhkdgx8ynjqengzoxnhqzf12ehel26lncheesujkqothmm8eipxqhvmmqh9cicwpknhpxqxiogkncxasccaj6jucr7zslv7e5blnybld9dyqzxu8n8i8zx8qbvde1ws0nbajh0vuay0yczomlmbgocenauicaqqxiowzdtdqqxdllijleslxekdf2gwhhpcos1mgipafnkc2odbvtxhnmws1plzdujjq2fkmtuf9gwk1m63qhnrx7nxeppdw3xdhbn14rsrjnxp6mev8r65fa0yhtp6n9zv0ljngbnjxrqyqlmvhbcubq6c5wls4zsq4pcqr3qz4r5lr5tou21uq6wbvztncfdadi1zyb3gjp6ivplift8txslcdlfand1oc33dnkqrzgmbs3ravrgoleygs97xumet7ur39wflsgzfws1o5ndh2fnvqabvboylatexitwk1wkktext', 1);('featurestopk positive wordsfigure2 illustrationoftheproposed hiteawerstgeneratetwodierenttemporalviewsfortheinputvideowherethelongviewisthevideoitselfandtheshortviewisrandomlytruncatedfromtheinputvideo toexplorethemomentrevealedintheshortview', 1);('crossmodalmoment exploration', 1);('candidate words input text', 1);('lcme', 1);('multimodal temporal relationexploration', 1);('temporal relations', 1);('videotext pairs dierent', 1);('lmtre', 1);('themultimodal encoders text', 1);('sharedcapabilities reasoning understanding causality', 1);('previous', 1);('eorts eld', 1);('imagetext models videotext tasks', 1);('concatenatingvideo frame', 1);('additional temporal encoder', 1);('switchthe image encoder video encoder learning spatiotemporal contexts', 1);('hero', 1);('thecorrect temporal order', 1);('nonethelessatp4andsingularity25revealtheexistenceofastaticappearancebiasinpopularvideolanguagedatasetsandtheydevelop', 1);('singleframe models', 1);('explicit temporal', 1);('ssv2', 1);('datasets test thetemporal ability', 1);('dierent', 1);('temporal resolutions generatetwo', 1);('video construct temporal hierarchy equips model ability', 1);('moment information temporal relations atthe time3', 1);('method31 overviewfigure2sketchestheoverviewofthe hiteainconcreteour', 1);('unimodal encoders encodingvideo text', 1);('multimodal encoder videoandtextinteractionandatextdecoderforgenerationwhichis', 1);('appendixfor', 1);('video representation', 1);('previous methods 262832encode', 1);('whole input video singleview', 1);('rich temporal details', 1);('videothus rst', 1);('views dierent time resolutions', 1);('hierarchy input', 1);('event information video segment', 1);('input video theshortview', 1);('asvs use video encoder encode arbitraryview video', 1);('v2rt\x02h\x02winto', 1);('sequence embeddingsvfvclsv1\x01\x01\x01vmg2rm\x02d wheremis thenumberofattenedpatches', 1);('dishiddensizeand', 1);('vclsistheembeddingofthevisualclstokenwhichprovidesglobalrepresentation video text representation usethe text encoder transform text', 1);('tinto', 1);('sequence ofembeddingstfwclsw1\x01\x01\x01wng2rn\x02dwherenis length text multimodal encodertakes video', 1);('vand', 1);('text featurestas inputs andyields multimodal representation vclsfor videoin order', 1);('full advantage dierent view ofthe video introduce crossmodal moment explorationcem explore', 1);('proper words phrases inputtext align shortview video', 1);('lcmefor', 1);('capturingthe moment information section', 1);('tomodel relations shortview video containingmomentinformationandthelongviewvideowitheventinformation', 1);('mtretoalignthemultimodalrepresentationof3shortview', 1);('longview videos', 1);('lmtrein', 1);('introduce overall', 1);('model section', 1);('crossmodal moment explorationto', 1);('moment information thevideo', 1);('short temporal range ie shortview videoshould', 1);('corresponding text', 1);('howeversincethevideoispartiallyalignedwiththepairedtextwhichdescribes', 1);('whole video', 1);('shortview ofvideo', 1);('noise model learninganddegradetheperformance', 1);('thereforeweproposeanovelpretraining', 1);('crossmodal moment explorationcmewhichenablesthemodeltounderstandnegrainedmoment informationformallywerstdiscoverthepossiblepositivewordsforthe video shortview', 1);('cosine similarityof word', 1);('sequence fw1\x01\x01\x01wngfrom textencoder shortview video representation vsclsfromvideo encoder askf\x191\x01\x01\x01\x19kg 1where\x19f1\x01\x01\x01ng f 1\x01\x01\x01ngis permutationfunction', 1);('sw\x191vscls\x15 \x01\x01\x01 \x15sw\x19nvscls andkis', 1);('word indiceskis number', 1);('words sxy xtykxk2kyk2represents cosine similarity xandy', 1);('afterobtainingthewordsforthevideoinshortviewas', 1);('positive pair crossmodal moment explorationlosslcmeiscomputedwithnegativepairsfromotherwordsin input text', 1);('aslcme\x001bbxi1 1jkjxk2klogexpvsclsiwik', 1);('pnn1expvsclsiwin', 1);('learnable temperature hyperparameter thatcontrols sharpness output distribution', 1);('consequence model ableto', 1);('crossmodal exploration scheme33', 1);('multimodal temporal relation explorationwhile', 1);('video encoder', 1);('eectiveness learning temporal representation', 1);('112868itremainsachallengetodiscovertheinherenttemporalrelations result', 1);('task temporal reasoning', 1);('video text', 1);('temporal cuesto end introduce multimodal temporal relationexploration', 1);('novel temporalaware pretrainingtaskthatimprovesmodelscapacitiesincapturingtemporalcorrelationofmomentsinvideowithnegrainedtextguidance', 1);('specially', 1);('shortview videovswould', 1);('moment information withrespect', 1);('whole video contrary longviewvideovlexpresses event topical information', 1);('toobtainthetextguidedvideofeatureswefeedvideosindifferent', 1);('video encoder individuallythen text', 1);('interact thevideo', 1);('multimodal encoder yield', 1);('video representations vlcls2rdandvscls2rdasfollowsvlclsffvlclsvl1\x01\x01\x01vlmgfwclsw1\x01\x01\x01wng3vsclsffvsclsvs1\x01\x01\x01vsmgfwclsw1\x01\x01\x01wng4wherefvtrepresents multimodal encoder withvideo featuresvand text featurest', 1);('whole text', 1);('improper videotext pairs', 1);('yield noisy multimodal representation', 1);('performance modeltherefore thanks', 1);('crossmodal moment exploration calibrate representationfor shortview video byvsclsffvsclsvs1\x01\x01\x01vsmgfwk1\x01\x01\x01wkkg5whereki2kis index', 1);('positivewords aim', 1);('representation producedtextguidedvideofeaturesindierentgranularitiesinorder', 1);('enable model', 1);('past futurefrom shortview video benets capturingthe general structure video', 1);('simsiam', 1);('negativecosine similaritydpszl \x00pskpsk2\x01zlkzlk2 6wherepshgvsclsandzlgvlcls', 1);('thegandhareprojectionmlpheadandpredictionmlphead714 minimizingdpszlis', 1);('meansquare error psandzl encourages thevideos dierent temporal magnitudes', 1);('loss aslmtre 12\x02dplsgzs', 1);('dpssgzl\x037where', 1);('sg\x01is stopgradient operation prevents themodel collapse training 64msrvtt', 1);('didemo lsmdc activitynet captionmethod pt data r1 r5 r10 r1 r5 r10 r1 r5 r10 r1 r5 r10clipbert', 1);('allinone', 1);('xclip', 1);('842hitea 17m', 1);('867table 1performance comparison texttovideo retrieval results', 1);('gray methods thatuse', 1);('data fair comparison', 1);('pt data', 1);('number videotext pairs pretraining34', 1);('pretraining objectivesapartfromthetwoproposedtemporalawarepretrainingtasks', 1);('approaches22832toadoptthestandardpretrainingtasksincludingvideotext contrastive', 1);('vtmmasked', 1);('precisely vtc vtm', 1);('align video text theglobal perspective', 1);('mlm prexlm', 1);('contributetomultimodalunderstandingandgenerationcapabilitiesofthe model', 1);('wesimplycombinetheseasthebasetrainingobjectivelbasefor', 1);('thefull pretrainingobjective', 1);('experiments41 experiment setuppretrainingdatasets followingtherecentwork212252832', 1);('pretrain model', 1);('webvid2m', 1);('25m videotext pairs aimagetext dataset', 1);('google conceptual captions cc3m52', 1);('3m imagetext pairs', 1);('previous methodswe pretrain model largescale videotextdatasets', 1);('howto100m', 1);('136m videotext pairsandyttemporal180m72duetotheheavycomputationforscalingupwealsotrainedourmodelonthewidelyusedimagetext', 1);('ms coco', 1);('genome', 1);('sbu captions', 1);('conceptual12m', 1);('setting 17m corpusdownstream', 1);('datasets', 1);('videolanguage benchmarks', 1);('videotext retrieval video question', 1);('video captioningtasks', 1);('videoqacan', 1);('multiplechoice mc openended oe', 1);('settings evaluation datasets', 1);('appendixvideotext retrieval msrvtt', 1);('mc tgifaction tgiftransition', 1);('lsmdcmc', 1);('nextqa60videoqa oe tgifframe', 1);('msrvttqamsvdqa', 1);('activitynetqa70video captioning msrvtt', 1);('hiteaisbasedonpytorch46 indetailweinstantiatethevideoencoderwithmvitbasemodel36pretrainedonimagenet21k48 thetextencoderisinitializedfromrstsixlayersofpretrainedbertbase10andthemultimodalencoderis', 1);('bertbasewe', 1);('hiteafor', 1);('batch size of16', 1);('nvidia a100 gpus', 1);('optimizer weight decay', 1);('098thelearningrateisrstwarmedupto5e5intherst1000iterations decays', 1);('cosine schedule', 1);('short andlongviewwhilepreservingtheirorderinbetweenandresizethemto', 1);('theduration', 1);('ofshortview isrestrictedas', 1);('whole video duration', 1);('mask ratio', 1);('appendix42 comparison', 1);('artsinthissectionwecompare hiteawithnumerousstateoftheart', 1);('methods severaldownstream datasets', 1);('msrvtt lsmdc msvd activitynetmethod pt data', 1);('transition frame mc qa mc fib qa qaclipbert', 1);('clover', 1);('451hitea 17m', 1);('performance', 1);('comparison video question', 1);('accuracy', 1);('pt data msrvtt msvdunivl', 1);('swinbert', 1);('clip4caption', 1);('1429hitea 5m', 1);('1451hitea 17m', 1);('1469table 3performance comparison video', 1);('cider57', 1);('texttovideo retrievaltable1summarizestheresultsonmsrvtt64didemo1lsmdc49andactivitynetcaption21undernetuning', 1);('settings method outperforms existingvideolanguage', 1);('underthe data scale', 1);('particular method yields 66lift terms', 1);('r1 msrvtt', 1);('5m videotext pairs', 1);('recent works utilize powerfulencoder', 1);('clip', 1);('notice ourmethodachievesthebestresultamongalloflistedmethodson', 1);('lsmdc', 1);('dataset proves model leverage', 1);('various moments', 1);('fruitful movie clipswith crossmodal moment exploration422', 1);('video', 1);('current stateoftheartapproachesonninevideoqadatasets', 1);('itcanbenoticedthat', 1);('performance ofvideoqa datasets', 1);('specifically', 1);('absolute improvement', 1);('tgifframeqa22onmsrvttmc15onmsrvttqa02onmsvdqaand33onactivitynetqawebelieve', 1);('crossmodal explorationare', 1);('clue answers', 1);('videoqa423 video captioningtable3compares hiteawithexistingmthodsonvideocaptioning', 1);('msrvtt msvd', 1);('hiteastillobtainssignicantimprovementcomparedtothoselargescalepretrainedmodels onmsrvttcaptionourmethodsurpassessotamethodmvgpt51by25cider notethatmvgptispretrainedformultimodalvideocaptioninganditleveragestheasrtranscriptsfromaudioastheadditionalinput bycontrastourmethodonly', 1);('utilizes video input generation43', 1);('discussioninthissectionwediscussthetemporalcharacteristicsofour', 1);('model datasetsimpact', 1);('individual loss terms results', 1);('table4 itcanbeobservedthatthecombiningboth lcmeandlmtreimproves', 1);('performance texttovideo retrievaland video question', 1);('inaveragerecall andaverage', 1);('nd performance', 1);('lcmesurpassesthat', 1);('appearance information', 1);('crossmodal moment exploration loss notonlyselectthepositiveverbsforthevideofromthetextbutalsochoosetheactingobjectforalignmentwhichcanboostthe retrieval performanceevaluation', 1);('temporalaware tasks leiet', 1);('retrieval datasets', 1);('temporal information', 1);('somethingtosomethingv2 ssv2 template ssv2 label', 1);('retrieval datasets to6methodmsrvttretrieval', 1);('ssv2 templateretrieval nextqa hard msvdqar1 r5 r10 aver r1 r5 r10 aver accc acct acclbase', 1);('videolanguage tasks texttovideo retrieval', 1);('r1 r5 r10and', 1);('report accuracyssv2label', 1);('ssv2templatemethod pt data r1 r5 r10 r1 r5 r10frozen', 1);('994hitea 5m', 1);('somethingtosomething ssv2', 1);('texttovideo retrievalmethod', 1);('pt data accc acct accd accfull sethuman', 1);('543hitea 5m', 1);('splitatp', 1);('hga', 1);('wereport', 1);('causal', 1);('temporal descriptive dsplits', 1);('overall accuracy validation', 1);('usingclip initialization visual encoderdataset', 1);('original shuedgapmsrvtt', 1);('template', 1);('label', 1);('temporal information texttovideoretrieval datasets temporal', 1);('test average', 1);('weevaluatethe', 1);('performance drop', 1);('input inferenceoriginal', 1);('datasetrelies temporal information model utilizes temporal information', 1);('tasktest models', 1);('true temporal', 1);('understanding moment temporal relation', 1);('objects information', 1);('performance thesedatasets', 1);('observedthathiteaachieves signicant improvement 85dataset', 1);('original shuedgapmsrvttqa', 1);('hard', 1);('temporal information video', 1);('datasets temporal', 1);('test report accuarcy dataset', 1);('inadditionweevaluateourmodelonnextqa60datasetthatexplicitly', 1);('temporal causal understandingaspresentedintable6ourmethodsignicantlysurpassesits', 1);('competitive counterparts', 1);('methods equippedwith', 1);('powerful imagetext', 1);('quantitativelyhiteaobtainsanabsoluteimprovement', 1);('intrinsic temporal relationrecentlybuch etal4lteroutthetrivialquestionforthedataset', 1);('causality temporal ofthemodel', 1);('aswecanseeinthetableevenforthequestionsthat', 1);('causality model', 1);('relative gain', 1);('model specic designfor', 1);('model solelydepend static appearancetemporalrelianceofdatasets', 1);('previousmethods122628', 1);('performance models', 1);('superiority methodshowever', 1);('buch', 1);('lei', 1);('temporal reliance evaluateddatasets', 1);('specicallywecomputetheperformancechangesbetweenrunninginference', 1);('video versus', 1);('dataset lessspatialbiasandneedsfortemporalinformation', 1);('table7andtable8concludetheperformancegapbetweenorderedandshuedinputvideofortexttovideoretrievalandvideoqadatasets', 1);('texttovideo retrieval task', 1);('ssv2 template7cartoon', 1);('characters talkingto pokemonvideo', 1);('framesbaselineoursvideo framespoking', 1);('visualizations', 1);('crossattention maps multimodal encoder', 1);('present samples', 1);('hiteaattends', 1);('objects motion', 1);('didemo lsmdcmethod pt data r1 r5 r10 r1 r5 r10 r1 r5 r10frozen', 1);('violet', 1);('clip4clip', 1);('292hitea 5m', 1);('398hitea 17m', 1);('442table 9zeroshot evaluation texttovideo retrieval results', 1);('pt data msrvttqa msvdqajust', 1);('116merlot reserve', 1);('frozenbilm', 1);('117hitea 5m', 1);('372hitea 5m', 1);('zeroshot', 1);('evaluation video question answeringaccuracy', 1);('inputvideo demonstrates', 1);('dynamic information', 1);('assumption thecontrary performance', 1);('mean recall', 1);('static objects', 1);('temporal information video question', 1);('dataset weobserve', 1);('msvdqa activitynetqa', 1);('thisisbecausethesetwodatasetscontainmorequestionsrequiringframeregioninformation', 1);('object categories scenes specieswebelievethiscanbeusedtoevaluatethetemporalrelianceof datasets', 1);('utilization temporal cue bymodels future workqualitativeanalysis', 1);('toverifythatourmodelcancapturethemotioninformationwithrespectedtothegiventextratherthan', 1);('static signal', 1);('present querytext', 1);('attention map ofatomic action', 1);('focuses mouse thecartooncharacterswhilethebaselinelargelyfocusingonthecharacters', 1);('method understandthe moment', 1);('example word spins canreveal trajectory object', 1);('zeroshot generalizabilityto', 1);('zeroshot evaluation onvideolanguage', 1);('summarizes theperformanceofourmodelandcomparedapproachesontexttovideo retrieval observe model', 1);('videotext pairs', 1);('method surpasses', 1);('models terms', 1);('lsmdcdataset', 1);('superiority models generalizability', 1);('zeroshot performance onvideoqa task', 1);('method attains competitivezeroshot performance', 1);('msrvttqa msvdqadatasets', 1);('help audio signal supervision 718oradditionalgeneratedvideoquestionpairs65', 1);('inparticularlesspretrainingdata', 1);('wealsoevaluatethezeroshotperformanceofmodelssupervisedonvqa', 1);('nd method surpasses thepowerful multimodal', 1);('sota', 1);('methods eg mplug 27with 5m', 1);('generalization ability', 1);('hitea5 conclusionin', 1);('work introduce', 1);('novel hierarchical temporalaware videolanguage', 1);('frameworkwith understanding generation capabilities', 1);('wevary', 1);('video dierent', 1);('model crossmodalalignment moments texts', 1);('temporal relations hierarchical way', 1);('toexplore alignment text video momentwhich', 1);('semantic alignmentbetween video text', 1);('temporal relations momentsand event', 1);('video multimodaltemporal relation exploration', 1);('hiteastill', 1);('lisa anne hendricks oliver wang eli shechtman josefsivic trevor darrell bryan russell localizing', 1);('moments video', 1);('natural language', 1);('oftheieeeinternationalconferenceoncomputervision pages58035812', 1);('maxbainarshanagraniglvarolandandrewzissermanfrozen', 1);('time joint video image encoder endtoendretrieval', 1);('proceedingsoftheieeecvfinternationalconference computer vision', 1);('gedas bertasius heng wang lorenzo torresani isspacetime', 1);('attention need video understandinginicml volume', 1);('shyamal buch cristbal eyzaguirre adrien gaidon jiajunwu li feifei juan carlos niebles revisiting', 1);('thevideo videolanguage understanding', 1);('davidchenandwilliambdolan collectinghighlyparalleldataforparaphraseevaluation proceedingsofthe49thannualmeetingoftheassociationforcomputationallinguisticshuman', 1);('language technologies pages', 1);('xinlei chen kaiming exploring', 1);('simple siameserepresentation learning', 1);('xinlei chen saining xie kaiming', 1);('empirical study training', 1);('vision transformers', 1);('inproceedings ieeecvf', 1);('international conference oncomputer', 1);('yenchun chen linjie li licheng yu ahmed el kholyfaisalahmedzheganyuchengandjingjingliu uniteruniversal', 1);('imagetext representation learning', 1);('europeanconference', 1);('computer vision pages', 1);('springer20209 jaemin cho jie lei hao tan mohit bansal unifyingvisionandlanguage', 1);('text generation', 1);('pages 19311942pmlr', 1);('jacob devlin mingwei chang kenton lee kristinatoutanova bert pretraining', 1);('deep bidirectionaltransformers language understanding arxiv preprintarxiv181004805', 1);('tsujui fu linjie li zhe gan kevin lin william yangwang luan wang zicheng liu violet endtoendvideolanguagetransformerswithmaskedvisualtokenmodelingarxiv', 1);('preprint arxiv211112681', 1);('yuying ge yixiao ge xihui liu dian li ying xiaohu qie ping luo bridging', 1);('videotext retrieval withmultiple choice questions', 1);('yash goyal tejas khot douglas summersstay dhruv batra devi parikh making', 1);('v vqa matter', 1);('elevatingtheroleofimageunderstandinginvisualquestionansweringinproceedings ieee', 1);('conference computer visionand pattern recognition pages', 1);('jeanbastien grill florian strub florent altch corentintallec pierre richemond elena buchatskaya carl doerschbernardoavilapireszhaohanguomohammadgheshlaghi azar', 1);('bootstrap', 1);('new approachto', 1);('jingjia huang yinan li jiashi feng xiaoshuai sun', 1);('towardsauniedvideolanguagealignment', 1);('fusion model arxiv preprint arxiv220707885', 1);('yunseok jang yale', 1);('youngjae yu youngjin kim', 1);('kim tgifqa toward', 1);('spatiotemporal reasoningin visual question', 1);('proceedings ieeeconferenceoncomputervisionandpatternrecognition', 1);('chaojiayinfeiyangyexiayitingchenzaranaparekhhieu pham quoc', 1);('yunhsuan sung zhen li tomduerig scalingupvisualandvisionlanguagerepresentationlearningwithnoisytextsupervision internationalconferenceonmachinelearning', 1);('pin jiang yahong han reasoning', 1);('heterogeneousgraph alignment video question', 1);('proceedings aaai', 1);('articial intelligence', 1);('wonjae kim bokyung', 1);('ildoo kim vilt visionandlanguage', 1);('convolution region su9pervisionin', 1);('internationalconferenceonmachinelearning', 1);('diederik p kingma jimmy ba adam', 1);('method forstochastic optimization arxiv preprint arxiv14126980', 1);('ranjay krishna kenji hata frederic ren li feifei', 1);('carlos niebles densecaptioning', 1);('events videos', 1);('international conference computer vision pages', 1);('ranjay krishna yuke zhu oliver groth justin johnsonkenji hata joshua kravitz stephanie chen yannis kalantidis lijia li david shamma', 1);('visual', 1);('language vision', 1);('internationaljournalofcomputervision', 1);('thao minh', 1);('vuong', 1);('svetha venkatesh truyentran hierarchical', 1);('conditional relation networks multimodal video question', 1);('international journal ofcomputer', 1);('kuanghuei lee xi chen gang hua houdong hu', 1);('stacked', 1);('cross attention imagetext', 1);('inproceedingsoftheeuropeanconferenceoncomputervision eccv', 1);('jie lei tamara', 1);('berg mohit bansal revealing', 1);('singleframe bias videoandlanguage learning arxiv preprintarxiv220603428', 1);('jie lei linjie li luowei zhou zhe gan tamara', 1);('bergmohit bansal jingjing liu less clipbert', 1);('forvideoandlanguage learning', 1);('chenliang li haiyang xu junfeng tian wei wang mingyan bin bi jiabo ye hehong chen guohai xu zhengcao', 1);('eective', 1);('crossmodal skipconnections arxiv preprintarxiv220512005', 1);('dongxu li junnan li hongdong li juan carlos nieblesandstevenchhoi alignandprompt videoandlanguagepretraining', 1);('entity prompts', 1);('theieeecvf conference', 1);('junnan li dongxu li caiming xiong steven hoiblip bootstrapping', 1);('icml202230 junnanliramprasaathselvarajuakhileshgotmareshaqjoty caiming xiong steven chu hong hoi align', 1);('language representation learning withmomentumdistillation', 1);('advancesinneuralinformationprocessing systems', 1);('linjie li yenchun chen yu cheng zhe gan lichengyuandjingjingliu hero hierarchicalencoderforvideolanguage', 1);('arxiv preprintarxiv200500200', 1);('linjie li zhe gan kevin lin chungching lin zichengliu ce liu luan wang lavender unifying', 1);('arxivpreprint arxiv220607160', 1);('ping li qinghao ye luming zhang li yuan xianghuaxu ling shao exploring', 1);('global diverse attention viapairwise temporal relation video summarization', 1);('patternrecognition', 1);('wei li gao guocheng niu xinyan xiao hao liujiachen liu hua wu haifeng wang unimo towardsuniedmodalunderstandingandgenerationviacrossmodalcontrastivelearning', 1);('xiujunlixiyinchunyuanlipengchuanzhangxiaoweihu lei zhang luan wang houdong hu li dong furuwei', 1);('oscar objectsemantics', 1);('europeanconferenceoncomputervision', 1);('springer', 1);('yanghao li chaoyuan wu haoqi fan karttikeya mangalam bo xiong jitendra malik christoph feichtenhofer mvitv2 improved', 1);('multiscale vision transformers forclassicationanddetection', 1);('proceedingsoftheieeecvfconference computer vision pattern recognition', 1);('kevin lin linjie li chungching lin faisal ahmed zhegan zicheng liu yumao lu luan wang swinbert endtoendtransformerswithsparseattentionforvideocaptioning proceedings ieeecvf conferenceon computer vision pattern recognition', 1);('tsungyi lin michael maire serge belongie james hayspietroperonadevaramananpiotrdollrandclawrencezitnick microsoft', 1);('common objects context', 1);('ineuropean', 1);('conference computer vision pages 740755springer', 1);('huaishao luo lei ji botian shi haoyang huang nanduan tianrui li jason li taroon bharti ming zhouunivl', 1);('video language', 1);('model formultimodal understanding generation arxiv preprintarxiv200206353', 1);('huaishao luo lei ji ming zhong yang chen wen leinan duan tianrui li clip4clip', 1);('empirical studyof clip end end video clip retrieval', 1);('yiwei guohai xu xiaoshuai sun ming yan ji zhangand rongrong ji xclip endtoend', 1);('contrastive learning videotext retrieval arxiv preprintarxiv220707285', 1);('tegan maharaj nicolas ballas anna rohrbach aaroncourville christopher pal', 1);('dataset explorationof models understanding video data llintheblank', 1);('nicola messina giuseppe amato andrea esuli fabriziofalchi claudio gennaro stphane marchandmailletfinegrained', 1);('visual textual alignment crossmodal retrieval', 1);('transformer encoders', 1);('acm transactions', 1);('computing communications applicationstomm', 1);('antoine miech jeanbaptiste alayrac lucas smaira ivanlaptev josef sivic andrew zisserman endtoend10learning', 1);('visual representations', 1);('instructional videos', 1);('proceedings ieeecvf conferenceon computer vision pattern recognition', 1);('vicente ordonez girish kulkarni tamara bergim2text describing', 1);('adam paszke sam gross francisco massa adam lererjames bradbury gregory chanan trevor killeen zeminglinnataliagimelsheinlucaantigaetal pytorch animperative', 1);('style highperformance', 1);('learning library', 1);('neural information processing systems', 1);('alec radford jong wook kim chris hallacy adityaramesh gabriel goh sandhini agarwal girish sastryamanda askell pamela mishkin jack clark', 1);('transferable visual models', 1);('natural language supervision', 1);('talridnikemanuelbenbaruchasafnoyandlihizelnikmanor imagenet21k', 1);('masses arxivpreprint arxiv210410972', 1);('annarohrbachmarcusrohrbachnikettandonandberntschiele', 1);('dataset movie description', 1);('ieee', 1);('conference computer vision pattern recognition pages', 1);('karsten roth oriol vinyals zeynep akata integratinglanguage', 1);('metric learninginproceedings', 1);('computervision pattern recognition', 1);('paul hongsuck seo arsha nagrani anurag arnab', 1);('endtoendgenerativepretrainingformultimodal', 1);('piyush sharma nan ding sebastian goodman radusoricut conceptual', 1);('image alttext dataset', 1);('automatic image', 1);('proceedings acl', 1);('chen sun austin myers carl vondrick kevin murphyand cordelia schmid videobert', 1);('joint model videoand language representation learning', 1);('haotanandmohitbansallxmert learningcrossmodalityencoder', 1);('representations transformers arxiv preprintarxiv190807490', 1);('mingkangtangzhanyuwangzhenhualiufengyunraodian li xiu li clip4caption clip', 1);('video captioninproceedingsofthe29thacminternationalconferenceonmultimedia pages', 1);('atousa torabi niket tandon leonid sigal learninglanguagevisual', 1);('movie understanding withnaturallanguage arxiv preprint arxiv160908124', 1);('ramakrishna vedantam', 1);('lawrence zitnick deviparikh cider consensusbased', 1);('image description evaluation', 1);('conference computervision pattern recognition pages', 1);('alexjinpengwangyixiaogeruiyanyuyinggexudonglin guanyu cai jianping wu ying xiaohu qie', 1);('zheng shou', 1);('exploring', 1);('arxiv preprint arxiv220307303', 1);('zirui wang jiahui yu adams wei yu zihang dai yuliatsvetkov yuan cao simvlm simple', 1);('visual languagemodel', 1);('weak supervision arxiv preprintarxiv210810904', 1);('junbin xiao xindi shang angela yao tatseng chuanextqa', 1);('explainingtemporal actions', 1);('junbin xiao pan zhou tatseng chua shuicheng yanvideographtransformerforvideoquestionanswering', 1);('arxivpreprint arxiv220705342', 1);('dejing xu zhou zhao jun xiao fei wu hanwang zhangxiangnan yueting zhuang video', 1);('attention appearance motion', 1);('inproceedings', 1);('acm', 1);('multimedia', 1);('hu xu gargi ghosh poyao huang dmytro okhonkoarmen aghajanyan florian metze luke zettlemoyer', 1);('videoclip contrastivepretrainingfor', 1);('zeroshot videotext understanding arxiv preprintarxiv210914084', 1);('jun xu tao mei ting yao yong rui msrvtt', 1);('conference computer vision andpattern recognition pages', 1);('antoine yang antoine miech josef sivic ivan laptev', 1);('schmid', 1);('questionsfrom millions', 1);('antoine yang antoine miech josef sivic ivan laptevand cordelia schmid zeroshot', 1);('video question answeringvia frozen bidirectional language models arxiv preprintarxiv220608155', 1);('lewei yao runhui huang lu hou guansong lu minzheniu hang xu xiaodan liang zhenguo li xin jiang', 1);('filip finegrainedinteractivelanguageimagepretraining', 1);('arxiv preprint arxiv211107783', 1);('qinghaoyexiyueshenyuangaoziruiwangqibipingli guang yang temporal', 1);('video highlightdetection lowrank audiovisual fusion', 1);('proceedingsof ieeecvf', 1);('youngjae yu jongseok kim gunhee kim', 1);('jointsequence fusion model video question', 1);('european conference oncomputer', 1);('vision eccv', 1);('zhou yu dejing xu jun yu ting yu zhou zhao yueting zhuang dacheng tao activitynetqa', 1);('datasetfor understanding', 1);('complex web videos', 1);('inproceedings aaai', 1);('articialintelligence', 1);('rowanzellersjiasenluximingluyoungjaeyuyanpengzhaomohammadrezasalehiadityakusupatijackhesselali farhadi yejin choi merlot', 1);('neural', 1);('scriptknowledge vision language', 1);('rowan zellers ximing lu jack hessel youngjae yujae sung', 1);('jize cao ali farhadi yejin choi merlot multimodal', 1);('neural script knowledge models', 1);('linchao zhu yi yang actbert learning', 1);('globallocalvideotext representations', 1);('proceedings ieeecvfconferenceoncomputervisionandpatternrecognition', 1);('pages87468755 202012a', 1);('additional experimental resultsin', 1);('section weprovide', 1);('experimental results forcompleteness', 1);('transfer imagetext downstream taskssince', 1);('singleframe videoswe', 1);('method imagetext tasks', 1);('imagetext retrieval visual question answeringimagetext', 1);('imagetotext', 1);('andtexttoimage retrieval', 1);('datasets resultsaresummarizedintable11', 1);('wecanobservethatourmethodsurpasses singularity', 1);('amount pretraindata', 1);('recall1 texttoimage', 1);('methods830leverage4mdatasetwhichcontainsthecocodatasetas part', 1);('hiteacan', 1);('answering', 1);('methodon visual question', 1);('concludes theimage question', 1);('vqav2', 1);('datasetswe observe', 1);('hiteademonstrates', 1);('competitive performance', 1);('tasks worthwhile', 1);('datasets indicatesthe videotext', 1);('performance ofimagetext', 1);('gapwithstateoftheartimagetextpretrainedmodelssinceourmethod use indomain data eg', 1);('future direction use imagetext data duringvideotext', 1);('additional ablation studiesimpact', 1);('positive candidate words size', 1);('kwe', 1);('positive words size', 1);('kduring', 1);('kthe', 1);('performance dataset', 1);('inadditionthereisatradeobetweenthechoiceofkand', 1);('dierent datasetsandk 5givesrelativegoodresultsamongthesedatasetsitalsosuggeststhatthesmall', 1);('kwouldgivemoredeterministicresultssincethemodelwouldonlyselectthewordwiththe', 1);('thenasnumberofpositivewordsincreasedmore', 1);('accurate words', 1);('align shortview video', 1);('benets fromcrossmodal moment exploration', 1);('large enoughiek 11ork', 1);('candidate wordsmethod', 1);('pt datacoco', 1);('5k testtr', 1);('irr1 r5 r10 r1 r5 r10vilt', 1);('878hitea 5m', 1);('methods imagetext retrievalon', 1);('dataset show results text retrieval imagetotext retrieval', 1);('tr', 1);('image retrieval', 1);('irmethod pt data', 1);('testdev teststdclipbert', 1);('vlbart', 1);('02m 7130lxmert', 1);('vqatemporalevaluationoflossterms tofurthervalidatethetemporaldependencyfortheproposedmethodweadopttheshuingtestformodelswithdierentlosstermsasshownin', 1);('shows loss terms contributemore', 1);('lcmeandlmtreconsistentlyimprovetheperformancesoforiginalandgaponmoretemporal', 1);('datasets ie', 1);('ssv2template ssv2labelfor', 1);('example model', 1);('surpassesthe baseline model metric', 1);('ssv2template ssv2labelrespectivelygeneralizationtoothervisionbackbone table14showsthat', 1);('generalizable dierent visionbackbones', 1);('indetailsweinstantiatethevideoencoderwith timesformer', 1);('cme mtre', 1);('model performance', 1);('hierarchical temporalaware', 1);('framework worthnotingthattimesformergenerateslongvideotokenscom131', 1);('kmsrvtt', 1);('original shuedgaporiginalshuedgaporiginalshuedgaplbase', 1);('methods temporal dependency temporal', 1);('performance drop whenshuingtheinputduringinference', 1);('originalandshueddenotetheoriginalandshuedinputvideosrespectivelyandgapisthedierence original shued', 1);('dataset relies temporal information modelutilizes temporal information', 1);('msrvtt didemo ssv2templatetimesformer lbase5730', 1);('eectivenessoftheproposedmethodsondierentvideobackbone', 1);('timesformer', 1);('generalization ability proposedmethod', 1);('fortexttovideoretrievalthemeanrecallofrecall1recall5andrecall10isreported forvideoquestionanswering', 1);('task report', 1);('top1', 1);('multiscale vit', 1);('extramemorycostforthemultimodalencoderanddecodersincethe computation selfattention quadratic makestimesformer', 1);('expensive scale input frames withlonger sequencesinuence language', 1);('insteadofutilizingthelanguagesignalswedirectlyadopt', 1);('video representation vclsfrom video', 1);('learning results', 1);('understanding actions indicatesthatourmethodcanbetterunderstandingtheactionsvia multimodal temporal relation exploration', 1);('wealso notice performance model', 1);('didemo ssv2template5060708090100recallmean629771259310631371599425642072089521mtre', 1);('textmtre textmtre selected textfigure', 1);('language duringmultimodal', 1);('temporal relation exploration mtre', 1);('mean recall recall1 recall5 recall10rect', 1);('multimodal pairs surpasses model', 1);('bymultimodal pairs text', 1);('improper videotext pair yields noisy multimodal representation', 1);('performance modelb', 1);('discussionb1 qualitative analysiswe', 1);('sample videos', 1);('corresponding texts andcompute similarities words videos', 1);('figure6', 1);('gure model eectivelycapture moments', 1);('etc video', 1);('essential under14figure', 1);('examples', 1);('similarities words videos', 1);('method method captures', 1);('atomic actions thevideos', 1);('crossmodal moment explorationstandingvideos', 1);('besideswecannoticethatthevideowouldalsoattendtheobjectappearedinthevideoshowingthecapability', 1);('moment informationb2', 1);('finegrained methodssomeeorts244367havebeenmadetolearnthenegrained', 1);('correlation alignment', 1);('filip', 1);('teran', 1);('aggregate themaximum', 1);('token similarity scores', 1);('optimalpatchword transport matrix', 1);('scan', 1);('utilizes similarityscorestoattendeachtokensforsoftnegrainedalignment approaches', 1);('dierent imagetext pretrainingvideotext', 1);('atomic actions proposedcrossmodal moment exploration leverages shortviewofvideotoreectthemomentinformationanddiscovertherelationship shortview videos words whichresults', 1);('moment representations videolanguage pretrainingb3', 1);('limitations boarder impactdespitetheeectivenessoftheproposedmethodonvariousdownstreamtasksourmethodstillhassomelimitationsthat', 1);('promising directions future work1', 1);('currently', 1);('pretrain model 5m data withthe basesize encoders scalability model isnot', 1);('indepth investigation inthe', 1);('method shares', 1);('similar risks', 1);('consist bias unsafe content', 1);('analysisbefore deploymentmethod', 1);('parameterclipbert', 1);('198mhitea 297mtable15', 1);('comparisontoothermodelsinthenumberofparametersc', 1);('detailsc1 number parametersweincludesomeofpreviousmodelswiththeirparametercounts', 1);('original paper calculatedbyfollowupworkandwecomparethemwith', 1);('hiteain', 1);('compared', 1);('models model', 1);('ofcomparable model size', 1);('performance terms ofboth videolanguage understanding generationc2', 1);('model architectureas', 1);('unimodal encoders text video', 1);('multimodal encoder videotext interaction text decoder generation', 1);('arbitrary viewof videov2rt\x02h\x02wis encoder sequence ofembeddingsfvclsv1\x01\x01\x01vmg2rm1\x02d wheremis number', 1);('patches video', 1);('cls', 1);('global representation video text encoder transforms text sequence embeddings15video', 1);('multimodal encodertext decodervtmmlmtruncated textprefix lmvtccls caucasian', 1);('ice cream licks', 1);('textvideofigure', 1);('architecture', 1);('objectivesfwclsw1\x01\x01\x01wng2rn1\x02d wherenis numberof words text', 1);('encode multimodalinformation', 1);('unimodal information wefuse video text', 1);('output multimodal encoderfvclsv1\x01\x01\x01vmwclsw1\x01\x01\x01wng2rmn2\x02disfedintoatransformerdecoderforsequencetosequencegeneration equips', 1);('hiteawith', 1);('capabilities bothmultimodal understanding generationc3', 1);('pretraining objectivesduring', 1);('videotext contrastive learning lvtcvideotext', 1);('lvtm masked language modelinglmlm prex language modeling lprexlm thevtc', 1);('task rst', 1);('align unimodal representation video text multimodal representationcan', 1);('vtm mlm', 1);('upon', 1);('thevideolanguage representations', 1);('multimodalencoder decoder', 1);('loss textcompletion taskvideotext', 1);('contrast vtc following', 1);('speciallythesoftmaxnormalized', 1);('videototext texttovideo similarities', 1);('employ memory queues', 1);('moco', 1);('toincrease number', 1);('negative samples', 1);('videotext contrastive loss', 1);('aslv2t\x001bbxi1logexpsvitipbj1expsvitj9lt2v\x001bbxi1logexpsvitipbj1expsvjtilvtc12lv2tlt2vwhereviandtjaretheprojectedrepresentationsof vclsandwclsforith videotext pair batchvideotext', 1);('task aims predictwhether video text', 1);('themultimodal representation', 1);('hardnegativevideotextpairsareselectedbasedonthesimilarityof video text contrastive learning', 1);('formally', 1);('vdenotesthevideofeatures', 1);('language modeling mlm', 1);('bert', 1);('where15 tokens text', 1);('themodel needs', 1);('formallythemaskedlanguagemodeling', 1);('word tokenprex', 1);('language modeling prexlm', 1);('pretext taskrequires model', 1);('ongivenvideosandprexsequenceoftruncatedtexts2729the model', 1);('likelihood ofthe', 1);('text autoregressive manner', 1);('formallythe', 1);('prex language', 1);('aslprexlm \x00ewv24lxllplogpwljwlplwlpv3512whereldenotes total number words text andlpis length prex sequence tokens', 1);('downstream', 1);('task implementation', 1);('detailswe', 1);('texttovideo retrieval openended videoqa multiple', 1);('videoqa videocaptioning', 1);('asfollowsfor retrieval tasks', 1);('loss videotext alignment', 1);('inference rst', 1);('select topk candidatesbycomputingthedotproductsimilaritybetweenthe video text', 1);('scores kis setto', 1);('optimizer learning rate weight decay lr schedule batch size', 1);('gpus epochsmsrvttret', 1);('endtoend', 1);('congurations videolanguage', 1);('rst generate video featuresandtextfeatureswithtwounimodalencodersandthen fuse multimodal encoder outputof multimodal', 1);('text decoder', 1);('generation use language', 1);('lossto optimize model inference answerwould', 1);('text decoderfor multiplechoice', 1);('videoqawe', 1);('treatthe problem asthetexttovideoretrievaltaskwherethecorrectanswershould', 1);('duringtraining', 1);('scores candidate', 1);('video optimize model withcross entropy loss inference answerwith', 1);('score prediction answerfor', 1);('fromvideo encoder', 1);('feed text decoderfor caption generation language', 1);('model optimizationfor videolanguage', 1);('tasks resizevideoframesto', 1);('duringnetuningfollowing2528werandomlysample12framesfortexttovideoretrieval16framesforvideoquestionansweringandvideocaptions', 1);('probability data augmentation hyperparameters', 1);('video captiontask use prex', 1);('datasets descriptioninthissectionwedescribeallofthedownstreamvideolanguagedatasetsusedduringevaluation thedetailsofthedatasets', 1);('ssv2 label', 1);('contains 10k', 1);('sourcedvideos 200k text descriptions', 1);('153240we train video 9k videos', 1);('rest1k video', 1);('contains 10k videos', 1);('flickrand', 1);('descriptions video', 1);('283241we concatenate', 1);('descriptions samevideo paragraph', 1);('paragraphtovideoretrieval performance number video trainingset 8k', 1);('1k validation', 1);('1k test setlsmdc 49consistsof118kvideoclipsfrom202moviesand clip', 1);('caption videoscripts 101k video clips training 1k clipsfor', 1);('standard splits', 1);('videos with100k captions', 1);('10k videos', 1);('report performance val1', 1);('with49k videos', 1);('thetextqueries ssv2template', 1);('textqueries specic object information eg', 1);('throwing17keys', 1);('ssv2templatemainlyfocusesontemporalunderstandingofactionswhilessv2label', 1);('comprehensive understanding ofboth appearance temporal dynamicmultiplechoicevideoqa', 1);('fivedatasetsareevaluatedformultiplechoice', 1);('tgifactionandtgiftransition', 1);('evaluatemodels capability', 1);('actions andstate transitions', 1);('candidate answers concatenate thequestion', 1);('text use', 1);('video candidate texts', 1);('tgifactioncontains', 1);('training 2k', 1);('tgiftransitions', 1);('gifquestion', 1);('pairs training 6kfor', 1);('msrvttmc', 1);('multiplechoice video', 1);('model nd theoptimal caption', 1);('candidatetextsnextqa 60isexplicitlydesignedfortemporalandcausal understanding', 1);('descriptive temporal causaleachquestioninthedatasetarepairedwith5candidateanswers', 1);('modelsability video question', 1);('video qa', 1);('weevaluate model datasets', 1);('msrvttqa', 1);('2k videos 47k questionstgifframes 16collectstheanswerablewithjustasingleframeinthevideoandisdividedintotrainingsetwith35kquestions test', 1);('14k questions', 1);('model needs', 1);('word theblank', 1);('video sentence blank', 1);('itcontains297ksentencesfortrainingand30ksentencesfortestingactivitynetqa', 1);('5for video', 1);('beforemsrvtt iscomposedof10kvideoswith20captionspervideo', 1);('msvdcontains', 1);('2k videos', 1);('wefollowthestandardsplitsfrom3237duringinferencewegeneratethecaptionwithbeamsearchuntil', 1);('model outputs', 1);('sep', 1);('end ofsentence', 1);('maximum generation step4018', 1);