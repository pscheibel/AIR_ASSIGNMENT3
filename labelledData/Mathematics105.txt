('pomdps', 50);('side information', 47);('pomdp', 45);('irl', 41);('information asymmetry', 28);('\x17 \x1b', 27);('\x16 \x1b', 25);('figure', 24);('scpforward', 23);('causal entropy', 19);('optimal policy', 18);('reward function', 18);('maze', 17);('mdps', 16);('ltl', 16);('a\x1b', 15);('r\x12', 15);('task specications', 12);('specically', 12);('mdp', 12);('expert demonstrations', 11);('s\x1b', 11);('sarsop', 11);('gail', 10);('evade', 10);('exit point', 10);('\x16\x17 \x1b', 10);('problem', 9);('proceedings', 9);('\x16\x16 \x1b', 9);('scp', 8);('pr\x1b', 8);('avoid', 8);('true reward function', 8);('visitation counts', 7);('obstacle', 7);('positive reward', 7);('robotics', 7);('temporal logic', 6);('performant policies', 6);('distr', 6);('bellman', 6);('cost function', 6);('\x16 ^\x1b', 6);('intercept', 6);('representative results', 6);('slip ]', 6);('international conference', 6);('machine learning', 6);('aaai', 6);('inverse reinforcement learning', 5);('markov', 5);('task specication', 5);('verication step', 5);('s\x02a7', 5);('rocks', 5);('current position', 5);('memoryless policies', 5);('time steps', 5);('p. abbeel', 5);('a. bagnell', 5);('citeseer', 5);('full information', 4);('data efciency', 4);('scalable manner', 4);('unity', 4);('partial information', 4);('nonconvex optimization problem', 4);('nonconvex problem', 4);('additionally', 4);('memoryless policy', 4);('fsc', 4);('a0', 4);('furthermore', 4);('2s\x02a\x00log\x17 \x1b', 4);('lp', 4);('high-level side information', 4);('\x17sp \x1b', 4);('solvepomdp', 4);('target state', 4);('rock', 4);('parameters n', 4);('view radius', 4);('computation time', 4);('b. d. ziebart', 4);('articial intelligence', 4);('u. topcu', 4);('m. wen', 4);('valuable rock', 4);('side informationr \x1b', 4);('decision processes', 3);('similar behavior', 3);('army research', 3);('work [', 3);('reward functions', 3);('temporal logic specications', 3);('learning process', 3);('ground robot', 3);('bayesian', 3);('r+', 3);('policy \x1b', 3);('remark', 3);('belief state', 3);('convex optimization problem', 3);('unknown reward function', 3);('belief updates', 3);('state-action visitation counts', 3);('nonconvex constraint', 3);('s02sx 2ap', 3);('compute', 3);('trust region', 3);('ow constraints', 3);('\x16 ~\x1b', 3);('cost c', 3);('specications', 3);('\x16sp \x1b', 3);('learning agent', 3);('stepswith', 3);('top row', 3);('atomic proposition', 3);('side information alleviates', 3);('compute policies', 3);('\x1b time', 3);('[ n', 3);('action scan', 3);('slippery probability', 3);('target states', 3);('a. y', 3);('ng', 3);('international journal', 3);('t. osa', 3);('ieee transactions', 3);('learning', 3);('a. k. dey', 3);('d. hsu', 3);('w. s. lee', 3);('ieee', 3);('k. bogert', 3);('p. doshi', 3);('instance', 3);('negative reward', 3);('ground truth reward', 3);('task-guided irl', 2);('craig lennon', 2);('such specications', 2);('intrinsic nonconvexity', 2);('algorithm learns reward functions', 2);('inverse', 2);('such information asymmetry', 2);('computational complexity', 2);('policy synthesis', 2);('maximum causal entropy', 2);('mce', 2);('nite-memory policies', 2);('partially', 2);('s7', 2);('m-fsc', 2);('memory states', 2);('memoryless policies \x1bon', 2);('pomdp mas', 2);('action sequence =f', 2);('time step', 2);('pomdpm', 2);('stochastic processes', 2);('z\x1b', 2);('time index t', 2);('tjjs0', 2);('ajs', 2);('return expectation', 2);('nx', 2);('b 2dx bi2b ix s2sbi', 2);('nonconvex formulation', 2);('state visitation', 2);('st', 2);('at=', 2);('indicator function', 2);('=p [', 2);('at=ajst=s', 2);('visitation counts \x16 \x1band\x17 \x1b', 2);('tp [', 2);('=\x16 \x1b', 2);('input', 2);('algorithm', 2);('maximize \x16 \x1b', 2);('slack variables', 2);('previous solution', 2);('nonconvex policy constraint', 2);('and\x17 ^\x1b', 2);('afne constraint', 2);('linear program', 2);('policy ~\x1b', 2);('\x17 ~\x1b', 2);('\x16sp ^\x1b', 2);('2s\x02aksp s', 2);('csp', 2);('sequential convex scheme', 2);('trust regions', 2);('theorem', 2);('for- ward problem', 2);('^\x1bvia linear constraint', 2);('numerical experiments', 2);('prob- lem', 2);('prism-pomdp', 2);('simulation', 2);('benchmark set', 2);('unknown routes', 2);('partial observation', 2);('side informationr\x12 \x1b', 2);('utilize side information suffer', 2);('bottom row', 2);('incorporate side information', 2);('memory leads', 2);('number', 2);('total reward', 2);('na na na na obstacle', 2);('na na', 2);('\x00 \x00', 2);('3600\x00 \x00', 2);('optimal policies', 2);('[ n ]', 2);('environ- ment', 2);('unknown reward', 2);('penalty cost', 2);('temporal logic task specication', 2);('agents ability', 2);('reward learning', 2);('partial observability', 2);('initial state distribution', 2);('] considers', 2);('special case', 2);('laboratory', 2);('ofce', 2);('naval', 2);('u.s.', 2);('apprenticeship learning', 2);('european conference', 2);('springer', 2);('s. j. russell', 2);('a. d. dragan', 2);('n. sugita', 2);('m. mitsuishi', 2);('online trajectory', 2);('systems', 2);('automation', 2);('principle', 2);('m. l. littman', 2);('theory', 2);('j.-p. katoen', 2);('proc', 2);('symp', 2);('i. papusha', 2);('demonstrations', 2);('finite-state controllers', 2);('s. junges', 2);('n. jansen', 2);('uai', 2);('k.-e. kim', 2);('j. peters', 2);('joint', 2);('occlu-', 2);('autonomous agents', 2);('ow constraint', 2);('p1', 2);('\x15\x17 \x1b+', 2);('\x15\x16\x16 \x1b+', 2);('\x15\x16 \x1b', 2);('2s\x02a\x00\x15\x17 \x1b', 2);('off-the-shelf solvers', 2);('current location', 2);('dangerous rock', 2);('stepsr', 2);('stepsmean', 2);('effects', 2);('experiment', 2);('exact setting', 2);('essentially', 2);('high probability', 2);('additional reward', 2);('optimal reward', 2);('scales franck djeumou\x03', 1);('ellis\x03\x03', 1);('murat cubuktepe\x03', 1);('ufuk topcu\x03 abstract', 1);('learning agent infers', 1);('reward function en-', 1);('unrealistic assumption', 1);('exces- sive', 1);('reward functionin', 1);('algorithm reduces', 1);('com- mon source', 1);('algorithmic complexity', 1);('sequential linear', 1);('introduction', 1);('trivial task', 1);('reinforcement learning', 1);('in- fers', 1);('\x03the university', 1);('texas', 1);('austin', 1);('\x03\x03the university', 1);('massachusetts', 1);('dartmouth', 1);('laboratory email', 1);('fdjeumou @ utexas.edu', 1);('franck djeumou', 1);('cellis3 @ umassd.edu', 1);('ellis', 1);('mcubuktepe @ utexas.edu', 1);('murat cubuktepe', 1);('craig.t.lennon.civ @ army.mil', 1);('utopcu @ utexas.edu', 1);('ufuk topcu', 1);('preprint', 1);('elsevier january', 1);('2023arxiv:2301.01219v1 [ cs.lg ]', 1);('dec', 1);('wide range', 1);('various domains', 1);('ac- robatic helicopter ight [', 1);('future actions', 1);('people [', 1);('human-autonomy interaction [', 1);('robotic surgery [', 1);('robotic manipulation tasks [', 1);('full state observations', 1);('envi- ronment [', 1);('internal states', 1);('works [', 1);('such information limitations', 1);('intrinsic assumptions', 1);('imperfect information', 1);('partial', 1);('observ- ability', 1);('rst challenge', 1);('so-called information asymmetry', 1);('partial view', 1);('experts demonstrations', 1);('hypothetical case', 1);('optimal pol-', 1);('solv-', 1);('optimal', 1);('in- nite memory', 1);('observations [', 1);('whereas memoryless policies', 1);('additional limitation', 1);('expressiv- ity', 1);('complex tasks [', 1);('describe requirements', 1);('target location', 1);('multiple locations', 1);('certain order', 1);('such requirements', 1);('temporal logic [', 1);('recent work', 1);('stochastic process', 1);('problem statement', 1);('opti- mization criterion', 1);('traditional entropy results', 1);('least-committal policy', 1);('experts demonstra- tions', 1);('additional assumptions', 1);('task speci- 2cations', 1);('task requirements', 1);('feasible behaviors', 1);('task re- quirements', 1);('new solver', 1);('improves computational tractability', 1);('task knowledge', 1);('temporal logic speci- cations', 1);('information asymmetry problem', 1);('nonconvex optimization problems [', 1);('convex optimiza- tion [', 1);('similar algorithmic benets', 1);('major difference', 1);('syn- thesis', 1);('iterative algorithm', 1);('adapting sequential convex', 1);('previous iteration', 1);('algorithm introduces', 1);('ex- tensions', 1);('algorithm computes', 1);('optimal solution', 1);('algorithm scales', 1);('observa- tions outperform memoryless policies [', 1);('nite-memory pol-', 1);('per- formant policies', 1);('incorporate memory', 1);('nite-state controllers [', 1);('utilize memory', 1);('d unity', 1);('algo- rithm', 1);('straight adaptation', 1);('performance gap', 1);('task specica- 3tions', 1);('extensive comparisons', 1);('state-of- the-art', 1);('preliminaries', 1);('prerequisite understanding', 1);('notation', 1);('real numbers', 1);('proba- bility distributions', 1);('xbydistr', 1);('sequences x0', 1);('function gof', 1);('random variables', 1);('xandyby ex', 1);('[ g', 1);('markov decision process', 1);('decision process', 1);('sequential interaction', 1);('observable environment', 1);('noisy observation', 1);('.we dene', 1);('m=', 1);('andzare nite', 1);('function \x160', 1);('r+provides', 1);('initial distribution', 1);('agents state', 1);('discount factor', 1);('innite planning horizon', 1);('decision time', 1);('action 2a', 1);('transition function', 1);('state s02sgiven', 1);('current state s2s', 1);('state transition', 1);('observation z02', 1);('function o', 1);('probability o', 1);('z0at states0', 1);('rencoding', 1);('innite-horizon problems', 1);('z\x02a', 1);('pomdp mmaps', 1);('m-', 1);('nite-state controller', 1);('initial memory state', 1);('q\x02z7', 1);('decision function', 1);('q\x02z\x02a7', 1);('memory transition function', 1);('theaction', 1);('memory state nand', 1);('observation z2z', 1);('memory update \x0e', 1);('joint execution', 1);('4memory state', 1);('memoryless fscs', 1);('observation z', 1);('reduction to memoryless policies', 1);('synthesize optimal', 1);('m-fscs', 1);('so-called for- ward problem', 1);('so-called product', 1);('memory update \x0eand', 1);('belief update', 1);('tgandtis', 1);('belief state species', 1);('initial belief b0=\x160', 1);('bayes', 1);('rule bt+1', 1);('ran- dom variables', 1);('andz\x1b ttake values st2s', 1);('tcausally-conditioned', 1);('=qt t=0p', 1);('atjs0', 1);('correlation be- tween', 1);('atis', 1);('s0', 1);('future variables', 1);('st+1', 1);('t. leth', 1);('ea', 1);('s [ \x00logp', 1);('conditional entropy', 1);('agiven', 1);('s.', 1);('nite-horizon setting', 1);('h\x1bin-', 1);('policy \x1bis', 1);('h\x1b', 1);('[ \x00logp', 1);('tjjs\x1b', 1);('] =pt t=0h', 1);('innite-horizon setting', 1);('causal entropy [', 1);('=x1 t=0 th', 1);('=x1 t=0 tea\x1b t', 1);('t [ \x00logp', 1);('tjs\x1b t', 1);('theentropy', 1);('future policy decisions [', 1);('] show', 1);('reinforcement learning techniques', 1);('tran- sition function', 1);('ltl specications', 1);('general linear temporal logic', 1);('complex task specications', 1);('apof', 1);('atomic propositions', 1);('boolean', 1);('truth values', 1);('state sor observation z', 1);('= truejaj', 1);('logic negation', 1);('xanduare', 1);('anduntil temporal operators', 1);('temporal operators', 1);("u'and g", 1);('policy \x1bon', 1);('pomdpm.a', 1);('problem formulation', 1);('pomdp mwith', 1);('ralong', 1);('policy \x1bthat in- duces', 1);('expert trajectory', 1);('tg', 1);('similarly', 1);('belief trajectory b =fb0', 1);('time index i', 1);('belief trajectoriesd=fb', 1);('ngfrom', 1);('total number', 1);('rby', 1);('differentiable function', 1);('rd', 1);('traditional representations', 1);('rdis', 1);('basis functions', 1);('function approximator', 1);('neural networks', 1);('traditional linear', 1);('parameter \x12deningr\x12and', 1);('policy \x1bsuch', 1);('return expectation \x16r\x12of', 1);('expert demonstration', 1);('d.', 1);('=1x t=0 tes\x1b t', 1);('t [', 1);('j\x1b ] and\x16r\x12=1', 1);('above condition', 1);('r\x12with', 1);('feature function', 1);('policy \x1bthat', 1);('policy \x1band weight\x12such', 1);('priori high-level side informa- tion', 1);('linear temporal logic formula', 1);('policy \x1band weight \x12such', 1);('parameter \x15\x150', 1);('parameter \x15that species', 1);("'is as-", 1);('optimal \x15', 1);('saddle points', 1);('nonconvex functions', 1);('such saddle points', 1);('state memory pairs [', 1);('substituting visitation counts', 1);('time dependency', 1);('r+andstate-action', 1);('=est [ 1x t=1 t1fst=sgj\x1b ] and\x17 \x1b', 1);('[ 1x t=1 t1fst=s', 1);('gj\x1b ]', 1);('=p 2a\x17 \x1b', 1);('concave expression', 1);('=x1 t=0 tes\x1b t', 1);('t [ \x00log', 1);('] =x1 t=0x', 1);('t=s ] =x', 1);('rst equality', 1);('state-action visitation', 1);('above expression', 1);('linear expression', 1);('\x17 \x1bfor', 1);('\x1b=1x t=0x', 1);('t= ] =x', 1);('ow constraint [', 1);('state s2s', 1);('saddle', 1);('formulation', 1);('policy \x1bthat satises', 1);('constraintr\x12 \x1b=\x16r\x12might', 1);('empirical estimate', 1);('d. additionally', 1);('full observation', 1);('saddle point computation problem', 1);('weight vector \x12and policy\x1bof', 1);('= max\x1bh \x1b+', 1);('function fcorresponds', 1);('inner optimization problem', 1);('reward parameter', 1);('policy \x1bthat maximizes', 1);('\x1b+r\x12 \x1bof', 1);('causal 8algorithm', 1);('weight vector \x12and policy\x1bsolution', 1);('lagrangian', 1);('feature', 1);('expectation \x16r fromd', 1);('initial weight \x120', 1);('step size\x11', 1);('n7', 1);("priori side information 'and\x152 [", 1);('\x1b0 uniform policy .initialize uniform policy', 1);('.compute\x12via gradient descent', 1);("optional 'and\x15", 1);('\x12k+1 \x12k\x00\x11', 1);('.gradient step', 1);('\x12k entropy', 1);('current estimate', 1);('entropy term', 1);('reward weights', 1);('gradient descent', 1);('random uniform', 1);('weight \x120is', 1);('nonzero vector', 1);('policy\x1bk+1= arg max\x1bh \x1b+', 1);('r\x12k', 1);('current reward estimate', 1);('r\x12kgiven', 1);('weight \x12', 1);('gradientr\x12fwith respect', 1);('=x s', 1);('2s\x02a\x17 \x1b', 1);('high-level task specications', 1);('forward problem', 1);('weight vector \x12k', 1);('formulate theforward problem', 1);('=x 2a\x17 \x1b', 1);('constant \x00\x16r\x12kfrom', 1);('above optimization problem', 1);('sequential linear programming formulation', 1);('sequential convex program-', 1);('in-', 1);('sound policies', 1);('visi- tation counts', 1);('de- scribe', 1);('high-level task specication', 1);('prob-', 1);('slight modications', 1);('optimization problem', 1);('linearizing nonconvex optimization problem scpforward', 1);('nonconvex constraints', 1);('pre- vious solution', 1);('linear subproblem [', 1);('ensure feasibility', 1);('non- convex problem', 1);('problem deviate', 1);('utilize trust region constraints [', 1);('past iterations', 1);('linearizing nonconvex constraints', 1);('adding slack variables', 1);('.we linearize', 1);('slack variables ks', 1);('=\x16 ^\x1b', 1);('\x00 \x16 \x1b', 1);('\x00\x16 ^\x1b', 1);('\x01x z2zo', 1);('region constraints', 1);('.the linearization', 1);('solution deviates', 1);('trust region constraints', 1);('term \x00', 1);('convex causal entropy cost function', 1);('2s\x02a\x00 ks', 1);('\x00\x10\x17 ^\x1b', 1);('\x11 \x16 \x1b', 1);('+\x10 log\x17 ^\x1b', 1);('+ 1\x11 \x17 \x1b', 1);('inaccurate visitation counts ~\x17 ~\x1b', 1);('~\x16 ~\x1bmight', 1);('fea- sible', 1);('potential infea- sibility', 1);('optimal cost', 1);('feasible visiation counts', 1);('schemes linearizes', 1);('inaccurate solutions for~\x17 ~\x1b', 1);('~\x16 ~\x1b', 1);('verication step solves', 1);('sound solution', 1);('\x16 ~\x1bby', 1);('\x16 ~\x1b\x150is', 1);('computes\x17 ~\x1b', 1);('=\x16 ~\x1b', 1);('current iteration', 1);('2s\x02a\x00log\x17 ~\x1b', 1);('\x16 ~\x1b\x17 ~\x1b', 1);('previous cost c', 1);('verication step rejects', 1);('con- tracts', 1);('previous solutions ^\x1b', 1);('otherwise', 1);('\x16 ~\x1band\x17 ~\x1b.by', 1);('verica- tion step', 1);('incorporating high-level', 1);('agent tasks', 1);('rst compute', 1);("'to nd", 1);('sett \x12s', 1);("satisfying 'with probability 1by", 1);('reachability specications', 1);('t.', 1);("satisfying 'is", 1);('target states s2t', 1);('state visitation count\x16sp \x1b', 1);('=p s2t\x16sp \x1b', 1);('unless', 1);('\x16sp \x1b6=\x16 \x1b', 1);('new variables \x16sp \x1b', 1);('adequate constraints', 1);('incorporating undiscounted visitation variables', 1);('linearized problem', 1);('.we append', 1);('new constraints', 1);('variables \x16 \x1b', 1);('\x17 ^\x1bare', 1);('ksp s', 1);('\x17sp ^\x1b', 1);('constraint \x16sp \x1b', 1);('+x s02sntx 2ap', 1);('such that\x16sp \x1b', 1);('counts transitions', 1);('non-target states', 1);('relaxing specication constraints', 1);('.to incorporate', 1);('=x s2t\x16sp \x1b', 1);('+ \x00sp\x15\x15', 1);('introduce \x00sp\x150as', 1);('\x00 sp\x00spto penalize', 1);('positive hyperparameter', 1);('updating verication step', 1);('.we modify', 1);('linearization inaccuracy', 1);('new policy constraint', 1);('and\x17sp \x1b', 1);('accurate \x16sp ~\x1bof', 1);('current pol- icy~\x1bthrough', 1);('~\x1b= minf0', 1);('s2t\x16sp ~\x1b', 1);('specication constraints', 1);('convergence', 1);('optimum solution', 1);('.the convergence guarantees', 1);('general convergence', 1);('sequential convex', 1);('4:7of [', 1);('weak convergence', 1);('algorithm generates', 1);('convergent subsequences', 1);('rst-order conditions [', 1);('strict sense', 1);('potential oscillation', 1);('limit points', 1);('convergence 12algorithm', 1);('linear', 1);('policy \x1bk+1that maximizes', 1);('current', 1);('weight estimate \x12k', 1);('policy ^\x1b=\x1bk', 1);('trust region \x1a >', 1);('penalization coefcients', 1);('contract trust region', 1);('threshold \x1alimfor trust region contraction', 1);('find\x16', 1);('\x17 ^\x1b=\x16 ^\x1b', 1);('realized', 1);('find\x16sp', 1);('\x17sp ^\x1b=\x16sp ^\x1b', 1);("^\x1b .if'is", 1);('specications violation', 1);('while\x1a > \x1a limdo .trust region threshold', 1);('find', 1);('optimal ~\x1bto', 1);('\x17 ^\x1b', 1);('\x17sp ^\x1b.we', 1);('\x00 sp\x00spto', 1);('\x16sp ~\x1b', 1);('\x17sp ~\x1b', 1);('f^\x1b ~\x1b', 1);('\x1a \x1a\x1a0gifc', 1);('elsef\x1a \x1a=\x1a0g.verication step', 1);('= ^\x1b claims', 1);('nonlinear optimization schemes fall', 1);('right regularity assumptions', 1);('local optimum solution', 1);('super- linear convergence rate', 1);('straightforward variant', 1);('state-of-the-art solvers', 1);('irl pomdps', 1);('hand-crafted pomdp instances', 1);('instances ex-', 1);('inavoid', 1);('certain preset', 1);('good rock', 1);('static obstacles', 1);('trap states', 1);('variants', 1);('learned', 1);('experts .we', 1);('uses side information', 1);("temporal speci- cation'or", 1);('memory size m =', 1);('orm =', 1);('rst expert', 1);('envi- ronment', 1);('memory size', 1);('m =', 1);('recall', 1);('there-', 1);('rst type', 1);('expert corresponds', 1);('ofine computation', 1);('doing', 1);('signicant advantage', 1);('high performance', 1);('corresponding policies', 1);('similar patterns', 1);('detailed', 1);('14no information asymmetry', 1);('100\x00200204060finite-memory policy', 1);('100memoryless policy', 1);('steps figure', 1);('performance drop', 1);('left column', 1);('algorithm outperforms', 1);('expert reward', 1);('value 47:83for', 1);('maze example', 1);('pomdpmis', 1);('cell labels', 1);('o=fo1', 1);('observations o6ando7denote', 1);('bad states', 1);('transition model', 1);('states s13ands14lead', 1);('=\x001for alls', 1);('reachings13with target', 1);('ifs=s13and target', 1);('bad states s12ands14with', 1);("formula '=g", 1);('current state s=s12ors=s14', 1);('signicant performance drop', 1);('policies 15with side information', 1);('stepstotal reward figure', 1);('performant', 1);('.the results', 1);('policies improves', 1);('learning policies', 1);('incorporating', 1);('nite-memory policy decreases', 1);('androcks benchmarks', 1);('side information improves data efciency', 1);('low data regime', 1);('trajectoriestotal rewardwithout', 1);('ltl opt', 1);('rew', 1);('left shows', 1);('right shows', 1);('sarsop solvepomdp problem', 1);('jsj js\x02oj joj', 1);('na na na na maze', 1);('12:219:83 0:05\x00 \x00', 1);('10:2819:83 13:71\x00 \x00', 1);('13:1819:81 81:19\x00 \x00', 1);('3600\x00 \x00 \x00 \x00', 1);('9:19\x00 \x00', 1);('210:47\x00 \x00', 1);('\x00 \x00 \x00 \x00', 1);('time limit', 1);('andavoid [ n', 1);('empty cell', 1);('na', 1);('side information improves performance', 1);('play- ers', 1);('players speed', 1);('andslip specify', 1);('side information induces', 1);('left', 1);('clearpath warthog', 1);('right', 1);('scpforward yields', 1);('scalability', 1);('andprismpomdp runs', 1);('large state space', 1);('state spaces', 1);('ef- ciency', 1);('small-size problems', 1);('obser- vations', 1);('continuous 3-d', 1);('clearpath', 1);('screen shot', 1);('corresponding trajectory', 1);('environment contains', 1);('terrain types', 1);('state space', 1);('of3350 states', 1);('944total observations', 1);('grass gravel road', 1);('gridworld', 1);('gure shows', 1);('unity environment', 1);('experimental scenarios', 1);('partial observations', 1);('ground vehicle contains', 1);('autonomy stack', 1);('main subsys-', 1);('omni- mapper', 1);('] performs', 1);('simultaneous localization', 1);('slam', 1);('lidar', 1);('imu', 1);('perception subsystem', 1);('pixel level semantic segmentation', 1);('video stream', 1);('rgb', 1);('object classes', 1);('semantic image', 1);('terrain projection algorithm', 1);('nbinary', 1);('nis', 1);('planning subsystem uses', 1);('previous subsystems', 1);('reward feature encoding', 1);('10demonstra- tions', 1);('implicit preference', 1);('linear combination', 1);('r\x12=\x121', 1);('road+\x122 gravel+\x123 grass+\x124 time+\x125 goal', 1);('corresponding state', 1);('shortest path', 1);('feature time penalizes', 1);('further-', 1);('learner exploit-', 1);('mdp expert pomdp', 1);('evolution', 1);('cumulative reward', 1);('environment inter- actions', 1);('expert mdp', 1);('expert pomdp', 1);('opti- mal policies', 1);('ground truth reward function', 1);('impact', 1);('values \x12= [', 1);('ground truth reward weight vector', 1);('above ground truth reward', 1);('traverses gravel', 1);('high penalty reward', 1);('modeling robot dynamics', 1);('ground truth map', 1);('learners behavior', 1);('entire state space', 1);('entire map', 1);('radius r=', 1);('grid cells', 1);('incorporate uncertainty', 1);('sensor classication', 1);('probabilityp= 0:9the prediction', 1);('.in addition', 1);('ugoal', 1);('.figure 7a shows', 1);('gravel com-', 1);('7b shows', 1);('side 20information increases', 1);('ad-', 1);('demonstrator trajectories', 1);('eval- uation', 1);('expert demonstra- tions', 1);('related', 1);('classical maximum-margin-', 1);('suboptimal demon- strations', 1);('alternatives [', 1);('efcient off-the-shelf solvers', 1);('outperforms off-the-shelf solvers', 1);('magni- tude', 1);('incorporate task specications', 1);('formulations [', 1);('probabilistic models', 1);('expert demonstrations [', 1);('algorithmic benets', 1);('reference', 1);('future trajectories', 1);('methods incorporate side information', 1);('aug- ment', 1);('previous work', 1);('authors incor- porate side information', 1);('temporal logic specication', 1);('refer-', 1);('ence [', 1);('task 21specication', 1);('conclusion', 1);('partial obser- vation', 1);('learning scheme', 1);('al- gorithm', 1);('outperforms state-of- the-art', 1);('large number', 1);('limitations', 1);('.this work assumes', 1);('observation functions', 1);('acknowledgements', 1);('cooperative agreement number', 1);('arl w911nf-20-2-0132', 1);('arl w911nf-19-2-0285', 1);('onr n00014-22-1-2254', 1);('ofcial policies', 1);('copyright notation herein', 1);('references', 1);('a. coates', 1);('autonomous helicopter aerobatics', 1);('k. m. kitani', 1);('m. hebert', 1);('activity forecasting', 1);('computer vision', 1);('d. hadeld-menell', 1);('cooperative inverse reinforcement learning', 1);('nips', 1);('s. s. srinivasa', 1);('policy-blending formalism', 1);('shared', 1);('dynamic envi-', 1);('surgical', 1);('automation.', 1);('force control', 1);('surgical tasks', 1);('engineering', 1);('c. finn', 1);('s. levine', 1);('guided', 1);('deep inverse optimal con-', 1);('policy optimization', 1);('pmlr', 1);('a. l. maas', 1);('maximum entropy inverse reinforcement learning', 1);('z. zhou', 1);('m. bloem', 1);('n. bambos', 1);('innite', 1);('horizon maximum causal entropy inverse reinforcement learning', 1);('automatic', 1);('modeling interaction', 1);('s. c. ong', 1);('s. w. png', 1);('robotic tasks', 1);('h. bai', 1);('integrated perception', 1);('continuous', 1);('pomdp approach', 1);('s. zhang', 1);('j. sinapov', 1);('s. wei', 1);('p. stone', 1);('robot behavioral exploration', 1);('mul-', 1);('perception', 1);('symposium', 1);('interactive multisensory object perception', 1);('embodied agents', 1);('k. akash', 1);('k. polson', 1);('t. reid', 1);('n. jain', 1);('improving human-machine collabora-', 1);('transparency-based feedbackpart', 1);('human trust', 1);('workload model', 1);('ifac-papersonline', 1);('x. liu', 1);('a. datta', 1);('modeling context aware dynamic', 1);('hidden markov model', 1);('n. vlassis', 1);('d. barber', 1);('stochastic controller optimization', 1);('acm trans', 1);('comput', 1);('o. madani', 1);('s. hanks', 1);('a. condon', 1);('undecidability', 1);('probabilistic', 1);('innite-horizon partially', 1);('markov decision problems', 1);('j. fu', 1);('c. isbell', 1);('j. macglashan', 1);('environment-', 1);('independent task', 1);('gltl', 1);('arxiv preprint arxiv:1704.04341', 1);('c. baier', 1);('principles', 1);('model checking', 1);('mit', 1);('a. pnueli', 1);('programs', 1);('annu', 1);('found', 1);('computer sci.', 1);('washington', 1);('d.c.', 1);('usa', 1);('4657. doi:10.1109/', 1);('sfcs.1977.32', 1);('f. memarian', 1);('z. xu', 1);('b. wu', 1);('active task-inference-guided deep inverse reinforcement learning', 1);('deci-', 1);('cdc', 1);('twenty-sixth', 1);('joint con-', 1);('maximum causal en-', 1);('estimating interacting processes', 1);('information theory', 1);('yuan', 1);('recent advances', 1);('region algorithms', 1);('mathematical pro-', 1);('mao', 1);('m. szmuk', 1);('x. xu', 1);('b. ac', 1);('successive', 1);('convergent algorithm', 1);('non-convex optimal control problems', 1);('arxiv preprint arxiv:1804.06539', 1);('h. yu', 1);('d. p. bertsekas', 1);('near optimality', 1);('set', 1);('average', 1);('mathematics', 1);('operations', 1);('r. wimmer', 1);('t. quatmann', 1);('l. winterer', 1);('b. becker', 1);('parameter synthesis', 1);('n. meuleau', 1);('l. p. kaelbling', 1);('a. r. cassandra', 1);('solving pomdps', 1);('nite policies', 1);('c. amato', 1);('d. s. bernstein', 1);('s. zilberstein', 1);('optimizing fixed-size stochastic con-', 1);('decentralized pomdps', 1);('aamas', 1);('j. ho', 1);('s. ermon', 1);('generative', 1);('adversarial imitation learning', 1);('advances', 1);('neural information processing systems', 1);('j. massey', 1);('causality', 1);('int', 1);('inf', 1);('theory applic', 1);('isita-90', 1);('g. kramer', 1);('directed information', 1);('channels', 1);('feedback', 1);('l. p. hansen', 1);('t. j. sargent', 1);('g. turmuhambetova', 1);('n. williams', 1);('robust', 1);('model misspecication', 1);('j. choi', 1);('en-', 1);('s. a. seshia', 1);('enforcing', 1);('almost-sure reachability', 1);('computer aided verication', 1);('e. walraven', 1);('m. spaan', 1);('accelerated vector pruning', 1);('optimal pomdp solvers', 1);('h. kurniawati', 1);('efcient', 1);('pomdp planning', 1);('reachable belief spaces.', 1);('g. norman', 1);('d. parker', 1);('x. zou', 1);('verication', 1);('observable probabilistic systems', 1);('real-time systems', 1);('j. pajarinen', 1);('g. neumann', 1);('j. bagnell', 1);('algorithmic perspective', 1);('imitation learning', 1);('foundations', 1);('trends', 1);('n. d. ratliff', 1);('m. a. zinkevich', 1);('maximum margin', 1);('inverse reinforcement learn-', 1);('algorithms', 1);('inverse reinforcement learning.', 1);('d. ramachandran', 1);('e. amir', 1);('bayesian inverse reinforcement learning.', 1);('ij- cai', 1);('a. boularias', 1);('o. kr', 1);('structured apprenticeship learning', 1);('knowledge discovery', 1);('databases', 1);('multi-robot inverse reinforcement learning', 1);('interactions', 1);('multi-agent systems', 1);('toward estimating', 1);('transition', 1);('multi-robot irl', 1);('twenty-fourth', 1);('j. f.-s. lin', 1);('d. kulic', 1);('expectation-maximization', 1);('hidden data', 1);('inter-', 1);('national conference', 1);('multiagent systems', 1);('inverse optimal', 1);('regular language specications', 1);('annual', 1);('american control conference', 1);('acc', 1);('appendices', 1);('supplementary derivations', 1);('a. concavity', 1);('derivations', 1);('bellman constraints', 1);('rst recall', 1);('in- verse reinforcement learning', 1);('additional details', 1);('concave causal entropy', 1);('.we rst recall', 1);('state visitation counts', 1);('est', 1);('t=1 t1fst=sg ]', 1);('eat', 1);('t=1 t1fst=s', 1);('g ]', 1);('state-action visitation counts \x16 \x1band\x17 \x1b', 1);('= \x1bs', 1);('section 4that', 1);('=\x17 \x1b', 1);('claim thath \x1bis aconcave fucntion', 1);('function f', 1);('variables \x17 \x1b', 1);('\x16 \x1band\x16\x17 \x1b', 1);('2s\x02a\x00log\x15\x17 \x1b', 1);('\x15\x17 \x1b', 1);('log\x15\x17 \x1b', 1);('log\x17 \x1b', 1);('log\x16\x17 \x1b', 1);('rst inequality', 1);('well-known log-sum inequality', 1);('x1logx1 y1+x2logx2 y2\x15', 1);('logx1+x2 y1+y2', 1);('nonnegative numbers x1', 1);('substitution x1= \x15\x17 \x1b', 1);('y1=\x15\x16 \x1b', 1);('inequality f', 1);('and\x16 \x1b', 1);('bellman flow constraint', 1);('valid policy', 1);('\x17 \x1band\x16 \x1bmust', 1);('bellman ow constraint', 1);('=es\x1b th1x t=0 t1fs\x1b t=sgi =\x160', 1);('es\x1b', 1);('th1x t=0 t1fs\x1b t+1=sgi =\x160', 1);('+ 1x t=0x s02sx 2a tp', 1);('t= ] =\x160', 1);('experimental tasks', 1);('benchmark examples', 1);('demon- strate', 1);('additional numerical simulations', 1);('computation resources', 1);('assets', 1);('intel core', 1);('cpu', 1);('ram', 1);('python', 1);('supplementary material', 1);('required tools.', 1);('stormpy', 1);('ofstorm [', 1);('gurobipy', 1);('storm', 1);('nite state con- troller', 1);('synthesis problem', 1);('tof', 1);("specication 'via graph prepro-", 1);('gurobi', 1);('feasible solution', 1);('forward problem.', 1);('approximate value iterations', 1);('incremen- tal', 1);('state-of-the-art vector', 1);('methods [', 1);('prismpomdp', 1);('nite memory strategy', 1);('approximate solution', 1);('default settings', 1);('readme les', 1);('b.2', 1);('off-the-shelf solver', 1);('current state', 1);('different memory policies', 1);('agent navigates', 1);('sam- ple', 1);('dangerous rocks', 1);('agent realizes', 1);('noisy sensors', 1);('neighbor cells', 1);('300050100finite-memory policy', 1);('300050100memoryless policy', 1);('priori knowledge', 1);('informa- tion asymmetry', 1);('instance.', 1);('ve static obstacles', 1);('exit state', 1);('parameter nspecies', 1);('100\x002000200400finite-memory policy', 1);('100\x002000200400memoryless policy', 1);('androck examples', 1);('side in- formation alleviates', 1);('.evade [ n', 1);('feature functions', 1);('high negative reward', 1);('small negative reward', 1);('scan action', 1);('standard deviation', 1);('different memory size policies', 1);('policy obtains', 1);('comparable policies', 1);('optimal memoryless policy', 1);('similar performance', 1);('.intercept [ n', 1);('player exits', 1);('eventually', 1);('location coincides', 1);('players location', 1);('b.3', 1);('additional experiments', 1);('side information speeds', 1);('reward parameters', 1);('side infor- mation', 1);('demonstra- tions', 1);('ex- periments', 1);('learning algorithm', 1);('lineariza- tions', 1);('gradient steps', 1);('reward updates', 1);('high probabil- ity', 1);('low probability', 1);('true reward', 1);('op- timal reward', 1);('goal state', 1);('etarget', 1);('subopti- mal', 1);('b.4', 1);('summary', 1);('temporal logic specications alleviates', 1);('33these examples', 1);('partial observability reduces', 1);('learning procedure', 1);('performance', 1);('memory improves', 1);