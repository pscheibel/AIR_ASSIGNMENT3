('infopro', 21);('e2e', 14);('gpu', 13);('international conference', 10);('swintiny', 9);('transformer', 9);('deconv', 9);('proceedings', 9);('transformers', 8);('figure', 7);('machine learning', 7);('pmlr', 6);('infoprotransformer', 5);('superior performance', 4);('cnns', 4);('local learning', 4);('swin transformer', 4);('imagenet', 4);('auxiliary block', 3);('cifar10 cifar100', 3);('linear', 3);('contrast', 3);('corr', 3);('cnn', 2);('approach outperforms', 2);('memory network dividedinto', 2);('memory network', 2);('novel local learning framework', 2);('training strategy method', 2);('wang', 2);('learning', 2);('input images', 2);('swintiny transformer', 2);('swmsa', 2);('number tokens', 2);('overview', 2);('swint', 2);('local modules', 2);('local module', 2);('memory consumption', 2);('comparison', 2);('bold', 2);('interp', 2);('mb', 2);('accuracy', 2);('cifar10', 2);('gb', 2);('cifar100', 2);('stl10', 2);('ablation', 2);('cifar100 stl10 mem mbe2e', 2);('swinbase', 2);('machinelearning', 2);('proceedings machine learningresearch', 2);('jun', 2);('jul', 2);('ieee', 2);('computational linguistics', 2);('research pages', 2);('advances neural information processing systems', 2);('international conference oncomputer', 2);('learning transformers', 1);('feature reconstructionpriyank pathak jingwei zhang dimitris samarasstony', 1);('brook university', 1);('stony', 1);('ny', 1);('usafprpathak', 1);('jingwezhang samaras gcsstonybrookeduabstracttransformers', 1);('popular due totheir', 1);('conventional convolutionalneural networkscnns', 1);('amount memory train', 1);('cnnswhich', 1);('prevents application', 1);('low resource settings', 1);('local learning divides network', 1);('distinct modules trains', 1);('alternative endtoend', 1);('training approach', 1);('amount memory training toincrease parallelism paper rst', 1);('locallearning', 1);('transformers purpose', 1);('local learning method', 1);('reconstructs input images module', 1);('entire image generalizewell paper', 1);('new mechanism eachlocal module', 1);('entire image reconstruct input', 1);('previous modules', 1);('decoder structures onswintiny experiments', 1);('infoprotransformer infopro transfomerbackbone', 1);('cifar10cifar100 stl10 svhn', 1);('compared e2e', 1);('introductiontransformers', 1);('computer vision tasks image classication segmentation detection usage', 1);('computer vision communitythe major advantage', 1);('conventional convolutional neural networkscnns', 1);('compared cnns transformers', 1);('local global selfattention', 1);('convolutional layers improveperformance', 1);('due quadratic complexity ofthe selfattention mechanism', 1);('muchlarger memory training', 1);('similar conditions', 1);('similar accuracy', 1);('deitb transformer', 1);('times moreparameters', 1);('regnety8g', 1);('convolutional modelthis', 1);('large memory requirement prevents applicationin', 1);('low resource devices', 1);('strong needfor techniques', 1);('memory requirements themodels', 1);('paradigm training', 1);('frameworks endtoend', 1);('backpropagation error gradients', 1);('rst layer mechanism memory inefcient', 1);('entire computationalgraph needs', 1);('memory intermediate variables', 1);('convolutional model', 1);('consecutive modulesthe primary advantage local learning', 1);('memory cost', 1);('entire computation graph model', 1);('higherdegree parallelization facilitates', 1);('appliedto realworld applications gigapixel imageclassication', 1);('privacy protection 11the major drawback local learning methods thatthey', 1);('inferior performance', 1);('toe2e approaches', 1);('recently wang', 1);('theinfopro method local learning', 1);('encourages network', 1);('possible respect input image', 1);('layers achievethis', 1);('reconstruction input images', 1);('whichbrings signicant memory overhead 26in paper', 1);('framework local module', 1);('entire input image', 1);('reconstruct modules inputfeatures', 1);('previous modules bestarxiv221214215v1 cscv', 1);('dec', 1);('2022knowledge rst local learning paper', 1);('method baseline simplyintegrates', 1);('transfomers infopro', 1);('framework whichwe', 1);('backbone method outperforms', 1);('cifar10 cifar100 stl10 svhndatasets', 1);('comparedto e2e', 1);('lessgpu memory network', 1);('accuracydrop show approach', 1);('swinlargeand swinbase transformers', 1);('related workvision transformers transformers', 1);('natural language problems achievedgreat success eld models', 1);('bert', 1);('vision transformer vit', 1);('model computer vision', 1);('cvtasks', 1);('images patches', 1);('multihead selfattention', 1);('intensive method', 1);('context longrange dependency tokens', 1);('trainable positional', 1);('different vanilla', 1);('touvron', 1);('dataefcient vision', 1);('performance thanvit', 1);('pyramid vision transformerpvt', 1);('tokens network wentdeeper', 1);('pyramid fashion likecnns', 1);('vit', 1);('denseprediction tasks', 1);('20is rst', 1);('time segmentation detectiontasks', 1);('local windows', 1);('relative positional', 1);('pvt', 1);('pyramid structuresome', 1);('recent works', 1);('broughtdown complexity', 1);('measures training', 1);('amemoryintensive tasklocal', 1);('training train neural networks dividesdeep learning models', 1);('consecutive modules', 1);('error gradients', 1);('specic modules gradient', 1);('local loss functions approaches', 1);('cascade learning9', 1);('module approaches', 1);('trainedall modules', 1);('additional classier toeach local module', 1);('nal target evaluatedtheir method', 1);('classication loss eg cross entropy', 1);('taskrelevant information distinguishingfeatures', 1);('early layersmodules', 1);('major improvement', 1);('previous work', 1);('taskrelevant information', 1);('infoproreconstructed', 1);('information gain input imagesand', 1);('reconstruction ofthe', 1);('entire input images', 1);('high computational cost', 1);('information gain', 1);('auxiliary decoder', 1);('towardthe choice auxiliary network contrast', 1);('previous approaches method relies', 1);('residual network regularization purposes', 1);('entire image', 1);('memory consumption gigapixel images notprovide analysis', 1);('input image', 1);('guo', 1);('showedimprovements performance', 1);('input albeit cost', 1);('small memory overhead', 1);('sharing', 1);('basic units', 1);('transformerwould', 1);('huge memory overhead3', 1);('methodology31', 1);('swin transformerin', 1);('architectureas backbone', 1);('solid lines', 1);('input image xwith height', 1);('hand', 1);('widthw network rst converts input image', 1);('patch generation thusthe', 1);('input generatesh4\x02w4number patches rstlayer', 1);('linear embedding', 1);('embeds patches tokensfeatures patches', 1);('4transformer stages', 1);('number oftransformer blocks blocks', 1);('pair namelywmsa', 1);('layers', 1);('local learning framework', 1);('dottedlines', 1);('modules blocks', 1);('thearchitecture auxiliary block', 1);('local learning gradients', 1);('red arrows', 1);('modulesthe stages', 1);('yellow lightblue', 1);('pink rectangles', 1);('stages respectively32', 1);('frameworkour local learning framework rst divides entiretransformer network layer layer', 1);('kconsecutivemodules g1g2 gk', 1);('optimizes separatelysuch network', 1);('pairs xyon whichxdenotes input image ythe', 1);('corresponding labela module', 1);('contains layers', 1);('original networkfor example rst module contains 2transformer encoders', 1);('patch merging', 1);('assumethe input module', 1);('gii', 1);('1k isxi\x001and outputisxigixi\x001 andx0is input image overviewof approach', 1);('indicatedto train module', 1);('giwith', 1);('input xi\x001locallywe use', 1);('auxiliary block fi', 1);('compute loss aslifigixi\x001xi\x001y', 1);('fixixi\x001y modulegiis', 1);('li', 1);('modulegi1has input xigixi\x001and', 1);('auxiliary block fi1using', 1);('method abovethis method', 1);('gkis', 1);('thenal classier', 1);('h linear embedding', 1);('module andis', 1);('rst module33', 1);('network', 1);('patch', 1);('transformer blockswin', 1);('conguration k2 k4swintiny', 1);('conguration swintiny swinlarge', 1);('means thenumber', 1);('blocks stage', 1);('andk4indicate distribution', 1);('blocks modulesection 33for implementation convenience', 1);('multiple ways', 1);('kmodules', 1);('4modules 1st', 1);('transformer blocks', 1);('division ofother variations', 1);('swin transformer swinbase swinlarge', 1);('memory footprints', 1);('possible way', 1);('conguration minimizes', 1);('maximum amount computationalmemory local modules34', 1);('auxiliary blockour', 1);('overall design', 1);('auxiliary block fiis', 1);('similar tothat', 1);('ith modulegiwhich outputs xigixi\x001xiis', 1);('fithe', 1);('orange rectanglethe', 1);('local loss', 1);('lito', 1);('supervisethe local module', 1);('gi', 1);('block xiis rst', 1);('layer normalization ln', 1);('followedby convolutional block dimensionality reduction', 1);('ln cb', 1);('contrastive loss reconfigure', 1);('architecture section', 1);('giis', 1);('ith module input xi\x001 output', 1);('block', 1);('gi1', 1);('blue arrow', 1);('red arrow', 1);('error gradientsln refers layer normalization', 1);('cb', 1);('refers convolutional blockand', 1);('mlp', 1);('linear layerstruction lossin contrastive loss', 1);('layer linear layer generate featurevector', 1);('corresponding label yare', 1);('generate contrastive loss', 1);('lixy chen', 1);('ntxent', 1);('contrastive loss maximize thesimilarity embeddings samples', 1);('idea increasethe similarity embeddings samples', 1);('fromthe classin reconstruction loss', 1);('toa decoder generate', 1);('reconstructfeaturexi\x001 reconstruction loss', 1);('lixxis', 1);('map xi\x001to measure similarityfigure', 1);('shows difference reconstructionof method', 1);('infoproshown', 1);('reconstructs input image', 1);('multiple times', 1);('module method', 1);('bwhich reconstructs input', 1);('notefor', 1);('rst module reconstruct input patch embeddings', 1);('original image x0', 1);('feature', 1);('spatial dimensionality', 1);('input image makesour method', 1);('infopro35 optimizationthe', 1);('settinglifi\x10gixi\x001xi\x001y\x111 ilixx ilixy 2whereirepresents ith module hyperparameters i iare regularization terms irepresents emphasis reconstruction irepresents', 1);('oftarget labels', 1);('reconstructing', 1);('vs b', 1);('oursthe', 1);('modeli 1k\x001 idecreases', 1);('binary', 1);('entropy bce', 1);('forthe reconstruction loss', 1);('lixxand ntxent', 1);('theconstractive loss', 1);('lixy', 1);('gk', 1);('tolkcehgkxk\x001y 3wherehrepresents nal classier', 1);('ce', 1);('representsthe cross', 1);('entropy', 1);('experiments41 datasetswe', 1);('models performance', 1);('svhn22 stl10', 1);('images 10000test images with10and100target labels 32\x0232resolution respectivelystl10', 1);('training images 10classes 8000test images 96\x0296resolution', 1);('svhn', 1);('training images', 1);('test images at32\x0232resolution scale input image sizes to224\x0222442 implementation', 1);('detailsunless', 1);('allour experiments training pipeline', 1);('identical toswin', 1);('adamw', 1);('optimizer weight decay', 1);('base learning rate of00005 cosine decay learning rate scheduler', 1);('werun', 1);('nvidia titan rtx', 1);('rtx', 1);('pytorch', 1);('method maxmemoryallocatedsplit', 1);('decoder method cifar10 cifar100 stl10 svhn memory mb e2e', 1);('datasets compareinfopro approach', 1);('underline indicatesthe overall', 1);('bilinear interpolation', 1);('layer decoder', 1);('deconvolution layers', 1);('red percentage', 1);('percentage memory', 1);('refers megabyte', 1);('evaluation network', 1);('therst layer', 1);('layer auxiliary blocks', 1);('role training', 1);('losshyperparameters i i', 1);('values range between3010202010300040as', 1);('deeperin model', 1);('i i range between3010004043', 1);('resultswe', 1);('overall accuracy', 1);('top1 accuracy', 1);('themain metric evaluation method weperform', 1);('extensive study', 1);('method withinfoprotransformer method', 1);('themethod introduce', 1);('backbone ininfopro', 1);('fair comparison methodwe use', 1);('simplicity performance method', 1);('infopromethod', 1);('structure decoder provethe', 1);('superior performance method', 1);('usedstructures decoders rst', 1);('combination bilinear interpolation', 1);('structure thatinfopro', 1);('224\x02224input case', 1);('ussage fork', 1);('simple stack deconvolution layers 10with', 1);('factor 2\x022and4\x024 use', 1);('deconvto', 1);('networkwe rst', 1);('common scenario image classications', 1);('epochs batch size', 1);('results areshown', 1);('datasetour method surpasses', 1);('method thecifar10 dataset', 1);('decoder number', 1);('allmethods statute dataset improvement signicant 3datasets', 1);('similar resultsfork', 1);('comparable theinfoprowe', 1);('method endtoend', 1);('e2etraining', 1);('comparable accuracywith', 1);('evaluation gpu', 1);('memory consumptiona major advantage local learning', 1);('withe2e training', 1);('network 2modules', 1);('compared', 1);('withe2e training consumes', 1);('memoryour method reduces memory consumption', 1);('thismemory', 1);('whenthe network', 1);('alittle sacrice accuracywe', 1);('decoder method cifar10 cifar100 stl10 e2e', 1);('superior performance betweenour approach', 1);('performanceconsumption method reconstructs', 1);('reconstructs entireimages', 1);('image resolution 224\x02224isa', 1);('costly operation minimum upscale factor 64h8w8hw forf1in', 1);('maximum upscale factor', 1);('method cases', 1);('decoder method', 1);('reduces by1215', 1);('mb gpu', 1);('memory requirement', 1);('similar memory footprint', 1);('similar memory overhead', 1);('multiple deconvolutions', 1);('upscale factor 16\x0216for image reconstruction', 1);('memory overhead', 1);('randominitializednetworkdespite comparison', 1);('networks section', 1);('datasets themodels batch size', 1);('experiments onstl10', 1);('epochs experiments oncifar10', 1);('dueto', 1);('enormous training cost', 1);('common practice', 1);('thatall training settings', 1);('method consistentlyoutperforms', 1);('2andk 4when network', 1);('acomparable performance', 1);('interpdecoder', 1);('deconvdecoder', 1);('surpasses endtoend', 1);('cifar100and stl10', 1);('method uses', 1);('cifar100achieves', 1);('theinfopro method', 1);('improvement experiments434', 1);('method surpasses endtoend', 1);('modelwhich counterintuitive hypothesize contrastive loss', 1);('training pipeline isthe reason', 1);('superior performance conductan ablation study losses', 1);('wecompare', 1);('cifar100with', 1);('greedy approaches reconstruction', 1);('auxiliary blocks', 1);('greedy approach ourmethod', 1);('othergreedy approach replaces contrast loss cross entropyloss approach', 1);('severalcross entropy lossesas', 1);('greedy approach cross entropy loss', 1);('network greedy approach constrictive loss', 1);('thee2e network shows', 1);('major performance gain isbecause constrictive loss', 1);('method acc mem mb e2e', 1);('14769k2greedy cross entropy', 1);('decoder greedy approach training', 1);('loss local moduleswhile cross entropy uses cross entropy loss onlysplit', 1);('classication', 1);('accuracy comparison', 1);('cifar100 stl10', 1);('input', 1);('resolution 384\x02384witha batch size', 1);('epochsalso model', 1);('amplies performance', 1);('gain accuracy435', 1);('variations swinin', 1);('order show methodology work typesof transformers', 1);('swin transformers', 1);('swinbase swinlarge', 1);('abatch size', 1);('input image resolution 384\x02384because', 1);('excessive compute training costs restrictourselves', 1);('wesee gain performance', 1);('whilek 4is', 1);('e2e cifar100', 1);('observe interestingphenomenon', 1);('performanceswinlarge results', 1);('serious compute limitations', 1);('trainingwith input resolution', 1);('batch size of80', 1);('epochs observe', 1);('cifar100 k', 1);('results surpasses', 1);('rate convergencein summary', 1);('memory withour technique', 1);('swinlarge accuracy', 1);('decoderinput resolution 224\x02224with batch size', 1);('for200 epochs5', 1);('conclusionin', 1);('transformers different', 1);('previous approachof', 1);('entire images', 1);('local module ourmethod reconstructs', 1);('previous module', 1);('memoryour method', 1);('classication datasets', 1);('superior performance gainover', 1);('previous method', 1);('superior memory', 1);('overendtoend training pipeline', 1);('particular evaluatedour method', 1);('decoder structures', 1);('cifar10 cifar100 stl10and svhn', 1);('comparedto', 1);('memory whenthe network', 1);('orno accuracy drop approach', 1);('workfor variations', 1);('swinbase transformers', 1);('hangbo bao li dong furu wei wenhui wang nan yangxiaodong liu yu wang jianfeng gao songhao piao mingzhou', 1);('unilmv2 pseudomasked', 1);('language models', 1);('language model', 1);('eugene belilovsky michael eickenberg edouard oyallon greedy', 1);('layerwise learning scale', 1);('imagenet kamalika chaudhuri ruslan salakhutdinov', 1);('eugene belilovsky michael eickenberg edouard oyallon decoupled', 1);('greedy learning', 1);('cnns hal daum', 1);('aarti singh', 1);('research pages 736745pmlr', 1);('ting chen simon kornblith mohammad norouzi geoffrey hinton', 1);('simple framework contrastive learning visual representations', 1);('37thinternational conference', 1);('machine learning icml20jmlrorg', 1);('adam coates andrew ng honglak lee', 1);('analysisof singlelayer networks', 1);('ingeoffrey gordon david dunson miroslav dud', 1);('k editors', 1);('proceedings fourteenth', 1);('articial intelligence statistics', 1);('research pages 215223fort', 1);('lauderdale fl usa', 1);('apr', 1);('jia deng wei dong richard socher lijia li kai liand li feifei imagenet', 1);('largescale hierarchical imagedatabase', 1);('conference computer vision andpattern recognition pages', 1);('jacob devlin mingwei chang kenton lee kristinatoutanova bert', 1);('deep bidirectional transformers language understanding', 1);('jill burstein christydoran thamar solorio', 1);('north american chapter association', 1);('language technologies naaclhlt', 1);('minneapolis mn usa june27', 1);('short papers', 1);('alexey dosovitskiy lucas beyer alexander kolesnikovdirk weissenborn xiaohua zhai thomas unterthinermostafa dehghani matthias minderer georg heigold sylvain gelly jakob uszkoreit neil houlsby', 1);('image isworth', 1);('image recognition atscale', 1);('iclr', 1);('xin', 1);('katayoun farrahi mahesan niranjan information', 1);('bottleneck theory', 1);('exploration cascade learningentropy', 1);('jun fu jing liu yuhang wang hanqing lu stackeddeconvolutional', 1);('network semantic segmentation', 1);('tao guo', 1);('guo jiewei zhang wenchao xu junxiao wang efcient', 1);('towards', 1);('selective removal input attributes', 1);('wenzhe guo mohammed e fouda ahmed eltawil', 1);('n salama backlink supervised', 1);('local training withbackward links', 1);('furong huang jordan ash john langford robertschapire learning', 1);('resnet', 1);('jennifer dy andreas krause', 1);('international conference onmachine', 1);('proceedings machinelearning', 1);('jul2018', 1);('max jaderberg wojciech marian czarnecki simon osindero oriol vinyals alex graves david silver koraykavukcuoglu decoupled', 1);('neural interfaces', 1);('icml17', 1);('jmlrorg', 1);('skander karkar ibrahim ayed emmanuel bezenac', 1);('gallinari blockwise', 1);('training residual networksvia', 1);('movement scheme workshop', 1);('aaaiconference articial intelligence', 1);('alex krizhevsky vinod nair geoffrey hinton cifar10canadian', 1);('seung hoon lee seunghyun lee byung cheolsong vision', 1);('transformer smallsize datasets', 1);('yahui liu enver sangineto wei bi nicu sebe brunolepri marco de nadai efcient', 1);('training visual transformers', 1);('small datasets', 1);('beygelzimer dauphinp liang j wortman vaughan', 1);('ze liu han hu yutong lin zhuliang yao zhenda xieyixuan wei jia ning yue cao zheng zhang li dongfuru wei baining guo swin', 1);('transformer v2', 1);('scalingup', 1);('capacity resolution', 1);('vision pattern recognition cvpr', 1);('ze liu yutong lin yue cao han hu yixuan wei zhengzhang stephen lin baining guo swin', 1);('transformerhierarchical vision transformer', 1);('inproceedings ieeecvf', 1);('vision iccv', 1);('enrique marquez jonathon hare mahesan niranjan deep', 1);('cascade learning', 1);('ieee transactions neuralnetworks learning systems', 1);('yuval netzer tao wang adam coates alessandro bissacco bo wu andrew ng reading', 1);('digits naturalimages', 1);('nips', 1);('deep learning unsupervised feature learning2011', 1);('arild nkland lars hiller eidnes training', 1);('neural networks', 1);('local error signals', 1);('kamalika chaudhuri', 1);('salakhutdinov', 1);('seungeun oh jihong', 1);('praneeth vepakomma sihunbaek ramesh raskar mehdi bennis seonglyun kimlocfedmixsl localize', 1);('federate mix improvedscalability convergence latency', 1);('inproceedings acm web', 1);('www', 1);('york ny usa', 1);('associationfor computing machinery', 1);('shraman pal mansi uniyal jihong', 1);('praneethvepakomma ramesh raskar mehdi bennis moongu jeonand jinho choi serverside', 1);('local gradient', 1);('rate acceleration', 1);('aaai2022', 1);('federated learning', 1);('adeetya patel michael eickenberg eugene belilovskylocal', 1);('learning neuron groups', 1);('cells societies collective learning', 1);('scales', 1);('alec radford karthik narasimhan tim salimans ilyasutskever improving', 1);('language understanding', 1);('ilija radosavovic raj prateek kosaraju ross girshickkaiming piotr doll', 1);('designing', 1);('network designspaces', 1);('ieeecvf', 1);('computer visionand pattern recognition cvpr', 1);('hugo touvron matthieu cord matthijs douze franciscomassa alexandre sablayrolles herve jegou training', 1);('dataefcient image transformers distillation throughattention', 1);('marina meila tong zhang', 1);('ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan n gomez', 1);('kaiser illia polosukhin attention', 1);('guyonu v luxburg bengio h wallach r fergus vishwanathan r garnett', 1);('curran associatesinc', 1);('wenhai wang enze xie xiang li dengping fan kaitaosong ding liang tong lu ping luo ling shaopyramid', 1);('vision transformer', 1);('versatile backbone denseprediction', 1);('computer vision', 1);('yulin wang zanlin ni shiji', 1);('yang gao huangrevisiting', 1);('alternative endtoend training', 1);('learningrepresentations', 1);('haiping wu bin xiao noel codella mengchen liuxiyang dai lu yuan lei zhang cvt introducing', 1);('convolutions vision transformers', 1);('zhenda xie zheng zhang yue cao yutong lin jianminbao zhuliang yao qi dai han hu simmim asimple', 1);('computer vision pattern recognition cvpr', 1);('yunyang xiong zhanpeng zeng rudrasis chakrabortymingxing tan glenn fung yin li vikas singhnystr', 1);('omformer nystr', 1);('proceedings aaai conferenceon articial intelligence', 1);('jingwei zhang xin zhang ke rajarsi gupta joel hsaltz maria vakalopoulou dimitris samaras gigapixelwholeslide', 1);('images classication', 1);('pengchuan zhang xiyang dai jianwei yang bin xiao luyuan lei zhang jianfeng gao multiscale', 1);('vision longformer', 1);('new vision transformer highresolution', 1);('iccv', 1);