('me', 162);('mer', 88);('mes', 56);('dfme', 52);('ieee', 31);('aus', 31);('china', 19);('au', 19);('vol', 16);('no', 16);('august', 16);('sur', 16);('part', 15);('international conference', 15);('of latex class files', 14);('wang', 13);('frame rate', 13);('samm', 12);('fear', 12);('ieee transactions', 12);('ustc', 12);('mae', 11);('apex frame', 11);('oth', 11);('maes', 11);('cas', 10);('dis', 10);('emotion categories', 10);('uf1', 10);('r3d', 10);('i3d', 10);('g. zhao', 10);('key-frame sequence', 9);('casme ii', 9);('table', 9);('video clips', 9);('emotion labels', 9);('au24', 9);('uar', 9);('p3d', 9);('resampling', 9);('computer vision', 9);('s.-j', 9);('class imbalance', 8);('smic', 8);('casme', 8);('sad', 8);('ang', 8);('au7', 8);('d3d', 8);('gesture recognition', 8);('micro-expression recognition', 7);('hefei', 7);('liu', 7);('happiness', 7);('neg', 7);('cbloss', 7);('x. fu', 7);('automatic face', 7);('fg', 7);('pattern recognition', 7);('computer', 6);('figure', 6);('fig', 6);('onset frame', 6);('con', 6);('disgust', 6);('sadness', 6);('au14', 6);('au10', 6);('w.-j', 6);('yan', 6);('x. huang', 6);('learning models', 5);('ekman', 5);('offset frame', 5);('mmew', 5);('pos', 5);('anger', 5);('apex frames', 5);('au4', 5);('au17', 5);('class imbalance problem', 5);('acc', 5);('x. li', 5);('y.-j', 5);('affective computing', 5);('acm', 5);('multimedia', 5);('shifeng liu', 4);('tong xu', 4);('enhong chen', 4);('zhao', 4);('emotion category', 4);('rep', 4);('rgb', 4);('neutralization paradigm', 4);('dataset [', 4);('facial', 4);('video sequence', 4);('] rst', 4);('furthermore', 4);('negative emotions', 4);('temporal adaptive', 4);('accuracy', 4);('baseline models', 4);('anger contempt disgust', 4);('happiness sadness surprise', 4);('predicted', 4);('labelanger contempt disgust', 4);('backbone models', 4);('balanced loss', 4);('m. pietik', 4);('pattern analysis', 4);('machine intelligence', 4);('micro-expression', 4);('proceedings', 4);('icdm', 4);('huaying tang', 3);('xinglong mao', 3);('hanqing tao', 3);('hao wang', 3);('different solutions', 3);('anhui', 3);('e-mail', 3);('data', 3);('com-', 3);('lbp-top', 3);('tlcnn', 3);('facial movements', 3);('small sample size', 3);('sample size', 3);('statistical properties', 3);('excellent performance', 3);('nir', 3);('meview', 3);('long video clips', 3);('surprise', 3);('elicitation videos', 3);('psychology', 3);('laboratory environment', 3);('low intensity', 3);('neural networks', 3);('optical ow', 3);('based', 3);('eulerian', 3);('sun', 3);('led', 3);('elicitation materials', 3);('facial expressions', 3);('total number', 3);('fleisss kappa', 3);('au1', 3);('au6', 3);('au5', 3);('au23', 3);('unweighted f1-score', 3);('unweighted average recall', 3);('major classes', 3);('pseudo-3d resnet', 3);('densenet', 3);('convnet', 3);('confusion matrices', 3);('recognition performance', 3);('comparison', 3);('effective number', 3);('uniform sampling adaptive sampling', 3);('t. pster', 3);('y. zhang', 3);('e. chen', 3);('x. hong', 3);('cvpr', 3);('phd', 3);('research interests', 3);('b.s', 3);('m.s', 3);('aaai', 3);('kdd', 3);('journal of latex class files', 2);('spontaneous micro-expression', 2);('genuine emotions', 2);('essential technical support', 2);('data hunger', 2);('dynamic facial micro-expressions', 2);('future research', 2);('subsequently', 2);('sirui zhao', 2);('macro- expression', 2);('how-', 2);('com- puter vision', 2);('pster', 2);('local binary pattern', 2);('orthogonal planes', 2);('transferring', 2);('novel two-stage learning', 2);('target learning', 2);('siamese 3d convolutional neural network', 2);('large number', 2);('past decade', 2);('short duration', 2);('data shortage', 2);('specically', 2);('data quality', 2);('mean=22.03', 2);('unc', 2);('possur', 2);('negsur', 2);('repsur', 2);('posrep', 2);('negrep', 2);('corresponding original paper', 2);('positively', 2);('negatively', 2);('neutral face', 2);('hs', 2);('vis', 2);('participants self-reports', 2);('action units', 2);('chinese academy', 2);('according', 2);('previous datasets', 2);('internet', 2);('high ecological validity', 2);('re- sults', 2);('movement characteristics', 2);('deep learning', 2);('hoof', 2);('deep neural networks', 2);('temporal information', 2);('peng', 2);('inspired', 2);('image recognition', 2);('capsulenet', 2);('recently', 2);('li', 2);('multi-model fusion network', 2);('cnn-lstm', 2);('transfers knowledge', 2);('khor', 2);('enriched', 2);('model input', 2);('me-plan', 2);('participant', 2);('acquisition memory', 2);('gigabit', 2);('video sequences', 2);('individual differences', 2);('self- reports', 2);('sample selection stage', 2);('coding', 2);('annotation results', 2);('nal result', 2);('facs', 2);('au6+au12', 2);('a1', 2);('a2', 2);('me samples', 2);('emotion annotation', 2);('k=', 2);('video clip', 2);('kx', 2);('piis', 2);('au12', 2);('l/r-au12', 2);('l/r-au2', 2);('au15', 2);('interpretation', 2);('statistical results', 2);('high frequency', 2);('au9', 2);('au20', 2);('ni', 2);('kkx', 2);('evaluation baseline', 2);('model size', 2);('inated', 2);('kinetics', 2);('happiness sadness surprisetrue label16.48 %', 2);('future works', 2);('key problems', 2);('mer performance', 2);('training models', 2);('r3d p3d d3d i3d35.037.540.042.545.047.550.052.555.0acc', 2);('uar fig', 2);('different classes', 2);('class-balanced loss', 2);('different tasks', 2);('springer', 2);('w. v', 2);('friesen', 2);('psychological', 2);('dynamic', 2);('spontaneous micro- expression recognition', 2);('f. xu', 2);('s. zhao', 2);('t. xu', 2);('spontaneous micro-expression database', 2);('m. h. yap', 2);('j. zhang', 2);('j. li', 2);('y. liu', 2);('spontaneous micro-expressions', 2);('h. li', 2);('affective comput-', 2);('y. li', 2);('s. zafeiriou', 2);('facial micro-expressions', 2);('x. feng', 2);('m. peng', 2);('z. zhang', 2);('t. chen', 2);('deep', 2);('k.', 2);('w. wang', 2);('learning', 2);('iccv', 2);('york', 2);('t.-y', 2);('lin', 2);('automatic micro- expressions analysis', 2);('human-computer interac- tion', 2);('hci', 2);('acm multimedia confer-', 2);('computer science', 2);('automatic micro-expressions analysis', 2);('conference papers', 2);('sci-', 2);('ph.d.', 2);('conference proceedings', 2);('ieee tkde', 2);('neurips', 2);('database', 1);('frame rates sirui zhao', 1);('member', 1);('senior member', 1);('abstract', 1);('important psychic stress reactions', 1);('transient facial expressions', 1);('human beings', 1);('lie detection', 1);('psychological analysis', 1);('cutting-edge data-driven', 1);('recent efforts', 1);('tiny amount', 1);('data scale', 1);('classical spatiotemporal', 1);('valuable reference', 1);('comprehensive experimental results', 1);('new benchmark', 1);('index terms emotion', 1);('facial micro-expression', 1);('datasets f', 1);('ntroduction facial', 1);('transmit emotional information', 1);('daily communication [', 1);('particular facial expression', 1);('micro- expression', 1);('subtle facial movements', 1);('individual tries', 1);('real emotions un- der pressure', 1);('patient conversation', 1);('concretely', 1);('pleasant conversation', 1);('southwest university', 1);('mianyang', 1);('sirui @ mail.ustc.edu.cn', 1);('tang', 1);('hqtaog @ mail.ustc.edu.cn \x0fxinglong', 1);('mao', 1);('lsf0619g @ mail.ustc.edu.cn', 1);('cheneh g @ ustc.edu.cn', 1);('possible publication', 1);('copyright', 1);('corresponding', 1);('manuscript', 1);('december', 1);('xx xx', 1);('xx.lie detection', 1);('public safety [', 1);('psychic stress reaction', 1);('ordinary facial expression', 1);('short dura- tion', 1);('partial movement', 1);('low movement intensity', 1);('image sequence', 1);('early research', 1);('manual analysis', 1);('manual analysis relies', 1);('expert experience', 1);('low recog- nition accuracy', 1);('powerful perception', 1);('recent years', 1);('mian directional mean op-', 1);('flow', 1);('mdmo', 1);('convolutional nerual network', 1);('recognition accuracy', 1);('actual scenarios', 1);('there-', 1);('dataset.arxiv:2301.00985v1 [ cs.cv ]', 1);('jan', 1);('onset 0apex 1.25offset 2.08second', 1);('onset 0apex 0.19offset 0.36second', 1);('examples', 1);('noteworthy', 1);('expression changes', 1);('white arrows', 1);('general directions', 1);('individual attempts', 1);('paper constructs', 1);('con- secutive years', 1);('popular spatiotemporal video', 1);('subsequent research', 1);('\x0fthis paper focuses', 1);('multiple high frame rates', 1);('review re-', 1);('elab- orate', 1);('comprehensive dataset evaluation', 1);('research conclusions', 1);('future work', 1);('r elated work', 1);('rst review', 1);('public sponta- neous', 1);('deep learning technologies', 1);('micro-expression datasets', 1);('affective com-', 1);('statistical information', 1);('current spontaneous me datasets me datasetsparticipants samples', 1);('mes annotation labels numbergender', 1);('male/female', 1);('age number frame rate resolution emotion facs au hs', 1);('smic vis', 1);('mean=28.171', 1);('tense', 1);('mean=22.5957', 1);('mean=33.24159', 1);('/ /', 1);('mean=22.35', 1);('mean=23.5', 1);('mean=27.8267', 1);('yesgrayscale', 1);('depth', 1);('dfme part', 1);('mean=22.43969', 1);('detection task', 1);('statistical data', 1);('positive', 1);('negative', 1);('amu', 1);('amusement', 1);('hap', 1);('repression', 1);('unclear', 1);('repressively', 1);('high-quality datasets', 1);('usf-hd', 1);('polikovsky', 1);('] datasets', 1);('imi- tation', 1);('current emotionalstate', 1);('subsequent researchers', 1);('spon- taneous', 1);('strong emotional stimuli', 1);('elicit expressions', 1);('certain degree', 1);('high-pressure mechanism', 1);('datasets', 1);('4dme [', 1);('standard visual camera', 1);('emo- tion categories', 1);('series datasets', 1);('] contains', 1);('corresponding onset', 1);('offset frames', 1);('sample frame', 1);('relevant video content', 1);('amuse- ment', 1);('facial area resolution', 1);('subtle changes', 1);('2dataset [', 1);('] embodies', 1);('\x02480 pixels', 1);('different', 1);('rst part', 1);('emotion tags', 1);('elicitation effect', 1);('formal start', 1);('stimulus videos', 1);('expression sample', 1);('compared', 1);('parameter setting', 1);('movement informa- tion', 1);('4dme dataset [', 1);('signicant innovations', 1);('multi- modality video data', 1);('4d facial data', 1);('3d facial meshes sequences', 1);('traditional 2d frontal facial grayscale', 1);('depth videos', 1);('4dme contains', 1);('ve emotion labels', 1);('multiple emotion labels', 1);('real high-pressure scenes', 1);('manual annotation', 1);('actual life scenarios', 1);('uncontrollable factors', 1);('frequent camera shot', 1);('full human', 1);('3dataset [', 1);('mock crime paradigm', 1);('3also contains', 1);('neutraliza- tion paradigm', 1);('part contains', 1);('learning task', 1);('various methods', 1);('small-scale datasets', 1);('mod- els', 1);('insufcient sample size', 1);('data augmentation', 1);('uncontrollable noises', 1);('composite datasets', 1);('different datasets', 1);('different parameter settings', 1);('simple fusion', 1);('micro-expression recognition approaches', 1);('rst attempt', 1);('3dhog [', 1);('variants [', 1);('complex expert knowledge', 1);('current mer', 1);('high-level expression', 1);('emotion classi- cation', 1);('model training', 1);('model considers', 1);('current deep learning', 1);('single', 1);('intensity frame', 1);('optical-ow format', 1);('resnet-10', 1);('large- scale image dataset', 1);('classication network', 1);('encouragingly', 1);('recognition accuracy exceeds', 1);('hand-crafted methods', 1);('lbp- top', 1);('capsule models', 1);('quang', 1);('expression-identity disentangle network', 1);('pixel- level', 1);('frequency domain', 1);('learning architecture', 1);('global information', 1);('liong', 1);('optical-ow image', 1);('resnet-18', 1);('optical- ow image', 1);('domain adversarial training strategies', 1);('rst place', 1);('megc2019', 1);('zhou', 1);('feature renement', 1);('fr', 1);('optical-ow information', 1);('gong', 1);('overall', 1);('large-scale im- ages', 1);('temporal informa- tion', 1);('maes.2.2.2 video', 1);('important expression states', 1);('kim', 1);('cnn', 1);('expression state', 1);('apex transition', 1);('offset transition', 1);('lstm', 1);('convo-', 1);('nerual network', 1);('small sample', 1);('large-scale expression data', 1);('recurrent convolutional network', 1);('el- rcn', 1);('temporal enrichment', 1);('different input data', 1);('3d convolution network', 1);('dual tempo-', 1);('scale convolutional neural network', 1);('dtscnn', 1);('optical-ow sequences', 1);('different frame rate', 1);('] pro-', 1);('em-ced', 1);('global attention module', 1);('rich spatiotemporal information', 1);('xia', 1);('deep recurrent convolutional networks', 1);('facial appearance', 1);('geom- etry', 1);('high- level', 1);('optical-ow sequence', 1);('crucial temporal sequences', 1);('deep teacher neural network', 1);('shallow student neural network', 1);('deep prototypical learning framework', 1);('key-frame sequences', 1);('3d residual prototypical network', 1);('local-wise attention module', 1);('deep learning technology', 1);('excellent neural networks', 1);('gcn', 1);('full use', 1);('spatial-temporal information', 1);('corresponding model', 1);('structural complexity', 1);('faces seri- ous', 1);('current small-scale', 1);('primary task', 1);('pivotal role.journal', 1);('reflector umbrellas', 1);('participants monitor', 1);('high-speed', 1);('configurable frame rate', 1);('collector collectors', 1);('mes collectors', 1);('optical fiber transmission line', 1);('experimental', 1);('similarly', 1);('high recognition rate', 1);('sufcient training', 1);('local-movement characteristics', 1);('construct large-scale', 1);('college students', 1);('standard deviation =', 1);('formal exper- iment', 1);('experimental procedure', 1);('possible benets', 1);('voluntary participation', 1);('consent form', 1);('facial images', 1);('academic paper', 1);('reector umbrellas', 1);('stable light source', 1);('participants faces', 1);('congurable frame rates', 1);('optical ber transmission line', 1);('elicitation material', 1);('procedure', 1);('me-eliciting', 1);('video', 1);('mes video id', 1);('emotion category mean score', 1);('ecological validity', 1);('natural scenes', 1);('irrelevant body', 1);('mouth move- ments', 1);('specic details', 1);('elicitation process', 1);('elicitation materials determines', 1);('high emotional valence', 1);('crucial [', 1);('effective stimulus materials', 1);('evalua- tion process', 1);('main emotion', 1);('stimulus level', 1);('emotional class', 1);('stimulus intensity values', 1);('specic', 1);('statistical details', 1);('specic seat', 1);('focal length', 1);('basic emotional types', 1);('obvious expressions', 1);('posture upright', 1);('excessive head movements', 1);('full attention', 1);('affective grade scale', 1);('emotional experience', 1);('self-report in-', 1);('emo- tion category', 1);('subsequent annotators', 1);('cognitive differences', 1);('emotional orientation', 1);('internal emotional expe- rience', 1);('whats', 1);('external expressions', 1);('true inner emotions', 1);('me annotation', 1);('two-stage annotation', 1);('cate- gories', 1);('valid expression samples', 1);('long video sequences', 1);('rst stage', 1);('facial muscle action units', 1);('annotation agreement test', 1);('3.3.1 sample', 1);('selection', 1);('manual segmen- tation', 1);('participants facial information', 1);('sev- eral shorter video fragments', 1);('video annotation software', 1);('original video sequences frame', 1);('facial muscle movements', 1);('guid- ance', 1);('facial movementswere expressions', 1);('interfer- ence data', 1);('dry eyelids', 1);('habitual mouth opening', 1);('eye movements', 1);('categories labeling', 1);('previous sample selection stage', 1);('key- frames', 1);('facial muscle action unit', 1);('emotion category labels', 1);('mo- ment', 1);('facial expression changes', 1);('rst round', 1);('ve annota- tors', 1);('expression clip', 1);('median value', 1);('time limit', 1);('foffset\x00fonset +', 1);('moment index', 1);('key-frame k.', 1);('multiple categories', 1);('obscure ones', 1);('ac-', 1);('actual induction', 1);('different categories', 1);('upper face', 1);('miscellaneous actions', 1);('corresponding face actions', 1);('r=', 1);('allau', 1);('allauis', 1);('categories intojournal', 1);('key aus included', 1);('dfme upper face', 1);('units lower face', 1);('units miscellaneous actions au1 inner brow raiser au9 nose wrinkler au18 lip pucker au31 jaw clencher au2', 1);('brow raiser au10 upper lip raiser au20 lip stretcher au38 nostril dilator au4 brow lowerer au12 lip corner puller au23 lip tightener au39 nostril compressor au5 upper lid raiser au14 dimpler au24 lip presser m57 head forward au6 cheek raiser au15 lip corner depressor au25 lips part m58 head', 1);('au7 lid tightener au16 lower lip depressor au28 lip suck au17 chin raiser', 1);('au4+au5', 1);('left-au6+ left-au12', 1);('au4+au7+ au10', 1);('au1+au4+ au7+au20', 1);('au1+au2+ au5', 1);('sur- prise', 1);('prototypical emotion cat- egories', 1);('nal label', 1);('reference basis', 1);('different emotion categories', 1);('category confusion', 1);('self-emotional cognition', 1);('different partici- pants', 1);('whole piece', 1);('comprehensive analysis', 1);('elicitation material contents', 1);('data collection process', 1);('corresponding timestamps', 1);('elicitation ma- terials', 1);('fig.3', 1);('basic emotion categories', 1);('annotation agreement', 1);('reliable emotion categories', 1);('vital sig- nicance', 1);('fleissskappa', 1);('test [', 1);('work [', 1);('excellent indicator', 1);('accurate self-report', 1);('corresponding elicitation material content', 1);('others g.', 1);('annotation personnel', 1);('nindicate', 1);('j-th category', 1);('calculate pj', 1);('j-th emotion', 1);('n\x02nnx', 1);('possible pairs', 1);('pi=1', 1);('j=1n2 ij', 1);('\x00n ]', 1);('p=1 nnx', 1);('occurrence', 1);('emotion categories anger', 1);('disgust fear happiness sadness surprise au', 1);('1au pct', 1);('l/r-au12278.7 au4', 1);('l/r-au10', 1);('au2', 1);('l/r-au1', 1);('l/r-au5', 1);('statistical range', 1);('l/r', 1);('left/right', 1);('pe', 1);('pe=kx', 1);('j=1p2 j', 1);('calculate \x14by', 1);('\x14=p\x00pe 1\x00pe', 1);('\x14= 0:72through', 1);('emotion annotators', 1);('substantial agreement', 1);('fleisskappa test', 1);('poor', 1);('agreement 0.01-0.20', 1);('slight', 1);('agreement 0.21-0.40 fair agreement 0.41-0.60', 1);('moderate', 1);('agreement 0.61-0.80', 1);('substantial', 1);('agreement 0.81-1.00', 1);('perfect agreement', 1);('part c.', 1);('frame rate setting', 1);('data size', 1);('strong control', 1);('dataset contains', 1);('emotion category label', 1);('fig.4', 1);('considerable reliability', 1);('emotion-au correspondence rule', 1);('high- occurrence', 1);('existence preference', 1);('different emotions', 1);('emotional category imbalance problem', 1);('combina- tions', 1);('emotion', 1);('au combinations', 1);('mes emotion categories au combinations anger au4+au5', 1);('au6+l/r-au12 disgust au4+au7+au10', 1);('au14 fear au14+au24', 1);('au1+au4', 1);('au4+au5 happiness au6+au12', 1);('au12 sadness au14', 1);('au14+au24 surprise au1+au2+au5', 1);('au1+au2', 1);('au5 shared1au4', 1);('au4+au7', 1);('shared', 1);('nose wrinkler', 1);('lip stretcher', 1);('certain facial muscles', 1);('brow lowerer', 1);('lid tightener', 1);('lip presser', 1);('different negative emotions', 1);('positive emotion', 1);('left/right-au12', 1);('disgust surprise happiness fear sadness anger', 1);('contempt others', 1);('combined', 1);('positive surprise negative', 1);('distribution', 1);('total sample number', 1);('deep show', 1);('positive feelings', 1);('d ataset evaluation', 1);('comprehensive experiments', 1);('auto- matic', 1);('inuential spatiotemporal', 1);('evaluation dataset', 1);('clear emotion labels', 1);('experimental dataset', 1);('data preprocessing', 1);('facial expression recognition', 1);('head poses', 1);('unequal video lengths', 1);('nal recognition results', 1);('prepro- cess', 1);('irrelevant variables', 1);('face alignment', 1);('style aggregated network', 1);('san', 1);('procrustes', 1);('analysis [', 1);('afne transformation', 1);('reference image', 1);('signicant impact', 1);('such landmarks', 1);('face cropping', 1);('facial area', 1);('necessary step', 1);('different backgrounds', 1);('retinaface', 1);('different me', 1);('different lengths', 1);('deep learning models', 1);('input size', 1);('sample lengths', 1);('temporal length', 1);('video classication models', 1);('video length', 1);('processing strategy', 1);('subtle movements', 1);('fol-', 1);('previous studies [', 1);('popular video classication models', 1);('work extracts', 1);('strategy [', 1);('evaluation protocols', 1);('metrics', 1);('leave-one-subject-out strategy', 1);('efcient 10-fold cross-validation strategy', 1);('classication indicators', 1);('accu-', 1);('true positive', 1);('tpi', 1);('false positive', 1);('fpi', 1);('false negative', 1);('fni', 1);('kclasses', 1);('average results', 1);('common metrics', 1);('overall performance', 1);('recognition method', 1);('=kx i=1tpi', 1);('i-th class', 1);('f1-score', 1);('uf1i=2\x01tpi', 1);('intractable problem', 1);('evaluation metric', 1);('methods performance', 1);('able metric', 1);('spatiotemporal convolution models', 1);('amazing performance', 1);('video classication tasks', 1);('unique features', 1);('large 3d models', 1);('large-scale dataset', 1);('backbone selection', 1);('extensive data', 1);('standard backbone networks', 1);('3d convolution architecture', 1);('validation experiments.4.4.1 3d-resnet', 1);('hara', 1);('video classication', 1);('basic idea', 1);('2d convolu- tional kernels', 1);('spatiotemporal 3d kernels', 1);('2d-resnet [', 1);('] network structure', 1);('3d model back- bone', 1);('good results', 1);('video tasks', 1);('key point', 1);('convolution lter', 1);('spatial domain convolution lter', 1);('temporal domain convolution lter', 1);('hence', 1);('improves training efciency', 1);('mental performance', 1);('4.4.3 3d-densenet', 1);('image tasks', 1);('residual connection', 1);('resnet', 1);('video eld', 1);('cai', 1);('densenet-based', 1);('in- ation', 1);('2d model', 1);('data requirements', 1);('large-scale video dataset', 1);('] simulta-', 1);('evaluation', 1);('settings', 1);('nvidia geforce rtx', 1);('gpus', 1);('nvidia a100-pcie-', 1);('gpu', 1);('following', 1);('original settings', 1);('input image', 1);('cross-entropy loss', 1);('stochastic gradi- ent descent', 1);('sgd', 1);('model parameters', 1);('batch size', 1);('initial learning rates', 1);('learning rates', 1);('evaluation baseline results', 1);('recognition confusion matrix', 1);('baseline model', 1);('r3d model', 1);('happiness sadness surprisetrue label23.75 %', 1);('d3d model', 1);('happiness sadness surprisetrue label22.78 %', 1);('i3d fig', 1);('average accuracy', 1);('eyes [', 1);('above experimental results', 1);('recognition confusion matrices', 1);('obviously', 1);('class im- balance problem', 1);('distinguishable spatiotemporal', 1);('vital exploration direction', 1);('various baseline models models', 1);('acc uf1 uar r3d', 1);('evaluation discussion', 1);('various key-frame sequence', 1);('4.7.1 class imbalance', 1);('inevitably', 1);('disgust samples', 1);('negative samples', 1);('negative impact', 1);('disgust class', 1);('effective solution', 1);('major categories', 1);('metrics resampling1acc uf1 uar r3dw/o', 1);('p3dw/o', 1);('d3dw/o', 1);('i3dw/o', 1);('class rebal-', 1);('main idea', 1);('equal probability', 1);('minor classes', 1);('acc r3d p3d d3d i3d0.360.380.400.420.440.460.480.50uf1unweighted f1-score', 1);('uf1 r3d p3d d3d i3d0.360.380.400.420.440.460.480.50uarunweighted average recall', 1);('information loss', 1);('reweighting', 1);('approaches attempt', 1);('actual impact', 1);('focal loss', 1);('different domains', 1);('experi- ments', 1);('result demonstrates', 1);('various models', 1);('similar problems', 1);('different models', 1);('ne- tune', 1);('various conditions', 1);('actual number', 1);('different losses metrics losses acc uf1 uar r3dcross entropy loss', 1);('p3dcross entropy loss', 1);('d3dcross entropy loss', 1);('i3dcross entropy loss', 1);('strategies', 1);('concise description', 1);('original video', 1);('contains key information', 1);('raw video', 1);('im- portant factor', 1);('accurate recognition', 1);('video-relatedtable', 1);('cost-sensitive reweighting losses', 1);('softmax probability', 1);('sample number', 1);('class y', 1);('class- balanced loss', 1);('= 0:999in', 1);('loss equation', 1);('entropy loss lce=\x00log', 1);('lcb=\x001\x00', 1);('1\x00 nylog', 1);('recognition tasks', 1);('xed-length key-frame sequence', 1);('spatial-temporal space', 1);('previous', 1);('studies [', 1);('key-frame temporal adaptive', 1);('key moments', 1);('corresponding recognition performance', 1);('mer performace', 1);('different key-frame sequence sampling strategies', 1);('metrics sampling method1acc uf1 uar r3dadaptive', 1);('p3dadaptive', 1);('d3dadaptive', 1);('i3dadaptive', 1);('result suggests', 1);('different baseline models', 1);('different sam-', 1);('acc r3d p3d d3d i3d0.360.380.400.420.440.46uf1unweighted f1-score', 1);('uf1 r3d p3d d3d i3d0.360.380.400.420.440.46uarunweighted average recall', 1);('adaptive key-frame sampling', 1);('uniform key-frame sampling', 1);('onclusion and future work', 1);('multiple frame rates', 1);('knowl- edge', 1);('spatiotemporal visual', 1);('particularly', 1);('analysis research', 1);('multiple natural scenes', 1);('high accuracy', 1);('uncertain labels', 1);('actual scenes', 1);('acknowledgments', 1);('micro-expression laboratory', 1);('special thanks', 1);('references', 1);('a. mehrabian', 1);('communication', 1);('communica-', 1);('tion theory', 1);('routledge', 1);('e. a. haggard', 1);('k. s. isaacs', 1);('micromomentary', 1);('facial ex- pressions', 1);('ego mechanisms', 1);('methods', 1);('nonverbal', 1);('psychiatry', 1);('s. porter', 1);('l. ten brinke', 1);('reading', 1);('identifying', 1);('universal facial expressions', 1);('telling', 1);('clues', 1);('ww norton', 1);('s. weinberger', 1);('intent', 1);('deception detection', 1);('catch terrorists', 1);('sharon weinberger', 1);('close look', 1);('nature', 1);('l. hunter', 1);('l. roland', 1);('a. ferozpuri', 1);('emotional', 1);('expression processing', 1);('depressive symptomatology', 1);('eye-tracking', 1);('reveals differential importance', 1);('middle facial areas', 1);('inter- est', 1);('depression', 1);('recognising', 1);('sponta- neous facial micro-expressions', 1);('m. pietikainen', 1);('texture recognition', 1);('local binary patterns', 1);('j.-k. zhang', 1);('main directional', 1);('b.-j', 1);('x. ou', 1);('long-term convolutional neural network', 1);('neuro-', 1);('h. tao', 1);('k. zhang', 1);('z. hao', 1);('two-stage 3d cnn', 1);('neurocomputing', 1);('spon- taneous micro-expression database', 1);('inducement', 1);('automatic', 1);('y.-h. chen', 1);('baseline evaluation', 1);('plos', 1);('p. e86041', 1);('a. k. davison', 1);('c. lansley', 1);('n. costen', 1);('k. tan', 1);('spontaneous micro-facial movement dataset', 1);('x. ben', 1);('y. ren', 1);('k. kpalma', 1);('w. meng', 1);('video-based', 1);('facial micro-expression analysis', 1);('z. dong', 1);('s. lu', 1);('y. ma', 1);('c. huang', 1);('generation facial', 1);('depth information', 1);('high eco-', 1);('logical validity', 1);('m. shreve', 1);('s. godavarthy', 1);('d. goldgof', 1);('s. sarkar', 1);('macro-and', 1);('long videos', 1);('spatio-temporal strain', 1);('s. polikovsky', 1);('y. kameda', 1);('y. ohta', 1);('micro-expressions recognition', 1);('high speed camera', 1);('3d-gradient descriptor', 1);('q. wu', 1);('f. qu', 1);('s. wu', 1);('spontaneous macro-expression', 1);('s. cheng', 1);('m. behzad', 1);('j. shen', 1);('m. pantic', 1);('spontaneous 4d micro-expression dataset', 1);('hus', 1);('j. cech', 1);('j. matas', 1);('spotting', 1);('computer vision winter', 1);('retz', 1);('r. chaudhry', 1);('a. ravichandran', 1);('g. hager', 1);('r. vidal', 1);('his-', 1);('binet-cauchy kernels', 1);('non- linear dynamical systems', 1);('human actions', 1);('x. liu', 1);('discriminative', 1);('spatiotemporal local binary pattern', 1);('integral projection', 1);('spontaneous facial micro-expression recog- nition', 1);('a. moilanen', 1);('towards', 1);('compara- tive study', 1);('recogni- tion methods', 1);('z. wang', 1);('microexpression', 1);('facial dynamics map', 1);('ieee transac-', 1);('z. wu', 1);('micro expression recognition', 1);('small datasets', 1);('x. zhang', 1);('s. ren', 1);('residual learning', 1);('n. van quang', 1);('j. chun', 1);('t. tokuyama', 1);('micro- expression recognition', 1);('b. xia', 1);('s. wang', 1);('micro-expression recognition framework', 1);('pro-', 1);('joint', 1);('global information learning', 1);('apex frame detection', 1);('image processing', 1);('s.-t. liong', 1);('k. wong', 1);('r. c.-w. phan', 1);('less', 1);('sig-', 1);('processing', 1);('image communication', 1);('h.', 1);('l. zheng', 1);('t. gedeon', 1);('neural micro- expression recognizer', 1);('l. zhou', 1);('q. mao', 1);('f. zhang', 1);('feature', 1);('fusion method', 1);('w. gong', 1);('cheng', 1);('j. gonz', 1);('` alez', 1);('meta-', 1);('meta-learning', 1);('acm transactions', 1);('multimedia computing', 1);('communications', 1);('applications', 1);('tomm', 1);('d. h. kim', 1);('w. j. baddar', 1);('y. m. ro', 1);('recog- nition', 1);('international con- ference', 1);('h.-q', 1);('r. c. w. phan', 1);('w. lin', 1);('long-term recurrent convolutional network', 1);('facial micro-expression recog- nition', 1);('s. ji', 1);('w. xu', 1);('m. yang', 1);('k. yu', 1);('3d convolutional neural net-', 1);('human action recognition', 1);('c. wang', 1);('g. liu', 1);('dual', 1);('temporal scale convolutional neural network', 1);('frontiers', 1);('y. wang', 1);('h. ma', 1);('x. xing', 1);('z. pan', 1);('3dcnn architecture', 1);('facial micro-expression recognition', 1);('multimedia modeling', 1);('z. xia', 1);('x. gao', 1);('spatiotemporal', 1);('recurrent convolutional networks', 1);('] b', 1);('s. cao', 1);('d. li', 1);('l. yu', 1);('knowledge distillation', 1);('h. tang', 1);('s. liu', 1);('h. wang', 1);('c. guan', 1);('deep prototypical learning', 1);('local atten- tion network', 1);('dynamic micro-expression recognition', 1);('neural', 1);('ofcial journal', 1);('neural network', 1);('h.-x', 1);('xie', 1);('l. lo', 1);('h.-h. shuai', 1);('w.-h. cheng', 1);('au-assisted', 1);('graph attention convolutional network', 1);('envi-', 1);('nonverbal behavior', 1);('j. l. fleiss', 1);('measuring', 1);('nominal scale agreement', 1);('x. jiang', 1);('y. zong', 1);('w. zheng', 1);('c. tang', 1);('w. xia', 1);('c. lu', 1);('j. liu', 1);('dfew', 1);('large-scale database', 1);('dynamic facial ex- pressions', 1);('x. dong', 1);('y. yan', 1);('w. ouyang', 1);('y. yang', 1);('style', 1);('facial landmark detection', 1);('j. c. gower', 1);('generalized', 1);('procrustes analysis', 1);('psychometrika', 1);('j. deng', 1);('j. guo', 1);('y. zhou', 1);('j. yu', 1);('i. kotsia', 1);('reti-', 1);('single-stage', 1);('dense face localisation', 1);('arxiv', 1);('k. hara', 1);('h. kataoka', 1);('y. satoh', 1);('spatiotemporal 3d cnns retrace', 1);('ieee/cvf', 1);('z. qiu', 1);('t. yao', 1);('t. mei', 1);('spatio-temporal representa- tion', 1);('pseudo-3d residual networks', 1);('g. huang', 1);('z. liu', 1);('k. q. weinberger', 1);('densely', 1);('convolutional networks', 1);('l. cai', 1);('w. dong', 1);('h. fang', 1);('3d densenet', 1);('squeeze-and-excitation networks', 1);('appl', 1);('soft comput', 1);('j. carreira', 1);('a. zisserman', 1);('quo', 1);('action recognition', 1);('new model', 1);('kinetics dataset', 1);('m. frank', 1);('m. herbasz', 1);('k. sinuk', 1);('a. keller', 1);('c. nolan', 1);('training', 1);('annual meeting', 1);('munication association', 1);('sheraton', 1);('megc', 1);('grand challenge', 1);('y. cui', 1);('m. jia', 1);('s. j. belongie', 1);('class-balanced', 1);('ieee/cvf confer-', 1);('goyal', 1);('r. b. girshick', 1);('doll', 1);('focal', 1);('dense object detection', 1);('puter science', 1);('acm tomm', 1);('research interests lie', 1);('data sci-', 1);('acm multimedia', 1);('gifted y', 1);('re- search interests', 1);('deep learn-', 1);('natural language processing', 1);('represen- tation learning', 1);('ieee tac', 1);('icme', 1);('associate researcher', 1);('main research inter- ests', 1);('representation learn-', 1);('recommender sys- tems', 1);('tkde', 1);('tois', 1);('associate professor', 1);('anhui province key laboratory', 1);('data analysis', 1);('ap-', 1);('50+ journal', 1);('social network', 1);('social media analysis', 1);('ieee tmc', 1);('ieee tmm', 1);('sensor member', 1);('vice dean', 1);('computer sci-', 1);('general area', 1);('research in- cludes data', 1);('machine learning', 1);('social network analysis', 1);('recommender systems', 1);('knowledge', 1);('data engineer-', 1);('mobile computing', 1);('cikm', 1);('program committees', 1);('numerous conferences', 1);('sdm', 1);('national science foundation', 1);('distinguished y', 1);('scholars', 1);