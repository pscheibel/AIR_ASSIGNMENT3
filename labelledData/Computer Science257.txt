('rgb', 11);('international conference', 9);('hri', 8);('gradcam', 7);('cls', 6);('fig', 6);('semantic segmentation', 6);('slowfast', 6);('cutout', 6);('posedataset', 5);('proceedings ieeecvf', 4);('aug deepfake', 4);('video clips', 4);('ieee', 3);('spatiotemporal', 3);('baseline', 3);('video augmentation techniques', 3);('recognition', 3);('optical flow', 3);('stgcn', 3);('social robots', 3);('international foundation', 3);('iop', 2);('andrea thomaz guy hoffman maya cakmak', 2);('autonomous agents multiagentsystems richland sc', 2);('robotics', 2);('machine learning robot perception', 2);('personalized', 2);('ognjen rudovic jaeryoung lee miles dai bjrn schuller rosalind wpicard', 2);('pattern recognition', 2);('picard', 2);('processing', 2);('york ny usa', 2);('computing machinery', 2);('cynthia breazeal', 2);('mixed', 2);('mutual information andcrossentropy', 2);('cam', 2);('resnet50', 2);('augmentations', 2);('original dataset', 2);('clear signs', 2);('social robot', 2);('action recognition models', 2);('i3d', 2);('timesformer', 2);('skeletonbased', 2);('lstm', 2);('recent years', 2);('previous study', 2);('similarly', 2);('base models', 2);('june', 2);('large public datasets', 2);('understanding capability', 2);('joint engagement classification', 2);('machine learning applicationsicmla ieee', 1);('recognition videos survey in2021', 1);('human action', 1);('current advanceson deep learningbased', 1);('yixiao zhang baihua li hui fang qinggang meng', 1);('ieee transactions circuits systems video', 1);('model audiovisual emotionrecognition', 1);('shiqing zhang shiliang zhang tiejun huang wen gao qi tian', 1);('topics', 1);('role detection multiparty interactions ieee', 1);('multistream recurrent neuralnetwork', 1);('lingyu zhang richard j radke', 1);('international conferenceon computer vision', 1);('localizable features', 1);('strong classifierswith', 1);('strategy train', 1);('cutmix regularization', 1);('yoo', 1);('sangdoo yun dongyoon han seong joon oh sanghyuk chun junsuk choe', 1);('image video processing', 1);('networkeurasip journal', 1);('saliencyaware motion enhancement', 1);('human activity', 1);('zhengkui weng wuzhao li zhipeng jin', 1);('computer vision image understanding', 1);('networks video actionrecognition', 1);('dual attention', 1);('efficient', 1);('lu', 1);('dafeng wei ye tian liqing wei hong zhong siqian chen shiliang pu', 1);('arxiv preprintarxiv201110250', 1);('interaction understanding', 1);('lagnet logicaware graph network', 1);('zhenhua wang jiajun meng jin zhou dongyan guo guosheng lin jianhuazhang javen qinfeng shi shengyong chen', 1);('materialsscience engineering vol', 1);('conference series', 1);('new model human action recognition', 1);('i3dlstma', 1);('xianyuan wang zhenjiang miao ruyi zhang shanshan hao', 1);('conference oncomputer vision pattern recognition workshops', 1);('visual explanationsfor convolutional neural networks', 1);('scorecam scoreweighted', 1);('fan yang zijian zhang sirui dingpiotr mardziel xia hu', 1);('haofan wang zifan wang mengnan', 1);('symposium robot humaninteractive communication roman', 1);('awareness thefocus attention conversation robotcentric reinforcement learningapproach', 1);('maintaining', 1);('vzquez steinfeld e hudson', 1);('acmieee internationalconference humanrobot interaction hri', 1);('body orientation gaze', 1);('conversations understanding', 1);('mcdorman j forlizzi steinfeld e hudson2017 towards robot autonomy', 1);('vzquez e j carter', 1);('foundations trends robotics', 1);('humanrobot interaction', 1);('computational', 1);('computationalhumanrobot interaction foundations trends robotics', 1);('multilabel classification', 1);('reviewof methods', 1);('adane nega tarekegn mario giacobini krzysztof michalak', 1);('dev', 1);('2001maternal responsiveness childrens achievement language milestoneschild', 1);('catherine tamislemonda marc h bornstein lisa baumwell', 1);('computing machinerynew york ny usa', 1);('multimodal interactionseattle washington usa icmi', 1);('acm', 1);('interactions robots inproceedings', 1);('dominance', 1);('classification childrens', 1);('sarah strohkorb iolanda leite natalie warren brian scassellati', 1);('internationalconference autonomous agents multiagent systems singapore singaporeaamas', 1);('robot tutors proceedings', 1);('affectawarestudent', 1);('samuel spaulding goren gordon cynthia breazeal', 1);('internationalconference autonomous agents multiagent systems stockholm swedenaamas18', 1);('proceedings', 1);('pronunciationsocially interactive agents', 1);('modeling childrens', 1);('robot', 1);('samuel spaulding huili chen safinah ali michael kulinski cynthia breazeal2018', 1);('fully automatic analysis engagement relationshipto personality humanrobot interactions ieee access', 1);('isabelle hupont hatice gunes mohamedchetouani', 1);('hanan salam oya', 1);('intelligent robots systems iros', 1);('ieeersjinternational', 1);('autism', 1);('culturenet deep learning approach engagementintensity estimation face images', 1);('andr w', 1);('schuller', 1);('ferrer', 1);('rudovic utsumi j lee j hernandez e', 1);('engagement autism therapy science', 1);('andengagement autism therapy science', 1);('j child lang', 1);('socioeconomic statusknowledge child development child vocabulary skill', 1);('relation', 1);('childdirected', 1);('meredith rowe', 1);('andbehavioral adaptation', 1);('user', 1);('silvia rossi franois ferland adriana tapus', 1);('technology journal', 1);('affectivelearning manifesto bt', 1);('strohecker', 1);('breazeal cavallot machover resnick roy', 1);('blumberg', 1);('bender', 1);('picard papert', 1);('july', 1);('affective computing challenges int j humcomputstud', 1);('rosalind', 1);('hawaii', 1);('robot companion early literacyeducation proc aaai aaai', 1);('autonomous', 1);('modelfree affective reinforcement learning approach', 1);('ishaan grover samuel spaulding louis gomez cynthiabreazeal', 1);('hae', 1);('forrobustness arxiv220106494 csai37', 1);('augly data augmentations', 1);('zoe papakipos joanna bitton', 1);('applicationsof computer vision', 1);('dataset proceedings ieeecvf winter', 1);('contextaware personality inference dyadic scenarios introducing', 1);('cs jacques albertclaps alexa mosegu zejian zhang david gallardo georgina guilera', 1);('cristina palmero javier selva sorina smeureanu julio', 1);('conference computer vision andpattern recognition', 1);('graph convolutions', 1);('ziyu liu hongwen zhang zhenghao chen zhiyong wang wanli ouyang2020 disentangling', 1);('selfcorrection humanparsing ieee transactions pattern analysis machine intelligence', 1);('peike li yunqiu xu yunchao wei yi yang', 1);('multimodalinteraction', 1);('playtherapy proceedings', 1);('quality assessment', 1);('improving movementsynchrony estimation', 1);('jicheng li anjana bhat roghayeh barmaki', 1);('translational', 1);('video 3dreconstruction', 1);('novel method measure motherinfant interaction', 1);('interaction', 1);('achard missonnier kerenr feldman chetouani david cohen', 1);('leclre avril viauxsavelon n bodeau', 1);('conferenceon image processing icip ieee', 1);('slowfastselfattention network action recognition', 1);('myeongjun kim taehun kim daijin kim', 1);('whats yourmind nico kiknstliche intelligenz', 1);('matthias kerzel jakob ambsdorf dennis becker wenhao lu erik strahl josuaspisak connor gde tom weber stefan wermter', 1);('pages httpsdoiorg101145260002129', 1);('oct', 1);('article', 1);('multiparty humanrobot interaction acm trans interactintell syst', 1);('machinelearning', 1);('humanrobot interaction hri ieee26327328 simon keizer mary ellen foster zhuoran wang oliver lemon', 1);('acmieee', 1);('humanrobot interaction 201712th', 1);('affective', 1);('jung', 1);('malte', 1);('apsipa asc ieee', 1);('annual summit', 1);('signal andinformation', 1);('asiapacific', 1);('shorttermmemory audiovisual emotion recognition', 1);('attentivelycoupled', 1);('jiahao hsu chunghsien wu', 1);('contextsensitive recognition conversationalengagement', 1);('transparentframework', 1);('alexander heimerl tobias baur elisabeth andr', 1);('neurocomputing', 1);('multitask ensemble learning multiplefeatures', 1);('visualaudioemotion', 1);('hao weihua cao zhentao liu min wu peng xiao', 1);('pmid', 1);('psychologybulletin', 1);('geoffrey haddock gregory r maio karin arnold thomas huskinson2008 persuasion affective cognitive moderating effectsof need affect need cognition personality', 1);('ieee robotics automation', 1);('explainable solution forneurorobotic systems', 1);('deep generalization ofperipheral', 1);('toward', 1);('paras gulati qin hu farokh atashzar', 1);('communication', 1);('patterns cooperation parentchildinteractions relationship nonverbal verbal communicationhuman', 1);('parental', 1);('tsfira grebelskylichtman', 1);('phoenix arizona', 1);('languageskills proc aaai aaai', 1);('robot tutor childrens', 1);('personalization', 1);('goren gordon samuel spaulding jacqueline kory westlund jin joo lee lukeplummer marayna martinez madhurima das cynthia breazeal', 1);('frontiers computer', 1);('automatic emotion recognition', 1);('kexin feng theodora chaspari', 1);('internationalconference computer vision', 1);('proceedings ieee', 1);('2019slowfast networks video recognition', 1);('christoph feichtenhofer haoqi fan jitendra malik kaiming', 1);('arxiv200404730 cscv18', 1);('x3d expanding architectures efficient videorecognition', 1);('christoph feichtenhofer', 1);('arxiv210413586 cscv17', 1);('haodong duan yue zhao kai chen dian shao dahua lin bo dai', 1);('pyskl towardsgood practices skeleton', 1);('haodong duan jiaqi wang kai chen dahua lin', 1);('real world arxivpreprint arxiv210705593', 1);('leveragingexplainability', 1);('fethiye irmak dogan gaspar melsion iolanda leite', 1);('regularization ofconvolutional neural networks cutout arxiv preprint arxiv170804552', 1);('improved', 1);('taylor', 1);('terrance devries graham', 1);('andbenchmark httpsgithubcomopenmmlabmmflow13', 1);('mmflow openmmlab optical flow toolbox', 1);('mmflow contributors', 1);('generation video understanding toolbox benchmark', 1);('openmmlabs', 1);('mmaction2 contributors', 1);('studies', 1);('child', 1);('assessment', 1);('nonverbalcommunication parentchild relationships', 1);('review', 1);('vivienne colegrove sophie havighurst', 1);('vision', 1);('international conference oncomputer', 1);('topology refinement graph convolution skeletonbasedaction recognition', 1);('chen ziqi zhang chunfeng yuan bing li ying deng weiming hu2021 channelwise', 1);('multimedia', 1);('fidelity face swapping mm', 1);('simswapan efficient framework', 1);('chen xuanhong chen bingbing ni yanhao ge', 1);('abs200809207 20208renwang', 1);('damip2c parentchildmultimodal interaction dataset arxiv', 1);('park2020 dyadic speechbased affect recognition', 1);('breazeal h', 1);('weninger rosalind picard', 1);('chen zhang', 1);('multimodal interaction virtual event netherlands icmi20', 1);('damip2cparentchild multimodal interaction dataset proceedings', 1);('dyadic speechbased affect recognition', 1);('andhae park', 1);('chen yue zhang felix weninger rosalind picard cynthia breazeal', 1);('computers', 1);('emotive engagement', 1);('reciprocal peer learning', 1);('impact', 1);('learningwith children', 1);('chen hae', 1);('dyadic affect parentchild multimodal interactionintroducing damip2c dataset preliminary analysis ieee transactionson affective computing', 1);('chen sharifa mohammed alghowinem soo jung jang cynthia breazealand hae', 1);('arxiv210205095 cscv4huili', 1);('spacetimeattention need video understanding', 1);('ieee3gedas bertasius heng wang lorenzo torresani', 1);('automatic faceand gesture recognition', 1);('gesture head movement analyses dyadic parentchild interactionas indicators relationship ieee', 1);('alghowinem huili chen hae', 1);('joint engagement ratinginventory technical', 1);('adamson r bakeman k suma', 1);('withdata augmentation visualization techniques robots affectrecognition multiperson humanrobot interaction wildreferences1lb', 1);('potential stateoftheart endtoend video understandingmodels', 1);('asthe firststep', 1);('furtherenhance performance model predictions multipersoncontextin conclusion work', 1);('culturesensitive joint engagement models', 1);('individuals eg', 1);('models outperforms onesizefitsall generic modelswhen', 1);('individual human groupsit', 1);('final layer', 1);('deep neuralnetwork layer', 1);('account forindividual differences', 1);('holisticand humanlike representations joint engagementour', 1);('data onemodality', 1);('challenging realworld situations', 1);('applicability multiperson', 1);('multiple modalities thedataset', 1);('utilizing', 1);('multiple data modalities audio video jointlylearn joint engagement', 1);('data augmentation visualization', 1);('multiple waysfirst', 1);('framework extensible', 1);('additionally', 1);('representations endtoend models', 1);('various social behavioral cuescontribute', 1);('generalize framework acrossdatasets quantify', 1);('additional parentchild interactiondatasets', 1);('future plan use', 1);('belief endtoend models limitedinterpretability comparison', 1);('valuable tohumans', 1);('interpretable insights', 1);('modelsrecognition joint engagement work aims demonstratethat visualization models', 1);('specific social cues behavioral characteristics', 1);('representation pinpoint', 1);('analysis visualization models', 1);('generatedby simulation', 1);('internet', 1);('large datasets', 1);('train robots perception modelmay', 1);('realworld interaction datasets', 1);('small dataset demonstratesits potential benefits realworld multiperson', 1);('excellent performance', 1);('similar tasks', 1);('various data augmentationtechniquesanddeeplearningmodelspretrainedonlargerdata corpora', 1);('framework leverage', 1);('framework formultipersonaffectrecognitionhoweverthesmallsizeofthedatasetmotivates', 1);('limit applicability', 1);('small population size thedyads', 1);('machine learning 52we', 1);('exploration method', 1);('support diverse methodsof robot learning realtime human input human teacherssuch', 1);('thisinterpretable visualization', 1);('crucial information videos learning task', 1);('recognition models extract', 1);('visualization method', 1);('model instance function minimal computational resources', 1);('excessive computational resources realtime version ofthe', 1);('predictions realtime', 1);('prediction output trainedthey', 1);('image frames asinput generate', 1);('models endtoend models', 1);('prediction eg', 1);('performance comparisonfeature extraction', 1);('different video augmentationtechniques', 1);('visualization sample test video clip', 1);('necessary human identification human skeletonfigure', 1);('multiple layersof models', 1);('models usedfor realtime', 1);('autonomous robots multipersonhumanrobot interaction', 1);('action recognition tasks', 1);('superior performance interpretability endtoend models', 1);('interpretable mannerdue', 1);('representation ofparentchild joint engagement', 1);('findings insights indicatethat endtoend models', 1);('altogether', 1);('social cues indicative parentchild interaction', 1);('sensitivity subtle', 1);('representations endtoend', 1);('visualization ofthe', 1);('models performance', 1);('ctrgcn stgcn', 1);('human skeleton featureseg', 1);('joint engagementrecognition models', 1);('slowfast i3d', 1);('general video understandingeg', 1);('endtoend models', 1);('training data', 1);('complex humanhumanjoint affective states', 1);('performance visualization stateoftheart endtoend video classification models', 1);('discussion conclusionthe', 1);('coreinformation training5', 1);('thisis', 1);('focus thecore parts scene', 1);('populationsor backgrounds encourages model robust thechange backgrounds clothing etc focus dyads facesand bodies', 1);('corner thescene implies training', 1);('attention dyads', 1);('parents childs', 1);('distractedby backgrounds example', 1);('small regions', 1);('certain joint engagement classes onthe dataset', 1);('regions contribute classify', 1);('understanding models learnedwe use', 1);('visualizing interaction representationsto', 1);('core information42', 1);('joint engagement', 1);('rgbframes', 1);('modelto catch affective state eg facial', 1);('human actionrecognition tasks joint engagement recognition', 1);('patterns fromthe human skeleton information', 1);('models attempt', 1);('baseline endtoend models graph', 1);('top1 accuracy max acc', 1);('insight generalization video augmentationtechniques joint engagement recognitionthe results', 1);('shows allof endtoend models lack diverse', 1);('performance increases', 1);('focus behavioral aspects ratherthan', 1);('asthe model', 1);('performance notdegrade', 1);('baseline performancefor endtoend models', 1);('notimprove performance', 1);('lead accuracy 157and145', 1);('particularly timesformer x3d', 1);('baselines endtoend models', 1);('hiddenforannonymoussubmission41 joint engagement recognition evaluationas', 1);('gpus source', 1);('nvidia', 1);('models trainedon', 1);('pytorch', 1);('opensource toolbox', 1);('mmaction2 pyskl', 1);('different inputs task implementationwe', 1);('models tosee effect', 1);('joint engagement classification performance', 1);('capability improvejoint engagement recognition stateoftheart action recognitionmodels', 1);('augdeepfake mixed cutout', 1);('section conduct experiments', 1);('experiment resultsin', 1);('mutual information function isa crossentropy function image crop function4', 1);('age andgender', 1);('algorithm', 1);('overall process', 1);('box coordinates', 1);('separate values', 1);('boxes parents andchilds', 1);('equation6', 1);('images distributions', 1);('crossentropy1920212223returnand pass logsoftmax', 1);('mutual information131401201215', 1);('boundingboxes', 1);('score8 forrole part bbox', 1);('images012 model mi ce crop2output score34score5while0and1and2do6 boundingboxes07', 1);('evaluation algorithm1input', 1);('normalize pixel values imagesalgorithm', 1);('value operator respect thedistribution', 1);('distributions andthis', 1);('kullbackleibler', 1);('metric crossentropy whichcomes', 1);('theimage arrays compute bidimensional histogram oftwo image array samples', 1);('image distribution', 1);('mutual information', 1);('low signal', 1);('histogram binsand', 1);('variables metric high theattention map signal', 1);('mutual information dimensionless quantity metric measures mutualdependence', 1);('techniques', 1);('image', 1);('bottom parts body343', 1);('upper bodyparts', 1);('importance head', 1);('upper partsof body', 1);('social touch body closeness', 1);('dyads joint engagementlevel', 1);('insights ourannotation process', 1);('eachof body poses', 1);('gaussian', 1);('fig2', 1);('specify body segment', 1);('pose extractor', 1);('extract skeleton informationby', 1);('skeleton information', 1);('proper regionswe', 1);('aug deepfake mixed cutoutinputtimesformer cls', 1);('performance inall endtoend modelsbasline', 1);('joint engagement recognition task results stateoftheart endtoend top table skeletonbasedbottom table action recognition models', 1);('overview', 1);('optical flowand', 1);('references skeleton information', 1);('metric converts images distributions calculates', 1);('pipeline', 1);('ingradcam focus motion changes', 1);('displays color orientations', 1);('optical flowwhich', 1);('subtle motion changes', 1);('human joint engagement eg', 1);('social cues iscrucial', 1);('subtle display duration intensity', 1);('section31', 1);('scheme joint engagement', 1);('onthe annotation', 1);('2the primary hypothesis', 1);('scores reference calculate weightedaverage score weight value', 1);('2skeleton information', 1);('theimages distributions calculate', 1);('previous section concept ofgradcam', 1);('evaluation references', 1);('image frames', 1);('video format', 1);('gradcamwhich', 1);('global pooling', 1);('definedas 3where 4heredenotes', 1);('convolution layer amodel', 1);('gradcam consider', 1);('as1 1where1 2definition', 1);('class interest', 1);('convolution layer 1and', 1);('global poolinglayerafter', 1);('cam consider', 1);('cam gradcam', 1);('activation', 1);('definitions in55 class', 1);('following', 1);('effective visualization capability purpose validatethe quality', 1);('gradcams', 1);('supports intuitivevisualization models attention image techniquehas', 1);('combinesthe classspecific property', 1);('activation mapping cam', 1);('ageneralization class', 1);('activation mapping gradcam', 1);('evaluation metric341 gradientweighted', 1);('mid', 1);('largestnumber labels', 1);('black boxes totalwe', 1);('corresponding regions', 1);('detectionmodule detect parents childs faces', 1);('core information thescenes ie', 1);('validate themodels representation learning', 1);('robustness overall performance conductingclassification tasks work', 1);('dropout input space', 1);('masks square regions input', 1);('wellknown simple regularizationtechnique', 1);('mixed335 cutout cutout', 1);('soin', 1);('ratio dataset', 1);('augand deepfake', 1);('performance improvements combinedto', 1);('performance improvement', 1);('mixed finallywealsowantedtoseeifcombiningthedatasetsthat', 1);('augmentation', 1);('images total gathered24749 video clips', 1);('naturalvideo clips', 1);('table generates', 1);('realistic customizations eg race gender age accessoriesand hair type', 1);('dataset httpsgeneratedphotosfaces whichsupports', 1);('swappingin videos feed diverse', 1);('simswap', 1);('small populations', 1);('dyads faces', 1);('deepfake deepfake', 1);('video clips labels ratio333', 1);('dataset total', 1);('different types', 1);('semantic segmentations', 1);('hints semantics frame', 1);('msg3d', 1);('ctrgcn', 1);('frame 346m', 1);('frame 280m', 1);('x3dm', 1);('frame 376m', 1);('frame 1214m', 1);('inputs data modality params backbone epochstimesformer', 1);('model configuration stateoftheart modelsmodels year', 1);('comparisons', 1);('whole frame', 1);('encouraging themodels robust learning', 1);('indoor image blur background', 1);('todiversify background', 1);('various kinds augmentation techniques whywe', 1);('irrelevant patterns eg backgrounds', 1);('24749video clips', 1);('video clips tomake labels ratio total', 1);('low high joint engagement', 1);('8143video clips', 1);('video augmentations techniqueswe', 1);('labels total number video clips ensure afair comparison', 1);('video augmentation technique describedin', 1);('thedetails', 1);('labeldistribution motivates', 1);('sample size label noise etc', 1);('andthis poses classification task', 1);('label distribution', 1);('techniquesour dataset', 1);('video augmentation', 1);('pyskl', 1);('momentum rest thehyperparameters', 1);('nesterov', 1);('learningrate scheduler optimizer', 1);('cosineannealing', 1);('batch size 128and train models', 1);('initial learning rate', 1);('dataset format keypoint keypointscore framedir label imgshape originalshape totalframes', 1);('ntupose', 1);('skeletonbasedactionrecognitionmodels forskeletonbasedmodels', 1);('mmaction2', 1);('thedefault configuration', 1);('number layers eachbackbone hyperparameters', 1);('every20 epochs total', 1);('models weuse base learning rate', 1);('sections321 action', 1);('k1 accuracy crossentropyloss details', 1);('topk', 1);('shuffledwesplitthemintothetrainvalidandtestby702010ratiofor models measure', 1);('n24749', 1);('details model training evaluation video clip pose dataset', 1);('model training evaluationin', 1);('mutual information crossentropy32', 1);('quality calculate evaluation score', 1);('eachvideo clip', 1);('models generate', 1);('proposed', 1);('aamas2023figure', 1);('fragments familyon average1dataset publication review time submission paper', 1);('fivesecond video clips', 1);('annotation protocol', 1);('strictlyfollowing', 1);('model training', 1);('medium4968 high', 1);('classification levels', 1);('averagingthe ratings', 1);('annotators thefinal score', 1);('threshold good quality 07510after recordings', 1);('annotators measuredgiven evaluation criteria annotation quality icc', 1);('average fixedraters agreement', 1);('intraclass correlation icc type', 1);('generatecontinuous quality scales', 1);('low joint engagement valence videofragment interval target audiovisual recordings', 1);('corresponding cases parentchild', 1);('joint engagementvalence', 1);('corresponding cases theparentchild pair', 1);('seconds fivepointordinal scale', 1);('annotatorsgave ratings', 1);('specifically', 1);('previous work joint engagement parentchild interaction', 1);('scheme choice video interval threshold theannotation protocol', 1);('thecoding', 1);('annotators psychology education background', 1);('childs interaction parentto annotate parentchild joint engagement audiovisualrecordings parentchildrobot interactions', 1);('metric quantifies classifies verbal nonverbal behaviors', 1);('thisengagement', 1);('previous parentchild interaction studies', 1);('measure parentchild engagement', 1);('joint engagement rating inventory jerito', 1);('annotate quality parentchild engagement', 1);('weeks intotal triadic session audiovisual recordings capturedand', 1);('25minute sessions', 1);('activity parentand child course', 1);('families 37yearoldchildren', 1);('ourprevious deployment1 study', 1);('dataset annotationthe', 1);('objective metric', 1);('different video augmentation methods', 1);('section introduce robotparentchild interactiondataset models', 1);('methodsin', 1);('task joint engagement recognitionwith', 1);('suitable models', 1);('great power understanding generalhuman activities', 1);('breakthrough results actionrecognition tasks', 1);('transformer cnn graphconvolutional', 1);('based', 1);('skeleton processing', 1);('convolution neural gcn networks', 1);('spatialtemporal graph', 1);('direct informationpropagation', 1);('utilizes densecrossspacetime edges', 1);('effective longrange', 1);('disentangles importance nodes differentneighborhoods', 1);('schememsg3d', 1);('different channels multiscale aggregation', 1);('aggregates jointfeatures', 1);('different topologies', 1);('channelwise topologyrefinement graph convolution network ctrgcn', 1);('recently', 1);('nature compactness', 1);('sequenceof joint', 1);('dynamic motion featureson hand human skeletons video', 1);('slow pathway static spatial', 1);('combinethe benefits', 1);('dualpathway structure', 1);('convnets', 1);('image classification', 1);('model filters poolingkernels', 1);('multiple network axes space time width depth', 1);('small 2d image classification architecture', 1);('efficient video networks continuouslyexpand', 1);('sequence framelevel patchesx3d', 1);('spatiotemporalfeature learning', 1);('architecture video', 1);('transformer', 1);('selfattention space time adaptsthe', 1);('model trainingin action recognition tasks', 1);('basic typical modality', 1);('action recognition models beenthe mainstream approach', 1);('frames optical flowhuman skeleton audio waves', 1);('various modalities eg', 1);('central task video understanding', 1);('developedfor action recognition', 1);('video classification models', 1);('dataaugmentation techniquesstateoftheart', 1);('video classification', 1);('enhance robots ability comprehend affectivedynamics multiperson', 1);('learning techniques', 1);('novel framework', 1);('perception modelsfor', 1);('explainable visualization', 1);('learning research video augmentation techniques', 1);('real world', 1);('dyadic human interactions', 1);('development affectaware', 1);('recognition inwhich interlocutors interact', 1);('learning approach multiperson', 1);('handmovement speech context data audiovisual recordingswere', 1);('interpersonal dynamics dyadic', 1);('bayesian', 1);('framework identify individuals engagement context twoway conversation theirstudy hybrid approach', 1);('speakersanother work', 1);('withattention mechanisms identify individuals affective expression audio stream', 1);('learning methods', 1);('chen', 1);('multipersoninteractions eg', 1);('deep models', 1);('recent works havebegun', 1);('knowledge handful', 1);('social role fourperson meetingto', 1);('vision audio textto recongize participants', 1);('temporalfusion multimodal', 1);('zhang radke', 1);('individuals sociodemographic profiles eg gender relationship status mood 35in', 1);('method thatutilizes multimodal', 1);('recognition dyadic interactioncontext', 1);('personality', 1);('combination deepfeatures graph network logicaware module relationship interaction', 1);('learning models foraction recognition models', 1);('motion andposture', 1);('full body andbody parts individuals', 1);('number multiperson interactionperception models focus human action recognition handshakes eg', 1);('froma single modality eg', 1);('dyadic dynamics', 1);('deep learning models smallnumber studies', 1);('detection multiperson interactions', 1);('learning thedetection', 1);('widespread application', 1);('affectivestates range valence arousal engagement eg 42in contrast', 1);('asmodel input recognition eg', 1);('previous research audiovideoinput', 1);('examples ofdeep learning techniques', 1);('twostream autoencoder', 1);('aclstm', 1);('longshort termmemory', 1);('dbns', 1);('deepbelief', 1);('style speech prosody linguisticssentiment head body movement', 1);('human behavioral cues audiovisual recordingssuch facial expression', 1);('detection models', 1);('singleperson singlemodality', 1);('recognition models commercial affectextraction tools', 1);('deep learning approach affectrecognitionthe', 1);('autonomoussocial robots multiperson', 1);('catalyze development', 1);('advancement multiperson affectrecognition', 1);('complex humanhumaninteractions', 1);('unlock potential robotto', 1);('recognition models dyadic human groups', 1);('theirinteractive behavior research', 1);('robots affective perception', 1);('train robots behavior policy', 1);('robots affective perceptual capacity aminority studies machine learning reinforcement learning', 1);('behavior triggers thatdoes', 1);('behavior policy eg', 1);('wizardofozparadigms teleoperate robot', 1);('current multiparty', 1);('context triadichumanhumanrobot interactiondue limitation robots perception system majority', 1);('humans robots personality profiles', 1);('automatic analysisand classification engagement', 1);('human coders offline eg utterance type gaze interruptions', 1);('nonverbal behavioral', 1);('prediction model participants socialdominance group humanrobot interaction', 1);('separate study', 1);('ina', 1);('trackparticipants control orientation gaze robot', 1);('kinect', 1);('instance estimates user positions body orientations', 1);('socialaffective dynamicsof human group perception system', 1);('equip robotwith perception system', 1);('field studies eg', 1);('multipersonhri', 1);('extraction tools applicablein singleperson settings handful', 1);('vast majority', 1);('usersto date', 1);('social dynamics', 1);('able torecognize affective', 1);('indyadic human interaction interactive technology', 1);('social affective cues directedat interaction task robot contrast', 1);('sufficient anintelligent system', 1);('oneonone interactions withhumans', 1);('majority affectware', 1);('nonetheless', 1);('theirinteraction experience', 1);('individual users', 1);('timely interventions', 1);('recognition enhances effectiveness robots toprovide', 1);('affective signals incorporatedinto robots user cognitive skill estimation models improveuser model accuracy student vocabulary acquisition 47overall', 1);('behavior policy cognitive model human feedback therobots', 1);('social affective signals', 1);('eg 2037prior', 1);('promotechildrens learning', 1);('social robotswith', 1);('early childhood development', 1);('empirical significance', 1);('social robots hasbeen', 1);('signal perception systems', 1);('humanrobot interaction hrideveloping', 1);('cue understandingin', 1);('related works21 socialaffective', 1);('representation compares human annotation guidelines visualregions model', 1);('new metric', 1);('future multiperson', 1);('models affective communication context', 1);('experimental results todemonstrate use', 1);('comprehensive analysis', 1);('social cues2conduct', 1);('video augmentation techniques recognition psychological processesie joint engagement involves changes subtle', 1);('large public datasets human action activity recognition task', 1);('settingthe models', 1);('learning models tojoint engagement recognition task multiperson', 1);('1adapt endtoend', 1);('social cues humansattend gauge joint engagement ie annotation guidelines givento human annotatorsin summary work contributes', 1);('gradientweighted classactivation mapping gradcam', 1);('video framesto identify joint engagement', 1);('visualize model', 1);('inorder', 1);('social cues', 1);('continuous dynamics human', 1);('howthe model learns', 1);('crucial humans', 1);('representation model interpretability', 1);('interpretability models', 1);('novel evaluation metric', 1);('dec', 1);('video augmentation techniquesarxiv221214128v1 cscv', 1);('performance insocial cue understanding tasks', 1);('works305663 show', 1);('detect human activity motion', 1);('understanding joint engagement states parentchild dyads', 1);('social cuesthat', 1);('models representation learning order', 1);('video augmentationtechniques', 1);('stateoftheart action recognition algorithms suchas', 1);('mixed60', 1);('deepfake', 1);('aug', 1);('learning framework range video augmentation techniques', 1);('parentchild joint engagement combininga', 1);('novel hybridmethod', 1);('underexploredin effort field study', 1);('automatic perceptionof affective dynamics members group vianonverbal indicators', 1);('synchronization engagement attachment', 1);('important indicators gaugeinteraction quality', 1);('recognition systems comprehend humanhuman affective dynamics parentchild interactions parentand childs nonverbal behaviors headbody movement gestures postures', 1);('engagement dyadthe nonverbal cues people display group interaction presentexcellent data sources development contactfree unobtrusive', 1);('timely appropriate actions mediatethe flow interaction', 1);('understanding multiperson interaction context supportrobots comprehend interpersonal dynamics parentand child', 1);('pedagogical practices order', 1);('mediate parentchilddialogic interactions', 1);('potential facilitate humanhuman connection andconversation', 1);('questions backandforth conversations', 1);('richdialogic interactions', 1);('equal access', 1);('howevernot', 1);('social emotional cognitive linguistic development', 1);('reciprocalrelationships parents children', 1);('highquality', 1);('scenarios butalso assess quality model deploymentgiven impact parentchild affective exchanges childrensdevelopment growth parentchild interaction primary application domain research', 1);('objective research improvethe affective recognition understanding capabilities socialrobots multiperson humanrobot interaction', 1);('behavior adaptability capabilities', 1);('contributes toa robots user', 1);('wwwifaamasorg rights reservedcompetencies humanrobot interactions', 1);('autonomous agentsand multiagent systems', 1);('kingdom', 1);('may', 1);('b eds', 1);('yeoh n agmon', 1);('ricci', 1);('autonomous agents multiagentsystems aamas', 1);('higherlevelproc 22nd', 1);('basic robot capability', 1);('understanding capacity hasbeen', 1);('natural andreciprocal manner', 1);('humans intuitive', 1);('social signals emotionsis', 1);('interactive robot perceive human nonverbal cues', 1);('anda variety functions ability', 1);('strong links learning', 1);('essential humanhuman interaction', 1);('introductionaffective', 1);('ifaamas', 1);('kingdom may', 1);('autonomous agents multiagent systems aamas2023 london', 1);('video augmentation techniques multiperson humanrobot interaction proc', 1);('reference formatyubin kim huili chen sharifa alghowinem cynthia breazeal hae wonpark', 1);('understanding jointengagement recognition multi humanrobot interactionacm', 1);('recognition themultiperson humanrobot interaction wildkeywordsnonverbal communication multiperson', 1);('data augmentation visualization techniques', 1);('potential endtoend video understanding', 1);('joint engagement work', 1);('model interpretability', 1);('representation ofthe models', 1);('models inthe robotparentchild interaction context', 1);('experimental results', 1);('datasets toimprove joint engagement classification performance', 1);('aug deepfake cutout mixed', 1);('video augmentation techniquesgeneral', 1);('joint engagement recognition models', 1);('social robotat home', 1);('dataset ofparentchild dyads reading storybooks', 1);('various video augmentation techniques', 1);('deep learning frameworkwith', 1);('parentchilddyads joint engagement', 1);('present novel hybrid framework', 1);('herewe', 1);('complex subtle nonverbal exchanges', 1);('interplay members eg joint engagement presentsas', 1);('accurate perception users affectivestate eg engagement', 1);('interact group users intuitive reciprocalwayhoweverthechallengeofmultipersonaffectunderstandingcomes', 1);('essential social robots', 1);('video augmentationtechniques multiperson humanrobot interactionyubin kim huili chen sharifa alghowinem cynthia breazeal hae parkmit media labcambridge massachusettsabstractaffect', 1);