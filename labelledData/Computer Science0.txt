('tinymim', 50);('mae', 35);('mim', 24);('imagenet-1k', 19);('computer vision', 16);('transformer', 15);('proceedings', 15);('ade20k', 14);('relation distillation', 12);('transformers', 11);('vit-l', 11);('token distillation', 11);('deit', 10);('eq', 10);('international conference', 10);('vit-t', 9);('knowledge distillation', 9);('semantic segmentation', 8);('vit', 8);('vit-b', 8);('target block', 8);('tinymim-vit-b', 8);('ieee/cvf', 8);('vits', 7);('comparison', 7);('vit-s', 7);('ffn', 7);('raw image', 7);('imagenet', 7);('han hu', 7);('yue cao', 7);('zheng zhang', 7);('teacher network', 6);('small vision', 6);('q-k', 6);('rv v', 6);('rqk', 6);('feature distillation', 6);('sequential distillation', 6);('ema in1k', 6);('pattern recognition', 6);('zhenda xie', 6);('computation budget', 5);('report top-1 accuracy', 5);('ours', 5);('vision transformers', 5);('image classication', 5);('section 3.1.1', 5);('fi\x001', 5);('student network', 5);('yixuan wei', 5);('learning', 5);('hyper-parameters', 5);('% top-1 accuracy', 4);('beit', 4);('peco', 4);('distillation target', 4);('vision transformer', 4);('v-v', 4);('rqq', 4);('d=m', 4);('block', 4);('adaptive block', 4);('learning rate', 4);('fine-tuning', 4);('mae-vit-b', 4);('pixel in1k', 4);('supervised', 4);('li dong', 4);('furu wei', 4);('arxiv', 4);('jianmin bao', 4);('masked', 4);('cvpr', 4);('yutong lin', 4);('baining guo', 4);('network regularization', 3);('cls', 3);('vit-tiny', 3);('vit-', 3);('tiny size', 3);('inductive biases', 3);('previous works', 3);('different', 3);('clip', 3);('cnn', 3);('section 3.1.3', 3);('ln', 3);('q-q', 3);('k-k', 3);('rkk', 3);('head', 3);('student model', 3);('feature dimension', 3);('head number', 3);('batch size', 3);('drop path rate', 3);('distillation strategies', 3);('scratch', 3);('label in1k', 3);('mae-vit-l', 3);('dpr', 3);('hangbo bao', 3);('hugo touvron', 3);('herv', 3);('jegou', 3);('saining xie', 3);('ieee', 3);('dong chen', 3);('pmlr', 3);('dacheng tao', 3);('knowledge', 3);('dan hendrycks', 3);('hang zhao', 3);('batch', 3);('adam\x0f', 3);('adam', 3);('minimal', 3);('rate schedule', 3);('dropout', 3);('weight', 3);('pre- training', 2);('small models', 2);('mod- els', 2);('distillation framework', 2);('distilling', 2);('token relations', 2);('distil- lation', 2);('weak', 2);('vit-small', 2);('base size', 2);('alternative way', 2);('rst time', 2);('swin', 2);('mobilevitv3-s', 2);('imagenet-', 2);('large models', 2);('transform-', 2);('distillation', 2);('models [', 2);('specically', 2);('data augmentation', 2);('auxiliary losses', 2);('useful ndings', 2);('distillation targets', 2);('data', 2);('distillation strategy', 2);('vit-t.', 2);('figure', 2);('similar model size', 2);('natural language processing', 2);('simmim', 2);('maskfeat', 2);('teacher', 2);('data2vec [', 2);('milan', 2);('small model', 2);('systematic study', 2);('cnns', 2);('trans-', 2);('output feature', 2);('input image x', 2);('linear projection layer', 2);('attention feature', 2);('ffn feature', 2);('attention', 2);('layer norm', 2);('qm', 2);('ki', 2);('softmax qm', 2);('ikm it p', 2);('image ex', 2);('block number', 2);('teacher model', 2);('norm', 2);('l1', 2);('head alignment', 2);('mtheads', 2);('head dimension', 2);('sequential distillation strategy', 2);('adamw', 2);('weight decay', 2);('natural adversarial examples', 2);('imagenet-a', 2);('imagenet-r', 2);('18-th block', 2);('plain architecture', 2);('cim', 2);('cae', 2);('out-of-domain datasets', 2);('ablation', 2);('eq.6', 2);('different features', 2);('atten- tion', 2);('residual connection', 2);('300-epoch schedule', 2);('maey', 2);('moco', 2);('tinymim-vit-s in1k', 2);('dall-e dalle250m+in22k+in1k', 2);('sdae', 2);('vqgan in1k', 2);('deit-style', 2);('] knowledge distillation', 2);('loss top-1 acc', 2);('top-1 acc', 2);('v-v x', 2);('tinymim-vit-s', 2);('image', 2);('rst row', 2);('drop path', 2);('bert', 2);('learning representations', 2);('armand joulin', 2);('ping luo', 2);('xinlei chen', 2);('kaiming', 2);('dongdong chen', 2);('xiaoyi dong', 2);('lu yuan', 2);('zicheng liu', 2);('self-', 2);('machine learning', 2);('matthijs douze', 2);('national conference', 2);('steven basart', 2);('dawn', 2);('jacob steinhardt', 2);('byeongho heo', 2);('sangdoo yun', 2);('jin', 2);('choi', 2);('aaai', 2);('articial intelligence', 2);('bo chen', 2);('mingxing tan', 2);('weijun wang', 2);('advances', 2);('neural information processing systems', 2);('european conference', 2);('ze liu', 2);('zhuliang yao', 2);('alec radford', 2);('gabriel goh', 2);('sucheng ren', 2);('zhengqi gao', 2);('zihui xue', 2);('chen wei', 2);('alan yuille', 2);('contrastive', 2);('learning rivals', 2);('arxiv preprint arxiv:2205.14141', 2);('tete xiao', 2);('bolei zhou', 2);('qi dai', 2);('shan', 2);('chang xu', 2);('chao xu', 2);('peak', 2);('stochastic', 2);('layer-wise', 2);('empirical', 1);('distilling mim pre-trained', 1);('sucheng ren fangyun wei', 1);('zheng zhang han hu microsoft', 1);('asia abstract masked', 1);('large vision', 1);('real-world applications can-', 1);('ap- proach', 1);('explore distillation techniques', 1);('mim-based', 1);('different op- tions', 1);('sequential dis- tillation', 1);('intermediate layer', 1);('student mismatches', 1);('nd- ings', 1);('accuracy improve- ments', 1);('base models', 1);('+4.2 % /+2.4 % /+1.4 % gains', 1);('ae20k', 1);('image classica- tion', 1);('new record', 1);('small vision models', 1);('strong perfor- mance suggests', 1);('code', 1);('introduction masked', 1);('large portion', 1);('image area', 1);('original signals', 1);('thanks', 1);('corresponding', 1);('fawe @ microsoft.com', 1);('vit-t vit-s vit-b7074788286scratch mae tinymim', 1);('modelparam', 1);('flops top-1miou', 1);('deit-t', 1);('pvt-t', 1);('cit-t', 1);('edgevit-xs', 1);('mobilevitv1-s', 1);('architecture variants', 1);('backbone pa- rameter', 1);('classication layer', 1);('numerous follow-ups', 1);('research line', 1);('architectures [', 1);('reconstruction targets [', 1);('proper- ties [', 1);('1arxiv:2301.01296v1 [ cs.cv ]', 1);('jan', 1);('vit-t vit-s vit-b vit-l scratch', 1);('gap', 1);('+0.7 +2.4 +3.3', 1);('model size shrinks', 1);('model size', 1);('real-world applications', 1);('classi- cation', 1);('accuracy drops', 1);('certain inductive bias', 1);('architecture design [', 1);('additional architec- tural inductive biases facilitate optimization', 1);('expressive capacity', 1);('plain small vision', 1);('above questions', 1);('pretext task', 1);('uses distillation technology [', 1);('nice properties', 1);('not-', 1);('main work', 1);('different design options', 1);('dis- tillation targets', 1);('macro distillation strategy', 1);('main ndings re-', 1);('fea- ture maps', 1);('intermediate layers', 1);('optimal target layer', 1);('different down-stream tasks', 1);('small drop path rate', 1);('macro', 1);('se- quential distillation strategy', 1);('framework options', 1);('sig- nicant', 1);('accuracy improvements', 1);('different sizes', 1);('speci-', 1);('vit-base', 1);('+4.2 % /+2.4 % /+1.4 %', 1);('-t model', 1);('top-1 accuracy', 1);('architectural in- ductive biases', 1);('new accuracy record', 1);('tinymim-t', 1);('-t suggests', 1);('training methods', 1);('related', 1);('masked image modeling masked language modeling', 1);('mlm', 1);('incredible success', 1);('nlp', 1);('inspired', 1);('such success', 1);('dall-e', 1);('] nd', 1);('rgb', 1);('pixels results', 1);('favorable rep- resentations', 1);('asymmetric encoder-decoder architecture', 1);('visible tokens', 1);('high portion', 1);('com- putation burden', 1);('lightweight decoder', 1);('rich semantics', 1);('computer vi- sion', 1);('low-level information', 1);('recent works aim', 1);('local gradient', 1);('hog', 1);('descriptor [', 1);('] trains', 1);('image raw image factorsinput target feature relation head number', 1);('softmaxoutput feature block feature qkv features attention feature ffn feature res', 1);('ffn blocklast intermediate transformer block', 1);('-noutpu tfeature', 1);('multi-head attentionadd', 1);('normffnadd', 1);('norm attention featureffn featureblock feature raw image masked image feature', 1);('block softmax transformer block', 1);('transformer block', 1);('highlight', 1);('relationsfigure', 1);('new tokenizer', 1);('perceptual similarity', 1);('ibot [', 1);('ema', 1);('] adopts', 1);('similarly', 1);('beitv2', 1);('tokenizer training', 1);('various tokenizers/teachers', 1);('im- age', 1);('giant size [', 1);('paper explores', 1);('distillation technology', 1);('knowledge distillation knowledge', 1);('classical method', 1);('cumbersome models', 1);('original knowledge distillation frame- work adopts', 1);('classication logits', 1);('extensive variants', 1);('ef- fectiveness [', 1);('relations [', 1);('data augmentations', 1);('students [', 1);('regu- larization [', 1);('strategies [', 1);('architec- tures', 1);('contrastive learning', 1);('meth- ods [', 1);('cnnarchitecture', 1);('inductive bias', 1);('hard pseudo class labels', 1);('naive knowledge distillation [', 1);('distillation method', 1);('map distillation', 1);('approaches in-', 1);('instance contrastive', 1);('learning [', 1);('rst time studies', 1);('distillation frame- work', 1);('signicant gains', 1);('various sizes', 1);('small vision transformers designing', 1);('recent years', 1);('works study-', 1);('efcient vision', 1);('majority focus', 1);('archi- tectures [', 1);('sophisticated components', 1);('plain vision', 1);('small scale', 1);('main insight', 1);('necessary inductive biases', 1);('explicit architecture bias', 1);('plain 3vision', 1);('state-of-the-art accuracy', 1);('knowledge distillation manner', 1);('rst describe', 1);('section 3.1.2', 1);('distillation losses', 1);('different distillation target', 1);('sequential distillation strat- egy', 1);('factors', 1);('distillation target block feature', 1);('nnon-overlapping', 1);('npatches', 1);('patch em- beddingsf02rn\x02d', 1);('suppose', 1);('ltransformer', 1);('fi\x001of', 1);('fiof', 1);('current block', 1);('fi= transformer', 1);('i2 [', 1);('l ]', 1);('flfrom', 1);('self-attention layer', 1);('feed for- ward layer', 1);('hi= attention', 1);('fi=bhi+ehi', 1);('denotes self- attention layer', 1);('term bhiandehias attention', 1);('query/key/value features', 1);('self-attention layer con- sists ofmhead networks', 1);('maps input featurefi\x001to query', 1);('wq', 1);('km', 1);('wk', 1);('vm', 1);('wv', 1);('vi2rn\x02d mrepresent', 1);('m-th head network', 1);('query/key/value fea- tures', 1);('qi', 1);('vi2rn\x02d', 1);('i/km i/vm i', 1);('them-th head network', 1);('product relation', 1);('iqm it p', 1);('softmax km', 1);('softmax vm', 1);('ivm it p', 1);('q-q/k-k/v-v/q-k', 1);('stack ofm', 1);('m/rkk i', 1);('m/rqk i', 1);('input mim', 1);('high proportion', 1);('image patches', 1);('raw image xor', 1);('target block consider', 1);('appropriate target', 1);('mim pre-training', 1);('distillation target candidates', 1);('different knowledge distillation losses', 1);('various distillation targets', 1);('xde- note', 1);('input image', 1);('teacher raw image', 1);('headblock', 1);('-n student', 1);('loss', 1);('forward', 1);('backwardfigure', 1);('default knowledge distillation strategy', 1);('intermediate block', 1);('teachers head number', 1);('extra computational cost', 1);('fttofsby optimizingfswhile', 1);('kl', 1);('lkl', 1);('=tlogt p', 1);('l=lkl', 1);('extra linear layer', 1);('teachers target', 1);('ftandfsdenote', 1);('l=l1', 1);('fs', 1);('ft', 1);('l1is', 1);('default knowledge distilla- tion strategy', 1);('userqk t', 1);('m-th head', 1);('rela- tion target', 1);('relation prediction', 1);('similar way', 1);('lqk=1 mmx', 1);('lv v=1 mmx', 1);('s t', 1);('l=lqk+lv v', 1);('recall', 1);('relation distillation loss', 1);('head misalignment issue', 1);('concretely', 1);('msheads', 1);('ds', 1);('ds=ms', 1);('dsand', 1);('ds=mt', 1);('vit- l. intuitively', 1);('good teacher', 1);('representation capability', 1);('huge capacity gap', 1);('poor distillation results', 1);('following', 1);('experiments', 1);('details pre-training', 1);('100- epoch schedule', 1);('] training', 1);('lr=1.5e- 4\x02batchsize=256', 1);('cosine decay schedule', 1);('] optimizer', 1);('random horizontal', 1);('color jitter', 1);('input size', 1);('] image classication', 1);('] se- mantic segmentation', 1);('op- timizer', 1);('upernet', 1);('input image resolution', 1);('evaluation metric', 1);('var- ious out-of-domain', 1);('datasets [', 1);('different perturbations', 1);('common image corruptions', 1);('imagenet-a/r', 1);('mce error', 1);('imagenet-c', 1);('default setting', 1);('se- quential distillation', 1);('previous methods', 1);('% top-', 1);('fur- ther', 1);('b surpasses', 1);('+4.1 and+2.0', 1);('+6.4 and+4.6', 1);('unless', 1);('ablation studies', 1);('relation distillation strategy', 1);('re- construction loss', 1);('explicit supervision', 1);('q/k/v', 1);('com- pares', 1);('default relation distillation', 1);('re- lation', 1);('target relations', 1);('q-k/v-v', 1);('q-k/k-k/v-v', 1);('softmax', 1);('different distillation strategies', 1);('tokenizer/ tokenizer/teacher classication segmentation epochs teacher data top-1 acc', 1);('tiny-size', 1);('vit-t/16', 1);('small-size', 1);('vit-s/16', 1);('dino', 1);('tinymim-vit-b in1k', 1);('base-size', 1);('vit-b/16', 1);('sim', 1);('hog in1k', 1);('mae-vit-l in1k', 1);('tokenizer/teacher data', 1);('training data', 1);('ofcial code.', 1);('model adopts', 1);('method model size imagenet', 1);('in-adversarial', 1);('in-rendition', 1);('in-corruption', 1);('vit-t72.2', 1);('vit-s79.9', 1);('vit-b81.2', 1);('robustness', 1);('target blocks', 1);('7method reconstruction', 1);('mae x', 1);('cls x', 1);('feature res', 1);('ffn feature x', 1);('attention feature x', 1);('q/k/v features', 1);('eq.7', 1);('relation softmax top-1 acc', 1);('different relations', 1);('thblock yields', 1);('enable distillation', 1);('different teachers', 1);('integrating mae', 1);('ef- fective', 1);('knowledge distilla- tion manner', 1);('knowledge distillation loss', 1);('model size top-1 acc', 1);('vit-t72.2 mae', 1);('vit-s79.9 mae', 1);('vit-b81.2 mae', 1);('method model size', 1);('vit-b47.2 mae', 1);('300- epoch schedule', 1);('classication', 1);('segmentation', 1);('teacher acc', 1);('vit-smae-vit-b', 1);('vit-tmae-vit-s', 1);('x x', 1);('reconstruction loss', 1);('% image patches', 1);('visi- ble patches', 1);('comparison be- tween', 1);('addi- tion', 1);('input yields', 1);('drop', 1);('critical techniques', 1);('appropriate drop path rate', 1);('optimal drop path rate', 1);('drop path yields', 1);('conclusion', 1);('mask-and-predict pretext task', 1);('knowledge dis- tillation manner', 1);('comprehensive study', 1);('various factors', 1);('distillation input', 1);('extensive experiments', 1);('simplic- ity', 1);('strong performance', 1);('solid baseline', 1);('future research', 1);('references', 1);('alexei baevski', 1);('wei-ning hsu', 1);('qiantong xu', 1);('arun babu', 1);('jiatao gu', 1);('michael auli', 1);('data2vec', 1);('general framework', 1);('arxiv preprint arxiv:2202.03555', 1);('songhao piao', 1);('image transformers', 1);('mathilde caron', 1);('ishan misra', 1);('julien mairal', 1);('piotr bojanowski', 1);('emerg-', 1);('arxiv preprint arxiv:2104.14294', 1);('xiaokang chen', 1);('mingyu ding', 1);('xiaodi wang', 1);('ying xin', 1);('shen-', 1);('mo', 1);('yunhao wang', 1);('shumin han', 1);('gang zeng', 1);('jingdong wang', 1);('context', 1);('representation learning', 1);('arxiv preprint arxiv:2202.03026', 1);('empirical study', 1);('yinpeng chen', 1);('xiyang dai', 1);('mengchen liu', 1);('mobile-', 1);('bridging', 1);('yabo chen', 1);('yuchen liu', 1);('dongsheng jiang', 1);('xiaopeng zhang', 1);('wenrui dai', 1);('hongkai xiong', 1);('qi tian', 1);('jang hyun cho', 1);('bharath hariharan', 1);('navneet dalal', 1);('bill triggs', 1);('histograms', 1);('gra- dients', 1);('human detection', 1);('computer soci- ety conference', 1);('cvpr05', 1);('jacob devlin', 1);('ming-wei chang', 1);('kenton lee', 1);('kristina toutanova', 1);('deep bidirectional trans- formers', 1);('language understanding', 1);('american chapter', 1);('as-', 1);('computational linguistics', 1);('language technologies', 1);('computa-', 1);('linguistics', 1);('ting zhang', 1);('weiming zhang', 1);('fang wen', 1);('neng-', 1);('yu', 1);('perceptual', 1);('arxiv preprint arxiv:2111.12710', 1);('alexey dosovitskiy', 1);('lucas beyer', 1);('alexander kolesnikov', 1);('dirk weissenborn', 1);('xiaohua zhai', 1);('thomas unterthiner', 1);('dehghani', 1);('matthias minderer', 1);('georg heigold', 1);('syl-', 1);('gelly', 1);('image recognition', 1);('preprint arxiv:2010.11929', 1);('yuxin fang', 1);('xinggang wang', 1);('corrupted', 1);('arxiv preprint arxiv:2202.03382', 1);('zhiyuan fang', 1);('jianfeng wang', 1);('lijuan wang', 1);('lei zhang', 1);('yezhou yang', 1);('seed', 1);('self-supervised', 1);('visual representation', 1);('tommaso furlanello', 1);('zachary lipton', 1);('michael tschannen', 1);('laurent itti', 1);('anima anandkumar', 1);('born', 1);('neural networks', 1);('jianping gou', 1);('baosheng yu', 1);('stephen j. maybank', 1);('international journal', 1);('benjamin graham', 1);('alaaeldin el-nouby', 1);('pierre', 1);('levit', 1);('convnets clothing', 1);('yanghao li', 1);('piotr', 1);('ross girshick', 1);('scalable vision learners', 1);('norman mu', 1);('saurav kada-', 1);('frank wang', 1);('evan dorundo', 1);('rahul desai', 1);('tyler zhu', 1);('samyak parajuli', 1);('mike guo', 1);('justin gilmer', 1);('critical analysis', 1);('out-of-distribution generalization', 1);('iccv', 1);('thomas dietterich', 1);('benchmarking', 1);('neural network robustness', 1);('common corruptions', 1);('iclr', 1);('kevin zhao', 1);('jeesoo kim', 1);('hyojin', 1);('no-', 1);('kwak', 1);('comprehensive overhaul', 1);('ieee/cvf inter-', 1);('minsik lee', 1);('activation bound- aries', 1);('geoffrey hinton', 1);('oriol vinyals', 1);('jeff dean', 1);('distill-', 1);('neural network', 1);('arxiv preprint arxiv:1503.02531', 1);('zejiang hou', 1);('fei sun', 1);('yen-kuang chen', 1);('yuan xie', 1);('s. y', 1);('kung', 1);('andrew howard', 1);('mark sandler', 1);('grace chu', 1);('liang-chieh chen', 1);('yukun zhu', 1);('ruoming pang', 1);('vijay vasudevan', 1);('searching', 1);('mo- bilenetv3', 1);('andrew g howard', 1);('menglong zhu', 1);('dmitry kalenichenko', 1);('tobias weyand', 1);('marco an-', 1);('hartwig adam', 1);('mobilenets', 1);('efcient', 1);('convolu- tional neural networks', 1);('mobile vision applications', 1);('arxiv preprint arxiv:1704.04861', 1);('jangho kim', 1);('seonguk', 1);('nojun kwak', 1);('paraphrasing', 1);('complex network', 1);('network', 1);('seung hyun lee', 1);('dae ha kim', 1);('byung cheol', 1);('singular value de- composition', 1);('eccv', 1);('yanyu li', 1);('geng yuan', 1);('yang wen', 1);('eric hu', 1);('georgios evan-', 1);('sergey tulyakov', 1);('yanzhi wang', 1);('jian ren', 1);('ef-', 1);('vision', 1);('mobilenet speed', 1);('arxiv preprint arxiv:2206.01191', 1);('jia ning', 1);('transformer v2', 1);('scaling', 1);('stephen lin', 1);('swin transformer', 1);('hierarchical', 1);('arxiv preprint arxiv:2103.14030', 1);('ilya loshchilov', 1);('frank hutter', 1);('decoupled', 1);('weight de- cay regularization', 1);('sachin mehta', 1);('mohammad rastegari', 1);('mobilevit', 1);('light-', 1);('vision trans-', 1);('learning representa-', 1);('junting pan', 1);('adrian bulat', 1);('fuwen tan', 1);('xiatian zhu', 1);('lukasz dudziak', 1);('hongsheng li', 1);('georgios tzimiropoulos', 1);('brais martinez', 1);('edgevits', 1);('competing', 1);('light-weight cnns', 1);('mobile devices', 1);('springer', 1);('zhiliang peng', 1);('qixiang ye', 1);('visual tokenizers', 1);('arxiv preprint arxiv:2208.06366', 1);('jong wook kim', 1);('chris hallacy', 1);('aditya ramesh', 1);('sandhini agarwal', 1);('girish sastry', 1);('amanda askell', 1);('pamela mishkin', 1);('jack clark', 1);('transferable visual models', 1);('natural language supervision', 1);('inicml', 1);('a. ramesh', 1);('mikhail pavlov', 1);('scott', 1);('chelsea v', 1);('mark chen', 1);('ilya sutskever', 1);('zero-', 1);('shot text-to-image generation', 1);('tianyu hua', 1);('yong-', 1);('tian', 1);('shengfeng', 1);('co-advise', 1);('cross inductive bias distillation', 1);('june', 1);('adriana romero', 1);('nicolas ballas', 1);('samira ebrahimi kahou', 1);('antoine chassang', 1);('carlo gatta', 1);('yoshua bengio', 1);('fitnets', 1);('hints', 1);('thin deep nets', 1);('arxiv preprint arxiv:1412.6550', 1);('olga russakovsky', 1);('jia deng', 1);('hao su', 1);('jonathan krause', 1);('san-', 1);('satheesh', 1);('sean ma', 1);('zhiheng huang', 1);('andrej karpathy', 1);('aditya khosla', 1);('michael bernstein', 1);('alexander', 1);('berg', 1);('li fei-fei', 1);('large scale visual recognition challenge', 1);('ijcv', 1);('quoc', 1);('efcientnet', 1);('rethinking', 1);('convolutional neural networks', 1);('chenxin tao', 1);('xizhou zhu', 1);('gao huang', 1);('yu qiao', 1);('xiaogang wang', 1);('jifeng dai', 1);('siamese', 1);('vision representation learning', 1);('arxiv preprint arxiv:2206.01204', 1);('matthieu cord', 1);('francisco massa', 1);('alexandre sablayrolles', 1);('training', 1);('data-efcient image transformers', 1);('preprint arxiv:2012.12877', 1);('shakti n wadekar', 1);('abhishek chaurasia', 1);('mobilevitv3', 1);('mobile-friendly', 1);('effective fusion', 1);('arxiv preprint arxiv:2209.15159', 1);('wenhai wang', 1);('enze xie', 1);('xiang li', 1);('deng-ping fan', 1);('kaitao', 1);('ding liang', 1);('tong lu', 1);('ling shao', 1);('pyra-', 1);('mid vision transformer', 1);('versatile backbone', 1);('dense predic- tion', 1);('xiaojie wang', 1);('rui zhang', 1);('yu sun', 1);('jianzhong qi', 1);('kdgan', 1);('generative adversarial networks', 1);('haoqi fan', 1);('chao-yuan wu', 1);('christoph feichtenhofer', 1);('predic- tion', 1);('arxiv preprint arxiv:2112.09133', 1);('yingcheng liu', 1);('yuning jiang', 1);('jian sun', 1);('unied', 1);('scene understanding', 1);('ineccv', 1);('zigang geng', 1);('jingcheng hu', 1);('revealing', 1);('dark secrets', 1);('arxiv preprint arxiv:2205.13543', 1);('arxiv preprint arxiv:2206.04664', 1);('multimodal', 1);('knowledge expansion', 1);('junho yim', 1);('donggyu joo', 1);('jihoon bae', 1);('junmo kim', 1);('fast', 1);('network minimization', 1);('learn-', 1);('multiple teacher networks', 1);('acm sigkdd', 1);('knowledge discovery', 1);('data mining', 1);('single-teacher multi-student', 1);('xavier puig', 1);('sanja fidler', 1);('adela barriuso', 1);('antonio torralba', 1);('semantic', 1);('int', 1);('j. comput', 1);('vis.', 1);('jinghao zhou', 1);('huiyu wang', 1);('wei shen', 1);('cihang xie', 1);('tao kong', 1);('online tokenizer', 1);('arxiv preprint arxiv:2111.07832', 1);('hyper-parameters hyper-parameters', 1);('imagenet-1k pre-training', 1);('ta-', 1);('imagenet-1k image classication fine-tuning', 1);('following mobilenetv3', 1);('classi- cation layer', 1);('ade20k semantic segmentation fine-tuning', 1);('hyperparameter vit-t vit-s vit-b layers', 1);('hidden', 1);('patch', 1);('pre-training', 1);('learning rate 2.4e-3', 1);('learning rate 1e-5', 1);('cosine warmup', 1);('randomresizeandcrop input', 1);('color', 1);('imagenet-1k pre-training.hyperparameter vit-t vit-s vit-b peak', 1);('learning rate 5e-3 5e-3 2e-3', 1);('warmup', 1);('learning rate decay', 1);('learning rate 1e-6', 1);('cosine stochastic', 1);('label', 1);('gradient', 1);('erasing', 1);('input', 1);('rand', 1);('mixup', 1);('cutmix', 1);('image classication ne-tuning.\x03indicates', 1);('hyperparameter vit-s vit-b input', 1);('learning rate 1e-4', 1);('steps 160k', 1);('learning rate decay f0.65', 1);('linear warmup', 1);