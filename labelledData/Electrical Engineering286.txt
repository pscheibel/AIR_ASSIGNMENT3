('p1', 72);('p2', 72);('p1s', 41);('p2s', 36);('asw', 12);('mdp', 12);('p1 p2', 7);('mc', 6);('nash', 5);('markov', 5);('perceptual game', 5);('private actions', 4);('figure', 4);('p2s bsr', 4);('true game', 4);('strong opponent', 4);('quantitative', 3);('action deception', 3);('complete information', 3);('june', 3);('theory mind', 3);('yellow cell', 3);('switches strategy', 3);('proceedings', 3);('specifically', 2);('proc', 2);('international conference', 2);('international foundation', 2);('hypergame', 2);('incomplete information', 2);('tom p2', 2);('perfect observations', 2);('prefplays', 2);('reachability objective', 2);('hisher almostsure', 2);('deceptionwe', 2);('information structure', 2);('soccer game', 2);('detect deviation', 2);('response game', 2);('p1sbest', 2);('alternative hypothesis', 2);('boolean', 2);('if1', 2);('uses heat maps', 2);('across', 2);('vod', 2);('p2 fig', 2);('cham', 2);('learning', 2);('artificial intelligence', 2);('deception', 2);('infinite', 2);('july', 2);('deception concurrentstochastic gameschongyang shiuniversity floridagainesville', 1);('statescshiufledushuo hanuniversity illinois chicagochicago', 1);('stateshanshuouicedujie fuuniversity floridagainesville', 1);('statesfujieufleduabstractwe', 1);('study class twoplayer noncooperative concurrent stochastic games graphs reachability objectives', 1);('player1 aims', 1);('subset 1of game states player', 1);('aims toreach subset 2of game states', 1);('reachability objectives opponentdoes', 1);('information players game dynamicsis asymmetric', 1);('p2at', 1);('beginning interaction setup investigatep1s', 1);('game model employa', 1);('maximize value action deception', 1);('additional payoff', 1);('payoff thegame', 1);('anticipating p2', 1);('maydetect misperception game adapt', 1);('unpredictable ways construct planningproblem', 1);('game model incompletemodel theory mind opponent', 1);('perceptionso entice', 1);('actions benefit', 1);('deceptive planning algorithm maximizes', 1);('boundon value action deception', 1);('effectivenessof deceptive planning algorithm', 1);('robot motion planningproblem', 1);('soccer gameskeywordshypergames', 1);('deception theory mind markov decision processesacm reference formatchongyang shi shuo han jie fu', 1);('deception concurrent stochastic', 1);('22ndinternational conference', 1);('autonomous agents multiagent systemsaamas', 1);('london', 1);('kingdom may', 1);('ifaamas9', 1);('introductionasymmetrical', 1);('information players', 1);('conflict analysis security applications 6142224for adversarial interactions player leverage asymmetricinformation hisher opponents disinformation gain', 1);('additional benefits', 1);('hisher objective', 1);('deception 10proc 22nd', 1);('autonomous agents multiagent systems aamas', 1);('ricci', 1);('yeoh n agmon', 1);('b eds', 1);('may', 1);('kingdom', 1);('autonomous agentsand multiagent systems', 1);('wwwifaamasorg rights reservedin general deception techniques', 1);('intention deception mark misinformation disinformation intention deceiver', 1);('ingame', 1);('theory intention deception', 1);('payoff misperception players', 1);('different perceptions payoffmatrix game', 1);('literature focus class ofdeception', 1);('applications cyber defense deception', 1);('motion planning', 1);('class capability deception mark hasincomplete knowledge deceivers action perceptioncapabilitiesfor', 1);('interactions asymmetric informationbayesian games', 1);('bayesian', 1);('game solution approach transform gamewith', 1);('incomplete information game imperfect information', 1);('incomplete information type', 1);('variable whichis', 1);('observable players', 1);('deceptionsettings type space ie hypothesis space opponentsintention capabilities', 1);('public knowledge', 1);('onthe hand construct hierarchy perceptual games thatcaptures hierarchical information player', 1);('aboutthe game', 1);('playersince hypergame model', 1);('unawarenessof players work', 1);('hypergame model analyzeaction deception class twoplayer concurrent stochasticgames player reachability objective representedby', 1);('goal states', 1);('game startsplayer', 1);('additionally p1', 1);('deploy action deception', 1);('action capitalize gain', 1);('p2ssuboptimal', 1);('inliterature', 1);('twoplayer reachability games', 1);('case players symmetricand', 1);('solutions ofreachability games', 1);('games 111action deception reachability games', 1);('18the authors', 1);('perfect observations thedeceiver strategy', 1);('goal states probabilityone', 1);('private actions startingfrom state objective', 1);('solution isqualitative', 1);('games whereas', 1);('gainof action deception quantitative planning concurrent gameswhen', 1);('objective probability', 1);('hisobjectivearxiv230101349v1 eesssy', 1);('jan', 1);('2023a key observation quantitative planning', 1);('knowthere mismatch game', 1);('true gameby', 1);('deviation gameplay', 1);('formulateaction deception', 1);('response 21in game', 1);('p2sperceptual', 1);('response 1in truegame', 1);('theoptimal timing switch 21to1to maximize value ofaction deception', 1);('p2 p1s', 1);('payoff equilibrium thetrue game', 1);('action setwe', 1);('deceptive planning algorithm incorporates atheory mind', 1);('changedetection mechanism', 1);('time delay1that', 1);('reaction deviation', 1);('strategy inreaction', 1);('action deception employ solution conceptofsubjective', 1);('rationalizable strategies', 1);('perception gamehowever', 1);('detects mismatch hasnot', 1);('value actiondeception', 1);('optimal gain completionsof', 1);('additional state variables track', 1);('tom p2we', 1);('optimal solution', 1);('value action', 1);('preliminaries problemformulationnotations letrdenote', 1);('real numbers', 1);('rthe', 1);('setof realvectors', 1);('probability distributions overis', 1);('givend', 1);('supp021 concurrent stochastic', 1);('objectiveswe', 1);('standard model twoplayer stochasticgames', 1);('oftwo components game graph', 1);('players interactingdynamics pair players intentionsobjectives', 1);('asreachability properties player', 1);('goal statesto', 1);('pronoun hehimhis andplayer', 1);('pronoun sheherhersdefinition', 1);('concurrent', 1);('stochastic games graphs reachability objectives twoplayer concurrent stochastic game agraph tuple', 1);('componentsis finite', 1);('states12is finite', 1);('actions 1is', 1);('actions thatp2 performdis probabilistic transition function', 1);('atevery', 1);('chooses action', 1);('p2chooses', 1);('probability distribution 0is', 1);('initial state01is', 1);('target states 21is', 1);('p2starget', 1);('states reachability objective target', 1);('means player aims', 1);('game concurrent reachabilitygame play game', 1);('initial game state', 1);('select pair ofactions probability', 1);('state1 repeat game ends', 1);('players satisfieshisher objective', 1);('sequence states actions000111such that10for any0 prefix play finite', 1);('initial segment stateactionsequence', 1);('possible plays game', 1);('prefixes plays', 1);('reachability objective target', 1);('expressedby temporal logic formula', 1);('symbolis temporal operator eventuality1a play000111is', 1);('letbe', 1);('theth state play state', 1);('satisfyingthe formula', 1);('plays0', 1);('shownto measurablea', 1);('prefplaysd', 1);('player12is function assigns probability distribution actionsgiven prefix play', 1);('strategy spaceof player strategy profile 12is pair strategies onefor player strategy profile 12induces probabilitymeasure', 1);('pr12over prefplays', 1);('game playercan ensure matter opponent plays state willbe', 1);('region strategy followsdefinition', 1);('almostsure', 1);('strategyregion 8a strategy1is almostsure', 1);('pr1211for', 1);('pr12', 1);('isthe probability measure paths', 1);('thestrategy profile12', 1);('region player definedbyasw1122pr121which', 1);('exists almostsure', 1);('reachability objectives', 1);('formula simplify somenotations omit preliminaries temporal logic', 1);('region strategy', 1);('regions strategies concurrent stochastic games reachability objectivescan', 1);('game memoryless determinedlemma', 1);('8in concurrent reachability game', 1);('exists memoryless almostsure', 1);('strategy forp1', 1);('state player almostsure winningregion player', 1);('strategy whereas player chance', 1);('that12for state', 1);('players apositive probability', 1);('respective target', 1);('computea strategy state', 1);('utility function concept', 1);('utility', 1);('functions utility function player', 1);('rsuch', 1);('1221e01110where 01is', 1);('value 12is stochastic process inducedby strategy profile concurrent reachabilitygame andris reward function definedas1ifaswandaswand0otherwisein words state', 1);('utility player statemeasures', 1);('hisher almostsure winningregion', 1);('aswfrom', 1);('definition', 1);('equilibrium 11a', 1);('equilibrium astochastic game strategy profile 12with propertythat 1221we havefor', 1);('concurrent reachability game players almostsure', 1);('methods 8the ne', 1);('solutions zerosum', 1);('problem formulation', 1);('asymmetric information game whichenables', 1);('deceptive planninginformation', 1);('structure', 1);('player observes hisher interaction opponent', 1);('thecomponents game', 1);('information structure isconsideredboth', 1);('complete observations statesp2', 1);('actionsp1s action', 1);('proper subsetof1 ie', 1);('p2may', 1);('able observe', 1);('action deceptionhere', 1);('informal problem statementproblem', 1);('p1 p2how p1', 1);('lack information', 1);('actions forstrategic advantages23', 1);('illustrative example soccer gamewe', 1);('theabove concepts game field 35grid twoplayers', 1);('game b', 1);('figures', 1);('theball', 1);('star players', 1);('players canfigure', 1);('p1 p2 bfigure', 1);('target statesmove', 1);('right black arrows', 1);('toeach player', 1);('thetrue action space', 1);('notable rules soccer game1the players', 1);('select actionto', 1);('cell aprobability', 1);('players aim', 1);('ball opponents gatethat', 1);('blue circles', 1);('red circles', 1);('position eachplayer tuplewhereis players row theplayers column', 1);('1be position', 1);('2be positionof', 1);('ball01', 1);('way wecan', 1);('state soccer game 12ball', 1);('thereforep1s', 1);('142ballfor anyand2', 1);('targetset210ballfor anyand1at', 1);('thehidden action', 1);('willupdate knowledge', 1);('planning action deceptionin', 1);('section introduce', 1);('main algorithm action deception planning31', 1);('hypergame modeling value', 1);('introduce hypergame model characterize players interaction', 1);('respective information higherorder information', 1);('incomplete informationfirst', 1);('due incomplete knowledge', 1);('p1sactions p2s', 1);('game graph', 1);('incomplete incomplete view', 1);('construct perceptual game', 1);('p2definition', 1);('actions game graph', 1);('perceptual gameis', 1);('12where transitionfunction2is', 1);('byp1s actions 121and', 1);('formally21212if121', 1);('otherwise1wheremeans function', 1);('perceptual game player', 1);('asw21asw22together', 1);('almostsure winning asw', 1);('response strategies theopponent', 1);('perceptual gameto', 1);('asymmetric information', 1);('hypergame model', 1);('game setupdefinition', 1);('herein interaction', 1);('bythe hypergame212where12is', 1);('perceptual game level1hypergame game 2is', 1);('perceptual gamein level2 hypergame', 1);('true game andp2s perceptual game', 1);('notion action deception deceiver', 1);('hisactions mark time deviates themarks perceptual game example', 1);('strategy thatuses', 1);('action formalize deceptive planning todetermine deviate clarity notations specifiedin', 1);('table 1further', 1);('deceptive strategy', 1);('classof strategiesne', 1);('strategies ne in2asw strategies', 1);('notations', 1);('players strategiesdefinition', 1);('onetime', 1);('strategy function swprefplaysd121that assigns fora historyprefplays probability distribution twobest responses 21for', 1);('game 2and1for', 1);('theswitching', 1);('onetime satisfies', 1);('plays', 1);('point thatsw0 21for allandsw1 1forall1it', 1);('action immediatelyupon', 1);('response 1may notemploy', 1);('action till', 1);('response 21for', 1);('p1it', 1);('detect mismatch', 1);('game playfrom perceptual game', 1);('albeit delay', 1);('p1sdeceptive', 1);('incorporate theory mind', 1);('areasonable detection mechanism', 1);('time1 for10', 1);('p2shall', 1);('bsr23', 1);('p1s incomplete model p2s behaviorally subjectively', 1);('strategy p1s', 1);('incomplete model', 1);('rationalizable strategy', 1);('bsr p2', 1);('function2prefplaysd2 function 2is', 1);('history 0001112prefplaysof length let0be history', 1);('thebest response 21in2and1be history', 1);('p1follows', 1);('response 1in', 1);('true game let1be timestep', 1);('detects deviation 2be time step whenp2 learns', 1);('true action', 1);('response thegame2for all11220that', 1);('duringthis time span', 1);('strategy undefinedand all12202 ifasw 2asw2otherwisethat', 1);('cannotpredict strategy', 1);('detects gameshe', 1);('true gameis case', 1);('actions possiblethat', 1);('true game dynamics', 1);('partialobservations model', 1);('subjectiverationalizable strategy perceptual game', 1);('actions advantageto deviate subjective', 1);('rationalizable strategyacompletion', 1);('if2is definedthen', 1);('2d2can arbitrarydistribution', 1);('possiblecompletions for2 notion', 1);('value ofactiondeception followsdefinition', 1);('value action deception', 1);('strategy swof', 1);('thevalue action deception', 1);('initial state 0asw 1asw 2ie', 1);('p1p2vodswmin2210sw21012and', 1);('strategy swis thatswarg maxswsw1vodswwhere sw1is', 1);('p1can', 1);('select optimal value action deception', 1);('vodswby', 1);('definition value action deception', 1);('gain payoff', 1);('action deceptionthan', 1);('truegame dynamics', 1);('game mismatch', 1);('computation value actiondeception considers', 1);('case completion incompletemodel', 1);('variation soccer game illustratethe', 1);('different key events', 1);('onetime switchingstrategy1p1 switches strategy 21to12p2 detects', 1);('deviates equilibrium', 1);('actionconsider arena', 1);('blue cells', 1);('action forp1', 1);('perceptual gamethat', 1);('wall target states', 1);('arethe soccer game', 1);('subsection', 1);('p1 p2s', 1);('subjectiverationalizable strategy 2will', 1);('cellwhere intercept', 1);('top rowin meantime optimal strategy', 1);('thebottom corridor', 1);('switch strategy 1as game', 1);('showsthat event', 1);('occurs event', 1);('possible delay thisexample events', 1);('time general', 1);('inherent delay', 1);('soccer', 1);('detection mechanismto', 1);('complete theory mind', 1);('incorporate changedetection mechanism predicts detection time 1assumption', 1);('response 21in', 1);('perceptual game 2given', 1);('knowledge actionsequence01', 1);('null hypothesis', 1);('initial game state transition functionis', 1);('switch strategy', 1);('true game construct alternativehypothesis', 1);('10with theinitial state 00and transition', 1);('employpages likelihood ratio', 1);('cusum', 1);('point detection', 1);('1for remaining1 time steps knowledge', 1);('discrimination function', 1);('observation time 0max1hlogpr111pr011ithe update discrimination function', 1);('incrementalas follows0111maxn01logpr111pr0110oremark', 1);('action sequence 01then need construct', 1);('1pr01221for01the chain', 1);('analogouslythe assumption', 1);('knowledge informedopponentnext show', 1);('deceptive planning strategy againstsuch', 1);('deceptive planning', 1);('actual gameour formulation', 1);('semimarkov decision process', 1);('policies ratherthan primitive actions semi', 1);('asa oneplayer stochastic game', 1);('decision thenthe nature player determines stochastic outcome way wecan', 1);('natures choice affects theory mindof', 1);('p1definition', 1);('onetime switch action deception planning action deception', 1);('asemimarkov decision process1211where1b01is state', 1);('adecision state bincludes state theoriginal game', 1);('real number', 1);('booleanb01 hererepresents', 1);('value discriminationfunction', 1);('bkeeps track ofwhether', 1);('equilibrium 2b0112is state', 1);('naturedetermines probabilistic outcome211are', 1);('macroactions policies', 1);('p1is', 1);('probabilistic transition function', 1);('p1and', 1);('nature players statesfirst natures state b12 ifasw 1asw 2almostsure', 1);('player thetrue game probability', 1);('ie b12sink1whereis null action', 1);('natures probabilisticchoicesecond', 1);('state b', 1);('chooses strategy', 1);('then021012211222at state012 nature determines probabilistic outcome012012case', 1);('p1switched', 1);('strategies state', 1);('nature determines probabilistic outcome112112whereobs12is', 1);('value forthe discrimination function', 1);('observation thetransitioncase', 1);('macroaction isto', 1);('consider', 1);('state1 then1111211222note', 1);('ne perceptual game 2given state112 nature', 1);('then112112whereobs12otherwise 112sink11ris', 1);('reward function any10', 1);('1ifasw 11ifasw 2112ifasw 1asw 20otherwisein planning problem process terminates', 1);('ofthe players', 1);('regions thetrue game', 1);('detects deviation', 1);('response fromhis perceptual game semi', 1);('given2can', 1);('actions 121and', 1);('p2sactions', 1);('2asw22the proof', 1);('computation almostsure winningregions', 1);('concurrent reachability game', 1);('asw22asw', 1);('statement proventheorem', 1);('letswbe', 1);('optimal policy semi', 1);('thatvodswvodswwhereswis optimal', 1);('strategy actiondeception', 1);('def', 1);('35proof state sequence terminates', 1);('thesemimdp say00b000b00011b1bsinkwe identify', 1);('unique play', 1);('previous analysis', 1);('bep1s strategy', 1);('time 1be time', 1);('detects adeviation 12be time', 1);('learns truegame', 1);('cases possiblecase', 1);('p1sdeviation', 1);('case thatasw', 1);('strategy 2asw2 strategy suboptimal inthe', 1);('payoff 1is', 1);('theactual payoff', 1);('suboptimal strategymay', 1);('possibility game states', 1);('thisis', 1);('policy almostsure', 1);('asw22may', 1);('qualitative analysis ofreachability objectives 8case', 1);('deviationfrom21at time step', 1);('payoffof equilibrium', 1);('reward alower', 1);('assumes detection', 1);('assumption ignores thepossible delay', 1);('true game detectionfor strategy 2that', 1);('detection wehave that212212and thus112112because12is equilibrium zerosumgamegiven cases reward', 1);('actual payoff', 1);('bsr', 1);('addition process', 1);('thenthe reward', 1);('zero rewardof', 1);('p1 bsr', 1);('p2thus', 1);('optimal value action deception', 1);('bythe value optimal policy semimdp', 1);('remark', 1);('onetime switchingwith', 1);('usesprimitive actions', 1);('detection sincethe', 1);('interaction case', 1);('employ thechange detection', 1);('case onehidden action', 1);('multiplehidden actions4', 1);('experimentswe', 1);('soccer game example', 1);('example', 1);('mdp def', 1);('hybridstate space range discrimination function iscontinuous employ', 1);('statespace 0intointervals112 whereis length interval', 1);('fora', 1);('discrimination function value', 1);('label adiscrimination function value level label update discrimination function value midpoint ofthe interval', 1);('level instance', 1);('current levelis update 212obs12 theexperiments', 1);('hence', 1);('10levels with0and level41', 1);('value deception comparativeanalysiswe', 1);('show value action deception', 1);('differencesbetween state values', 1);('different strategies', 1);('initial statesie state', 1);('value action deceptionvodswgiven optimal', 1);('strategy swobtainedby', 1);('results clearer plot thefigure', 1);('ie range', 1);('vodswis', 1);('to0100instead of01 employ value iterationto', 1);('bellman', 1);('error isbelow 01with respect', 1);('reward figurefigure', 1);('vodswgiven', 1);('possible initial positionswe observe', 1);('benefit action deception whenp2 ball', 1);('allinitial states maximal', 1);('72289and minimal', 1);('is0 words', 1);('maximum gain', 1);('probability forp1', 1);('70for states', 1);('initial state wherevodswis', 1);('deviate andinstead', 1);('true gameto gain insight', 1);('showsa snapshot game state', 1);('thisfigure', 1);('gamestateasw22 ie almostsure', 1);('p2thinks', 1);('howeverif p2', 1);('thatp1 intercept ball probability', 1);('hiddenaction is221and20at currentstate', 1);('p1uses', 1);('increase hischance', 1);('delay detection', 1);('p1we', 1);('realisticchange detection semi', 1);('p2has', 1);('strategy sofor', 1);('p2figure', 1);('differences state valueie10sw2210so22for', 1);('different initial states', 1);('notethat', 1);('difference equals difference value deception', 1);('value deception againstthe', 1);('initial states maximal statevalue difference 46564and minimal state value difference is0', 1);('initial states', 1);('valuesof deception consistent case', 1);('resultalso highlights importance', 1);('detection delay42', 1);('sensitivity analysis detection thresholdin', 1);('construction semi', 1);('threshold ofthe', 1);('detection algorithm', 1);('possible thetrue detection threshold', 1);('different conductexperiments assess', 1);('effective deceptive strategies areagainst uncertainty detection thresholdthe', 1);('ourprevious', 1);('sensitive valuenext', 1);('different values', 1);('respectivelyfor eachvalue construct', 1);('corresponding semi', 1);('different semi', 1);('inthis', 1);('strategy performs', 1);('p2employs', 1);('detection threshold 15812while', 1);('p2sdetection', 1);('threshold 2the value swinis denoted1for15812', 1);('value swin', 1);('original semi', 1);('mdpgiven2is', 1);('maximum values 210sw10sw each15812figure', 1);('vodfor', 1);('detection delayfrom result observe performance degrade', 1);('initial state', 1);('maximum value of210sw10swis', 1);('policy swhas performance degradation', 1);('range 2515812maximum difference', 1);('3403original value 210sw9261', 1);('comparison', 1);('state value swin', 1);('different semimdp', 1);('conclusionin', 1);('algorithm action deceptionin twoplayer concurrent stochastic games asymmetric information players knowledge observations formallyprove', 1);('lowerbound value action deception', 1);('response strategy building resultthere', 1);('future directions', 1);('solution concepts', 1);('competitive interactions', 1);('general noncooperative interactions practiceasymmetric information prevalent multiagent interactionsand', 1);('possible agents intentions', 1);('actioncapabilities adaptive player', 1);('multiagent collaboration', 1);('anotherdirection', 1);('competitive settingbut general information structure instance whatif', 1);('partial observations state actionsequences', 1);('subclass gamesmay', 1);('tractable solutionsacknowledgmentsthis research', 1);('army research office', 1);('aroand', 1);('grant number w911nf2210034and grant number w911nf2210166', 1);('document authors notbe', 1);('official policies', 1);('army research office us', 1);('governmentthe', 1);('us government', 1);('reproduce distributereprints government purposes', 1);('copyrightnotation hereinreferences1kenshi', 1);('abe yusuke kaneko', 1);('offpolicy exploitabilityevaluation', 1);('zerosum markov', 1);('conferenceon autonomous agents multiagent systems aamas', 1);('alshaer jinpeng wei kevin', 1);('hamlen cliff wang', 1);('dynamicbayesian', 1);('adversarial defensive cyber deception autonomouscyber deception reasoning adaptive', 1);('evaluation honeythings springer', 1);('bakker arnab bhattacharya samrat chatterjee draguna', 1);('vrabie', 1);('hypergames cyberphysical', 1);('security control', 1);('systemsarxiv180902240', 1);('httparxivorgabs180902240 arxiv1809022404pg', 1);('bennett', 1);('toward', 1);('theory hypergames', 1);('omega', 1);('g bennett robin r bussel', 1);('hypergame theory methodologythe current', 1);('state art', 1);('management uncertainty approachesmethods applications luc wilkin ed springer netherlands dordrecht158181', 1);('e carroll daniel grosu', 1);('game theoretic', 1);('deception network', 1);('conferenceon computer communications networks', 1);('issn', 1);('chatterjee thomas henzinger', 1);('survey stochasticregular games', 1);('j comput', 1);('sci', 1);('alfaro thomas henzinger', 1);('concurrent omegaregulargames9yinuo', 1);('zimeng', 1);('stephanie milani', 1);('adaptivecyber deception game proc', 1);('autonomousagents multiagent systems auckland', 1);('zealand', 1);('andrew estornell sanmay das yevgeniy vorobeychik', 1);('deceptionthrough halftruths proceedings aaai', 1);('april', 1);('jerzy filar koos vrieze', 1);('competitive markov', 1);('decision processes', 1);('springerscience', 1);('media12 bahman gharesifard jorge cortes', 1);('equilibria misperceptions hypergames', 1);('american control conference', 1);('ieee san francisco ca', 1);('john', 1);('harsanyi', 1);('incomplete information played bayesianplayers iiii part', 1);('model management', 1);('publisherinforms14 joao p hespanha yusuf ateskan h kizilocak', 1);('noncooperative games partial information', 1);('darpajfacc symposium advances enterprise', 1);('citeseer', 1);('linan huang quanyan zhu', 1);('dynamic', 1);('approach proactive', 1);('strategies advanced persistent threats cyberphysicalsystems comput secur', 1);('mustafa karabag melkior ornik ufuk topcu', 1);('insupervisory control', 1);('ire transactions automatic', 1);('anagha kulkarni siddharth srivastava subbarao kambhampati', 1);('signaling', 1);('headfaking enemies', 1);('balancing goal obfuscation goal legibility proceedings', 1);('international conference onautonomous', 1);('agents multiagent systems aamas', 1);('autonomous agents multiagent systems richland sc', 1);('18891891texids kulkarnisignalingfriendsheadfaking2020 arxiv', 1);('abhishek n kulkarni jie fu', 1);('synthesis deceptive strategies', 1);('misperception proceedings twentyninth', 1);('joint', 1);('jointconferences artificial intelligence', 1);('yokohama japan', 1);('tze leung lai', 1);('sequential', 1);('changepoint detection quality control dynamical systems journal', 1);('statistical', 1);('society series b', 1);('methodological57', 1);('lening li haoxiang abhishek n kulkarni jie fu', 1);('dynamic hypergames synthesis deceptive strategies temporal logic objectivesunder', 1);('ieee transactions automation', 1);('engineering july2020', 1);('httparxivorgabs200715726 arxiv', 1);('robert mcnaughton', 1);('finite graphs', 1);('annals pureand applied logic', 1);('thanh h nguyen yongzhao wang arunesh sinha michael p wellman2019 deception finitely repeated', 1);('proceedings aaaiconference artificial intelligence', 1);('number', 1);('yasuo sasaki', 1);('subjective rationalizability hypergames advances', 1);('aaron schlenker omkar thakoor haifeng xu fei fang milind tambe longtranthanh phebe vayanos yevgeniy vorobeychik', 1);('deceiving cyberadversaries game theoretic approach', 1);('omkar thakoor shahin jabbari palvi aggarwal cleotilde gonzalez milindtambe phebe vayanos', 1);('exploiting bounded rationality riskbasedcyber camouflage', 1);('decision game theory', 1);('omkar thakoor milind tambe phebe vayanos haifeng xu christopher kiekintveld fei fang', 1);('cyber camouflage', 1);('deceptionindecision game theory', 1);('tansu alpcan yevgeniy vorobeychikjohn baras gyrgy dn eds vol', 1);('springer', 1);('httpsdoiorg101007978303032430831 series', 1);('titlelecture notes computer science27 wieslaw zielonka', 1);('graphs applications automata infinite trees', 1);('theoretical computer', 1);