('figure', 10);('dqn', 8);('crossroad', 7);('reinforcement learning', 6);('safety rules', 5);('unsafe actions', 5);('freeway', 4);('invaders', 4);('qnetwork', 4);('atari', 4);('mspacman', 4);('qsr', 3);('equation', 3);('frostbite', 3);('agent needs', 3);('qlearning', 3);('average', 3);('experiment', 3);('negative reward', 3);('safe \x0fgreedy policy', 3);('often', 2);('safety rule', 2);('real world', 2);('safety component', 2);('left', 2);('openai gym atari', 2);('qfunction', 2);('experience replay', 2);('safe action', 2);('unsafe action', 2);('size search space', 2);('guided', 2);('crossroad freeway frostbite mspacman', 2);('\x0fgreedy policy', 2);('crossroad freeway', 2);('base model', 2);('obvious limitation', 2);('maximum reward', 2);('garc j fern', 2);('andez f', 2);('dont safer reinforcement learning rulebased guidanceekaterina nikonovacheng xue jochen renzschool computingthe australian', 1);('universitycanberra australiaekaterinanikonova', 1);('training reinforcement learning systems interact withthe world', 1);('safety actionswhen', 1);('world systems', 1);('harm surroundings', 1);('dangerous situations', 1);('rules thatthe system', 1);('conditions example robot navigation', 1);('objects people workwe dene safety rules terms relationships betweenthe agent objects', 1);('reinforcement learning systems', 1);('new safe epsilongreedy algorithm thatuses safety rules override agents actions', 1);('unsafe experiments', 1);('safeepsilongreedy policy', 1);('increases safety theagent training improves learning efciency', 1);('performance base modelintroductionin reinforcement learning agent tries maximize thelongterm return', 1);('explores world randommanner', 1);('safety actions', 1);('whilesafety', 1);('safe interaction', 1);('necessary quality agent example delivery robot', 1);('important collide surroundingobstacles', 1);('package selfdrivingcar making', 1);('safe decisions', 1);('crucial safety thedriver others', 1);('ai', 1);('particular issue knownassafe exploration', 1);('5multiple approaches', 1);('safer reinforcement learning example', 1);('optimization criteria', 1);('safe exploration interaction', 1);('particular domain exampleconsider', 1);('realworld task delivery robot navigate streets', 1);('objects people task', 1);('important safety rule', 1);('immediate crash rule', 1);('spatial relationships robotand objects example', 1);('qualitative spatial representation', 1);('words dene', 1);('simple rulefor task object', 1);('close proximity agentand', 1);('collision notperform actionthe idea', 1);('long way', 1);('issac asimovsideas', 1);('laws robotics', 1);('basic safety rules', 1);('knowledge agent', 1);('theagent explores worldin reinforcement learning researchers', 1);('knowledge address', 1);('safe explorationproblem', 1);('possible way use', 1);('imitation learning', 1);('example theagent', 1);('imitate human demonstrations', 1);('thesafety constraints', 1);('policy optimization 26saunders', 1);('override agents actionsif', 1);('saunderss', 1);('idea tobe', 1);('human oversight override agent decisions', 1);('intervention policy human demonstrations', 1);('safe approach', 1);('human loopwhich', 1);('unrealistic ourwork', 1);('need constanthuman oversight', 1);('safety rules override agents actions ifthey unsafe', 1);('knowledge notfound method uses symbolic safety rules', 1);('toprevent agent', 1);('unsafe actionsin work focus domains', 1);('agentsto act', 1);('general framework', 1);('possible implementation framework', 1);('symbolic rules', 1);('inour', 1);('human knowledge', 1);('internal dynamics environment', 1);('constructsymbolic safety rules', 1);('framework workwith', 1);('different safety components', 1);('short experiment', 1);('rules agent explores world', 1);('wepropose', 1);('new method incorporate safety rules thereinforcement learning algorithm', 1);('specically', 1);('dene aarxiv221213819v1 csai', 1);('dec', 1);('general framework validate safety agents decision', 1);('right', 1);('example offramework implementation', 1);('decision wouldbe overwritten safer action downsafe\x0fgreedy policy uses symbolic rules validate thesafety actions overrides safer actions ifnecessary experiments', 1);('deep learning model', 1);('identical hyperparametersto ensure fair evaluation method demonstratethat', 1);('agent actions improves overallperformance increases safety agent', 1);('improves efciency agent', 1);('superhuman stateoftheart performance', 1);('experiments test method domains toydomaincrossroad', 1);('frostbite mspacman', 1);('14backgroundin work', 1);('markov decisionprocess mdp satr', 1);('wheresis statespaceais action space', 1);('ts\x02asthe', 1);('transitionfunctionrs\x02aris reward function y201the discount factorapolicy\x19sadetermines action', 1);('ineach state', 1);('typically', 1);('goal reinforcement learning tond policy maximizes', 1);('theqfunctionq\x19sa e\x19p1t0', 1);('trtjs0sa0ameasures performance agent', 1);('startsin states', 1);('action aand', 1);('policy \x19afterwardsthe', 1);('valuefunction v\x19s ea\x18\x19sq\x19sameasures', 1);('overall value state policythose functions optimal', 1);('q\x03sa', 1);('max\x19q\x19saandv\x03s max\x19v\x19s', 1);('optimal policycan', 1);('q\x03as', 1);('\x19\x03s argmaxaq\x03sain', 1);('nonlinear function approximator suchas neural network', 1);('qsa\x12i', 1);('where\x12iare weightsof', 1);('ith iteration', 1);('usinga nonlinear function approximator', 1);('diverge dueto', 1);('problems correlation sequenceof observations b correlations', 1);('values targetvaluesrt maxaqstaand c policy', 1);('sensitive changes', 1);('qnetwork dqn', 1);('addresses rst problem', 1);('experience', 1);('replay implementedby', 1);('agent technique removes correlation sequences observations', 1);('data dene experience etstatrt1st1 experience', 1);('mfe1etgin', 1);('order address', 1);('problem notionof target network', 1);('usedto calculate loss', 1);('li\x12i esars0\x18umr', 1);('maxa0qs0a0\x12\x00i\x00qsa\x12i2 whereiis iteration', 1);('discount factor \x12iare weights', 1);('\x12\x00iare weights target network', 1);('target network', 1);('csteps', 1);('acopy online network weights online network', 1);('iteration iguidance', 1);('symbolic rulesdenitionsqualitative spatial representation', 1);('order constructa symbolic representation states extract qualitative spatial relationships', 1);('objects theagent', 1);('particular use', 1);('directional representation', 1);('qualitative distance representation 30figure', 1);('shows example combination directionaland distance representations', 1);('combination toconstruct safety rules relational representation thestatesafety', 1);('obvious approach', 1);('denesafety terms state transitions', 1);('states 2however denition', 1);('agent havea model', 1);('atthe relationships objects', 1);('state dene safetyrule conjunction nary relationships betweenobjects action', 1);('compromisesafetysafetyrule r1o1onrmo1onfigure', 1);('qualitative', 1);('directional distance', 1);('safety rulesaction whereriis', 1);('relationship ojis object andaction ais action example safetyrule', 1);('action thatcould lead collision', 1);('nearby objects iecloseagento', 1);('nagento', 1);('action denesafety rules collection rules', 1);('rightdemonstrates', 1);('symbolic safety rules', 1);('freewayis', 1);('safe call action atto safein statest', 1);('violateany safety rule validate action', 1);('safe extract symbolic relationships state stasssymbt fr1o1onrmo1ong whereriis', 1);('relationship ojis object state st example wecan extract spatial relationships', 1);('nearby objectsand agent ie closeagentcar 1nagentcar 1we conjugate symbolic representationasymbt actionatieaction', 1);('deneisactionsafe stat1ifssymbtasymbtsafetyrules 0otherwise1wheressymbt relationships', 1);('standasymbt symbolic representation note thiswork extract spatial relationships objectsand agents fall', 1);('certain region', 1);('right consider figure', 1);('demonstrates aframework implementation', 1);('collision witha car example car', 1);('agentiessymb closeagentcar 1nagentcar', 1);('ifthe dqn', 1);('predicts action ie closeagentcar 1nagentcar 1action rule', 1);('violatedand action', 1);('safe actionrandom', 1);('select random', 1);('safe action thestatest deneselectrandomsafeaction st \x1aa2asafeifasafe6a2a otherwise2in', 1);('asafe', 1);('orauniformlyandasafefaitjisactionsafe stait 1ait2agis aalgorithm', 1);('guided explorationinput qstoutput', 1);('a1n\x18u012ifn\x0fthen3 selectrandomsafeaction st4else5 argmaxaqsta6end if7return acollection', 1);('safe actions actions', 1);('unsafe shouldbe', 1);('agent whenever possiblesafe\x0fgreedy policyin section', 1);('safety rulesas part', 1);('safer version \x0fgreedy algorithm', 1);('safe\x0fgreedy', 1);('prevents agentfrom', 1);('possible hypothesize', 1);('agent actions', 1);('learning saferbut', 1);('performancein timestep', 1);('safe \x0fgreedy leverages safety rules todetermine safety agents actions', 1);('safe \x0fgreedy algorithm', 1);('exploration andfullguidance', 1);('validates overrides agents', 1);('exploration phase', 1);('thewhole training process describe algorithms furtherin sectionrecall the\x0fgreedy policy 16\x19st \x1aa2a ifn\x0fn2u01argmaxaqstaotherwise3by denition epsilongreedy policy', 1);('selectsa random action exploration phase', 1);('safety check', 1);('select random safeactionalgorithm', 1);('guidanceinput qstoutput', 1);('a1n\x18u012ifn\x0fthen3 selectrandomsafeaction st4else5 argmaxaqsta6 ifnotisactionsafe stathen7 selectrandomsafeaction st8 end if9end if10return afigure', 1);('testing', 1);('invadersalgorithm', 1);('demonstrates safety rules', 1);('safer version', 1);('random action algorithm', 1);('safe action note', 1);('safe actionwill', 1);('mor', 1);('qtable intabular', 1);('rl', 1);('aid training feedback', 1);('algorithm', 1);('ensure agent', 1);('oncethe exploration phase agent', 1);('example agent encounters', 1);('unseen state', 1);('ensure agent behaves', 1);('agentwith guidance', 1);('whole training process asalgorithm', 1);('select safe actions exploration phase', 1);('addition action', 1);('algorithm checkwhether action', 1);('safe action hypothesize strict supervision ofthe actions', 1);('agent increase safetyof agent', 1);('result algorithm manyunsafe actions', 1);('sizeof search space', 1);('necessary step', 1);('maximum performance', 1);('prioritize safety', 1);('domains', 1);('work use', 1);('freeway frostbite mspacman', 1);('invadersfigure', 1);('atari2600 freeway', 1);('discrete action space ability tomove', 1);('place goal ofthe game cross road', 1);('car result', 1);('immediate end thegame agent', 1);('game won1', 1);('orstay place goal game cross road themaximum', 1);('possible number times timer runsout', 1);('car result end ofthe game agent', 1);('crosses allroads', 1);('theagent needs', 1);('ghosts eaten cases', 1);('extra reward', 1);('lastly atari', 1);('agentneeds shoot enemies', 1);('master thosegames agent needs', 1);('notjump water space', 1);('avoidthe bullets', 1);('highplaces addition domains method', 1);('settings', 1);('safe \x0fgreedy policy intabular', 1);('reinforcement learning settings representation learning focus work use symbolic state representation', 1);('crossroad ram', 1);('order extract symbolic representation forthe', 1);('aari', 1);('notehowever part', 1);('stateoftheart image processor extract location objects test', 1);('safe \x0fgreedy policy usingguidedexploration', 1);('fullguidance', 1);('measure impact ofthe method', 1);('complications instability', 1);('reinforcement learning tabular', 1);('test threeagents vanilla', 1);('qlearning qlearning guided exploration qlearning fullguidancefor', 1);('deep reinforcement learning experiments usedeep', 1);('relative simplicity', 1);('widespread research community', 1);('offpolicy algorithm asa substitute \x0fgreedy policy test', 1);('agents vanilladqn', 1);('dqn guided exploration dqnge dqn fullguidance dqnfg', 1);('note use samehyperparameters model architecture', 1);('agents difference', 1);('agentsis actionselection algorithm use training', 1);('algorithms', 1);('performance agents', 1);('gamesto performance humans random agent', 1);('with01 rewardmillion frames', 1);('results imagebaseddqn show difference', 1);('ram', 1);('imagestate representationsafety', 1);('testing domains', 1);('terms relationships objects actions work dene symbolic safety rules', 1);('domain manuallyhowever note', 1);('rules bothcumbersome', 1);('address wehypothesize rules', 1);('autonomouslylater paper', 1);('learning', 1);('present short experiment', 1);('rules work methodas wellin', 1);('agent collide cars dene safety rules terms relationships cars agent', 1);('rightfor', 1);('safe wecheck', 1);('agent notthen', 1);('similar rules', 1);('otherdirections actionsin', 1);('similarly', 1);('terms relationships platforms agent exampleto', 1);('safe checkwhether platform', 1);('andfreeway collide ghosts exceptionthat', 1);('ghosts eaten', 1);('bonus pointsas', 1);('terms relationships game', 1);('terms relationships bullets player bullet', 1);('close tothe player player needs', 1);('dodge providefull safety rules domains', 1);('appendixdifference reward shaping', 1);('logical questionthat', 1);('agent itsaction lead', 1);('similar performance', 1);('negative rewards actions', 1);('obvious downside thisapproach', 1);('agents learningrule agent', 1);('crossroadvent', 1);('unsafe actions agentwould', 1);('arisk hypothesize', 1);('need toexplore', 1);('search space', 1);('likely outperformthe safe\x0fgreedy agentsfigure', 1);('learning curves', 1);('agents incrossroad', 1);('originally', 1);('agent collides thecar', 1);('crosses roads', 1);('reward function experimentto', 1);('dqnnegreward', 1);('scenario human shapes reward functionfor domain experiment agent', 1);('negative reward outperform', 1);('safe \x0fgreedy agents norvanilla', 1);('time dene', 1);('hypothesise thatsuch rules', 1);('short experiment test hypothesis', 1);('rules note learning rules', 1);('focus work', 1);('fullscale experiment', 1);('future workfor experiment use', 1);('learning safetycomponent framework', 1);('train logistic regression collection symbolic states andclassify', 1);('safe unsafe categories statestwe use model', 1);('state st1followingthe actionatand use classier', 1);('full details classier andthe approach', 1);('appendix figure', 1);('shows learning curves', 1);('agents vanilla', 1);('qlearning qlearningwith', 1);('agentsthat use', 1);('qlearning guided', 1);('qlearning fullexploration', 1);('rules concludes rules', 1);('fullscale experifigure', 1);('number times agent', 1);('dqnor qlearning', 1);('lesser number safer training', 1);('collisions deathsment domains future workresults', 1);('discussionsin', 1);('present results experimentsin domains', 1);('safe \x0fgreedy policy increases safety agent training improves', 1);('performance thebase', 1);('safety safe\x0fgreedy', 1);('leverages safety rulesto override agents actions', 1);('unsafe hypothesize', 1);('numberof collisions deaths training', 1);('helpthe agent', 1);('safer policy', 1);('relative number times agent', 1);('training domains', 1);('safe \x0fgreedy agents', 1);('noticeable reduction number deaths', 1);('basedon', 1);('results hypothesize', 1);('real world ourmethod', 1);('potential cost training theagent', 1);('times note somedomains', 1);('deaths signicantas domains hypothesise', 1);('theshort horizon safety rules domains ie', 1);('agent needs plan actions', 1);('steps aheadto', 1);('states death', 1);('possible wayto', 1);('sophisticated safetycomponent validate safety agents', 1);('efciency', 1);('convergence speed', 1);('demonstrates howmany episodes', 1);('safe \x0fgreedy agents achievemaximum performance base model demonstratesthat domains efciency', 1);('demonstrates average rewards', 1);('byagents domains', 1);('safe', 1);('initial jumpstart performance infigure', 1);('number episodes', 1);('agent toachieve', 1);('maximum performance vanilla', 1);('dqn qlearning', 1);('lesser number efcient agentwas meaning', 1);('convergencesome cases', 1);('convergence speed notethat', 1);('25000episodes whereas', 1);('dqnge dqnfg', 1);('optimal performance', 1);('initial jumpstart', 1);('performancein rst episodebetter', 1);('performance', 1);('safety efciency', 1);('improves performance agents', 1);('agents training', 1);('maximum performance ofthe agents humans random agent', 1);('providethe results', 1);('performance impact', 1);('different representations domains safe\x0fgreedy agents', 1);('performance thanthe base models hypothesize', 1);('likely due thetwo', 1);('main reasons', 1);('initial jumpstartin performance', 1);('domains rstreason', 1);('efciency wouldresult', 1);('point learning', 1);('domains thatrequire agent behave', 1);('goodperformance hypothesize comparison someother domains safety correlates performancesince agent explores', 1);('safe states nds', 1);('policy fasterconclusions', 1);('limitationsin', 1);('domainspecic symbolicsafety rules', 1);('new safe \x0fgreedy algorithm injects symbolic rules training process reinforcement learning agent', 1);('versions thisalgorithm', 1);('restricts agent actions', 1);('entire training process thefigure', 1);('rewards agents', 1);('invadersdqn', 1);('episodes 20m frames imagedqn results', 1);('17exploration world', 1);('different domains experiments', 1);('increase efciency ofthe agent boost overall performance', 1);('additional experimentto show', 1);('learning efciency', 1);('integratedinto training', 1);('symbolic safety rules isthat', 1);('complex domains rules', 1);('hard dene', 1);('section hypothesize thatin', 1);('domains rules', 1);('goesbeyond paper', 1);('future workanother limitation method completelyeliminate unsafe actions', 1);('due tothe', 1);('short horizon', 1);('symbolic rules rules thatwe use', 1);('able model', 1);('leavesa number situations', 1);('unsafe action example', 1);('therecould situations agent enoughtime way', 1);('sophisticated complex rules neededwe hypothesize', 1);('neurosymbolic approach', 1);('paper andwe', 1);('future workin conclusion', 1);('safety rules learning process', 1);('safety efciency reinforcement learningagentsreferences1', 1);('comprehensive survey onsafe reinforcement learning', 1);('j mach learn res', 1);('hans schneega sch', 1);('udluft safeexploration', 1);('esann', 1);('thomas g luo safe reinforcementlearning imagining near', 1);('neurips', 1);('kobelrausch jantsch collisionfree deepreinforcement learning mobile robots', 1);('policy', 1);('international conference control', 1);('automation robotics iccar', 1);('gu yang', 1);('chen g walter', 1);('wang jyang', 1);('review safe reinforcementlearning methods theory applications arxiv', 1);('amodei olah', 1);('steinhardt j christiano pschulman j', 1);('man e', 1);('concrete problems aisafety arxiv', 1);('geibel p wysotzki', 1);('risksensitive reinforcement learning applied', 1);('constraintsj artif intell res', 1);('howard r matheson j risksensitive markov decision processes management', 1);('l utjens b', 1);('everett j safe reinforcementlearning model uncertainty', 1);('international conference', 1);('robotics automationicra', 1);('chow nachum', 1);('e ghavamzadeh lyapunovbased approach', 1);('reinforcement learning neurips', 1);('clouse j utgoff p', 1);('method reinforcement learning ml', 1);('safe exploration stateand', 1);('spaces reinforcement learning arxiv', 1);('geramifard redding j j intelligent cooperative', 1);('architecture framework performance improvement', 1);('safe learning', 1);('intelligent robotic systems', 1);('brockman g cheung v pettersson', 1);('schneiderj schulman j tang j zaremba', 1);('openai gymarxiv', 1);('anand racah e ozair bengio', 1);('c ote', 1);('hjelm r unsupervised', 1);('representation learningin atari arxiv', 1);('sutton r barto reinforcement learning anintroduction bradford book201817 mnih v kavukcuoglu k silver rusua veness j bellemare graves riedmiller fidjeland ostrovski g petersens beattie', 1);('sadik antonoglou kingh kumaran wierstra legg hassabis humanlevel', 1);('nature', 1);('clementini e felice p hern', 1);('qualitativerepresentation positional information artif intell', 1);('cohn renz j qualitative spatial representationand reasoning handbook knowledge representation', 1);('weld etzioni', 1);('robotics acall', 1);('aaai', 1);('pecka svoboda safe exploration techniquesfor reinforcement learning overview mesas', 1);('leike j martic krakovna v ortega p everittt lefrancq orseau', 1);('legg ai safety gridworlds arxiv', 1);('santara naik ravindran', 1);('das mudigere avancha kaul', 1);('rail riskaverseimitation learning aamas', 1);('asimov robot conn fawcett publications1950625 ghavamzadeh petrik chow safe policyimprovement minimizing robust baseline regretnips', 1);('achiam j', 1);('tamar abbeel p constrained policy optimization icml', 1);('saunders', 1);('sastry g stuhlm', 1);('evanso trial', 1);('error towards safe reinforcement learning', 1);('intervention arxiv', 1);('lazaridis fachantidis vlahavas deepreinforcement learning stateoftheart walkthrough j artif intell res', 1);('renz j mitra qualitative direction calculi', 1);('granularity pricai', 1);('frank qualitative', 1);('spatial reasoning distancesand directions geographic space', 1);('j vis lang comput3pp', 1);