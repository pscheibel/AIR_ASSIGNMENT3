('cs', 11);('rl', 10);('mw', 10);('figure', 8);('rnns', 6);('lstm', 6);('neural networks', 6);('nn', 5);('mlp', 5);('sgd', 5);('algorithm', 5);('springer', 5);('j schmidhuber', 5);('nns', 4);('intrinsic reward', 4);('experiments', 4);('equation', 4);('save', 4);('preprint', 4);('international conference', 4);('gans', 3);('cis', 3);('intrinsic curiosity reward', 3);('mwith', 3);('hence', 3);('appendix', 3);('mean', 3);('condence intervals', 3);('mwfor', 3);('execute', 3);('ddpg', 3);('reinforcement learning', 3);('proceedings', 3);('ieee', 3);('j schmidhuber learning', 3);('kolmogorov', 3);('initial experiments', 3);('number steps', 3);('important things science', 2);('finding', 2);('costly experiments', 2);('automatic generation', 2);('dec', 2);('mis', 2);('qas', 2);('external reward', 2);('cmight', 2);('mcan', 2);('ms', 2);('lstms', 2);('mhas', 2);('cm', 2);('p ower play', 2);('big net', 2);('ccan', 2);('cwill', 2);('simple experiments', 2);('rnn', 2);('halt', 2);('result', 2);('surprising experiments', 2);('cto', 2);('internal computations', 2);('adversarial', 2);('pure', 2);('short experiments', 2);('parameters w', 2);('mwis', 2);('generating', 2);('generate experiments', 2);('mwcannot', 2);('random experiments', 2);('1000experiments0005101520253035goal states', 2);('adjustedwith intrinsic reward', 2);('purple', 2);('goal states', 2);('differentiable controller', 2);('\x12in environment', 2);('set', 2);('neural computation', 2);('lncs', 2);('advances', 2);('proceedings genetic evolutionary computation', 2);('gecco', 2);('neural machine translation', 2);('blog', 2);('neural information processing systems', 2);('machine learning', 2);('pmlr', 2);('nips', 2);('joint', 2);('neural networks singapore', 2);('proc', 2);('j schmidhuber discovering', 2);('complexity high generalization capability', 2);('technical', 2);('j schmidhuber deep', 2);('force field', 2);('valuehidden', 2);('128hidden layers c', 2);('iteration c', 2);('001weight decay c 001experiment parameter range', 2);('einit', 2);('hyperparameters algorithm', 2);('execute rnn', 2);('trainmwon', 2);('data fromdfor', 2);('calculate', 2);('learning', 1);('abstract bit timethrough selfinvented experimentsencoded neural networksvincent herrmann1louis kirsch1jurgen schmidhuber1231the', 1);('ai lab idsiausisupsi lugano switzerland2nnaisense lugano switzerland3ai initiative kaust thuwal saudi arabiafvincentherrmann', 1);('louis juergen gidsiachabstractthere', 1);('questions b', 1);('comingup', 1);('good questions articial scientists', 1);('questions butalso', 1);('new questions', 1);('experiments akin thoseof mathematicians articial scientist expands knowledge', 1);('surprising outcomes', 1);('present empirical analysis', 1);('interesting experiments therst setting', 1);('environmentand show lead', 1);('effective exploration', 1);('weights recurrent neural networks', 1);('neural experimentgenerator', 1);('introduction previous workwe', 1);('answers togiven questions b', 1);('coming', 1);('good questions eg', 1);('standard problem computer science', 1);('creative part b articialsystems reinforcement learning', 1);('articial neural networks', 1);('andother machine learning methodsto', 1);('work articial scientists equippedwith articial curiosity creativity eg', 1);('rst articialqa system', 1);('questions intrinsic', 1);('ccprobabilistically', 1);('generates outputs', 1);('inuence environment', 1);('theworld model predicts environmental reactions', 1);('gradient descent', 1);('mminimizes', 1);('predictor zerosum game rewardmaximizingctries nd sequences output actions maximize error', 1);('mms', 1);('loss gain', 1);('c1arxiv221214374v1', 1);('application articial curiosity', 1);('generalcases sequential data', 1);('thegenerative adversarial', 1);('neural network predictive world model usedto maximize controllers intrinsic reward proportional models prediction errorswhich', 1);('instance trials', 1);('inbandit problems environment', 1);('controllers orgenerators output', 1);('questions action sequences', 1);('learning toanswer questions', 1);('mdoes', 1);('interest questions', 1);('series papers articial curiosity articial scientists', 1);('recent years', 1);('successful applications', 1);('simple principleand variants thereof sequential settings eg', 1);('2the approach', 1);('ne exploration strategy', 1);('deterministic environments stochastic environments', 1);('focus thoseparts environment', 1);('prediction errors', 1);('due randomness', 1);('tocomputational limitations example agent', 1);('front atv screen', 1);('unpredictable white noise eg', 1);('stochastic environments', 1);('reward errorsofm approximation rst derivative ofms errors', 1);('subsequent training iterationsthat', 1);('progress improvements', 1);('high errorsin front noisy tv screen', 1);('cwont', 1);('simple insight', 1);('lots followup work', 1);('approachfor articial curiosity stochastic environments', 1);('mlearned', 1);('topredict estimate probabilities environments', 1);('possible responses', 1);('aftereach', 1);('interaction environment', 1);('kldivergence', 1);('msestimated', 1);('probability distributions', 1);('new experiencethe informationgain', 1);('bayesian surprise', 1);('work informationgain', 1);('rl nns', 1);('6in general', 1);('setting environment', 1);('sec', 1);('candmmay', 1);('prot memory', 1);('previous events', 1);('towards', 1);('candmcan', 1);('encode history', 1);('hof', 1);('short codes', 1);('mconsiders', 1);('thatis', 1);('learning progress', 1);('lot concept compression progress', 1);('account bits ofinformation', 1);('specify general approach', 1);('algorithmic informationtheory eg', 1);('algorithmiccompression progress', 1);('scheme weights modelnetwork eg', 1);('scheme history observationsso', 1);('history science history ofcompression progress incremental discovery', 1);('simple laws', 1);('complexobservation sequences', 1);('cwere', 1);('all2the details future inputs eg pixels', 1);('thats', 1);('general adversarial', 1);('rlmachine', 1);('arbitrary abstract questionswith', 1);('computable answers', 1);('example', 1);('question run policy program awhile executes', 1);('internal storage cell number', 1);('different yesno outcomes computational experiments', 1);('thewinner', 1);('upwith questions', 1);('answers surprise', 1);('hard questions', 1);('present followingsectionsall systems', 1);('maximize sum thestandard external rewards', 1);('usergiven goals intrinsic rewards distortthe', 1);('times day curiosityreward systems ephemeral', 1);('additionalintrinsic reward', 1);('external reward tends', 1);('learnable environments', 1);('long run intrinsic reward', 1);('tothe external reward', 1);('applications care externalrewardour', 1);('rl qa', 1);('systems 1990s', 1);('enumerate questions', 1);('butthe', 1);('whatis', 1);('formalisable questions', 1);('question answeredby learning machine dene question need computational procedure', 1);('question ornot', 1);('possible questions problems searches', 1);('current policy', 1);('forgettingthe answers', 1);('qa', 1);('therepertoire cheapest onethe', 1);('p ower playin', 1);('empirical investigation section', 1);('complex computational experiments yesno outcomes', 1);('generationof experiments', 1);('model prediction error deterministic', 1);('cdriven', 1);('information gain generates', 1);('experimentsin form weight matrices', 1);('rnns2 selfinvented experiments encoded neural networkswe', 1);('arbitrary computational experiments', 1);('experiments binary yesno outcomes', 1);('boringin general controller', 1);('cand', 1);('time step 12cs input', 1);('current sensoryinput vector int external reward vector', 1);('ret', 1);('ritcmay', 1);('ormay interact', 1);('environment action outputs', 1);('cask', 1);('questions and3propose experiments', 1);('chas', 1);('output unit', 1);('start', 1);('active 05cuses', 1);('extra output units', 1);('weight matrix program \x12of', 1);('efor experiment', 1);('fast weight programmer style', 1);('18etakes sensory inputs environment', 1);('actions outputs', 1);('twoadditional output units', 1);('unit weights \x12are generatedat time stept0eis', 1);('es halt', 1);('unit exceeds05', 1);('time step t00', 1);('current experiment ends experiment computes ownruntime', 1);('experimental outcome rt00is1if activation result t00ofes', 1);('0otherwiseat timet0 experiment', 1);('compute output prt0201from\x12and history', 1);('inputs actions t0', 1);('previous experiments theiroutcomes prt0modelsms uncertainty nal binary outcome experiment willbe', 1);('yes', 1);('experiment runin shortcis', 1);('experimental question form \x12that yield binary answerunless time limit', 1);('eis rnn', 1);('general computer', 1);('weight matrix implementany program', 1);('executable traditional computer', 1);('computable experiment binaryoutcome', 1);('weight matrix', 1);('storage limitations nite', 1);('orother computers', 1);('appropriate weight matrix \x12ccan', 1);('computable solution words', 1);('scientic hypothesis', 1);('veriable falsiableatt00ms', 1);('previous prediction prt0is', 1);('outcome rt00of', 1);('csexperiment', 1);('spans t00\x00t0time steps', 1);('rit00is', 1);('proportionaltoms surprise calculate', 1);('probability rt00', 1);('thehistory observations', 1);('mby', 1);('gradient descent regularization', 1);('amount time', 1);('previous predictions', 1);('mcalledm\x03in', 1);('generalm\x03will compute', 1);('different prediction', 1);('prt0ofrt00', 1);('history t0\x001at timet00 contribution', 1);('rigt00tocs', 1);('curiosity reward proportional', 1);('apparent resultinginformation gain', 1);('kldivergencerigt00\x18dkl\x00prt0jjprt0\x01ifmhad', 1);('condent belief', 1);('particular experimental outcome belief', 1);('inthe wake', 1);('experiment major surprise', 1);('big insight', 1);('lotsof intrinsic curiosity reward c hand', 1);('mwas', 1);('unsure experimentaloutcome', 1);('unsure afterwards', 1);('mandcwillfail', 1);('interesting hypotheses experiments', 1);('mscurrent', 1);('deep beliefs', 1);('alternative intrinsic curiosity reward', 1);('basedon compression progress', 1);('entire experimental protocol responsibility \x12', 1);('through\x12emust', 1);('initializethe experiment eg', 1);('position isimportant', 1);('reliable results', 1);('sequence computationalsteps actions', 1);('data sequence nal abstract binary outcomeyes', 1);('nocis', 1);('experimental protocols \x12that surprise', 1);('mcwill', 1);('condence recall noisy tv', 1);('high condence', 1);('high condence4a', 1);('negative reward', 1);('time step encourages', 1);('physical actions environment cost', 1);('immediate negative reward', 1);('es', 1);('time step', 1);('physical actions', 1);('nondifferentiable environment fact', 1);('bias towardsthe', 1);('surprising ofcsinitial experiments', 1);('ceandmare', 1);('differentiable notonlymbut alsocmay', 1);('trainable backpropagation', 1);('slowerpolicy gradient methods', 1);('true reward function', 1);('alsodifferentiable respect', 1);('experimental evaluationhere', 1);('present initial studies', 1);('interesting experiments', 1);('intrinsic reward encourages experiments', 1);('differentiable environment sequences', 1);('continuous control actions', 1);('thediscovery goal states sparse reward setting', 1);('rnnswithout', 1);('environmental interactions', 1);('information gain rewardtogether', 1);('important aspects', 1);('abstractexperiments binary outcomes method', 1);('curious exploration creation interestingpure', 1);('integration setups singlesystem', 1);('future work31', 1);('generating experiments', 1);('environmentreinforcement', 1);('involves exploration environment nondifferentiabledynamics', 1);('methods policy gradients', 1);('simplify investigationand focus', 1);('experiments introduce', 1);('analytical policy gradients', 1);('backpropagation notlimit generality approach', 1);('continuous force eld environment', 1);('agent navigate througha 2d environment', 1);('external force eld force eld', 1);('different levels complexity states environment position velocity agent agents actionsare', 1);('force vectors', 1);('laziness bias', 1);('simple experiments time step', 1);('small negative reward \x0001 sparse', 1);('large reward100', 1);('whenever agent', 1);('goal state', 1);('life setting', 1);('episodic resets', 1);('additional', 1);('information force eld environment foundin', 1);('environment deterministic sufcient', 1);('generate experimentswhose results', 1);('mcannot', 1);('methodalgorithm', 1);('summarize process', 1);('interesting abstractexperiments binary outcomes goal test', 1);('generated', 1);('exploratory behavior', 1);('goal states51', 1);('differentiable force eld environment agent', 1);('red navigate goal stateyellow external force eld exerts forces agent', 1);('negative rewards proportion runtime experiments averageruntime increase time controller nd', 1);('outcomes model', 1);('model learns', 1);('yesno results experiments becomesharder controller', 1);('outcomes surprise modelthe', 1);('experiments form', 1);('ar wheree linear feedforward networkwith parameters sis environment state aare actions r201is experimentalresult sandaare', 1);('unit single scalar 2rdetermines number steps experiment run simplify setup experiment network feedforward', 1);('experimental result', 1);('differentiable respect runtime parameter predicts', 1);('gaussian', 1);('variance number steps actualresult ris expectation result unit rover distribution', 1);('details thiscan', 1);('appendix a1', 1);('result rhas value', 1);('0otherwisethe parameters \x12of experiment network parameters', 1);('runtime parameter ie\x12', 1);('state controller c generates experiments c s \x12c', 1);('multilayer perceptron', 1);('parameters and\x12denotes parameters', 1);('experimentthe modelmwis', 1);('mws\x12', 1);('o201for experiment', 1);('state sand parameters', 1);('iteration algorithm c generates experiment', 1);('current state softhe environment experiment', 1);('certain threshold eg', 1);('state experiment parameters \x12andbinary result', 1);('memory buffer', 1);('dof', 1);('state memory buffer', 1);('bafter', 1);('experiment execution model', 1);('number steps stochastic6environmen tmemoryresultintrinsic', 1);('calcula', 1);('statefigure', 1);('differentiable environment controller c', 1);('e\x12that', 1);('surprise model', 1);('execution theenvironment experiments binary results', 1);('memory model', 1);('onthe history', 1);('previous experimentsgradient descent', 1);('minimize losslmes\x12r \x18dbcemws\x12r 1where bce binary crossentropy loss functionthe', 1);('part iteration training controller c loss', 1);('islces\x18b\x00bce\x00mwsc src ss\x01\x00rec ss 2the function rmaps experiment parameters', 1);('continuous result ofthe experiment function', 1);('remaps', 1);('experiment parameters', 1);('state externalreward', 1);('gradient information ow', 1);('execution theexperiment', 1);('differentiable environment rst term corresponds intrinsic reward forthe controller encourages generate experiments', 1);('thesecond', 1);('term external reward environment punishes', 1);('long experiments', 1);('goal sparse', 1);('differentiable respect experiments actionsno information goal state', 1);('discussionto', 1);('rst hypothesis', 1);('3a shows', 1);('cumulative number times goal state', 1);('number environment interactions experiment', 1);('specically', 1);('shows hj', 1);('pjk1gknk', 1);('index generatedexperiment gkis1if goal state', 1);('kth experiment 0otherwise nkisthe runtime kth experiment method', 1);('themost goal states', 1);('environment interaction', 1);('purely', 1);('goal states70', 1);('external rewarda', 1);('number', 1);('times goal state', 1);('number environment interactions', 1);('adversarial intrinsic reward benet exploration thanrandom experiments', 1);('intrinsic motivation agent', 1);('goalstates sparse reward setting', 1);('1000experiments01020304050experiment runtime04030201000102prediction accuracy differenceb', 1);('blue average runtime experiment', 1);('difference result prediction accuracy', 1);('generatedexperiment average prediction accuracy thecurrentmwfor random experiments', 1);('differentiable force eld environmentbut', 1);('random exploration parameter space', 1);('apowerful exploration strategy', 1);('average runtime random experiments 50steps', 1);('229for experiments', 1);('c rule potential', 1);('unfair bias dueto', 1);('different runtimes', 1);('additional baseline random experimentswith average runtime 20steps', 1);('intrinsic adversarial reward controller', 1);('theexternal reward', 1);('bce term', 1);('surprising thissettingc', 1);('lccontainsno', 1);('information sparse goal rewardfigure 3b addresses', 1);('hypotheses c', 1);('tends prolong experimentsasmwhas', 1);('long runtimes', 1);('punitive external reward explanation', 1);('harderwith time c', 1);('thecorrect results', 1);('fact prediction accuracy', 1);('specically figure', 1);('3b shows difference prediction accuracy thecurrentmwfor', 1);('prediction accuracy', 1);('experiments accounts', 1);('general gain', 1);('mws', 1);('prediction accuracyover course training', 1);('beginning c', 1);('adversarial experiments surprise', 1);('experiments challenge', 1);('mw32 pure rnn thought experimentsthe', 1);('previous experimental setup uses feedforward', 1);('experiments intrinsic reward function', 1);('differentiable respect controllers weights section investigates', 1);('complementary setup', 1);('experiments environment interactions generated8algorithm', 1);('yesno experiments', 1);('differentiable environmentinput', 1);('randomly', 1);('c s\x02', 1);('differentiablemodelmws\x02\x02r empty experiment memory empty state memory b', 1);('random initialexperimentseinit', 1);('differentiable environmentoutput experiment memory', 1);('interesting experiments1for\x122e initdo2s', 1);('current environment state3', 1);('binary result r4', 1);('tuple s\x12r tod5', 1);('states experiment', 1);('b6end', 1);('current environment state9\x12 c s10', 1);('binary result r11', 1);('tuple s\x12r tod12', 1);('current environment state13 forsome steps do14 sample tuple s\x12r fromd15', 1);('update', 1);('rwbcemws\x12r16 end for17 forsome steps do18 sample', 1);('state sfromb19', 1);('environment state s20', 1);('c sin environment', 1);('continuous resultrand external reward', 1);('re21 update', 1);('r \x00\x00bcemwsc sr\x00re\x0122 end for23', 1);('environment state s24until', 1);('interesting experiments foundin form', 1);('methodin', 1);('new setup', 1);('important differencesan experiment', 1);('e\x12is rnn', 1);('form ht1rt1 t1', 1);('e\x12ht', 1);('wherehtis hiddenstate vector rt2f01gis binary result experiment time step', 1);('haltunit', 1);('result rofe\x12is thertfor experiment step twhere trst', 1);('05since external environment experiments', 1);('independent themodelmwis', 1);('experiment parameters \x12asinput', 1);('result prediction omw\x12o201as', 1);('intrinsic reward signal', 1);('nondifferentiable meansthatin contrast method', 1);('section 31the controller', 1);('informationaboutmwfrom gradients', 1);('infer the9historylstmgid00038gid00087gid00079gid00068gid00081gid00072gid00076gid00068gid00077gid00083gid00049gid00081gid00068gid00067gid00072gid00066gid00083gid00072gid00078gid00077', 1);('resultintrinsic reward calculationgid00056gid00068gid00072gid00070gid00071gid00083gid00082gid00036gid00078gid00077gid00083gid00081gid00078gid00075gid00075gid00068gid00081gid00046gid00078gid00067gid00068gid00075rewardrnnmlpmlphistory encodingfigure', 1);('previous experiments controller generates', 1);('environment interactions', 1);('history ofprevious experiments results rewards0', 1);('30000experiments130135140145150155160165experiment runtime004005006007008information gainfigure', 1);('empirical', 1);('blue averageruntime experiment', 1);('information gain reward', 1);('mwfrom', 1);('previous experiments intrinsic rewards', 1);('new surprising experiments controller c', 1);('new experiments', 1);('history past experiments c \x12 historyis sequence tuples \x12iriri wherei', 1);('index experiment containsexperiments', 1);('appendix bfor', 1);('information gain', 1);('wbemsweights training', 1);('steps data', 1);('memory andw\x03the weights training theinformation gain reward', 1);('experiment \x12isrig\x12ww\x03 1jdjx\x122ddklmw\x03\x12jjmw\x12 3where', 1);('output model', 1);('bernoulli', 1);('discussionfigure', 1);('shows information gain reward', 1);('new experiment c generateswe observe', 1);('short initial phase intrinsic information gain reward', 1);('similar observe prediction accuracy section', 1);('forthe controller generate experiments surprise model', 1);('anatural effect sinceas model', 1);('new additionalexperiment contributes average', 1);('information gain reward10an', 1);('interesting albeit', 1);('minor effect', 1);('setup average runtimeof', 1);('experiments increases', 1);('negative reward forlonger', 1);('experiments shorter experiments', 1);('beginning yield learning progress andmore information gain', 1);('interestingin comparison experiments', 1);('present ones', 1);('shorterruntime sideeffect experiments', 1);('rnns halt', 1);('experiments average runtime', 1);('conclusion', 1);('workwe', 1);('controllermodel cm', 1);('framework notion arbitrary', 1);('computational experiments binary outcomes', 1);('experimental protocols essentiallyprograms', 1);('weight matrices', 1);('thecontroller model', 1);('outcome experiment', 1);('outcomes surprise model controller curiouslyexplores environment', 1);('system analogous scientist whodesigns experiments gain insights', 1);('physical world', 1);('experiment akin thoseof mathematicianswe', 1);('empirical evaluation', 1);('simple instances systems', 1);('complementary aspects idea rst setup show', 1);('abstract experiments', 1);('feedforward networks', 1);('continuous control environment facilitatethe discovery', 1);('furthermore', 1);('time controller', 1);('negative external reward asshort experiments', 1);('surprise modelin', 1);('setup controller generates', 1);('pure abstract', 1);('experiments form ofrnns observe time', 1);('experiments result', 1);('intrinsic informationgain reward', 1);('runtime hypothesize thisis', 1);('lot information gain', 1);('time interval', 1);('insight anymorethese', 1);('empirical setups', 1);('initial steps', 1);('capable systems asthe', 1);('scaling', 1);('complex environments generation', 1);('sophisticated experiments', 1);('direct', 1);('generation andinterpretation', 1);('effective large deep networks', 1);('previous', 1);('work 3already', 1);('similar innovations facilitate generation abstract', 1);('experiments beyondthe', 1);('small scale setups', 1);('acknowledgmentswe', 1);('grateful friends', 1);('useful comments work', 1);('europeanresearch', 1);('advanced grant', 1);('74287011references1 c', 1);('berner g brockman', 1);('chan v cheung p debiak', 1);('dennison farhi q fischer hashme', 1);('hesse r j', 1);('gray c', 1);('olsson j pachocki petrov h pde oliveira pinto j raiman salimans j schlatter j schneider sidor sutskeverj tang', 1);('wolski zhang dota', 1);('large scale', 1);('corr', 1);('burda h edwards pathak storkey darrell efros largescale', 1);('study ofcuriositydriven learning', 1);('faccio v herrmann ramesh', 1);('kirsch j schmidhuber goalconditioned', 1);('policies arxiv preprint arxiv220701570', 1);('faccio', 1);('kirsch j schmidhuber parameterbased', 1);('value functions', 1);('preprintarxiv200609226', 1);('faccio ramesh v herrmann j harb j schmidhuber', 1);('general policy evaluation andimprovement learning identify', 1);('crucial states arxiv preprint arxiv220701566', 1);('v v fedorov theory', 1);('optimal experiments', 1);('academic', 1);('gers j schmidhuber', 1);('cummins learning', 1);('continual', 1);('prediction withlstm', 1);('j gomez j koutn', 1);('j schmidhuber compressed', 1);('networks complexity search', 1);('inspringer', 1);('parallel problem solving nature ppsn', 1);('j gomez j schmidhuber evolving', 1);('modular fastweight networks control', 1);('inw duch j kacprzyk e oja zadrozny', 1);('articial neural networks biologicalinspirations icann', 1);('springerverlag berlin heidelberg200510 goodfellow j pougetabadie mirza', 1);('xu wardefarley ozair courville', 1);('bengio generative', 1);('adversarial nets', 1);('advances neural information processing systemsnips', 1);('graves fern', 1);('j schmidhuber multidimensional', 1);('recurrent neural networksinproceedings 17th', 1);('articial neural networks september200712 graves liwicki fernandez r bertolami h bunke j schmidhuber', 1);('novelconnectionist system', 1);('handwriting recognition', 1);('ieee transactionson pattern analysis machine intelligence', 1);('ha dai q v', 1);('hypernetworks', 1);('arxiv preprint arxiv160909106', 1);('j harb schaul precup pl bacon policy', 1);('evaluation networks arxiv preprintarxiv200211833', 1);('hochreiter j schmidhuber flat', 1);('hochreiter j schmidhuber', 1);('shortterm memory neural computation', 1);('based tr fki20795 tum', 1);('huffman', 1);('method construction minimumredundancy codes', 1);('proceedings ire', 1);('k irie schlag r csord j schmidhuber going', 1);('linear transformers recurrent', 1);('fast weight programmers', 1);('advances neural information processing systems', 1);('itti p', 1);('baldi bayesian', 1);('surprise attracts human attention', 1);('advances neural information processing systems nips', 1);('mit', 1);('cambridge', 1);('p kaelbling', 1);('littman', 1);('moore reinforcement', 1);('learning survey journal ofai research', 1);('kirsch j schmidhuber meta', 1);('learning backpropagation', 1);('information processing systems', 1);('n kolmogorov', 1);('approaches quantitative denition information', 1);('problems', 1);('transmission', 1);('j koutn', 1);('g cuccu j schmidhuber', 1);('gomez evolving', 1);('largescale neural networks', 1);('amsterdam july', 1);('acm24 j koutn', 1);('k f', 1);('gomez j schmidhuber evolving', 1);('annual', 1);('genetic evolutionary computation', 1);('kullback r leibler', 1);('information sufciency', 1);('annals mathematicalstatistics', 1);('li p', 1);('vit', 1);('introduction kolmogorov complexity applications2nd', 1);('p lillicrap j j hunt pritzel n heess erez tassa silver wierstracontinuous', 1);('reinforcement learning arxiv preprint arxiv150902971', 1);('openai andrychowicz', 1);('baker chociej r jozefowicz', 1);('mcgrew j pachockia petron plappert g powell ray j schneider sidor j tobin p welinder', 1);('wengand', 1);('zaremba learning', 1);('dexterous inhand manipulation', 1);('international journal ofrobotics research', 1);('py oudeyer baranes', 1);('kaplan intrinsically', 1);('real world sensorimotor skills developmental constraints', 1);('g baldassarre mirolli', 1);('intrinsically motivated learning', 1);('articial systems springer', 1);('pathak p agrawal efros darrell curiositydriven', 1);('proceedings ieee', 1);('computer vision patternrecognition', 1);('workshops pages', 1);('j pino sidorov n ayan transitioning', 1);('facebook', 1);('plappert r houthooft p dhariwal sidor r chen x chen asfour p abbeel', 1);('andrychowicz parameter', 1);('space noise exploration arxiv preprint arxiv170601905', 1);('ramesh', 1);('kirsch', 1);('steenkiste j schmidhuber exploring', 1);('random curiosity general value functions', 1);('h oh agarwal belgrave k cho', 1);('j rissanen modeling', 1);('shortest data description', 1);('automatica', 1);('felder j schmidhuber statedependent exploration', 1);('policy gradientmethods w', 1);('european conference', 1);('machine learning ecml', 1);('andprinciples practice', 1);('knowledge discovery databases', 1);('part ii lnai', 1);('h sak senior k rao', 1);('beaufays j schalkwyk google', 1);('voice search', 1);('google', 1);('schlag k irie j schmidhuber linear', 1);('fast weight programmersininternational conference', 1);('schlag j schmidhuber learning', 1);('order tensor products', 1);('advancesin', 1);('j schmidhuber making', 1);('dynamic reinforcement learning planning nonstationary environmentstechnical', 1);('fki12690', 1);('httppeopleidsiach juergenfki12690revisedbwocrpdf', 1);('tech univ munich', 1);('online algorithm', 1);('dynamic reinforcement learning planning reactive environments', 1);('proc ieeeinns', 1);('neural networkssan diego', 1);('j schmidhuber curious', 1);('control systems', 1);('proceedings internationaljoint', 1);('dynamic links', 1);('proc internationaljoint', 1);('curiosity boredom', 1);('neural controllers', 1);('j meyer', 1);('wilson', 1);('simulation adaptive behavior animals animats', 1);('mitpressbradford', 1);('j schmidhuber reinforcement', 1);('markovian', 1);('nonmarkovian environments', 1);('ind lippman j e moody touretzky', 1);('advances neural informationprocessing systems', 1);('morgan kaufmann', 1);('control fastweight memories', 1);('alternative recurrent netsneural', 1);('computation', 1);('ratio learning complexity number', 1);('recurrent nets', 1);('international conference onarticial', 1);('neural networks amsterdam', 1);('prieditis russell', 1);('machine learning proceedings', 1);('international conference pages', 1);('morgan kaufmann publishers san franciscoca', 1);('neural nets', 1);('j schmidhuber whats', 1);('idsia3597 idsia', 1);('proc snowbird98', 1);('j schmidhuber articial', 1);('novel algorithmic predictabilitythrough coevolution', 1);('p angeline z michalewicz schoenauer x yao z zalzalaeditors congress evolutionary computation', 1);('j schmidhuber exploring', 1);('ghosh tsuitsui', 1);('computing', 1);('j schmidhuber hierarchies', 1);('nonenumerable universal measures', 1);('computable limit', 1);('international journal', 1);('foundations computerscience', 1);('j schmidhuber overview', 1);('articial curiosity', 1);('active exploration links publicationssince', 1);('j schmidhuber developmental', 1);('robotics optimal articial curiosity creativity music thene arts', 1);('j schmidhuber simple', 1);('algorithmic principles discovery subjective beauty selective attention curiosity creativity', 1);('intl conf algorithmic learning theory alt2007 lnai', 1);('alt', 1);('ds2007 sendai japan', 1);('j schmidhuber compression', 1);('progress algorithmic principle', 1);('curiosity creativity applications theory humor', 1);('min video', 1);('summit', 1);('york', 1);('city httpwwwvimeocom7441291', 1);('min excerptshttpwwwyoutubecomwatchvipomu0mlfai57', 1);('j schmidhuber driven', 1);('compression progress', 1);('simple principle', 1);('essential aspects subjective beauty novelty surprise interestingness attention curiosity creativity artscience music jokes', 1);('g pezzulo v butz sigaud g baldassarre', 1);('anticipatory behavior adaptive learning systems psychological theories articialcognitive systems', 1);('j schmidhuber formal', 1);('theory creativity fun intrinsic motivation', 1);('ieeetransactions autonomous mental', 1);('j schmidhuber overviews', 1);('articial curiositycreativity', 1);('active explorationwith links publications', 1);('j schmidhuber selfdelimiting', 1);('idsia0812arxiv12100118v1', 1);('ai lab idsia', 1);('j schmidhuber p ower play training increasingly', 1);('problem solver continuallysearching simplest', 1);('problem frontiers psychology', 1);('based', 1);('onarxiv11125309v1 csai', 1);('learning neural networks overview', 1);('tr', 1);('arxiv14047828 csne63', 1);('algorithmic', 1);('information theory novel combinations reinforcement learning controllers recurrent neural world models', 1);('preprintarxiv151109249', 1);('arxiv180208864 csai', 1);('february', 1);('learning miraculous year', 1);('j schmidhuber generative', 1);('adversarial networks', 1);('special cases articial curiosity 1990and', 1);('predictability minimization', 1);('abstract bit time', 1);('unpublished tech', 1);('idsia nnaisense', 1);('j schmidhuber articial curiosity creativity', 1);('httpspeopleidsiach juergenartificialcuriositysince1990html', 1);('ai blog202169 j schmidhuber scientic integrity history deep learning', 1);('turing lecture', 1);('turing', 1);('award httpspeopleidsiach juergenscientificintegrityturingawarddeeplearninghtml', 1);('ai blog', 1);('e shannon', 1);('mathematical theory communication parts', 1);('ii bell', 1);('technicaljournal xxvii379423', 1);('h siegelmann e sontag turing', 1);('computability neural nets', 1);('applied mathematicsletters', 1);('singh g barto n chentanez intrinsically', 1);('inadvances neural information processing systems', 1);('nips mit', 1);('cambridge ma200573 r j solomonoff', 1);('formal theory inductive inference', 1);('part information', 1);('r k srivastava', 1);('r steunebrink j schmidhuber', 1);('powerplayneural networks', 1);('autonomous learning1675 j storck hochreiter j schmidhuber reinforcement', 1);('information acquisition innondeterministic environments', 1);('articialneural networks paris', 1);('ec2 cie', 1);('sun', 1);('gomez j schmidhuber', 1);('optimal bayesian', 1);('dynamic environments', 1);('proc fourth', 1);('articial', 1);('intelligence agigoogle mountain', 1);('ca', 1);('r sutton barto reinforcement', 1);('learning introduction', 1);('cambridge mit press199878', 1);('steenkiste j koutn', 1);('k driessens j schmidhuber', 1);('york ny usa', 1);('acm79 vemula', 1);('sun j bagnell contrasting', 1);('exploration parameter action spacea zerothorder optimization perspective 22nd', 1);('articialintelligence statistics', 1);('vinyals babuschkin', 1);('czarnecki mathieu dudzik j chung h choir powell ewalds p georgiev', 1);('grandmaster', 1);('starcraft ii', 1);('multiagentreinforcement learning', 1);('nature', 1);('bringing magic amazon ai alexa apps aws', 1);('distributed', 1);('wallace boulton', 1);('information theoretic measure classication', 1);('computerjournal', 1);('wallace p r freeman estimation', 1);('journal theroyal', 1);('statistical', 1);('society series b', 1);('wiering', 1);('otterlo reinforcement learning springer', 1);('wierstra foerster j peters j schmidhuber recurrent', 1);('policy gradients', 1);('logicjournal igpl', 1);('r j williams simple', 1);('algorithms connectionist', 1);('h witten r neal j g cleary arithmetic', 1);('data compression', 1);('communications acm', 1);('wu schuster z chen q v', 1);('norouzi', 1);('macherey krikun cao q gaok macherey j klingner shah johnson x liu', 1);('kaiser gouws kato kudoh kazawa k stevens g kurian n patil', 1);('wang', 1);('j smith j riesa rudnicko vinyals g corrado hughes j dean googles', 1);('gap human machine translation', 1);('arxiv160908144 201617a', 1);('environmentthe', 1);('force eld environment', 1);('2d grid', 1);('force vectors geta', 1);('continuous force eld bicubic interpolation vectors grid', 1);('theresolution grid inuences complexity force eld', 1);('resolution intricateforce eld experiments grid resolution', 1);('f335577gthe random seed run affects force eld position goal state meansthat', 1);('unique environmenta1', 1);('experiment executionletrt201be', 1);('value result node step tof experiment', 1);('runtime determinedby parameter', 1);('maximum runtime', 1);('steps distribution overexperiment steps tis', 1);('followsp t exp\x0005t\x00 2p100u1exp\x0005u\x00 2the', 1);('continuous result experiment expectation result unit distributionret\x18p rt binary result experiment ris boolean value r05a2', 1);('hyperparameters', 1);('shows hyperparameters', 1);('output nodes c', 1);('generate parameters experiment network tanh output nonlinearity', 1);('predenedrange output node generates', 1);('range 0100the experiment parameters random baselines', 1);('tanhv wherev\x18n04i runtime parameter', 1);('range hyperparameters model', 1);('baseline external reward', 1);('usesthe hyperparameters', 1);('difference setting loss c', 1);('simplylces\x18b\x00rc ssinstead', 1);('rate c 00003weight decay', 1);('2noise input nodes c 8environment grid resolutions', 1);('7number iterations 1000number', 1);('1random longrandom', 1);('external rewardfigure', 1);('additional baseline', 1);('short random experiments anaverage runtime', 1);('additional resultsto', 1);('account potential bias', 1);('due experimental runtime', 1);('number ofgoal states baseline shorter random experimentsb', 1);('pure thought experimentsalgorithm', 1);('summarizes method', 1);('setup model', 1);('trainedto minimize', 1);('losslme\x12r\x18dbcemw\x12r 4efcient approximation policy gradients controller', 1);('controller c', 1);('encoder generatesa', 1);('representation history', 1);('previous experiments results', 1);('input history representation createdby', 1);('generates weights experiment', 1);('whereas critic', 1);('ahistory representation experiment weights input outputs scalar reward estimation', 1);('actorand', 1);('critic share', 1);('history encoder', 1);('gradient descent steps', 1);('history encoder sequence', 1);('beenexecutedthe experiment', 1);('rnns e\x12used', 1);('empirical evaluation 3hidden units inputs', 1);('theinitial', 1);('state h0is', 1);('part parameters \x12and', 1);('randomexperiments', 1);('a2', 1);('table 219algorithm', 1);('rnnsinput randomly', 1);('sequences ofthe form \x12iriri\x12i1ri1ri1', 1);('model mw', 1);('\x02rempty sequential experiment memory', 1);('einitoutput', 1);('experiment memory', 1);('experiments1for\x122e initdo2', 1);('binary result r3', 1);('tuple \x12rtod4', 1);('updatedweights w\x035', 1);('ririg\x12ww\x03equation', 1);('36w w\x037', 1);('saveritod8end', 1);('for9repeat10 sequence', 1);('d11\x12', 1);('binary result r13', 1);('weights w\x0314', 1);('ririg\x12ww\x0315w', 1);('saveritod17 trainc', 1);('intrinsic reward18until', 1);('interesting experiments foundhyperparameter', 1);('64hidden layers c', 1);('rate c 00001weight decay', 1);('3number iterations 30000number', 1);