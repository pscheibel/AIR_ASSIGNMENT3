('algorithm', 29);('irl', 14);('f l', 9);('rl', 8);('fig', 8);('ktin', 7);('theorem', 7);('pi', 5);('nash', 5);('kin', 4);('update', 4);('control vol', 4);('proc', 4);('ioc', 3);('gare', 3);('r0', 3);('unique solution', 3);('substituting', 3);('qi1in', 3);('datadriven', 3);('automatica', 3);('inverse reinforcement learning', 2);('zerosum game', 2);('policy iteration', 2);('lyapunov', 2);('t0', 2);('learner', 2);('learning', 2);('lin', 2);('algorithm1', 2);('i0step 2game policy correction', 2);('stop', 2);('otherwise', 2);('ii1 repeatsteps', 2);('algorith', 2);('convergence', 2);('converged', 2);('lemma', 2);('qtrttpt', 2);('algori', 2);('rlbased', 2);('comparison', 2);('imitation performance', 2);('h modares', 2);('lewis z jiang h', 2);('ieee trans autom', 2);('online', 2);('pro', 2);('acc', 2);('l b', 2);('xue', 2);('arxiv230101997v1 cslg', 1);('jan', 1);('zerosum gameswenqian xue bosen lian jialu fan tianyou chai frank', 1);('lewisabstract', 1);('paper formulate inverse', 1);('expertlearner interaction', 1);('th eoptimal performance intent expert target agent isunknown learner agent learner observes statesand', 1);('reconstruct theexperts', 1);('function intent', 1);('mimics experts ptimalresponse', 1);('noncooperative disturbances eek todisrupt learning stability learner agent', 1);('thi', 1);('leadsto formulation', 1);('new interaction', 1);('zerosum gameirl', 1);('irlproblem', 1);('p ito', 1);('unknown expert performance intentions comput edand noncooperative disturbances', 1);('frame workhas', 1);('parts value function control action update bas edon', 1);('cost function update', 1);('onstandard inverse optimal control', 1);('deve lopan offpolicy', 1);('knowledgeof expert learner agent dynamics performs', 1);('loop learning', 1);('rigorous', 1);('proofs analyses', 1);('fina', 1);('llysimulation experiments', 1);('effective nessof', 1);('new approachi', 1);('ntroductionfor', 1);('agent system', 1);('disturbances control input defender desires', 1);('con trolmission', 1);('control policy reject inuenc esof antagonistic input ie noncooperative disturbance thatintend disrupt mission', 1);('zerosum game sor minmax problems', 1);('realworld applications gentdynamics', 1);('unknown order', 1);('agentperform target trajectories', 1);('target age ntwith optimal policy optimal control theory assumes th eperformance cost function', 1);('control methods', 1);('compute optimalpolicy', 1);('states control actions', 1);('ingthe system dynamics', 1);('standard iterative form', 1);('rlis', 1);('real interactions operators', 1);('cost functions ie weights states inp utsas result optimal control methods', 1);('control performance', 1);('xue jialu fan tianyou chai', 1);('key', 1);('boratory ofsynthetical', 1);('automation process', 1);('interna', 1);('joint researchlaboratory integrated automation northeastern univer', 1);('shenyang110819 china', 1);('email xuewenqian23163com fanjialu gmailcom tychaimailneueducnbosen', 1);('lian frank', 1);('lewis uta', 1);('research institute university', 1);('texas arlington texas', 1);('usa', 1);('emailbosenlianmavsutaedu lewisutaeduinstead', 1);('cost function weights manyefforts', 1);('cost function weight sinverse optimal control', 1);('rl irl', 1);('construc tcost function weights', 1);('system control behaviors times', 1);('th eymay differ structure', 1);('stable control system', 1);('constructs costfunction', 1);('system behavior optimalthe cost function', 1);('lyapuno', 1);('vstability condition continuoustime', 1);('ct', 1);('17and discretetime', 1);('dt', 1);('consid ersnite horizon', 1);('online ioc', 1);('cost functio nin innite nite horizon', 1);('io cis', 1);('cont rollaws', 1);('minmaxor zerosum games', 1);('systems withunknown dynamicsirl', 1);('reconstructs reward cost functions fromexpert demonstrations optimal policy', 1);('apprenticeship learning imitation learning problems', 1);('markov', 1);('decision processes', 1);('mdps', 1);('2629where learner', 1);('imitate demonstrations', 1);('unknown experts reward function observeddemonstrations', 1);('methods construct reward function sin cereward function succinct robust transferabl edenition task policy', 1);('states toactions', 1);('andimitation problems differential systems', 1);('w here30 uses bilevel structure', 1);('th', 1);('isan optimal control problem', 1);('innerloop twoloop iteration', 1);('expensiv e', 1);('allof', 1);('minmaxor zerosum games work', 1);('effort datadriven control', 1);('model parameters', 1);('model identication', 1);('previous studies', 1);('merelysystem data zerosum games work 36considers zerosum games', 1);('method usinga twoloop iteration structure partial system dynamics paper considers expertlearner zerosum game thatis learner agent', 1);('noncooperative disturb anceswith', 1);('unknown dynamics', 1);('mimic behaviors theexpert agent optimal policy solution', 1);('anew interaction', 1);('noveldatadriven offpolicy', 1);('algorithm expertlearner z ero2sum games differential systems', 1);('gamesolution correction', 1);('costfunction weight reconstruction', 1);('ioc usi', 1);('behavior data expert learner learneragent learns', 1);('unknown cost function objective theoptimal control policy mimic experts behavior', 1);('thisalgorithm', 1);('identify system modelsand performs singleloop learning procedure', 1);('solv ingoptimal problems', 1);('inner loops', 1);('theproperties', 1);('datadriven rewellanalyzednotations bardblbardblis', 1);('euclidean', 1);('inisnnidentity', 1);('matrix diag abis diagonal matrix abin diagonalline vector xx1xntrn xdefinesx21 2x1x22x1xnx22 2xn1xnx2nt matrix', 1);('aai', 1);('irl p roblem formulationwe', 1);('dynamical agents target expert agentexhibits demonstrations', 1);('e xpert performance cost function learner agent attempts todetermine', 1);('unknown cost function objective expertagent mimic behavior learner agent knowsthe target agents control actions state behavior esnot', 1);('performance cost function system dynamicsa', 1);('target', 1);('optimal controlconsider target expert agentxtaxtbutddt 1where xtrnis target state utrmis target inputanddtrzis noncooperative disturbance', 1);('matrices abanddhave', 1);('appropriate dimensions pair', 1);('abis', 1);('input target', 1);('utktxtthat minimizes', 1);('target performance cost funct ionagainst dtvtxtintegraldisplaytxttqtxtuttrtut2tdttdtd 2where', 1);('qtqtt0 rtrtt0', 1);('isattenuation factor input utand', 1);('disturbance dtare', 1);('byutktxtr1tbtptxt 3adtltxt12tdtptxt 3band form', 1);('equilibriumvtxtminutmaxdtintegraldisplaytrtdxttptxt 4where rtxttqtxtuttrtut2tdttdt', 1);('ptptt0satises', 1);('bellman', 1);('equationaxtbutddttptxtxttptaxtbutddtxttqtxtuttrtut2tdttdt0 5and target game algebraic', 1);('riccati', 1);('gareatptptaqtptbr1tbtpt12tptddtpt06the gare', 1);('guarantees uniqueness', 1);('optimalcontrol policy 3a', 1);('performance cost function', 1);('control problemconsider learner agent controlledxaxbudd 7where xrnurmdrzare learners state controlinput disturbance respectivelyassumption', 1);('target cost function', 1);('vt2', 1);('targetcontrol policy 3a', 1);('unknown learner', 1);('qtrttinvt', 1);('optimal strategy', 1);('disturbance dtltand', 1);('ptare', 1);('target behaviour dataxtutanddtdenition', 1);('expertlearner zerosum game', 1);('usingthe behaviour data expert', 1);('learner desires reconstruct', 1);('unknown performancecost function', 1);('exhibit control actions utin3a states xtas expert', 1);('squarethe learner', 1);('wayas target', 1);('learner 7with', 1);('hence', 1);('control goal determinethe', 1);('unknown cost function objective', 1);('optim alcontrol input ukxwith', 1);('kktusing', 1);('target dataofxtutdtand learner data xudiii', 1);('odel based irl', 1);('rameworkin', 1);('irlframework', 1);('cost function 2and use knowledge compute control input ut suchthat behavior trajectories utxtmimic observedtarget trajectories uttxtt section', 1);('nallypropose datadriven', 1);('algorithm need anysystem dynamicsa', 1);('optimal controllet', 1);('review zerosum game', 1);('learner7 arbitrary', 1);('cost functionvxintegraldisplaytxtqxutru2dtdd 8where', 1);('qqt0rrt0', 1);('optimal inputuoand', 1);('byuokxr1btpx 9dwlx12dtpx 103anduodwforms', 1);('equilibriumvxminumaxdintegraldisplaytrdxtpx 11where rxtqxutru2dtd', 1);('ppt0', 1);('gareatppaqpbr1btp12pddtp0', 1);('expertlearner', 1);('zerosum game solutionwe', 1);('present theorem show conditions thesolution expertlearner zerosum game', 1);('cost weights', 1);('q rand', 1);('equationabkttppabktq12pddtpkttrkt0 13then', 1);('corresponding control strategy', 1);('9equals thetarget strategy', 1);('k tin3aproof rewrite', 1);('asabkttppabktq12pddtpkttrkktrktktrk0 14by', 1);('havekttrktkttrkktrktktrkktktrktk0 15since', 1);('kkt', 1);('function game control policyto nd', 1);('qrpsatisfying theorem', 1);('select 0and', 1);('iterative procedure', 1);('game controlpolicy', 1);('pusing', 1);('setcurrent', 1);('iteration step ii01and', 1);('qiandli', 1);('iterative form', 1);('optimal control 9and', 1);('update strategy', 1);('ki1andli1basedon', 1);('piby', 1);('17a 17b respectivelynow', 1);('update cost function weight estimate', 1);('qi1based', 1);('pi ioc', 1);('ioccomputation', 1);('stability condition', 1);('qiqtandktkiin', 1);('rl pi algorithm', 1);('modelbased irl', 1);('algorithm expertlearnerzerosum gamesstep', 1);('initialize r0', 1);('l00and', 1);('pibyabkttpipiabktqikttrkt2litli', 1);('16step 3input update', 1);('control input disturbancegain', 1);('10ui1ki1xr1btpix 17adi1li1x12dtpix 17bstep 4cost function weight construction', 1);('update qi1byqi1atpipiaki1trki12li1tli1', 1);('nalysis algorithm', 1);('1the convergence stability optimality proposedalgorithm', 1);('piqieven', 1);('target strategy', 1);('kikta convergence', 1);('q0such', 1);('0q0q andl00where', 1);('q0is', 1);('r0and0 algorithm', 1);('converges ii thesolutions', 1);('qipikiliconverge qpklthat', 1);('satisfyatppaqpbr1btp12pddtp019akktr1btp 19bl12dtp 19ciii solutions', 1);('qpklsatisfy theorem', 1);('qifor', 1);('kiinto', 1);('qiyieldsatpi1pi1akitrkiqi2litli', 1);('20which rewritten', 1);('givesabkttpi1pipi1piabktkikttrkikt 22it', 1);('kikttrkikt0 hurwitzabktthat pi1piholds', 1);('iterations pair of4piqi1satises', 1);('correspond eachother', 1);('pi1piifqiqi137', 1);('qi1qi0', 1);('qi1qiholds ktki1', 1);('ktki1is', 1);('goal algorithmnow show', 1);('qiis', 1);('letq0 p0', 1);('group solution', 1);('thatisatppaqpbr1btp12pddtp0', 1);('23b yieldsabkttppabktqkttrkt2ltl024ifqi2litliq2ltlholds', 1);('willsolve 0pipwith', 1);('hurwitz abkt', 1);('3a 17a and23b', 1);('ares', 1);('23a rewritten asabki1tpipiabki1qi1ki1trki12li1tli1', 1);('hurwitz abki1', 1);('25a 25b', 1);('pipobtainsabki1tppi ppiabki1qi12li1tli1q2ltlki1kttrki1kt0', 1);('qi12li1tli1q2ltlholdsby', 1);('w ithaq0such', 1);('q0qandl00 qi0i01will', 1);('m1 convergesii', 1);('19a 19b 19c', 1);('pi1pipas', 1);('becomesktkrktk0 28where', 1);('kr1btp', 1);('ktkwhich', 1);('pproduces', 1);('lusing', 1);('19a 19b yieldsabkttppabktqkttrkt12pddtp0 29which', 1);('obviously', 1);('qpksatisfy theorem', 1);('stability', 1);('optimality analysiswe', 1);('3and optimality', 1);('exponentiallystabilizes learner agent 7with d0proof', 1);('rewriting', 1);('3a 17a yieldsatpipiaqipibr1btpi2litli0 30where', 1);('qiqiki1kttrki1kt withki1kttrki1kt0 qi0 qi0', 1);('pisolved', 1);('symmetricalpositive denite matrix satisfyingatpipiapibr1btpi2litli0 31and', 1);('piyields', 1);('ki1by', 1);('17a learner', 1);('qi12li1tli10', 1);('q00', 1);('32will hold i01 squarebefore optimality analysis', 1);('alemma importance', 1);('ioc15', 1);('twoplayer zerosum gameslemma', 1);('consider', 1);('twoplayer learner agent', 1);('withxt0 x0tt0and', 1);('assume', 1);('exists apositive denite symmetric matrix', 1);('prnnsuch', 1);('thatatppapbr1btp12pddtp0 34then optimal feedback control input uoand worstdisturbance dwsuch thatuor1btpxdw12dtpx 35and cost function weightqatppapbr1btp12pddtp 36the saddle point uodwmakes', 1);('value function 8reach', 1);('equilibriumvx0uodvx0uodwvx0udw 37proof', 1);('vxin', 1);('p0', 1);('asvxxtpx0v00 38it', 1);('hxuodw0', 1);('hamiltonian', 1);('ishxudxtqxutru2dtdaxbuddtpxxtpaxbudd 395one writeshxudhxudhxuodwuuoruuo2ddwtddw40and hencehx0uodhx0uodwhx0udw', 1);('qpklobtainedby algorithm', 1);('equilibriumof value function', 1);('xin8such thatvx0udvx0udvx0ud 42where ukx dlxproof', 1);('qi0', 1);('q0', 1);('andatppapbr1btp12ptddtp043from 19a', 1);('psatises', 1);('control strategy', 1);('19band disturbance gain', 1);('thisindicates', 1);('nashequilibrium', 1);('nonuniqueness', 1);('solutionin fact', 1);('qrpsatisfying', 1);('19a19c explainthe strategy', 1);('kktmay', 1);('unique bedifferent', 1);('actual target values', 1);('qtrttptshownin', 1);('multisolution phenomenon knownas illposedness property', 1);('tare', 1);('resultwe characterize relationship', 1);('ct gare', 1);('show conditions theoccurrence phenomenontheorem', 1);('recall q rtt ptsatisfying', 1);('6and 3band', 1);('q ro posatisfybtporor1tbtpt', 1);('44aqoatpopoakttrokt12tptddtpt12pddtp0 44bwhere', 1);('qqtqoand pptposatisfy', 1);('19a 19c', 1);('subtracting', 1);('pptporortr r0', 1);('44a giveskr1btpr1tbtptkt 46which 19b', 1);('yields 19a', 1);('9cthis proves relationship', 1);('solutionqrpand experts', 1);('44a 44b nonzero', 1);('thatisqrpassociate', 1);('strategy asqtrttpt ie', 1);('ktk qtneationslashqrtneationslashrtneationslashtherefore', 1);('multiple solutions 19a gen erate', 1);('equal target', 1);('3a squarethe', 1);('corollary shows', 1);('special case', 1);('qrof theorem', 1);('vxcvtxtcorollary', 1);('scalar c0', 1);('qcqtrcrtctwould', 1);('vxcvtxtin', 1);('kas', 1);('expert thatkktin 19b 3aproof', 1);('bring qrinto', 1);('qtrttsatisfy6', 1);('pcptand kkt', 1);('vxand', 1);('vtxtin', 1);('vxcvtxt', 1);('atadriven offpolicy irl lgorithmalgorithm', 1);('relies system dynamics', 1);('abdand', 1);('thetarget strategy', 1);('kt', 1);('requirement develophere datadriven', 1);('algorithm expertlearner zeros umgames', 1);('dataxtutdtof target agent', 1);('xudof learneragent', 1);('techniques similarto integral', 1);('game policy correctionin order update', 1);('piki1and li1in', 1);('target data xtutdt', 1);('idea offpolicyintegral', 1);('17a 17b 3a', 1);('sides f romttott', 1);('integral time period obtains50', 1);('piki1andli1are', 1);('ut ie utktxte persistenceof excitation condition', 1);('learning process', 1);('solutions converge', 1);('need knowledge agent dynamics', 1);('abdor', 1);('cost function weight reconstructionin order update', 1);('terms utbtpixanddtdtpix', 1);('rewritten asxtqi1xaxbuddtpixxtpiaxbuddxtki1trki1x2xtli1tli1x2utbtpix2dtdtpix 49where ucan', 1);('policy dcan random', 1);('different dtin learning', 1);('17a 17b', 1);('piki1and li1obtained', 1);('thm1 calculate', 1);('qi1without', 1);('system dynamicsalgorithm', 1);('algorithm expertlearner zerosum gamesstep', 1);('initialize r00q00 l00and', 1);('system data', 1);('stabilizingcontrol input u', 1);('set', 1);('ki1and', 1);('disturbance gain', 1);('li1byxttttpixtttxtttpixtt2integraldisplaytttetrki1xtd22integraldisplaytttdttli1xtdintegraldisplaytttparenleftbigxttqixtutetrute2xttlitlixtparenrightbigd', 1);('50step 3cost function weight construction', 1);('costfunction weight', 1);('qi1byintegraldisplaytttxtqi1xdxtttpixttxttpixtintegraldisplayttt2utrki1xxtki1trki1xd2integraldisplayttt2dtli1xxtli1tli1xd', 1);('remark', 1);('need system dynamicsmoreover iterates', 1);('innerloop iteration neededc implementation', 1);('analysis algorithm', 1);('2in order show', 1);('irl algorithm', 1);('data rst', 1);('kronecker', 1);('productatw b btatvecwfor', 1);('dene followingoperatorsoxtxtx2t12xt1xt2x2t22xt2xt3x2t ntdxtxtoxtxtttoxtxttoxtxttltoxtxttl1ttixtxtintegraldisplaytttoxtxtdintegraldisplaytlttl1toxtxtdtixtutintegraldisplaytttxtutdintegraldisplaytlttl1txtutdtixtdtintegraldisplaytttxtdtdintegraldisplaytlttl1txtdtdtixteintegraldisplaytttxtedintegraldisplaytlttl1txtedtpdxtxt2ixteinr22ixtdttrixttqixtutetrute2xttlitlixtipintegraldisplaytttridintegraldisplaytlttl1tridtpipi11pi12pi22pi23pinnt 52where lis group number', 1);('data belnn12nmnz', 1);('squares method', 1);('piki1li1can', 1);('deneoxxx212x1x2x222x2x3x2ntdxxoxxttoxxtoxxtktoxxtk1ttqiqi11qi12qi22qi23qinntqixxintegraldisplaytttoxxdintegraldisplaytkttk1toxxdtii1qtintegraldisplayttt2utrki1xxtki1trki1xd2integraldisplayttt2dtli1xxtli1tli1xdi1qdxxpiii1qtii1qtk1tt 54where kis group number', 1);('data beknn12', 1);('qi1can', 1);('byqi1tqq1tqi1q 55by', 1);('piqi1ki1li1in', 1);('adatadriven modeeach step', 1);('thatthe solution', 1);('and55 yield', 1);('unique solutions', 1);('l o0 ko0 l loandkkorankixtxtixtutixtdtnn12nmnz 56arankixxnn12 56bthen', 1);('unique solution respectivelyproof', 1);('thisis', 1);('show thatp0 57has', 1);('trivial solution', 1);('0by contradiction', 1);('assume xtvytvztvtrnn12nmnzis', 1);('nonzero solution', 1);('xvrnn12yvrmnzvrnz xvyvzvuniquely', 1);('xyzbyxvxyvvecyandzvvecz', 1);('wherexis symmetrical matrixdeneinxintegraldisplaytttxtxtdintegraldisplaytkttk1txtxtdt', 1);('ttottgivespinxvece2ixtutvecg2ixtdtvecf059whereeatxxakttryytrkt 60agbtxry 60bfdtx2z 60csince', 1);('eis', 1);('symmetrical matrix', 1);('inxveceixtxteusing', 1);('yieldspixtxt2ixtut2ixtdtevecgvecf0 61under 56a', 1);('ixxixuixdhas', 1);('full column rankand', 1);('e0', 1);('andvecf0 isatxxakttryytrkt0 62abtxry 62bdtx2z 62csince', 1);('abktis hurwitz', 1);('62b 62c into62a', 1);('x0', 1);('y0 z0', 1);('due tor0', 1);('thisconicts assumption nonzero', 1);('itconcludes 56a', 1);('unique solutionsecond integral', 1);('conclude thatwhen 56b', 1);('data squarevi', 1);('imulationwe', 1);('simulation experiments rst', 1);('performance comparison simulation bilevel', 1);('toshow reduction iteration steps', 1);('comparison simulation', 1);('control method', 1);('improvement ofcontrol performance cost function weights', 1);('simulation', 1);('2the system dynamics information target', 1);('simulation isabracketleftbigg1', 1);('17bracketrightbiggbbracketleftbigg03bracketrightbiggdbracketleftbigg10bracketrightbigg 63for target agent', 1);('actual target cost function obj ective', 1);('qtdiag812rt2i1t3 thedisturbance', 1);('dt0003rand1 experts', 1);('ltktandptarekt1986935779lt0416201472ptbracketleftbigg37459', 1);('23853bracketrightbigg 64for learner behaviour strategy generate dataiskb', 1);('disturbance d0003rand1', 1);('initial weights costfunction', 1);('initial disturbance gain', 1);('l0', 1);('integral time periodtare', 1);('1a captures iteration process initia lspot spot bardblki1ktbardbl001 nal values', 1);('kiqili piarekql p', 1);('followskbracketleftbig19827 35839bracketrightbigqbracketleftbigg22796', 1);('60151bracketrightbiggl103bracketleftbig04021 04131bracketrightbigpbracketleftbigg06441', 1);('11968bracketrightbigg 66where', 1);('kclosely', 1);('approximates target', 1);('qland pare', 1);('equal toqtltand', 1);('ptin', 1);('multiplesolution phenomenon', 1);('1b learners state xcanmimic trajectories target xtvery', 1);('anappropriate cost function optimal policy learne r tomimic target trajectoriesb', 1);('simulation case 1this subsection shows simulation results bilevelirl method', 1);('iterates twoloop show thereduction computational complexity terms iteratio nsteps expertlearner system', 1);('initial parameters for80', 1);('50update steps ia3436pipt0', 1);('thm 2algorithm', 1);('comparison method1fig', 1);('captures iteration process initialspot spot bardblki1ktbardbl001', 1);('thedifference iteration steps', 1);('methods innerloop iteration gures', 1);('nal values', 1);('kjqjlj pjofthe', 1);('outerloop iterations arekbracketleftbig19822 35691bracketrightbigqbracketleftbigg23186', 1);('60506bracketrightbiggl103bracketleftbig04053 04130bracketrightbigpbracketleftbigg06486', 1);('11897bracketrightbigg 67where', 1);('kapproximates', 1);('table shows total iteration steps methodis', 1);('outerloop updates', 1);('and2783 innerloop updates', 1);('step sin total', 1);('1a time learning process ofalgorithm', 1);('408s comparison method', 1);('is169936s proportional', 1);('colle cteddata', 1);('groups data comparisonmethod', 1);('groups data', 1);('data time bilevel comparisonmethod', 1);('600outerloop update steps j3436pjptfig', 1);('convergence kjqjlj pjusing', 1);('comparison method 1table', 1);('iiteration steps learning time algorithm', 1);('thecomparison method', 1);('1methods total updates', 1);('408scomparison method', 1);('simulation case 2in subsection', 1);('optimal trackingcontrol method', 1);('systems computes optimal control policy', 1);('cost function weights', 1);('show advantage', 1);('optimal control policy andcost function weightsthe system cost weights', 1);('anddiscount factor', 1);('optimal controllaw u11760', 1);('thetrajectory data imitation performance', 1);('metho dsis', 1);('error index', 1);('followste1nni1radicalbigg1aak1xiktxtikt2where n2a250', 1);('t0008s', 1);('iiteof algorithm', 1);('thegiven cost function weights', 1);('inappropriate th ecomparison method', 1);('obtains muchbetter imitation performancetable', 1);('iiimitation index algorithm', 1);('comparison method', 1);('error', 1);('tealgorithm', 1);('00162comparison method', 1);('imitation', 1);('performance learners xto target', 1);('comparisonmethod 2vii c', 1);('onclusionthis', 1);('novel datadriven offpolicy', 1);('irlapproach', 1);('cost function optimal control policy stabilize learner agent', 1);('noncooperative disturbances', 1);('target agents tra jectories', 1);('data agents', 1);('approachdoes need system dynamics guarantees stabilitynash optimality imitation performance singlelo opiteration rigorous theoretical proofs simulation experiments', 1);('bas', 1);('p bernhard hinnity', 1);('optimal control', 1);('minimaxdesign problems', 1);('dynamic game approach', 1);('springer', 1);('science business', 1);('media', 1);('lewis vrabie v syrmos optimal', 1);('john wiley', 1);('inc hoboken nj usa', 1);('g reinforcement', 1);('learning introduction', 1);('mitpress cambridge usa', 1);('lewis vrabie reinforcement', 1);('feedback control', 1);('ieee circ syst', 1);('mag vol', 1);('r g carrillo k vamvoudakis deeplearning', 1);('systems adversarial inputs', 1);('ieee transaerosp electron syst', 1);('unknown continuoustime systems', 1);('ieee trans neural netw learn syst', 1);('kiumarsi', 1);('control linear discretetime systems', 1);('offpolicy', 1);('reinforcement learning', 1);('vol 78pp', 1);('lewis linear', 1);('co ntrol ofpartiallyunknown continuoustime systems', 1);('reinfor cement learning', 1);('kleinman', 1);('iterative technique riccati equa tion computations', 1);('x chen monfort', 1);('ziebart p carr adversa', 1);('rial inverseoptimal control general imitation learning losses e mbodimenttransfer', 1);('conf uncertain artif intell', 1);('r self k coleman h bai r kamalapurkar onlin', 1);('inverse reinforcement learning arxiv201102057', 1);('levine v koltun continuous', 1);('inverse optimal co ntrol', 1);('optimal examples arxiv preprint arxiv12064617', 1);('n ab azar shahmansoorian davoudi', 1);('inv erseoptimal control inverse reinforcement learning histo rical reviewannu', 1);('rev', 1);('r e kalman', 1);('linear control system optimal', 1);('eng', 1);('haddad v chellaboina nonlinear', 1);('dynamical systems andcontrol', 1);('lyapunovbased', 1);('princeton', 1);('pressprinceton usa', 1);('johnson n aghasadeghi bretl inverse', 1);('opti mal control fordeterministic continuoustime nonlinear systems', 1);('ieeecdc', 1);('x cai z han inverse', 1);('optimal control nonlinear systems withstructural uncertainty', 1);('iee procc', 1);('ornelas e n sanchez g loukianov discret', 1);('etime nonlinear systems inverse optimal control control lyapunov functionapproach', 1);('proc ieee int conf', 1);('autom', 1);('e n sanchez', 1);('ornelastellez discretetime', 1);('inverse optimalcontrol nonlinear systems', 1);('crc', 1);('boca raton usa', 1);('molloy j j ford perez finitehorizon', 1);('nverse optimalcontrol discretetime nonlinear systems', 1);('inverse optimal control innite horizon', 1);('proc57th ieee cdc', 1);('inverse optimal control controlconst', 1);('discretetime systems nite innite horizons', 1);('inverse', 1);('optimal robust nonlinear attitud e control rigidspacecraft', 1);('aerosp sci technol', 1);('luo chu k v ling inverse', 1);('optimal adaptive control forattitude', 1);('ieee trans autom contr', 1);('vol 50no', 1);('tsai', 1);('molloy perez inverse', 1);('twoplayer zerosumdynamic games', 1);('aucc', 1);('p abbeel ng apprenticeship', 1);('inv erse reinforcement learning', 1);('conf mach learn icml', 1);('wulfmeier p ondruska posner maximum', 1);('entr opy deepinverse reinforcement learning arxiv preprint arxiv150704888', 1);('g neu', 1);('szepesv', 1);('apprenticeship', 1);('inversereinforcement learning gradient methods pp', 1);('ziebart maas j andrew bagnell k dey maximumentropy', 1);('aaai', 1);('choi kim h jin kim inverse', 1);('reinforcement l', 1);('controlfor trajectory', 1);('multirotor uav', 1);('int j', 1);('autom syst', 1);('r self abudia r kamalapurkar online', 1);('invers e', 1);('systems disturbances', 1);('k mombaur truong j p laumond', 1);('human h umanoidlocomotion inverse optimal control approach', 1);('auton robots', 1);('r kamalapurkar linear', 1);('inverse reinforcement', 1);('continuoustime space', 1);('xue k p j fan', 1);('chai', 1);('lewis inve', 1);('rse reinforcement learning', 1);('inverse opt imal controlieee', 1);('trans cybern', 1);('doi 101109tcyb20213061894', 1);('j fan chai', 1);('lewis inverse', 1);('inverse optimal contr ol', 1);('ieeetrans neural netw learn', 1);('doi 101109tnnls20213106635', 1);('lian', 1);('lewis chai online', 1);('nonlinear systems adversarial attacks', 1);('int j robustnonlinear', 1);('p lancaster', 1);('rodman algebraic riccati', 1);('clarendonpress oxford uk', 1);('menner n zeilinger convex', 1);('formulations algebraicsolutions linear quadratic inverse optimal control', 1);('pro blems', 1);('proc17th ecc', 1);('j inga e bischoff', 1);('molloy flad hohman', 1);('solutionsets', 1);('inverse noncooperative linearquadratic differ ential gamesieee control', 1);('syst lett', 1);('jiang z jiang computational', 1);('adaptive optimal control forcontinuoustime linear systems', 1);('unknown dy namicsautomatica vol', 1);('jiang j fan chai j li', 1);('lewis datadr', 1);('iven otationindustrial process operational optimal control', 1);('ieee trans ind informa', 1);