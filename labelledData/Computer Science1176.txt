('psrl', 20);('rl', 13);('tasc', 13);('international conference', 9);('pomdp', 7);('qfunction', 7);('eps', 6);('machine learning', 5);('ddqn', 5);('proceedings', 5);('acm', 5);('isbn', 5);('reinforcement learning', 4);('true state', 4);('oikarinen', 4);('asc', 4);('erto', 4);('tv', 4);('exit', 4);('springer', 4);('articial intelligence', 4);('markov', 3);('sun', 3);('wu v', 3);('fan', 3);('huang', 3);('specically', 3);('twoway', 3);('nominal reward', 3);('cav', 3);('learning representations', 3);('inneural information processing systems', 3);('neural networks', 2);('pattanaik', 2);('kumar', 2);('wu', 2);('duggirala', 2);('chen', 2);('kantarcioglu', 2);('madry', 2);('chiang', 2);('wang', 2);('salman', 2);('zhang', 2);('santa cruz shoukry', 2);('ht', 2);('ddqn van hasselt', 2);('safety reinforcement learningin', 2);('crown', 2);('otherwise', 2);('initial state', 2);('sequences actions', 2);('aois', 2);('certo', 2);('psrlat', 2);('ibp', 2);('r adial', 2);('leurent', 2);('episodes highway scenario', 2);('mse', 2);('computer aided verication', 2);('science pages', 2);('proceedings machine learning', 2);('research pages', 2);('pmlr', 2);('berlin heidelberg', 2);('york ny usa', 2);('neural information processing systems', 2);('arxiv221214115v1 cslg', 1);('dec', 1);('research vol', 1);('xx', 1);('safety reinforcement learning adversa', 1);('attacksjunlin wu junlin wuwustl eduhussein sibai sibai wustl eduyevgeniy vorobeychik yvorobeychik wustl educomputer', 1);('engineeringwashington', 1);('saint louisabstractfunction', 1);('remarkable advances', 1);('rltechniques', 1);('environments highdimensional inputs images endtoend fashion', 1);('lowlevel control', 1);('nev', 1);('vulnerable small adversarial input perturbations number ap proaches', 1);('certifyingrobustness endtoend', 1);('adversarial perturbations h ave', 1);('oncumulative reward', 1);('advers arial scenarios violation offundamental properties safety', 1);('ove rall reward', 1);('safety withefciency', 1);('properties safety', 1);('true state ratherthan highdimensional', 1);('raw inputs endtoend policie disentangle', 1);('nominal efciency andadversarial safety situate', 1);('deterministic partial', 1);('decision processespomdps goal', 1);('cumulative reward subj ect safety constraints thenpropose', 1);('advantage ofan', 1);('additional assumption', 1);('k nown training time presentthe rst approach', 1);('policies und er adversarial input perturbations andtwo adversarial training approaches', 1);('direct use', 1);('experiments demonstrateboth efcacy', 1);('saf ety adversarial environments andthe value', 1);('adversarial tra', 1);('high nominal reward highquality', 1);('true statekeywords', 1);('adversarial reinforcement learning safe reinforcement', 1);('certied robustness1 introductionrecent', 1);('remarkable advances reinforcemen learning', 1);('deepneural networks', 1);('qfunctions', 1);('policies applications rangingfrom autonomous', 1);('panesar', 1);('kiran', 1);('aspire transition advances practice highstakes applications h owever safety', 1);('series demonstrations', 1);('turbations observations constitute inputs', 1);('policies nearlyarbitrary adversarial ends', 1);('response number approaches', 1);('traditional reinfor cement learning algorithms orderto', 1);('empirical robustness l utjens', 1);('j wu h sibai v', 1);('safety reinforcement learning2022 zhang', 1);('recent efforts', 1);('policies adversarial perturbations', 1);('work robustness', 1);('adversarial perturbations isany', 1);('explicit consideration safety', 1);('reward signal', 1);('orthogonal aspects suc h efciency', 1);('conating', 1);('issuesis consequential practice adversarial events', 1);('efciency hand safety verication', 1);('central concern', 1);('formal analysis dynamical systems control', 1);('bayen', 1);('interpretable safety properties', 1);('withrespect contrast robust reinforcement learning nd robustness certication methods havebeen', 1);('endtoend approaches map highdime nsional', 1);('uninterpretable perceptual inputs', 1);('safety p roperties', 1);('speciedover inputs', 1);('consideredto bridge gap robustness certication approach es adversarial', 1);('verication methods dynamical systems control', 1);('environmentas deterministic', 1);('decision p rocess', 1);('pomdpare', 1);('enable safety specication observations highdimensional', 1);('conv entional control', 1);('additional ass umptions structure whereasconventional endtoend', 1);('map observations directl actions', 1);('observations state', 1);('conventional verication dene afety respect', 1);('true state andset goal agent', 1);('cumulative reward ove r nite horizon subject safetyconstraints addition', 1);('observable decision time weassume', 1);('training safetycritical application assumptionis', 1);('log annotate traini ng phase purposes testingand debuggingour rst contribution', 1);('reinforce ment learning framework', 1);('state obse rvations reinforcement learning thatlearns control policy lowdimensional', 1);('system state', 1);('supervision', 1);('prediction model eg mappingan image locations obstacles road', 1);('netunesuch models', 1);('performance poli', 1);('input distribution', 1);('thepsrl', 1);('complex compositions ofperception learning control', 1);('adopte safetycritical environments suchas autonomous drivingour', 1);('contribution approach adversarial', 1);('safe ty certication', 1);('policiesover nite time horizon', 1);('algori thm computes', 1);('magnitude adversarial perturbations observations safety', 1);('true states', 1);('adversarial training approaches ake use', 1);('animportant', 1);('tension context endtoend adv ersarial training updates', 1);('decisions adversarial training upda tes', 1);('componentswhich leverages', 1);('signal vestigate', 1);('variations adversarial training balance considerations', 1);('native ways nd hybrid approach2certifying', 1);('safety reinforcement learningthat', 1);('updates yields', 1);('od balance high', 1);('nominal rewardadversarial', 1);('accurate state predictio', 1);('work work', 1);('broad area adversarial machine learning', 1);('eit learning predictions', 1);('signicant progress eld inthe context', 1);('broad array app roaches', 1);('robustnessof learning adversarial perturbations', 1);('trainin g data', 1);('kearns li', 1);('servedio', 1);('liu', 1);('inputs prediction time', 1);('cai', 1);('whilemany', 1);('ml', 1);('robustness focus e mpirical robustness measuresa number aspire', 1);('robustness adversaria l perturbations', 1);('cohen', 1);('xu', 1);('train models exhibit', 1);('empirical robustness', 1);('wong kolter', 1);('line w orkmodern reinforcement learning approaches', 1);('sh suffer adversarialperturbations', 1);('number approaches', 1);('robustness l utjens', 1);('efcacy varies', 1);('severalrecent', 1);('efforts address robustness certication proble reinforcement learning adapting', 1);('reinfor cement learning', 1);('policy variance reward lowerbounds', 1);('rich literature control theory', 1);('formal metho ds', 1);('settings adversarialperturbations states observations dynamical syst ems research problems stabilityreachability robustness', 1);('formal safety verication correctbyconstruction control synthesis insuch settings', 1);('mitra', 1);('tabuada', 1);('frehse', 1);('bak duggirala', 1);('althoff', 1);('meyer', 1);('promising line research', 1);('safety properties inputoutput incl usion properties', 1);('various neural networksnn dynamical systems', 1);('nn', 1);('ivanov', 1);('dutta', 1);('tran', 1);('lowdimensional systems inputs', 1);('line research', 1);('robustness safety control systems hig hdimensional sensor inputs', 1);('dean', 1);('2020b adean', 1);('recht', 1);('katz', 1);('scenarios map sensor nputs semantic outputs perception module', 1);('smooth state', 1);('training data f perception module accurategenerative model', 1);('semantic states', 1);('realistic high dimensional sensor inputs availableor', 1);('accurate model environment known2', 1);('modelwe', 1);('formal model deterministic nitehorizon', 1);('pomdps pomdp', 1);('saorfd', 1);('wheresis statespaceais', 1);('ois', 1);('observations rsais reward function isa function', 1);('current state sand action asais deterministic transition function thatreturns', 1);('state ssafsis deterministic observation function ofsfor3certifying', 1);('safety reinforcement learningobservation', 1);('oand state anddis probability distribution', 1);('initial states', 1);('mo', 1);('reover weconsider setting agent', 1);('safety constraint', 1);('certain unsafe statesshould', 1);('udenote', 1);('unsafe states swe', 1);('dpf andrare', 1);('unknown agent', 1);('act inthis', 1);('environment addition', 1);('stat e spacesrmand observationspaceorn wherenm setup captures settings', 1);('true state sis comprisedof collection', 1);('meaningful variables whil e observations ofsare', 1);('onhighdimensional perceptual inputs images', 1);('lidar', 1);('point clouds etcat point time agent', 1);('history observations hto0ot', 1);('ingeneral', 1);('full history observations state observation spacesare nite', 1);('belief state sufcient statistic', 1);('difcult se ttingsince observations highdimensional', 1);('common approach', 1);('deep reinforcement learning', 1);('partiallyobservable environmen ts highdimensional observations tocondition policies nite sequence', 1);('obs ervations simplify exposition weassume policies', 1);('approaches dependence nite histories straightforward', 1);('icy byo', 1);('observationoto action', 1);('anystandard reinforcement learning method', 1);('bojarski', 1);('peng', 1);('deneht', 1);('o0s0o1s1otstto sequence observations states', 1);('policy suppose', 1);('sthtbe', 1);('goal theagent maximize total', 1);('sum rewards nite time horizon subject theconstraint unsafe states', 1);('way f', 1);('agents goal solvemaxebracketleftbiggtsummationdisplayt0rtbracketrightbiggstust 1where expectation respect randomness th e agents decisions', 1);('initial statesour primary focus robustness adversarial perturbatio ns observations', 1);('specicallyletbe', 1);('adversarial perturbation results observ', 1);('bein anpball ie bardblbardblp', 1);('0and norm pn', 1);('theadversarys', 1);('violation safety cons traint nite', 1);('toformalize', 1);('0t1be sequence adversarial perturbations', 1);('htbe', 1);('sequence observations state andstbe', 1);('subsetof states adversarys goal identify perturbatio n sequence bardbltbardblpfor allt andusteatioslash3', 1);('partiallysupervised reinforcement learningin', 1);('environment deterministic', 1);('p omdp', 1);('truesemantic states sover safety constraints', 1);('uare', 1);('stillin principle', 1);('policies map observedinputs lowlevel control aa', 1);('host reasons', 1);('undesirable insafetycritical domains', 1);('wrong difcult use policy oto yield4certifying', 1);('safety reinforcement', 1);('learningactionable insights', 1);('sufcie nt semantic information', 1);('large datasets', 1);('complementary domains', 1);('meaningfulmappings gofrom sensory inputs images states lo cation ego vehicleas', 1);('objects vehicles scene', 1);('consequen', 1);('common domains toeschew endtoend', 1);('favor approaches compose', 1);('ceptual learning reasoning', 1);('complex compositional app roaches learning act highstakesdomains key idea', 1);('situ ations', 1);('true state sis', 1);('training time', 1);('unknown decision time example rain anautonomous vehicle', 1);('true state act', 1);('available transitiona', 1);('physical environment', 1);('navi gate perceptual informationin', 1);('construct policy oas composition perceptual prediction module goand asemantic policy sswhich maps', 1);('state sto action', 1);('tolearn gand decision time', 1);('signicantly', 1);('gog maps highdimensional observations oto relativelylowdimensional states', 1);('standard regression loss g withl2losslgos bardblgosbardbl22 use', 1);('qlearning', 1);('actionvalue function', 1);('qsa', 1);('withss argmax aqsa', 1);('standard approach thesewould work training iteration update', 1);('qusingthe', 1);('approachin sequel address', 1);('key problems', 1);('obt highquality policies thatremain', 1);('adversarial perturbations highd imensional inputs o1', 1);('formally', 1);('certify safety properties', 1);('policies decision time respect adversarial perturbations2', 1);('learn', 1);('safety guarantees un der adversarial perturbations4', 1);('certifying safety adversarial perturbationswe', 1);('polici eso sgo goal ofrobustness certication', 1);('learning eithe r ascertain', 1);('prediction robustie invariant', 1);('perturbation budget certify budget perturbationunder budget constraint', 1);('extensions thisidea reinforcement learning', 1);('hat policy invariant adversarialperturbations', 1);('total reward context', 1);('sole concernis certify safety adversarial perturbations reason view adversarial encountersas', 1);('nominal policy work cases deployedas', 1);('issues efciency r ewards', 1);('issues safety vehicle e xample doesnt crashwe', 1);('dene adversarial safety certication', 1);('problem followsgiven policy identify', 1);('ust', 1);('allsuch thatbardbltbardblpand 0tt15certifying', 1);('nd goal return', 1);('policy unsafe', 1);('clearly asc', 1);('problem denition', 1);('whe n', 1);('enters unsafe stateeg car crashes difcult', 1);('simulate transition safor state sand action', 1);('veri cation dynamical systems literature', 1);('dynamical model', 1);('assumes specic structure discrepancy functions', 1);('hsieh', 1);('present algorithm adversarial safety cer tication', 1);('tasctreebased', 1);('adversarial safety certication algorithmconsider', 1);('observation policyss state prediction function', 1);('onadversarial perturbation key building block certication procedure r eturn setof actions', 1);('aowith', 1);('property adversary', 1);('select actionnot', 1);('aao psrl', 1);('steps rst cert ify', 1);('ofstatessosuch goso', 1);('ao asoasthe', 1);('actions state ssocan', 1);('steps asseparate robustness certication problems', 1);('input perturbations', 1);('ourcertication policy stakes', 1);('normsuchsobs wherebsisaball', 1);('state sgo use eg', 1);('calculate upperlower', 1);('qsaandqsafor', 1);('action aforsgo sis', 1);('values dete rmine action', 1);('aoas', 1);('theset actions afor', 1);('qsamaxaqsafor', 1);('goabove appeal', 1);('distinct approaches', 1);('onthe norm p', 1);('adversarial perturbations observations oto boundedinnorm', 1);('approach hand perturbations', 1);('eitherapproach', 1);('yields ballbsthat use certify policy', 1);('observation describe', 1);('proceeds prede', 1);('lettvbe', 1);('theverication horizon', 1);('different reward horizont', 1);('stvgenerated', 1);('initial state s02the rst step check', 1);('ustv', 1);('series iterations cer tication1 case simulate potential', 1);('future decisiontime certication', 1);('safe state concern adversarial perturbations', 1);('purposes cert ication', 1);('actual interaction theenvironment2 assumption', 1);('l oss generality', 1);('dene dummyinitial state', 1);('i1 attempt', 1);('additionthrough iterations', 1);('initial state s0root node tree tree', 1);('sparse ubtree thefull tree', 1);('tvlong', 1);('s0 node rinthis tree corresponds state', 1);('sequence actions', 1);('s0 aswell', 1);('observation addition node rwe', 1);('ai1r', 1);('node f ollows', 1);('nominal path', 1);('node rthereby', 1);('certify setof actions', 1);('airunderi ifairai1r', 1);('update certicate node rtoiand', 1);('additional exploration subtree', 1);('ifaireatioslashai1r', 1);('ofnew actions', 1);('i1 action aairiinduces', 1);('nominal historyhtrr wheretris', 1);('verication horizon', 1);('node r associatedset states', 1);('strr ifustrreatioslash', 1);('return i1', 1);('rone node timein depthrst fashion', 1);('anyunsafe state', 1);('entire subtree', 1);('unsafe statewe', 1);('ifor noder', 1);('nominal trajectory successfu', 1);('tvsteps', 1);('nominal trajectory fashion w verifyi', 1);('full version paper seehttpsvorobeychikcompsrlsafetycertpdf', 1);('important properties', 1);('straightf orward', 1);('realtime isto return', 1);('adversarial safety', 1);('arbitrary time limit', 1);('longas procedure', 1);('propertieslet c', 1);('aothat', 1);('robust policy observationounder', 1);('adversarys budget', 1);('norm pdenition', 1);('action aaowherewithbardblbardblpsuch', 1);('complete ifaao aandbardblbardblpaao', 1);('sound returns iscomplete ifproposition', 1);('ifcerto', 1);('iscomplete max i1ii1for nite discrete schedule adversarial noise bounds wherenis', 1);('absolute upper boundproof', 1);('sketch suppose', 1);('returns ifor iinthe discrete sequence adversarial noise bounds', 1);('conside', 1);('r tree', 1);('sound tree', 1);('aofor', 1);('node othrough depth', 1);('th etree contains nodes adversary', 1);('identify ofthese unsafe soundness followsfor completeness', 1);('setting te', 1);('nis', 1);('case subtree', 1);('unsafe state', 1);('exists path', 1);('state unsafe state that7certifying', 1);('safety reinforcement learningan', 1);('perturbations i1', 1);('ii1 theresult follows5', 1);('learning robust policiesa', 1);('natural complement certication problem bolst', 1);('robustness learning', 1);('asmentioned', 1);('conventional learning max imize total reward subject safetyconstraints', 1);('equation', 1);('learning problem', 1);('nom inal rewardand', 1);('safety violations', 1);('nominal pr ior adversarial perturbations sequenceof states', 1);('robust learning', 1);('goal maximize certication', 1);('problem ismaxustebracketleftbiggtsummationdisplayt0rtbracketrightbigg1 2where twithbardbltbardblpnote', 1);('problem', 1);('different typical goal robust', 1);('maximize worstcase sum rewards wh ere certication', 1);('playthe role', 1);('worstcase cumu lative reward', 1);('crucial separate primary goal operative', 1);('normal circumstances tomaximize sum', 1);('nominal rewards robustness pertain', 1);('critical considerationssuch violation safety constraints', 1);('rare adversarial encountersone', 1);('basic techniques', 1);('robustness achine learning', 1);('reinforcement learningis adversarial trainin g', 1);('adversarial training augments', 1);('loss adversarial counterpart area number ways', 1);('adversarial loss mo st', 1);('qactoroausing', 1);('observation oas proxy state', 1);('common adversarial counterpart', 1);('qactoroa', 1);('adversarial perturbation', 1);('gradient descent aois policy inducedby actor', 1);('softmax layer', 1);('baseline version adversarial training', 1);('atan', 1);('bounds actorqfunction', 1);('ibp gowal', 1);('maximize thedifference', 1);('value action', 1);('nominal poli cyand', 1);('qvalue', 1);('r adial notethat r adial', 1);('architecture single', 1);('policy architecturein addition', 1);('novel approaches adversarial training', 1);('advantage ofthe', 1);('compositional architecture rst', 1);('loss forthe policy sor', 1);('loss g theformer approach analogous', 1);('adversarial counterpart 2loss', 1);('bardblgosbardbl22bardblgosbardbl22 wheregandgare', 1);('case loss case 2loss takean approach', 1);('bounds actor', 1);('safety reinforcement learningour', 1);('approach hybrid', 1);('r adial psrlat specically', 1);('rst usethe', 1);('loss function update', 1);('full composite', 1);('component g addition use adversarial', 1);('loss gtoperform', 1);('update minibatch', 1);('psrlh ybrid', 1);('experimentsexperiment setup', 1);('highway autonomous', 1);('full version paper details experiment', 1);('environment highway', 1);('twoway exit', 1);('position ego vehicle', 1);('simplify setup', 1);('highwayscenario', 1);('direction threelane road vehicles thanthe ego vehicle', 1);('constant speed ego vehicle', 1);('inthe rightmost lane', 1);('high speed', 1);('unsafe state whichwe dene', 1);('unit distanc e vehicles', 1);('twowayscenario', 1);('vehicles drive twolane road', 1);('vehicles thesame lane ego vehicle', 1);('lane eg vehicle', 1);('reward 05for high speed penalty unsafe states co llision', 1);('scenario issimilar highway scenario', 1);('exit lane', 1);('right rewardof', 1);('successful exit', 1);('distance th e exitour safety certication uses', 1);('context involves', 1);('neural networksgo maps image input positions vehic les', 1);('sceneand theqnetwork', 1);('state sie', 1);('vehicles closestto ego vehicle scene input', 1);('true state check safety violation collision', 1);('qnetworkwe', 1);('use policy', 1);('agreedy policy ie', 1);('policy sswith probability', 1);('random action', 1);('train gfor', 1);('scenario 1000episodes', 1);('scenariofor variants adversarial training train', 1);('target level thereafterwe', 1);('values target adversarial training', 1);('present theresults perturbations results', 1);('full details experiment setup', 1);('full version paperefcacy', 1);('adver sarial safetycertication algorithm', 1);('maximum total number n odes', 1);('cases experi ments affectthe medians', 1);('short horizon', 1);('needs toexplore', 1);('small fraction search tree cases belo w', 1);('horizon incr eases', 1);('fraction nodes cases sce narios', 1);('fraction nodes', 1);('complete', 1);('details p', 1);('full version paper9certifying', 1);('safety reinforcement learningefcacy adversarial training', 1);('efcacy adversarial training ap proaches', 1);('adversarial training referto', 1);('vanilla atstandard', 1);('adversarial training', 1);('radial oikarinen', 1);('adversarial error c', 1);('psrlat psrlhybrid', 1);('measures efcacy', 1);('nominal reward cer', 1);('state predi ction prediction', 1);('actualobservation oavg err', 1);('upper avg err ub', 1);('avg err lb bounds prediction errorafter adversarial perturbation', 1);('crown wang', 1);('figure', 1);('1presents results', 1);('column ce', 1);('high nominal reward', 1);('noratyield meaningful safety certication', 1);('te rms', 1);('psrlhybridconsistently', 1);('nominal reward safet certication', 1);('radial', 1);('cases relat', 1);('thusthe', 1);('hybrid approach', 1);('scenarios0250501005v anilla', 1);('30010v anilla', 1);('3005v anilla', 1);('r adial psrla psrlhybrid000000050010000005010avg', 1);('err avg err ub avg err lb000001a', 1);('tr adialpsrla tpsrlhybridfigure', 1);('average', 1);('safety middle', 1);('state prediction error rightfor highway', 1);('mse psrlat', 1);('outperform adversarial training alternatives', 1);('styleadversarial training methods', 1);('additional adv antage consistency performance andinterpretability', 1);('bservations endtoend approaches7', 1);('conclusionwe', 1);('rst framework adversarial safety cert ication reinforcement learning thecontext', 1);('system model', 1);('effectiv e adversarial safety certicationalgorithm', 1);('adversarial training thods', 1);('psr', 1);('l leverages observations truestate training time show making use', 1);('adve rsarial training yield strongnominal performance high', 1);('highquality predictions ofstate', 1);('high level interpretability10certifying', 1);('safety reinforcement learningreferencesm althoff', 1);('introduction cora', 1);('proc', 1);('applied verication', 1);('hybrid systems', 1);('bak parasara sridhar duggirala hylaa', 1);('tool fo r', 1);('simulationequivalentreachability linear systems', 1);('hybridsystems computation', 1);('control pages', 1);('bayen eva cruck claire tomlin guarantee', 1);('overapproximations unsafesets', 1);('continuous hybrid systems', 1);('hamilto njacobi equation', 1);('claire tomlin mark r greenstreet', 1);('edito rshscc volume', 1);('lncs', 1);('bojarski ben firner', 1);('flepp larry jackel urs muller karol zieba davide deltesta endtoend', 1);('deep learning', 1);('nvidia developer technical blog', 1);('cai chang liu dawn', 1);('curriculum', 1);('adversaria l training', 1);('jointconference articial intelligence', 1);('chen erika abrah sriram sankaranarayanan flow', 1);('analyzer nonlinear hybridsystems', 1);('natasha sharygina helmut veith', 1);('lecture notes computer', 1);('springer berlin heidelberg', 1);('chiang michael curry ahmed abdelkader aounon k', 1);('john dickerson tomgoldstein detection', 1);('certied', 1);('object detec tion median', 1);('neuralinformation processing systems', 1);('cohen elan rosenfeld j zico kolter certie', 1);('adversarial robustness', 1);('dean benjamin recht certainty', 1);('equivalent percep', 1);('learning dynamics contr', 1);('ol volume', 1);('june', 1);('urlhttpsproceedingsmlrpressv144dean21ahtml sarah dean nikolai matni benjamin recht vickie ye ro', 1);('bust guarantees', 1);('alexandre bayen ali jadbabaie george pappas pablo parrilo benjamin recht claire tomlin melanie zeilinger', 1);('learning dynamics', 1);('control volume', 1);('jun', 1);('urlhttpsproceedingsmlrpressv120dean20ahtml sarah dean andrew j taylor ryan k cosner benjamin recht ames guaranteeingsafety', 1);('perception modules', 1);('measurementrobu st control barrier functions', 1);('arxiv', 1);('abs201016001 2020bparasara', 1);('sridhar duggirala sayan mitra mahesh viswanath matthew potok c2e2 averication', 1);('tool stateow models', 1);('christel baier cesare tinelli', 1);('tools', 1);('safety reinforcement learningalgorithms', 1);('analysis systems', 1);('berlin heidelbergsouradeep dutta xin chen sriram sankaranarayanan reachability analysis neural feedback systems', 1);('regressive polynomial', 1);('infe', 1);('rence page', 1);('computing machinery', 1);('urlhttpsdoiorg10114533025043311807 chuchu fan bolun qi sayan mitra mahesh viswanathan', 1);('datadriven', 1);('verication andcompositional reasoning automotive systems', 1);('page 441461springer', 1);('frehse colas', 1);('guernic alexandre donz', 1);('scott cot', 1);('rajarshi ray olivier lebeltelrodolfo ripado antoine girard thao dang oded maler', 1);('scalable verication ofhybrid systems', 1);('gowal krishnamurthy dvijotham robert stanforth ru', 1);('bunel chongli qin jonathanuesato relja arandjelovic timothy mann pushmeet koh', 1);('li effectiveness intervalbound propagation training', 1);('robust models arxiv preprint arxiv181012715 2018chiao', 1);('hsieh keyur joshi sasa misailovic sayan mitra verifying', 1);('controllers convolutional neural', 1);('perception case intel ligible', 1);('safe precise abstractionsarxiv preprint arxiv211105534 2021chao', 1);('huang jiameng fan wenchao li xin chen qi zhu rea', 1);('reachability', 1);('analysis ofneuralnetwork', 1);('acm trans embed comput syst', 1);('185s oct', 1);('issn15399087', 1);('url', 1);('ling huang anthony joseph blaine nelson benjamin ip rub', 1);('j doug tygar adversarial', 1);('workshop security', 1);('pages 43582011zhenqi', 1);('huang chuchu fan alexandru mereacre sayan mitra marta z kwiatkowska invariant', 1);('verication nonlinear hybrid automata networks ca rdiac cells', 1);('notes computer', 1);('ivanov james weimer rajeev alur george j pappas insup lee verisig', 1);('verifyingsafety properties hybrid systems neural network', 1);('con trollers', 1);('acm hscc', 1);('katz anthony', 1);('corso christopher strong mykel j kochenderferverication', 1);('neural network controllers usi ng generative models journal', 1);('aerospace information systems', 1);('urlhttpsdoiorg1025141i011071 michael kearns ming li learning', 1);('presence malic ious errors', 1);('siam', 1);('ravi kiran ibrahim sobh victor talpaert patrick mannio', 1);('ahmad al sallab senthil yogamani patrick p', 1);('deep', 1);('reinforcement learning utonomous', 1);('ieeetransactions intelligent', 1);('systems', 1);('safety reinforcement learningaounon kumar alexander levine soheil feizi policy', 1);('robust reinforcement learning', 1);('environment autonomous', 1);('dec isionmakinghttpsgithubcomeleurenthighwayenv 2018chang', 1);('liu bo li yevgeniy v', 1);('alina oprea rob', 1);('ust linear regression training data', 1);('security pages 911022017phil', 1);('rocco servedio learning', 1);('largemargin halfsp aces malicious noise', 1);('2011bj orn l utjens', 1);('michael everett jonathan p certi', 1);('adversarial robustness deepreinforcement learning conference', 1);('robot learning', 1);('madry aleksandar makelov ludwig schmidt dim', 1);('tsipras', 1);('vladu towards', 1);('deep learning models', 1);('resistant adv ersarial attacks', 1);('urlhttpsopenreviewnetpdfidrjzibfzab pierrejean meyer alex devonport murat arcak tira', 1);('oolbox interval reachability analysis', 1);('hyb', 1);('systemscomputation', 1);('hscc', 1);('computing machinery isbn', 1);('urlhttpsdoiorg10114533025043311808 sayan mitra verifying cyberphysical systems path safe autonomy mit', 1);('urlhttpsmitpressmiteducontributorssayanmitra tuomas oikarinen wang zhang alexandre megretski luca da', 1);('tsuiwei weng robustdeep', 1);('reinforcement learning adversarial loss', 1);('panesar machine', 1);('ai', 1);('pattanaik zhenyi tang shuijing liu gautham bommann girish chowdhary robustdeep', 1);('reinforcement learning adversarial attacks arxiv preprint arxiv171203632 2017baiyu', 1);('peng qi sun shengbo eben li dongsuk kum yuming yin junqing wei tianyu guendtoend', 1);('automotive innovation', 1);('salman jerry li ilya razenshteyn pengchuan zhang h', 1);('zhang sebastien bubeck', 1);('yang provably', 1);('safety reinforcement learningulices santa cruz yasser shoukry nnlanderverif', 1);('neu ral network', 1);('formal verication framework', 1);('autonomous air craft', 1);('nasaformal methods', 1);('symposium nfm', 1);('pa', 1);('ca usamay', 1);('springerverlag isbn', 1);('urlhttpsdoiorg101007978303106773011 jianwen sun tianwei zhang xiaofei xie lei yan zheng k', 1);('chen yang liustealthy', 1);('efcient adversarial attacks', 1);('rei nforcement learning', 1);('aaai', 1);('sun haitham khedr yasser shoukry formal', 1);('veri cation neural network controlledautonomous systems', 1);('hybrid systems computati', 1);('andcontrol pages', 1);('tabuada verication', 1);('hybrid systems symbolic appro', 1);('incorporated', 1);('1st edition', 1);('tran xiaodong yang diego manzanas lopez patr', 1);('musau luan viet nguyenweiming xiang stanley bak taylor johnson nnv', 1);('n eural network verication toolfor', 1);('cyberphysi cal systems', 1);('shuvendu k lahiriand chao wang', 1);('van hasselt arthur guez david silver deep', 1);('reinfo rcement learning double', 1);('qlearning aaai', 1);('murat kantarcioglu adversarial machine learning morgan claypool', 1);('wang huan zhang kaidi xu xue lin suman jana choju hsieh j zicokolter betacrown efcient', 1);('propagation pern euron', 1);('constraints neural network robustness verication', 1);('urlhttpsopenreviewnetforumidahyilrbecfw eric wong j zico kolter', 1);('provable defenses adve rsarial examples', 1);('convex outeradversarial polytope', 1);('wu linyi li zijian huang yevgeniy v', 1);('ding z', 1);('bo li crop certifying', 1);('robust policies reinforcement learning fun ctional', 1);('internationalconference learning representations', 1);('wu yevgeniy v', 1);('robust', 1);('deep reinforcem ent learning bootstrappedopportunistic curriculum', 1);('xu huan zhang shiqi wang yihan wang suman jana xue lin chojui hsieh fastand', 1);('enabling', 1);('complete neural network vericatio n', 1);('parallelincomplete veriers', 1);('urlhttpsopenreviewnetforumidnvztxbi6lnn', 1);('safety reinforcement learninghuan zhang hongge chen chaowei xiao bo li mingyan liu', 1);('boning chojui hsiehrobust', 1);('deep reinforcement learning adversarial pe rturbations state observations', 1);('zhang endtoend learning autonomous driving phd', 1);('york universityusa', 1);