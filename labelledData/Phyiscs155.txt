('lyapunov', 13);('empowerment', 10);('d. polani', 10);('intrinsic motivation', 9);('empowerment maximization', 7);('ta', 7);('state space', 7);('cef', 6);('nal state', 6);('fig', 6);('p. abbeel', 6);('figure', 5);('control signal', 5);('variational', 5);('advances', 5);('iclr', 5);('dynamical systems', 4);('time horizon', 4);('crucially', 4);('empowerment landscape', 4);('cte', 4);('s. levine', 4);('c. salge', 4);('c. l. nehaniv', 4);('s. tiomkin', 4);('engineering', 3);('systems', 3);('speci', 3);('te', 3);('wiener', 3);('reward function', 3);('time step', 3);('gaussian', 3);('channel capacity', 3);('singular values', 3);('top', 3);('\x01 t', 3);('proceedings', 3);('learning representations', 3);('a. sharma', 3);('c. glackin', 3);('a. s. klyubin', 3);('r. zhao', 3);('neurips', 3);('i. nemenman', 3);('dynamical', 2);('theory', 2);('in- trinsic motivation', 2);('mutual information', 2);('future states', 2);('reinforcement learn-', 2);('arti cial agents', 2);('machine learning', 2);('mutual information [', 2);('intrinsically', 2);('di erent versions', 2);('analytical results', 2);("agent 's actions", 2);('dec', 2);('~ ajx0', 2);('ta=', 2);('causal entropic forcing', 2);('generalized empowerment', 2);('continuous time', 2);('con- trol', 2);('reinforcement learning', 2);('rl', 2);('tasks [', 2);('empowerment c', 2);('achievable mutual information', 2);('teand', 2);('= max p', 2);('xte', 2);('ate\x001', 2);('current state', 2);('arbitrary length sequences', 2);('channel power', 2);('various versions', 2);('intrinsic', 2);('free fall acceleration', 2);('observation noise', 2);('linear approximation', 2);('horizontal axis', 2);('bottom', 2);('te= 0:5s', 2);('empower- ment maximization', 2);('` c2cos\x122', 2);('double pendulum', 2);('traversing', 2);('cart-pole', 2);('dynamical system', 2);('dy- namics', 2);('con-', 2);('frontiers', 2);('reinforcement', 2);('neural', 2);('intrinsic control', 2);('interna-', 2);('tional conference', 2);('s. gu', 2);('v. kumar', 2);('robotics', 2);('b. eysenbach', 2);('j. ibarz', 2);('di-', 2);('learning', 2);('international conference', 2);('h. edwards', 2);('guided self-organization', 2);('inception', 2);('springer', 2);('universal agent-centric measure', 2);('ieee congress', 2);('evolutionary computation', 2);('t. jung', 2);('p. stone', 2);('con- tinuous agent|environment systems', 2);('adaptive behavior', 2);(':16 {', 2);('e\x0ecient', 2);('a. d. wissner-gross', 2);('c. e. freer', 2);('causal', 2);('entropic forces', 2);('physical', 2);('review letters', 2);('neural information processing systems', 2);('ai', 2);('d. pathak', 2);('t. darrell', 2);('a. efros', 2);('ieee transactions', 2);(':1 {', 2);('national academy', 2);('automated', 2);('systems stas tiomkina', 1);('ilya nemenmanb', 1);('daniel polanie', 1);('naftali tishbyf', 1);('g acomputer', 1);('charles w. davidson', 1);('engineering san jose', 1);('state university', 1);('ca', 1);('physics', 1);('biology', 1);('modeling', 1);('emory', 1);('atlanta', 1);('usa', 1);('research group', 1);('hertfordshire', 1);('hat eld', 1);('uk', 1);('rachel', 1);('selim benin', 1);('computer', 1);('lilly safra', 1);('elsc', 1);('hebrew', 1);('jerusalem', 1);('israel abstract biological', 1);('ex- plicit reward signal', 1);('computational principles', 1);('information-theoretic approach', 1);("agent 's em- powerment", 1);('past ac- tions', 1);('approach generalizes', 1);('previous attempts', 1);('formalize intrinsic moti- vation', 1);('e\x0ecient algo- rithm', 1);('necessary quantities', 1);('benchmark control problems', 1);('information-theoretic control function', 1);('fundamental properties', 1);('dynam- ical system', 1);('agent-environment system', 1);('practical ar- ti cial', 1);('animal behaviors', 1);('dynamical properties', 1);('keywords|', 1);('information capacity jsensitivity gainjsta- bilizationjpredictive information', 1);('introduction', 1);('generate behaviors', 1);('generic mechanism', 1);('useful behaviors', 1);('in- trinsic motivation [', 1);('computational algorithms', 1);('intrinsic properties', 1);('organism-environment dynam- ics', 1);('increasingly', 1);('reward structure [', 1);('speci c class', 1);('such intrin- sic motivation algorithms', 1);('arti cial systems', 1);('prof. naftali tishby', 1);('de- velopment', 1);('stas tiomkin', 1);('phd', 1);('au- thors', 1);('senior author', 1);('obtained.empowerment maximization', 1);('poten- tial actions', 1);('subsequent future state', 1);('world [', 1);('future world states', 1);('behavior options', 1);('synthetic agents', 1);('atypi- cal', 1);('simple living systems', 1);('interestingly', 1);('future ac- tions', 1);('key part', 1);('training algorithms [ 8,23,24 ]', 1);('re- mains unclear', 1);('general intrinsic motivation principle', 1);('unknown [ 20,23,25 ]', 1);('additionally', 1);('simulational case studies', 1);('link em- powerment', 1);('better-understood characterizations', 1);('mutual in- formation', 1);('general case', 1);('challenging task [', 1);('simple cases', 1);('intrinsic moti- vation', 1);('empowerment maximization paradigm', 1);('main contribution', 1);('empowerment-like quantities', 1);('agent-environment dynamics', 1);('connects empowerment maximization', 1);('well-understood properties', 1);('sensitive re- gions', 1);('dynamics potentiate', 1);('future behav- iors', 1);('intrinsic motivations', 1);('practical com- putational algorithm', 1);('com- plex scenarios', 1);('continuous time limit', 1);('sec- ond major contribution', 1);('algo- rithm', 1);('standard benchmarks', 1);('intrinsic motivation research [', 1);('e\x0ecient calculation', 1);('empowerment manages', 1);('extrinsic rewards', 1);('complex robotic', 1);('| empowerment', 1);('1arxiv:2301.00005v1 [ cs.lg ]', 1);('\x01t\x00 x0\x11x', 1);('\x01 = max p', 1);('[ fxte', 1);('xte\x001', 1);('x ta+\x01tg|', 1);('{ z } ~x\x00future states', 1);('ata\x002', 1);('0g| { z } ~a\x00possible actionsjx0\x11x', 1);('empowerment ta=te', 1);('cef ta=', 1);('uni', 1);('information theoretical intrinsic motivation', 1);('process sequence', 1);('time x0', 1);('potential actions', 1);('tatimes', 1);('ttime', 1);('future system trajectory', 1);('preliminaries notation', 1);('states x', 1);('dynamics fwith', 1);('stochastic perturbations \x11', 1);('via', 1);('control gain g', 1);('hered\x11denotes', 1);('system noise', 1);('pro- cess', 1);('stochastic control process', 1);('variance \x1b2', 1);('models potential e ect', 1);('null action', 1);('various quantities', 1);('continuous version', 1);('integer index', 1);('\x01 tdenotes', 1);('physical time step', 1);('current physical time', 1);('tethe', 1);('time index', 1);('tato', 1);('action sequence', 1);('perturbation trajectories', 1);('nite equidistant times', 1);('ft+k\x01\x01tgt k=0', 1);('xte 0\x11fxkgte k=0', 1);('ata 0\x11fakgta k=0', 1);('and\x11te 0\x11f\x11kgte k=0', 1);('control theory literature', 1);('reverse order', 1);('xte 0=', 1);('continuous nature', 1);('continuous times', 1);('uses reinforce- ment learning', 1);('task-speci c', 1);('agent needs', 1);('extrinsic feedback', 1);('reward func- tion', 1);('precise construction', 1);('short training time [', 1);('signi cant degree', 1);('amongst reward functions', 1);('equivalent performance [', 1);('behavior intoa', 1);('concrete reward function', 1);('furthermore', 1);('complex behaviors consist', 1);('shorter sequences', 1);('designing', 1);('re- ward function', 1);('such parts', 1);('realistic time', 1);('hard [', 1);('task-unspeci c learning', 1);('endows or- ganisms', 1);('specializa- tion', 1);('task-unspeci c behaviors', 1);('speci c tasks', 1);('extrinsic reinforcement', 1);('information- theoretic quantities [ 4,23,30,34 {', 1);('pseudo-utility function', 1);('system dynamics', 1);('formally', 1);('conditional probability dis- tributionp', 1);('state x0and', 1);('action se- quenceate\x001', 1);('channel capacity [', 1);('control action sequence', 1);('state x0', 1);('ate\x001 0jx0', 1);('herep', 1);('probability density', 1);('probability distri- bution function', 1);('iis', 1);('xtejx0', 1);('xtejate\x001', 1);('variable means', 1);('conditional distribution', 1);('notational convenience', 1);('locally', 1);('actions atypical', 1);('natural dynamics', 1);('em- powerment measures', 1);('achievable future states', 1);('agent {', 1);('empowerment quanti es', 1);('inten- tional control', 1);('en- 2tropy', 1);('passive di usion process', 1);('state variables', 1);('fur-', 1);('quanti es diversity', 1);('future action sequences', 1);('empow- erment maximization principle [', 1);('pseudo- utility function', 1);('agent chooses', 1);('agent climbs', 1);('a\x03\x00 x', 1);('\x01 = argmax a2ae\x11\x02', 1);('c\x00', 1);('hereais', 1);('\x01 t0is', 1);('small time step', 1);('actual behavior', 1);('time step \x01', 1);('agent generates', 1);('action selection procedure', 1);('decision step', 1);('general analytical solutions', 1);('e\x0ecient algo- rithms', 1);('numerical estimation', 1);('arbitrary dynamical systems', 1);('em- powerment maximization principle', 1);('speci c approximations', 1);('linear response approximation', 1);('relate empowerment', 1);('traditional quantities', 1);('describe dynamical systems', 1);('control signal ain', 1);('interesting cases', 1);('chal- lenge', 1);('linear time-variant dynamics', 1);('autonomous dynamics', 1);('de- ne \x16xsas thes-th step', 1);('deterministic approximation', 1);('\x16 x3=f', 1);('+ g', 1);('recur- sive', 1);('\x16 x0to \x16xsbyf', 1);('time step sto', 1);('time step rcan', 1);('di erentia- tion chain rule', 1);('state derivative', 1);('dynamics f', 1);('@ \x16xs @ ar=sy =r+2r\x16xf', 1);('\x16x \x001', 1);('thedx\x02dxjacobian matrix', 1);('approx- imatesfup', 1);('linear order', 1);('-th entry ofr\x16xf', 1);('@ fi', 1);('@ \x16x', 1);('vectors xandf', 1);('@ \x16xr+1 @ ar=g', 1);('linear response', 1);("system 's states xs2s1to", 1);("agent 's actions \x01 ar2r1fs1", 1);('= @ \x16xs2 @ ar2 @ \x16xs2 @ ar2\x001', 1);('@ \x16xs2 @ ar1 @ \x16xs2\x001 @ ar2 @ \x16xs2\x001 @ ar2\x001', 1);('@ \x16xs2\x001 @ ar1 ......', 1);('... @ \x16xs1 @ ar2 @ \x16xs1 @ ar2\x001', 1);('@ \x16xs1 @ ar12', 1);('s+ \x01t+r\x001 =te', 1);('agent applies', 1);('rtime steps', 1);('gap observes', 1);('+ \x01t', 1);('control sequence', 1);('notice', 1);('dy- namical system', 1);('fs0', 1);('r0 2in', 1);('sensitivity matrix', 1);('fs1', 1);('r0 1=r0 2=', 1);('ands0 1=te', 1);('thens0 2=te', 1);('sensitivity matrix collapses', 1);('sensitiv- ity', 1);('fte', 1);('= @ \x16xte @ a0', 1);('blue block', 1);('overall sensitivity matrix', 1);('linear response regime', 1);('e ect', 1);('\x01ar2r1+ ~\x11', 1);('\x01aand \x01xare', 1);('small actions', 1);('.here ~ \x11models', 1);('total noise', 1);('process noise d\x11from', 1);('subsequent observation', 1);('state per- turbation \x01 xs2s1', 1);('entire dynamics', 1);('e ects', 1);('xte ta+\x01t', 1);('ata\x001', 1);('tadenotes', 1);('time steps', 1);('tis', 1);('time gap', 1);('action se- quence', 1);('teis', 1);('maximum mutual information be- tween', 1);('plugging', 1);('arbitrary discretization step', 1);('arbitrary time horizon', 1);('tereduces', 1);('traditional calculation', 1);('large number', 1);('dimensions re', 1);('= max \x1bi\x150p i\x1bi=p1 2dxx i=1ln', 1);('appropriate sub- matrixfs0', 1);('traditional empowerment corresponds', 1);('control signal \x01 aover', 1);('whole control period', 1);('overall power', 1);('i-th singular value', 1);('usual water-', 1);('procedure [', 1);('denotepas power', 1);('control-theoretic convention', 1);('time interval', 1);('units ofpare', 1);('weak control assumption', 1);('pto', 1);('empowerment be-', 1);('shows explic-', 1);('sensitivity matrix f', 1);('char- acterize dynamics', 1);('arbitrary dy- namical system', 1);('long time horizons', 1);('ar- bitrary', 1);('small discretization steps', 1);('linear response matrix', 1);('f.', 1);('analytical di erentiation', 1);('numerical di erentiation', 1);('whenever fis', 1);('modern computers', 1);('channel powers \x1bito', 1);('available total power', 1);('pin', 1);('empower- ment value', 1);('connecting generalized empowerment', 1);('related quan-', 1);('generalized', 1);('di erent durations', 1);('observation sequences', 1);('various quan- tities', 1);('intrinsic motivation [', 1);('causal en-', 1);('forcing', 1);('future trajectories', 1);('immediate con- sequences', 1);('time horizonte', 1);('maximizingcte', 1);('maximize susceptibility', 1);('ver- sion', 1);('rst action', 1);('uncontrolled future variability', 1);('action planning', 1);('green submatrix', 1);('top right corner', 1);('te= ta=', 1);('s0 2=s2ands0 1=s0', 1);('appropriate submatrix', 1);('fis', 1);('\x03\x11lim s2', 1);('\x10 @ \x16xs2 @ ar1\x11\x10 @ \x16xs2 @ ar1\x11y', 1);('@ \x16xs2 @ ar1is', 1);('special case', 1);('control gain', 1);('\x03 reduces', 1);('usual characteristic', 1);('dynamical system [', 1);('general control gain', 1);('speci c indices', 1);('controllable subspace', 1);('various sensitivties', 1);('controllable dynamical sys- tem [', 1);('weak agents', 1);('e\x0ecient control principle', 1);('double pole', 1);('paradigmatic models', 1);('important phenom- ena', 1);('solutions', 1);('stabilization problem', 1);('accumulate energy', 1);('pendulum upright', 1);('solution needs', 1);('finding', 1);('indirect control policy', 1);('tra- ditional reinforcement learning', 1);('nontrivial [', 1);('long time', 1);('informative rewards indi-', 1);('such situations', 1);('local properties', 1);('present trajectory', 1);('future variations', 1);('inverted', 1);('simple task', 1);('external reward', 1);('upright vertical', 1);('\x13 = _\x12', 1);('dt g lsin', 1);('angular velocity', 1);('nal state ~\x12=\x12+ ~\x11obs', 1);('power level', 1);('observation noise e', 1);('end state', 1);('process noise dw', 1);('gain sequence', 1);('sys- tem', 1);('empowerment values', 1);('di erent states', 1);('control protocol', 1);('eqs', 1);('deterministic part', 1);('discrete time', 1);('empowerment landscapes', 1);('vertical axis', 1);('rad =s', 1);('black', 1);('white lines', 1);('red dots', 1);('control signals', 1);('actions consist', 1);('orig- inal empowerment', 1);('time horizons', 1);('discretization \x01 t= 10\x003are', 1);('con- trol action', 1);('upper row', 1);('show trajec- tories', 1);('row shows time traces', 1);('maximum allowable torque', 1);('around', 1);('em- powerment', 1);('pendulum accumulates', 1);('small values', 1);('upright position', 1);('empowerment land-', 1);('convergence', 1);('time resolution', 1);('empowerment es- timation', 1);('numerical stability', 1);('limit approximation', 1);('optimal value func- tion', 1);('standard optimal control techniques', 1);('reward speci', 1);('top position [', 1);('particular discretization \x01 t= 10\x003s', 1);('dif- ferent \x01t', 1);('maximum value', 1);('original empowerment', 1);('black dot', 1);('\x01 t.', 1);('estimate converges', 1);('discrete time dy- namics', 1);('consistent approximation', 1);('continuous time dynamics', 1);('pendulum', 1);('empowerment maximization formalism', 1);('double pendulum [', 1);('equa- tions', 1);('=\x001 d1', 1);('\x12 d2', 1);('=1 m2 ` 2c2+i2\x00d2', 1);('\x12 da', 1);('\x00m2 `', 1);('` c2_\x121', 1);('=m1 `', 1);('1+ `', 1);('c2+ `', 1);('=\x00m2 `', 1);('` c2_\x12', 1);('\x002m2 `', 1);('` c2_\x122', 1);('m1 ` c1+m2 `', 1);('=m2 ` c2gcos', 1);('scalar control signal ja', 1);('` i', 1);('` ci', 1);('andii stand', 1);('i-th link', 1);('i2 [', 1);('respec- 5figure', 1);('control torque', 1);('slices', 1);('subplot shows', 1);('particular slice', 1);('4d landscape', 1);('axes _\x122', 1);('di erent scale', 1);('instantaneous empowerment', 1);('nal positions', 1);('torque', 1);('middle joint', 1);('original empow- erment', 1);('phase space', 1);('nonetheless', 1);('local gradient', 1);('maximum empowerment', 1);('vertical position', 1);('applies torque', 1);('pendulum halves', 1);('arbitrary directions', 1);('surprisingly', 1);('fig.5', 1);('` _\x122', 1);('m+msin2\x12', 1);('\x00m ` _\x122', 1);('m+m', 1);('cart mass', 1);('pole length', 1);('discussion', 1);('intrinsic motivation models', 1);('mimic decision-making abilities', 1);('biological or- ganisms', 1);('various situations', 1);('explicit reward signals', 1);('information-theoretic formulation', 1);('dynami- cal equations', 1);('controller improves', 1);('fu- ture', 1);('ac- tion sequence', 1);('subsequent responses', 1);('sensitive points', 1);('simple reinforcement learning algorithms', 1);('pen- dula', 1);('depending', 1);('past actions', 1);('future responses', 1);('approach interpolates', 1);('original formulation', 1);('agent- environment pair', 1);('behaviors pro- 6figure', 1);('left', 1);('control force', 1);('x-y plane', 1);('right', 1);('seconds t2 [', 1);('] s.', 1);('di erent motivation functions', 1);('big challenge', 1);('information-theoretic quantities', 1);('signi cant contribu- tion', 1);('explicit algorithm', 1);('arbitrary lengths', 1);('small noise/small control approximation', 1);('interesting regime', 1);('empowerment gradient', 1);('es- timation', 1);('real control appli- cations', 1);('deep neural networks', 1);('relevant dynamical landscapes [', 1);('exact form', 1);('potential limitation', 1);('oppor- tunities', 1);('complex scenarios', 1);('work suggests', 1);('spec- trum', 1);('trajectory divergence', 1);('small arbitrary perturbation', 1);('initial perturbation', 1);('controllable directions', 1);('systematic study', 1);('future work', 1);('analysis relates', 1);('social in- teractions', 1);('interacting', 1);('own intrinsic moti- vations', 1);("'s ability", 1);('understanding', 1);('multiple agents interact', 1);('con ict', 1);('promising area', 1);('future research', 1);("'s empowerment", 1);('distin- guishes', 1);('social interactions', 1);('physical interactions', 1);('nearby individuals', 1);('acknowledgements st', 1);('califor-', 1);('nia state university', 1);('sjsu', 1);('in', 1);('simons', 1);('investi-', 1);('gator award', 1);('simons-emory consortium', 1);('nih', 1);('grant 2r01ns084844', 1);('dp', 1);('acknowledges partial support', 1);('ec h2020-641321', 1);('fet proactive', 1);('pazy foundation.references', 1);('p.-y', 1);('oudeyer', 1);('f. kaplan', 1);('intrinsic motiva- tion', 1);('computational approaches', 1);('r. s. sutton', 1);('a. g. barto', 1);('mit', 1);('k. doya', 1);(':219 {', 1);('s. mohamed', 1);('d. j. rezende', 1);('informa- tion maximisation', 1);('reinforce- ment learning', 1);('neural information pro-', 1);('k. gregor', 1);('d j. rezende', 1);('d. wierstra', 1);('arxiv preprint arxiv:1611.07507', 1);('k. baumli', 1);('d. warde-farley', 1);('s. hansen', 1);('v. mnih', 1);('relative', 1);('variational intrinsic control', 1);('aaai', 1);('arti', 1);('intelligence', 1);(':6732 {', 1);('t. kwon', 1);('levine', 1);('k. hausman', 1);('dynamics-aware', 1);('arxiv preprint arxiv:1907.01657', 1);('ahn', 1);('k. haus-', 1);('emergent', 1);('real-world robotic skills', 1);('o -policy reinforcement learning', 1);('rss', 1);('gupta', 1);('learning repre-', 1);('r. houthooft', 1);('x. chen', 1);('y. duan', 1);('j. schulman', 1);('f. de turck', 1);('vime', 1);('infor- mation', 1);('neural in- formation processing systems', 1);('j. achiam', 1);('d. amodei', 1);('option discovery algorithms', 1);('arxiv preprint arxiv:1807.10299', 1);('j. choi', 1);('h. lee', 1);('s. s. gu', 1);('vari-', 1);('ational empowerment', 1);('representation learning', 1);('d polani', 1);('em-', 1);('keep', 1);('prin- ciple', 1);('sensorimotor systems', 1);('plos one', 1);('empower- ment estimation', 1);('neural information processing sys-', 1);('deeprl', 1);('t. m.', 1);('a. thomas', 1);('elements', 1);('information theory', 1);('john wiley', 1);('empow-', 1);('vol- ume', 1);('ieee', 1);('a. gupta', 1);('e. kiciman', 1);('a. d. dragan', 1);('ave', 1);('assistance', 1);('w. bialek', 1);('n. tishby', 1);('predictabil-', 1);(':2409 {', 1);('c. holmes', 1);('estimation', 1);('mutual infor- mation', 1);('phys rev e', 1);('k. lu', 1);('em- powerment estimation', 1);('a. y. ng', 1);('d. harada', 1);('s. russell', 1);('policy', 1);('reward transformations', 1);('p. dayan', 1);('g. e. hinton', 1);('feudal', 1);('neural information processing sys- tems', 1);('t. d. kulkarni', 1);('k. narasimhan', 1);('a. saeedi', 1);('j. tenen-', 1);('hierarchical', 1);('deep reinforcement learning', 1);('inte-', 1);('temporal abstraction', 1);('inadvances', 1);('y. burda', 1);('a. storkey', 1);('large-scale', 1);('curiosity-driven learning', 1);('p. agrawal', 1);('curiosity-driven', 1);('predic- tion', 1);('icml', 1);('m. karl', 1);('m. soelch', 1);('p. becker-ehmck', 1);('d. benbouzid', 1);('p.', 1);('van der', 1);('smagt', 1);('j. bayer', 1);('unsupervised', 1);('real-time control', 1);('variational empowerment', 1);('approximation', 1);('continuous domain', 1);('complex systems', 1);('t. anthony', 1);('general self-motivation', 1);('strategy identi cation', 1);('case', 1);('computational intelligence', 1);('h. j. charlesworth', 1);('m. s. turner', 1);('collective motion', 1);(':15362 {', 1);('j. schmidhuber', 1);('formal', 1);('autonomous mental', 1);(':230 {', 1);('h. d. abarbanel', 1);('r. brown', 1);('m. b. kennel', 1);('local lya- punov exponents', 1);('nonlinear', 1);(':343 {', 1);('l. c. evans', 1);('mathematical optimal control theory version', 1);('a. d. kuo', 1);('pendulum analogy', 1);('hu-', 1);('man movement science', 1);(':617 {', 1);('b. c. daniels', 1);('adaptive inference', 1);('phenomenological dynamical models', 1);('nature', 1);('s. l. brunton', 1);('j. l. proctor', 1);('j. n. kutz', 1);('discovering', 1);('sparse identi cation', 1);('nonlinear dynamical systems', 1);(':3932 {', 1);('b. chen', 1);('k. huang', 1);('s. raghupathi', 1);('i. chandratreya', 1);('h. lipson', 1);('mental variables', 1);('experimental data', 1);('nature computational', 1);(':433 {', 1);