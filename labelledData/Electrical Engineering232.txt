('tsai', 12);('different modalities', 9);('tav', 8);('mult', 8);('safrlm', 8);('proceedings', 6);('fusion representations', 6);('multimodal', 5);('zadeh', 5);('text audio sequences', 5);('articial intelligence', 4);('ta', 4);('ncrossmodal', 4);('pham', 4);('figure', 4);('cmumosi cmumosei', 4);('lp', 3);('annual meeting', 3);('morency lp', 3);('cmumosei', 3);('cmumosi', 3);('experiment results', 3);('specically', 3);('proceedings aaai', 2);('computational linguistics', 2);('aaai', 2);('language processing', 2);('baseline models', 2);('glove', 2);('feature', 2);('rahman', 2);('audio sequences', 2);('youtube', 2);('experimental', 2);('2019our run', 2);('ven pham', 2);('modality acch7acch2f1hmaelcorrheflstm tav', 2);('input sequences', 2);('previous work', 2);('bigru', 2);('unimodal representations', 2);('ftandfa', 2);('text audio', 2);('adaptthe fusion representations', 2);('vaswani', 2);('robust jointrepresentations', 2);('robust fusion representations', 2);('performance metrics', 2);('public multimodal sentiment analysis datasets', 2);('adapt fusion representations', 2);('crossmodal adjustment transformer', 2);('full use interaction', 2);('different', 2);('selfadjusting fusion representation learning model safrlm', 2);('different modalities sequences', 2);('selfadjusting fusion representation learning model', 2);('processing', 1);('selected topics', 1);('ieee', 1);('learning information fusion applications', 1);('representation', 1);('yang z x deng', 1);('22362246zhang c', 1);('longpapers', 1);('interpretable dynamic fusiongraph', 1);('language analysis thewild', 1);('liang p p poria cambria e', 1);('8288zadeh b', 1);('ieee intelligent systems316', 1);('verbal messages', 1);('facialgestures', 1);('2016multimodal sentiment intensity analysis videos', 1);('nihpublic accesszadeh zellers r pincus e morency lp', 1);('articial intelligence aaai', 1);('recurrent network forhuman communication comprehension', 1);('multiattention', 1);('articial intelligence zadeh liang p p poria vij p cambria e', 1);('sequential learning proceedings thirtysecond aaai', 1);('memory fusion network', 1);('liang p p mazumder n poria cambriae morency lp', 1);('methods', 1);('conference onempirical', 1);('tensor fusion network multimodal sentiment analysis proceedings', 1);('chen poria cambria e morencylp', 1);('annual meeting association computational linguistics 37183727zadeh', 1);('annotation modality', 1);('chinese multimodal sentiment analysis dataset', 1);('chsims', 1);('zhu wu j zou jand yang k', 1);('xu h meng', 1);('international conference multimedia 521528yu w', 1);('acm', 1);('textaudio sentiment analysis', 1);('cmbert crossmodalbert', 1);('k xu h gao k', 1);('multimodal sentimentanalysis', 1);('network aspect', 1);('multiinteractivememory', 1);('chen g', 1);('n mao', 1);('multimodal languagechallengehml', 1);('proceedings grandchallenge', 1);('dnn', 1);('emotions video', 1);('recognizing', 1);('j kleinegesse comanescu r raduo', 1);('nonverbal behaviors', 1);('word representations', 1);('dynamically', 1);('words', 1);('shen liu z liang p p zadeh', 1);('systems 59986008wang', 1);('advances', 1);('attention', 1);('shazeer n parmar n uszkoreit j jonesl gomez n kaiser polosukhin', 1);('patternrecognition', 1);('interaction review', 1);('accessturk', 1);('nih', 1);('computational linguistics meeting', 1);('conference association', 1);('multimodal language sequences', 1);('h h bai liang p p kolter j z morencylp salakhutdinov r', 1);('integrating multimodalinformation large pretrained transformers proceedings', 1);('mao cmorency lp hoque e', 1);('hasan k lee zadeh', 1);('paper pdf', 1);('urlhttpss3uswest2', 1);('language understanding generative', 1);('improving', 1);('narasimhan k salimans', 1);('robust jointrepresentations cyclic translations modalitiesinproceedings', 1);('learning', 1);('found', 1);('andpoczos b', 1);('h liang p p manzini morency lp', 1);('papers', 1);('linguistics', 1);('association forcomputational', 1);('efcient lowrankmultimodal fusion modalityspecic factors proceedings', 1);('liang p pzadeh', 1);('z shen lakshminarasimhan v', 1);('empiricalmethods', 1);('language analysis recurrent multistagefusion proceedings', 1);('p p liu z zadeh', 1);('discriminative representation learning speechemotion recognition ijcai', 1);('r wu z jia j bu zhao meng h', 1);('theieee conference computer vision pattern recognition 770778li', 1);('residual learning image recognition', 1);('deep', 1);('k zhang x ren sun j', 1);('empirical methods naturallanguage processing', 1);('proceedingsof', 1);('intermodal attention multimodal sentiment analysis', 1);('contextual', 1);('akhtar chauhan poria ekbala bhattacharyya p', 1);('short papers', 1);('language technologies', 1);('associationfor computational linguistics', 1);('north american chapter', 1);('understanding proceedings', 1);('pretraining deep bidirectional transformers', 1);('ieeedevlin j chang mw lee k toutanova k', 1);('international conference acoustics speech signal processing icassp', 1);('collaborative voice analysis repository speech technologies', 1);('cov arepa', 1);('g kane j drugman raitio scherers', 1);('pattern analysis machine intelligence412 423443degottex', 1);('ieeetransactions', 1);('machine learning survey taxonomy', 1);('ahuja', 1);('language model unimodal multimodal toimprove performance multimodal sentiment analysisreferencesbaltru saitis', 1);('language model explore', 1);('fusion representations future becausethere', 1);('accurate predictionsafter', 1);('audio modality information account', 1);('qualitative analysis proves model revise sentiment intensity', 1);('additionally', 1);('datasets showthat', 1);('text audio information experiment results', 1);('protects characteristics modality core unitof model crossmodal adjustment transformer', 1);('previous works model', 1);('selfadjusting fusion representation learning model safrlm different', 1);('conclusionin', 1);('text audiosequences6', 1);('fusion representations robust', 1);('adjusts fusion representations combiningthe unimodal information', 1);('main reason model', 1);('tothe ground truth', 1);('prediction method', 1);('example2 example4', 1);('original characteristics ofdifferent modalities', 1);('different modalities alsomaximizes properties', 1);('fulluse interaction', 1);('multfrom example1 example3', 1);('prediction thetruth label', 1);('similar thesecond example model', 1);('example4', 1);('audio modalityinformation account', 1);('proves model revise sentiment intensity', 1);('wrong emotionaljudgment example', 1);('attentionto text modality causes', 1);('aright prediction', 1);('strong negative help intermodal interaction text audio modalities model', 1);('positive contrast audio information performs', 1);('example3', 1);('predictedsentiment result model', 1);('safrlm mult', 1);('strong negative sentiments', 1);('text andaudio information', 1);('example2', 1);('audio modality prediction probability', 1);('accurate prediction probability \x00115however', 1);('compared mult', 1);('positive truth label ofthe example is\x0010', 1);('strong negative toneof speaker performs weak', 1);('multin example1', 1);('behaviors showthe ground truth example sentiment predictions model', 1);('dataset example', 1);('analyze impact', 1);('mult52 qualitative analysiswe', 1);('positive example show ground truth prediction thesafrlm', 1);('dataset ground truth labels range', 1);('examples cmumosi', 1);('mild', 1);('right hair', 1);('fit actual sabre tooths', 1);('242know things', 1);('broken', 1);('073umm yeah', 1);('frustrated', 1);('051i cant', 1);('tosardonic tone08', 1);('im', 1);('pretty giant smile', 1);('truthsafrlmoursmulthi im', 1);('behaviors 1234ground', 1);('speak', 1);('different numbers crossmodal blocks themosi datasetspoken words', 1);('acch2', 1);('2class accuracy', 1);('14accuracynumber crossmodal blocksfigure', 1);('decreaseof binary accuracy7807837867897927957988018048078108132', 1);('number crossmodal blocks complexityof model increases', 1);('crossmodal blocks withthe', 1);('4to10 achievesthe', 1);('audio modalitythe results', 1);('14n2blocks usedfor text modality', 1);('nnumbers ofcrossmodal blocks n', 1);('acch2 safrlm', 1);('shows the2class accuracy', 1);('thecrossmodal adjustment transformer core unit ourmodel number crossmodal blocks', 1);('number crossmodal blocks', 1);('text audio sequenceseffect', 1);('improves performance', 1);('maximizes theprotection unimodal characteristics', 1);('compared multthe safrlm', 1);('recurrent networks convolutional networks', 1);('adopts transformer', 1);('model performance', 1);('datasets proves efciency generalizationof', 1);('superior performance', 1);('alsoachieves 0021and0017performance improvementsthe', 1);('onmaelandcorrh', 1);('improves 06onacch207onf1h 10onacch7', 1);('experimental conditions input text audio sequences ourmethod', 1);('whats', 1);('onmaeland improves 00170088oncorrh', 1);('percentage points regression task', 1);('807onacch2and812onf1h 1453and1555improvement baseline modelsin sentiment score classication task model achieves499onacch7 improves', 1);('modalities binarysentiment classication task model', 1);('thebaseline models', 1);('model use text audio sequences andit', 1);('similarto mosi', 1);('cmumoseidataset', 1);('generalization method otherdatasets', 1);('onmaelandcorrhour', 1);('performances theevaluation metrics approach improves 19onacch220onf1h 07onacch7', 1);('transformermodel fair comparison input unalignedtext audio sequences', 1);('acch7andmael', 1);('modalities model outperforms', 1);('compared multwhich', 1);('reduces 0080170onmaeland improvesabout', 1);('percentage points higherover baselines regression task', 1);('baseline models sentiment score classication task model', 1);('modalities binary sentiment classication task model', 1);('performance theevaluation metrics', 1);('experiment results thecmumosi dataset', 1);('baseline', 1);('baseline models addition wediscuss effect number crossmodal blocks onexperimental resultscomparison', 1);('section show performance proposedmodel', 1);('discussion51 quantitative analysisin', 1);('current stateoftheart method', 1);('adapts streams', 1);('distinct timesteps', 1);('directional pairwise crossmodal attention interactions multimodal sequences', 1);('multimodal transformer multuses', 1);('available datasets multimodal sentiment analysis emotionrecognitionmult', 1);('competitive performance', 1);('nonverbal cues itachieves', 1);('models negrainedstructure nonverbal subword sequences dynamicallyshifts word representations', 1);('recurrent attended variation embedding network ra ven', 1);('ven wang', 1);('sourcemodality inputra', 1);('joint representations', 1);('different modalitiesand', 1);('multimodal cyclic translation network mctn', 1);('contextual information eachmodality concatenate output', 1);('uses singlelstm model', 1);('late fusion lstm lflstm', 1);('contextual informationlflstm', 1);('lstm', 1);('concatenatesmultimodal inputs uses', 1);('early fusion lstm eflstm', 1);('previous methods inmultimodal sentiment analysis task methods', 1);('baselineswe', 1);('experimental results44', 1);('runsas nal', 1);('average result', 1);('select seeds', 1);('performance model exceptformae', 1);('value themetrics means', 1);('regression task', 1);('model predictions', 1);('corr', 1);('andthe correlation', 1);('mae', 1);('absolute error', 1);('binary sentiment classication task', 1);('f1', 1);('acc2 f1', 1);('sentiment score classication task 2class accuracy', 1);('acc7is', 1);('method 7class accuracy', 1);('tsaiet', 1);('experiments consistent', 1);('hz43 evaluation metricsin', 1);('glottal source parameters peak slopeparameters maxima dispersion quotients thesefeatures', 1);('mfccs', 1);('cepstral coefcients', 1);('melfrequency', 1);('dimensional vector including12', 1);('extract audio', 1);('cov arep degottex', 1);('work use', 1);('common crawl datasetaudio', 1);('dimensional word vectors', 1);('words sequences video transcripts', 1);('word embeddings', 1);('extraction methodfor text audio modalitiestext', 1);('previous works', 1);('feature extractionto', 1);('l1loss', 1);('learning rate', 1);('adam', 1);('process number batch size epoch setto', 1);('dropout rate', 1);('layers usedin model', 1);('number channels thetemporal convolutional layer', 1);('makesure validity experiment strategy', 1);('hz', 1);('movie review videoclips', 1);('multimodal sentiment emotion analysis dataset', 1);('similarlycmumosei', 1);('videos training validation test', 1);('positive negative data wesplit', 1);('training testingsets balance', 1);('hz consideringthe', 1);('cmumosiare', 1);('utterances utterance annotatedin range', 1);('movie reviews videos', 1);('opinion videosdownload', 1);('intensity cmumosei zadeh', 1);('cmu multimodal opinion sentiment', 1);('cmu multimodalopinionlevel sentiment intensity cmumosi zadehet', 1);('datasets experimental settingswe', 1);('introduce evaluation metrics andthe baseline models', 1);('extraction section', 1);('experimental settings section', 1);('subsections section 41shows information datasets', 1);('cmumosiand cmumosei', 1);('thepublic multimodal sentiment analysis datasets', 1);('experimentsin', 1);('asloss lossta0losst0alossta', 1);('loss', 1);('global classierat time objective function', 1);('train local classiers', 1);('global classier importantlywe optimize model single objective functionwhich', 1);('temporalinformation fusion representation output theselfattention transformer', 1);('corresponding local classier hand theselfattention transformer', 1);('hand fusion representations trainedby', 1);('onta0andont0a', 1);('transformer classier', 1);('blocks nal', 1);('onta0ofthe', 1);('ea0', 1);('blocks adjustonta0by', 1);('blocksonta0 introduce', 1);('flln mita0 mita0', 1);('ln mhita0oi\x001ta0', 1);('oita0is', 1);('16the output ith layers', 1);('qita0kita0pdvita0', 1);('asq1ta0lneta0 15mhita0softmax', 1);('andoi\x001ta0is output i\x001th crossmodal block multihead attention', 1);('qita0 ln oi\x001ta0kita0vita0 ln et', 1);('multihead attention ith blocks', 1);('query key value', 1);('layer normalization', 1);('residual connection', 1);('multihead attention feedforwardlayer', 1);('crossmodal block', 1);('et', 1);('text representations', 1);('fusion representationseta0by', 1);('layer normalization weadopt', 1);('lnpefta0ta0gxfta0ta0g14where lnrepresents', 1);('bylayer normalizationefta0ta0g', 1);('\x04d2\x05 weadd position', 1);('computedas followspepos2i sin pos100002id 12pepos2i1 cos pos100002id 13where pos', 1);('dimension position', 1);('sequencex2rl\x02dlrepresents sequence length drepresentsthe', 1);('pe', 1);('temporal information input sequences', 1);('xa0 topreserve', 1);('audio representations', 1);('xt', 1);('xta0', 1);('inputfusion representations', 1);('fusion representationsxta0as', 1);('xtandxa0to', 1);('xta0\x00xtxa0', 1);('introduce structure throughthe example', 1);('forconvenience', 1);('different unimodal information', 1);('fusion representations tobe', 1);('design crossmodal adjustment transformer', 1);('adjustment transformer based', 1);('introduce selfattention transformer classiercrossmodal', 1);('crossmodal adjustment transformer core unit', 1);('unimodal streams', 1);('original characteristics modalitythe', 1);('selfadjusting modulein', 1);('different unimodal representations bis bias34', 1);('10xt0awt0xt0waxabt0a 11where wtat0a0represents weight', 1);('xta0andxt0axta0wtxtwa0xa0bta0', 1);('different fusion representations', 1);('xtandxa0xt0andxarespectivelyand', 1);('xta0\x00xtxa0after', 1);('architecture crossmodal adjustmenttransformer', 1);('normfigure', 1);('normmulti head attentionfeed forwardlayer norm layer normblock', 1);('head attentionfeed forwardlayer norm layer normaudioadjusted fusionrepresentationblock', 1);('xa', 1);('xt0andxa0xt0ota xt', 1);('attentive representations', 1);('important information', 1);('differentmodalities focus', 1);('6oatsatxt 7then matrix multiplication', 1);('otaandoatotastaxa', 1);('soft attention compute modalitywise attentive representations', 1);('tanh mat', 1);('tanh mta', 1);('staandsatare', 1);('functionthe attention score matrices', 1);('softmax', 1);('function compute attention score', 1);('tanh', 1);('crossmodality informationmtaxtxa 2matxaxt 3then pass attention matrices', 1);('mtaandmat', 1);('rst compute pair attention matrices', 1);('enable onemodality', 1);('weemploy crossmodal collaboration attention', 1);('ghosal', 1);('inspired mmmuba', 1);('purpose fusion representation initialization module initialize fusion representations intermodal interaction text audio modalities', 1);('fusion representation initialization modulethe', 1);('initialize thefusion representations33', 1);('xtandxaare', 1);('alignedtext audio', 1);('extracttemporal information modality', 1);('kta sta 1where ktaandstarepresent number convolutional kernels strides text audio modalities', 1);('fa', 1);('taconv', 1);('different size convolutional kernelsand stridesconv', 1);('dimension setting', 1);('sequencesthrough 1d temporal convolutional layer', 1);('text audio sequencesafter', 1);('method aims', 1);('crossmodal alignment moduleour', 1);('adapt fusion representationsxta0andxt0a32', 1);('xtxaxt0andxa0to', 1);('textaudio sentiment analysis performance', 1);('xta0andxt0a thegoal', 1);('initialize fusion representations', 1);('xt0andxa0to', 1);('unimodal attentive representations', 1);('attention mechanismto', 1);('xtandxa', 1);('alignment text audio representations', 1);('rstlywe project', 1);('problem denitiongiven', 1);('modules model section 32section', 1);('thenwe', 1);('present problem denition section', 1);('text audio unimodal information followingwe rst', 1);('fusion representationsthrough intermodality interaction text audio modalities', 1);('fusion representation initialization module', 1);('map text audio', 1);('crossmodal alignment module', 1);('firstly', 1);('fusion representation learning model safrlm', 1);('section introduce architecture', 1);('methodologyin', 1);('selfadjusting fusion representation learning modeleach', 1);('overview', 1);('fusion text audio text would shorter conv1dconv1dbigrubigrutext audiocrossmodal alignment modulecrossmodalcollaboration attentionconcatenatelocal classifierlocal classifierglobalclassifierselfadjusting module fusion representation initialization moduletransformertransformercrossmodaladjustment transformercrossmodaladjustment transformeraudio figure', 1);('information offusion', 1);('problem design crossmodal adjustment transformer', 1);('original characteristics differentmodalities', 1);('focuses interaction modalities ignores', 1);('howeverthe mult', 1);('outperforms stateoftheart methods', 1);('lowlevelfeatures experiment results', 1);('multiple stacks pairwise bidirectionalcrossmodal attention blocks', 1);('multimodal transformer mult', 1);('unimodal multimodal', 1);('transformer modelon text modality', 1);('al2019all methods', 1);('devlin', 1);('natural language processing tasks', 1);('new stateoftheart resultson', 1);('contextin layers', 1);('bidirectional transformer networks generate contextual word representations', 1);('bidirectional encoder representations transformers bert', 1);('longterm dependencies text', 1);('generative pretraining gptwhich', 1);('adopttransformer networks', 1);('radford', 1);('models frameworks', 1);('recent years transformer networks', 1);('computation speed', 1);('attention mechanisms', 1);('recurrent neural networks convolution neural networks encoderand decoder transformer networks', 1);('machine translation task', 1);('transformer network rst', 1);('transformer networkthe', 1);('different unimodal information22', 1);('changefusion representations', 1);('2019in crossmodal adjustment transformer', 1);('crossmodal attention', 1);('interact text audio modalitiesbesides', 1);('fusion representation initializationmodule', 1);('featuresamong model crossmodal collaborationattention', 1);('attention onmultimodal representations', 1);('multimodal multiutterancebimodal attention mmmuba', 1);('lstms ghosal', 1);('different dimensions ofmemories system', 1);('crossviewand temporal interactions', 1);('adeltamemory attention network', 1);('important role multimodal sentiment analysis', 1);('successful use attention mechanism computer vision plays', 1);('ofthese methods need align multimodal sequences thewordlevelwith', 1);('nonverbal cues', 1);('recurrent attended variationembedding network ra ven', 1);('dynamic nature nonverbal intentswang', 1);('new stateoftheart resultin order', 1);('modalities onlyuses text data test creates', 1);('multimodal cyclictranslation network mctn', 1);('machine translation', 1);('inspired', 1);('2018b employ multiattention block anda longshort term hybrid memory', 1);('unimodal predictors', 1);('geta signicant improvement', 1);('fusion approach concatenate multimodal', 1);('williams', 1);('innovative models', 1);('performance sentiment analysis', 1);('emotional relevantinformation', 1);('internal correlation', 1);('xu mao chen', 1);('sentiment textaudio video modalities', 1);('new research area thataims', 1);('sentiment analysis', 1);('cmumosi cmumosei2 related works21 multimodal sentiment analysismultimodal', 1);('public sentiment benchmark datasets', 1);('new stateoftheart textaudio sentiment analysis result', 1);('modality information show', 1);('text audio sequences design novel crossmodal adjustment transformerwhich', 1);('sentiment analysis whichcan', 1);('selfadjusting fusion representationlearning model textaudio', 1);('asfollows introduce', 1);('main contribution paper', 1);('fusion representationsthe', 1);('accurate predictions', 1);('takinginto account audio modality information makemore', 1);('sentiment intensity', 1);('thebaseline models outperforms 18\x0084on mostof metrics addition qualitative analysis proves thatour model', 1);('compared', 1);('new stateoftheart performance onthe', 1);('2018c experiments showthat model creates', 1);('cmumosi zadeh', 1);('alarge number experiments', 1);('effectiveness method', 1);('singlemodal streamsto', 1);('module employedto', 1);('initialize fusion representations theintermodal interaction text audio sequencesin order maximize preservation characteristics modality', 1);('samedimension crossmodal collaboration attention', 1);('rst use crossmodal alignmentmodule project', 1);('different modalities butalso maximizes maintenance characteristics ofunimodal', 1);('methods model makesfull use interaction', 1);('different modalities paperwe design', 1);('2022nore characteristics', 1);('nov', 1);('multimodal sequences igarxiv221211772v1 cscl', 1);('basedon wordlevel', 1);('2018however approaches', 1);('liang', 1);('multiple stages focuseson subset multimodal signals', 1);('recent attempt decomposes multimodal fusion problem', 1);('forbetter', 1);('liu', 1);('enhances sentiment analysis results', 1);('efciency reduces parameters', 1);('lowrank tensors', 1);('researchers performmultimodal fusion', 1);('multimodal fusion representations', 1);('product ofmultimodal', 1);('innovative methodshave', 1);('recently', 1);('zhanget', 1);('certain extent', 1);('different modalities determines performance', 1);('core problems multimodal sentiment analysis', 1);('extensive eld affective', 1);('positivemultimodal sentiment analysis', 1);('voice itwill', 1);('negative contrary', 1);('sure angryvoice', 1);('various emotions differentcontexts', 1);('sure ambiguous express', 1);('interaction text audiomodalitieslustrate intermodal interaction text audiomodalities sentiment sentence', 1);('intermodal', 1);('unimodal crossmodalangry', 1);('behaviors sentimentangry', 1);('sure speaker', 1);('wwwaaaiorg rights reservedare', 1);('advancement articialintelligence', 1);('example ilcopyright', 1);('sentiment analysis performance', 1);('morecomprehensive emotional information', 1);('intermodal interaction text audio sequences', 1);('yu', 1);('liet', 1);('text shows sentiment variations voice characteristics pitch energy vocaleffort loudness', 1);('audio', 1);('easy identify emotions', 1);('information containedin', 1);('turk', 1);('life expresses emotion words phrases andrelations', 1);('essential modality', 1);('text', 1);('xu gao', 1);('ahuja morency', 1);('baltru', 1);('verbal nonverbal behaviors', 1);('express sentiment', 1);('introductionpeople', 1);('textaudio sequences1', 1);('singlemodal streams', 1);('original unimodal characteristics', 1);('model intermodal interaction text audio sequences initialize fusion representations core unitof', 1);('dimension crossmodalcollaboration attention', 1);('rstemploy crossmodal alignment module project differentmodalities', 1);('maximizes protection unimodal characteristics', 1);('previousworks model', 1);('robust crossmodal fusion representations', 1);('relevant information modality', 1);('indispensable role multimodal sentiment analysis', 1);('interaction plays', 1);('power universityabstractintermodal', 1);('china3northeast', 1);('technologyshijiazhuang', 1);('university science', 1);('engineering hebei', 1);('china2school information', 1);('beijing', 1);('tsinghua', 1);('computer scienceand', 1);('systems', 1);('textaudio sequenceskaicheng yang12 ruxuan zhang3 hua xu1kai gao21state key laboratory intelligent', 1);