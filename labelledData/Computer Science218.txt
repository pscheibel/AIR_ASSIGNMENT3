('sirl', 42);('international conference', 17);('jacorobot', 16);('vae', 16);('gridrobot', 14);('march', 13);('similarity queries', 7);('figure', 7);('multipref', 7);('tpa', 6);('similaritybased implicit representation learning hri', 5);('ee', 5);('true features', 5);('preference queries', 5);('10hrandommultipref 50hvae10', 5);('ieee', 5);('eq', 4);('state space', 4);('mlps', 4);('learning', 4);('adam', 4);('reward learning', 3);('singlepref', 3);('h1', 3);('singlepref multipref', 3);('lastly', 3);('robotics', 3);('vol', 3);('learning rate', 3);('representationversus spurious', 2);('aspects behavior', 2);('bycontrast', 2);('favorite mug', 2);('similar trajectories', 2);('inversereinforcement learning', 2);('useful learning', 2);('reward function', 2);('rthat', 2);('sec', 2);('trajectories', 2);('top', 2);('trajectory stays', 2);('bottom', 2);('output neurons', 2);('good representations', 2);('fpe', 2);('generalizable reward learning', 2);('10hrandommultipref 50hvaefigure', 2);('human data', 2);('learns representations predictive', 2);('specific preference', 2);('close laptop', 2);('heldout', 2);('similarity', 2);('f1', 2);('humans humans', 2);('pmlr', 2);('andreea bobu marius wiggert claire tomlin anca dragan', 2);('rank', 2);('machine learning pmlr', 2);('advances neural informationprocessing systems', 2);('humanrobot', 2);('kelvin xu ellis ratner anca dragan sergey levine chelsea finn', 2);('training', 2);('relu', 2);('loss function', 2);('representations unfrozen', 2);('simple environments', 2);('frozensirl unfrozensirlvae frozensirlvae unfrozen10', 2);('frozensinglepref unfrozensingleprefvae frozensingleprefvae unfrozen10', 2);('preference queries050607080910tp agridrobot test preference accuracy tp n1000multipref', 2);('10h frozenmultipref 10h unfrozenmultiprefvae 10h frozenmultiprefvae 10h unfrozen10', 2);('preference queries050607080910tp ajacorobot test preference accuracy tp n1000multipref', 2);('50h frozenmultipref 50h unfrozenmultiprefvae 50h frozenmultiprefvae 50h unfrozen10', 2);('sirl similaritybased implicit representation learningandreea bobuabobuberkeleyeduuniversity california berkeleyunited', 1);('americayi liuyiliu77berkeleyeduuniversity california berkeleyunited', 1);('americarohin shahrohinmshahdeepmindcomdeepmind researchunited kingdomdaniel browndsbrownberkeleyeduuniversity california berkeleyunited', 1);('americaanca draganancaberkeleyeduuniversity california berkeleyunited', 1);('americasimilarity', 1);('querypreference query', 1);('h1preference', 1);('h2preference', 1);('h3optimize optimize optimize figure', 1);('representations robot behavior', 1);('salient people', 1);('generalizable preference learning', 1);('low sample complexity', 1);('extract representation', 1);('people trajectory similarity queries leftwhere judge', 1);('similar use representation', 1);('different peoples preferences', 1);('different tasks rightabstractwhen robots', 1);('reward functions', 1);('high capacity modelsthat', 1);('raw state', 1);('input need', 1);('arepresentation matters task task', 1);('ifthey', 1);('fullreward function', 1);('easy end representation thatcontains spurious correlations data', 1);('new settings', 1);('ultimate goal', 1);('enable robots toidentify isolate causal', 1);('careabout use', 1);('states behavior ideais tune representation', 1);('users whatbehaviors', 1);('similar behaviors', 1);('similar theboth authors', 1);('research research', 1);('naval', 1);('onr', 1);('investigator', 1);('weill neurohub', 1);('aiml', 1);('sid reddy andrea bajcsy', 1);('hard copies part work', 1);('personal orclassroom use', 1);('distributedfor profit commercial advantage copies', 1);('full citationon', 1);('copyrights', 1);('thirdparty components work', 1);('honoredfor uses contact ownerauthorshri', 1);('stockholm sweden2023 copyright', 1);('isbn', 1);('97814503996472303httpsdoiorg10114535681623576989features matter', 1);('lowlevel behavior', 1);('thefeatures matter differs', 1);('robotto disambiguate needs', 1);('versus notion learning representations', 1);('nice parallel contrastive learninga', 1);('representation learning technique maps', 1);('similar data points', 1);('similar embeddings similarityis', 1);('designer data augmentation heuristics', 1);('representations people', 1);('preferences objectives', 1);('definitionof similarity simulation', 1);('user study show', 1);('taskinput alternativesacm', 1);('reference formatandreea bobu yi liu rohin shah daniel brown anca dragan2023 sirl similaritybased implicit representation learning proceedingsof', 1);('acmieee', 1);('humanrobot interactionhri', 1);('stockholm sweden acm', 1);('york ny usa12', 1);('pages httpsdoiorg10114535681623576989arxiv230100810v2 csro', 1);('jan', 1);('stockholm sweden tbd1 introductionimagine', 1);('morning home robot assistantwants place', 1);('fresh coffee table exactlywhere', 1);('depending', 1);('context willhave', 1);('different preference robot itstask days', 1);('close table toprevent', 1);('case slip remainyour', 1);('favorite mug days steam delicious mealis', 1);('robots perception youd', 1);('large clearance table', 1);('similarlysome', 1);('days mug anespresso shot', 1);('laptop toprevent clutter', 1);('rest table', 1);('open youthe reward function robot optimize changes whetherdue variations task', 1);('different users theexamples', 1);('different contexts', 1);('part therobot state eg', 1);('regular mug', 1);('representation top reward isbuilt ie', 1);('thetable laptop etc', 1);('robot learnsthis representation', 1);('task user context changes', 1);('metalearningand', 1);('multitask learning methods', 1);('representation user input', 1);('full reward', 1);('preferencequeries demonstrations contrast', 1);('learninggeneralizable representations goal', 1);('theuser input', 1);('full reward hopingto extract good representation', 1);('robots representations', 1);('easy arethe', 1);('care aboutwhile techniques advocate people', 1);('users teacheach', 1);('able toexplicate representation', 1);('concepts thatare', 1);('teachable work idea', 1);('tune representations people', 1);('themto proxy task', 1);('similarity behaviors', 1);('behaviorswill', 1);('similar features matter', 1);('different evenone', 1);('matter differs', 1);('enablethe robot', 1);('robots thatcan disambiguate needs', 1);('introduce novel type human input', 1);('robotextract persons representation trajectory similarity queries trajectory similarity query triplet trajectories theperson answers', 1);('infigure', 1);('person chooses', 1);('trajectories closeto table', 1);('theylook dissimilar results anchor', 1);('positive negative tripletthat', 1);('similaritybased implicit representation learning sirlour', 1);('method parallel', 1);('learning work', 1);('contrastive learning goal', 1);('good visualrepresentation training anchor', 1);('data augmentation techniques', 1);('thisnotion similarity', 1);('designedheuristics data augmentation', 1);('reflective ofwhat users', 1);('similar instance', 1);('images mightbe', 1);('different fact difference onlywith respect lowlevel aspects', 1);('relevantto distribution tasks people care', 1);('resultin representations', 1);('present humans representation method usessimilarity defer users judgement similaritywith goal', 1);('users representation course method', 1);('representations experiments', 1);('outperforms methods', 1);('full tasks simulation', 1);('learns representations', 1);('multiple moregeneralizable reward functions', 1);('alsopresent user study crowdsource similarity queries fromdifferent people', 1);('individual preferences study resultsdo show', 1);('significant effect effect size', 1);('attributable part interface difficulty', 1);('robot trajectories means work neededto', 1);('enable users accuratelyanswer similarity queries', 1);('different features means similarityqueries', 1);('nonetheless', 1);('ourresults underscore gains', 1);('robotand human representations', 1);('abyproduct learning rewards', 1);('standard queries2', 1);('related worklearning', 1);('input humanintheloop', 1);('paradigm robot uses human input toinfer policy reward function', 1);('behaviorin imitation learning robot learns policy essentiallycopies human demonstrations', 1);('training regime', 1);('irl', 1);('uses demonstrations extracta reward function', 1);('specific behavior desirablethus', 1);('unseen scenarios', 1);('recent', 1);('types human inputfor reward learning corrections', 1);('unless', 1);('alatent representation', 1);('respective human inputwe', 1);('preferenceagnostic latent spacethat', 1);('wefocus', 1);('learning models human reward functions', 1);('pairwisepreference queries', 1);('latent space learncan', 1);('useful learning types feedbackrepresentation', 1);('similarity learning', 1);('common representation learning approaches', 1);('human supervision', 1);('stockholm swedennot', 1);('person cares', 1);('priorwork', 1);('leverages task labels', 1);('trajectory rankings', 1);('learnlatent spaces', 1);('specific goals preferences', 1);('focus learning taskagnositic measures featuresimilarity', 1);('multiple preferences', 1);('somework', 1);('select features', 1);('active learningtechniques', 1);('learning lowerdimensional', 1);('representation allatonce', 1);('ata time', 1);('furthermore', 1);('human providephysical demonstrations learning good', 1);('space 1011we', 1);('accessible general form human', 1);('user triplets trajectories', 1);('themto label', 1);('triplet', 1);('similarity models', 1);('howhumans perceive objects', 1);('triplet loss', 1);('generaltaskagnostic similarity model humans perceive trajectoriesmeta', 1);('multitask reward function learning', 1);('learnmultiple models human reward functions', 1);('demonstrations learning', 1);('different rewardfunction cluster', 1);('methods requirea', 1);('large number demonstrations adapt', 1);('new rewardfunctions', 1);('metalearning', 1);('reward function initialization', 1);('test time', 1);('58multitask reward learning approaches pretrain reward functionon', 1);('multiple human intents finetune reward functionat test time', 1);('large set training environments contrast assumeany knowledge testtime task distribution priori notrequire access population', 1);('different reward functions training', 1);('focus learning taskagnostic featurerepresentation', 1);('reward learning tasks', 1);('particular test', 1);('representation thedownstream task learning models human reward functionsvia pairwise preference queries trajectories', 1);('methodwe', 1);('present method learning preferenceagnostic representations trajectory similarity queries intuition ahuman judges', 1);('similar representations', 1);('explicit threshold insteadpresent human triplet trajectories', 1);('dissimilar onewe use humans answers train representation thatsimilar trajectories embeddings', 1);('dissimilartrajectories map embeddings', 1);('robot uses thislatent space', 1);('multiple people', 1);('different preferences31', 1);('preliminarieswe', 1);('trajectory sequence states', 1);('possible trajectories humans preferenceover trajectories', 1);('human interaction robot reasons', 1);('approximation ofthe reward function whererepresents parameters aneural network', 1);('human preferencelabels trajectories', 1);('parameters thatmaximize likelihood human input robot thenuse', 1);('reward function score trajectories', 1);('order align behavior', 1);('particular humanspreferences focus', 1);('human input', 1);('learna good representation use representation', 1);('rewardspecific humaninput eg preferences demonstrations', 1);('therepresentation time reward function32', 1);('training feature representation', 1);('similarity querieswe', 1);('train latent space', 1);('preference learning tasks', 1);('learning apreferenceagnostic model human similarity', 1);('way learnsuch model', 1);('users judge', 1);('relativerather binary quantitative assessments similarity 3553thus', 1);('internal threshold', 1);('measure similarity insteadfocus qualitative trajectory similarity queries', 1);('theuser visualization', 1);('similar ones', 1);('dissimilar onethe humans queries form data', 1);('d12', 1);('where1and2are trajectories', 1);('similar thetrajectory dissimilar twowe', 1);('similarity dissimilarity distance function', 1);('trajectories 2feature distance', 1);('dataset oftrajectory similarity queries', 1);('triplet loss 7lmax01a form contrastive learning anchor thepositive example', 1);('negative example 0is marginbetween', 1);('positive negative pairs', 1);('explicit anchor', 1);('final loss followsld1l12l212we train similarity', 1);('minimizes similarity loss representation dimensionality intuition', 1);('loss pushtogether embeddings', 1);('theembeddings dissimilar trajectories training representation loss', 1);('sirl reward learninggiven', 1);('learning models ofspecific user preferences focus learning pairwisehri', 1);('stockholm sweden tbdpreferences', 1);('note principle', 1);('types human feedback', 1);('whenlearning', 1);('reward function human preferences', 1);('thesetwo human', 1);('humans preferences lens', 1);('bradleyterry', 1);('preference model', 1);('simple crossentropy lossld0log1log434', 1);('adapting different user preferenceswe', 1);('robots adapt changes', 1);('individual userspreferences', 1);('adapt tonew users preferences', 1);('new set human data training', 1);('new reward function study', 1);('canleverage latent space', 1);('accurateand sampleefficient multipreference learning learning anew users preference model robot use quicklylearn reward function', 1);('main idea becausethis', 1);('latent representation', 1);('preferenceagnosticsimilarity queries', 1);('multiple specific task objectives', 1);('furthermore sirluses', 1);('human input train hypothesize', 1);('human reward functionsthan latent space', 1);('experiments simulationwe', 1);('sirltrained', 1);('representations andtheir benefits preference learning', 1);('human inputin', 1);('environments ground truth rewards features41', 1);('environmentsgridrobot figure', 1);('2a 5by5gridworld', 1);('obstaclesand laptop', 1);('blue green black boxes', 1);('the19dimensional', 1);('andcoordinates eachstate', 1);('angle 9060300306090at end state', 1);('human answers queries', 1);('4featuresin world', 1);('euclidean', 1);('distances object andthe', 1);('absolute value angle orientationjacorobot', 1);('2b pybullet', 1);('environmentwith 7dof', 1);('jaco', 1);('robot arm tabletop human andlaptop environment', 1);('dimensions positions robot jointsand objects rotation matrices results 2037dimensional input space', 1);('gridrobot the4', 1);('human table', 1);('viewsquery answerstrajectory replayquery idb jacorobot', 1);('environment user study interfacefigure', 1);('visualization', 1);('experimental environmentsdistance robots', 1);('endeffector ee', 1);('table b upright', 1);('relative upright', 1);('upright c laptop plane distance', 1);('passes laptop anyheight proxemics', 1);('proxemic plane distance', 1);('tothe human', 1);('front human sidein', 1);('trajectory spacecan', 1);('continuous construct', 1);('shortest pathtrajectories', 1);('startgoal pairs', 1);('appa1', 1);('generate similarity preference queries', 1);('human answers similarity queriesby', 1);('spacefor preference queries', 1);('human computes groundtruth reward samples trajectory', 1);('rewardthe space', 1);('true reward functions', 1);('simulate preferencelabels', 1);('linear combinations', 1);('describedabove robot', 1);('access groundtruth featuresnor groundtruth reward function', 1);('fromsimilarity preference labels', 1);('raw trajectory observations42', 1);('qualitative examplesin figure', 1);('similar dissimilar trajectories', 1);('environment laptopand joint angle', 1);('cup side', 1);('learns trajectoriesthat share', 1);('dissimilar insirl', 1);('stockholm swedenfigure', 1);('similar trajectories toa query trajectory', 1);('similar features', 1);('dissimilar states', 1);('trajectories dissimilar', 1);('statesthe statespace', 1);('laptop andholds cup angle', 1);('learns trajectories holdthe cup upright stay', 1);('laptop dissimilar', 1);('similar statespace', 1);('experimental setupmanipulated variables', 1);('test importance user input thatis', 1);('withmultitask learning techniques generic preference queriesand', 1);('representation learning', 1);('baselines avae learns representation variational reconstruction loss', 1);('bmultipref multitask baseline', 1);('welearn representation', 1);('multiple rewardfunctions', 1);('initial layers', 1);('preference learningcsinglepref hypothetical method learns', 1);('ideal userwho', 1);('benefit human data isalso immune spurious correlations', 1);('human preference rewards', 1);('good coverage thereward space embeddings network size forgridrobot', 1);('units mappingto', 1);('units tohandle', 1);('input space', 1);('app a2', 1);('fair comparison', 1);('sirl singlepref multipref', 1);('equal amounts ofhuman data', 1);('andpreference queries', 1);('amongst humans', 1);('app a3dependent measures', 1);('test quality', 1);('feature prediction error fpe', 1);('preference accuracy tpa fpe', 1);('priorwork argues', 1);('goal measure', 1);('necessary information', 1);('groundtruthfeatures environment generate data', 1);('ground truth', 1);('linearregression layer top', 1);('vector giventrajectory', 1);('dinto', 1);('test pairs andfpeis', 1);('mse', 1);('vector ground truth', 1);('forthe', 1);('human query methods', 1);('fpewith', 1);('numberof representation training queries', 1);('fortpa', 1);('leadto good learning general preferences', 1);('embeddings base', 1);('test preference rewards generate', 1);('preference queriesd', 1);('training 20for test train reward model preference queriesper test reward', 1);('preference networks thesame architecture', 1);('therespective method', 1);('areward function trajectory preference labels', 1);('gridrobotwe', 1);('workedbetter unfrozen embeddings', 1);('app a3', 1);('asthe preference accuracy', 1);('reward models testpreference', 1);('test human preferenceshypotheses test', 1);('representations predictive ofthe', 1);('random representationsh2the', 1);('representations result', 1);('multipref singlepref', 1);('random representations44', 1);('resultsin figure', 1);('fpescore', 1);('representation queries', 1);('gridrobotboth', 1);('outperform baselines', 1);('humans performs', 1);('human preference seenwhile', 1);('data budget', 1);('humansit ends learning', 1);('thereis', 1);('diversity human trainingrewards', 1);('data rewardgets tradeoff', 1);('similarity queries areagnostic', 1);('particular human reward complexjacorobot versions', 1);('outperform baselines linewith', 1);('tpascore', 1);('environments witha', 1);('amount test preference queries', 1);('respective methodperforms', 1);('low amounts representationdata', 1);('case just100 queries', 1);('sirl vae', 1);('vae sirlwithout', 1);('random performance', 1);('frozenand preference baselines', 1);('random theyhri', 1);('stockholm sweden tbd100', 1);('representation queries0001020304fpegridrobot feature prediction error fpesirlvae ourssingleprefsirl oursmultipref', 1);('10hrandommultipref 50hvae100', 1);('representation queries0001020304fpejacorobot feature prediction error fpesirlvae ourssingleprefsirl oursmultipref', 1);('fpefor gridrobot', 1);('right environments', 1);('simple complex environmentswerent', 1);('versions ofsirl', 1);('data good representation', 1);('results support', 1);('h2 sirls', 1);('outperform baselines environments', 1);('jacorobotsirl', 1);('vae gridrobot', 1);('suggests reconstructionloss struggles', 1);('representation whenthe input space', 1);('hinders performancewhen', 1);('blank slate', 1);('trendthat preference humans', 1);('result betterperformance confirms observation', 1);('appropriate number human preferences', 1);('challenging problem', 1);('representation data', 1);('generalizable rewards', 1);('h2', 1);('recovers sensible', 1);('representationsit reduces amount human data', 1);('needs otherwiseit hurts performance', 1);('correlate training data arenot', 1);('representation isworse learning', 1);('user studywe', 1);('present user study novice users', 1);('experiment designwe', 1);('user study', 1);('table andlaptop', 1);('humanoid inthe environment', 1);('interface people clickand drag', 1);('view press buttons replay trajectoriesand record query', 1);('display theeuclidean path trajectory query traces foundthat', 1);('anotherthe study', 1);('phase introduce userto interface describe', 1);('becausesimilarity', 1);('queries preferenceagnostic describe examples ofpossible preferences akin ones', 1);('bias theparticipant', 1);('similarityqueries answers', 1);('similarity queries thesecond phase describe scenario environment hasa', 1);('theres', 1);('smoke thekitchen robot stay high table issmoke kitchen robots mug empty', 1);('differentpreference scenarios participant person practices', 1);('preference queries answers', 1);('preference queriesparticipants', 1);('campus community', 1);('queries usershad', 1);('technical background caution results speakto', 1);('sirls', 1);('usability population', 1);('variables guided', 1);('realistic baseline', 1);('collect100 similarity queries participant train sharedrepresentation', 1);('measures', 1);('sec4fpeandtpa tpa', 1);('preference queries eachusers', 1);('unique preference use', 1);('individual rewardnetworks', 1);('realwe', 1);('tpawith', 1);('splits demonstratehow', 1);('new people dont contribute learning similarity', 1);('users compute', 1);('heldout userspreference data', 1);('real data tends noisy', 1);('stockholm sweden10', 1);('agridrobot n100sirlvae ourssingleprefsirl oursmultipref', 1);('ajacorobot n100sirlvae ourssingleprefsirl oursmultipref', 1);('agridrobot n500sirlvae ourssingleprefsirl oursmultipref', 1);('ajacorobot n500sirlvae ourssingleprefsirl oursmultipref', 1);('preference queries050607080910tp agridrobot n1000sirlvae ourssingleprefsirl oursmultipref', 1);('preference queries050607080910tp ajacorobot n1000sirlvae ourssingleprefsirl oursmultipref', 1);('tpa gridrobot', 1);('recovers generalizablerewards', 1);('different rewards wealso', 1);('simulatedhypotheses', 1);('hypotheses study areh3', 1);('thana random representation', 1);('novice user datah4', 1);('representation results', 1);('novice similarity queries52', 1);('analysisfigure', 1);('summarizes results', 1);('recovers representation', 1);('h3a', 1);('ttest p', 1);('confirms suggests', 1);('canrecover aspects peoples', 1);('noisysimilarity queries novice users', 1);('real sirl', 1);('generalizable rewards average', 1);('random providingevidence', 1);('h4 furthermore', 1);('representation ona novel user', 1);('random andthe result', 1);('real', 1);('suggests similarity queries', 1);('novel user', 1);('improvesperformance methods', 1);('noise humanpreference data', 1);('anovas', 1);('method asa factor', 1);('significant main effect', 1);('humans thatwere', 1);('eg issmoke kitchen robots mug empty stay', 1);('fromthe table', 1);('performance slightlybetter', 1);('preference data subset users hintingthat', 1);('stockholm sweden tbdall humans000002004006008010fpesirl oursrandomall humans', 1);('humans000102030405060708tp areal heldout simulatedfigure', 1);('fpe tpawith', 1);('novice similarity queries', 1);('recovers representationsboth predictive', 1);('different user rewards baselineoverall quantitative results support', 1);('h3 h4', 1);('subjectively', 1);('2d interface', 1);('trajectory similarity', 1);('onthe viewpoint', 1);('natural artifact', 1);('3d world in2d', 1);('future work', 1);('comparingtrajectories part', 1);('internal representation inthewild studycould', 1);('others users timetoanswer', 1);('future work coulduse confidence metric', 1);('trust answer6', 1);('discussion limitationsin', 1);('work goal tackle problem learning goodrepresentations', 1);('representation learning rewards', 1);('different preferencesand tasks', 1);('generalizable models', 1);('different situations ratherthan', 1);('unable distinguish goodfrom', 1);('bad behaviors', 1);('new data idea', 1);('tapinto representation', 1);('similar iftheir representations', 1);('novel humaninput type trajectory similarity queries', 1);('tobetter representations', 1);('multitask learning', 1);('learning rewards thesame training data', 1);('rank behaviors test datathat', 1);('explicit beall endallsolution goal representations', 1);('seein simulation', 1);('perfect theyare', 1);('suffer issuepreference queries', 1);('multiple important features', 1);('similarthe advantage', 1);('particular tasks', 1);('ignore downweigh certainfeatures matter tasks similarity queries aretaskagnostic', 1);('distribution tasks thehumans head', 1);('specify task distribution formultitask learning similarity queries', 1);('user leverage generalpurpose representationof world', 1);('exciting ideas future work instancewhat', 1);('similarity querieson', 1);('current estimate', 1);('important features overtime representation', 1);('humansthe queries', 1);('specific featuresanother', 1);('obvious limitation inthewildstudy theory similarity queries', 1);('peoplealready robot', 1);('distribution tasks care', 1);('everyday contexts inour study', 1);('users contexts whatmight', 1);('internal representation robots', 1);('prevalent followupstudy users', 1);('possiblein sense', 1);('foundation model', 1);('hundreds queries', 1);('good representationwhile dont', 1);('data pretrainingis', 1);('significant desirableperformance improvement baselines samplecomplexity iscrucial address scale', 1);('complex robotic tasks similarity queries task agnostic crowdsourcethe queries', 1);('multiple people user study', 1);('economy scale alleviate user burden', 1);('personfor informative similarity queries', 1);('data amountsa avenue work', 1);('policy learning learning exploration functions', 1);('similarity queriesare replacement', 1);('able toleverage', 1);('finetune representationhow', 1);('open questionoverall similarity queries step', 1);('state art andcan benefit exploration', 1);('withother inputs selfsupervision', 1);('interfaces query selection algorithmssirl', 1);('stockholm swedenreferences1pieter abbeel andrew ng', 1);('apprenticeship', 1);('machine learning icml', 1);('acm2sameer agarwal josh wills lawrence cayton gert lanckriet david kriegmanand serge belongie', 1);('generalized', 1);('nonmetric multidimensional', 1);('inartificial intelligence statistics pmlr', 1);('amid aristides gionis antti ukkonen', 1);('kernellearningapproach semisupervised clustering relative distance comparisonsvol', 1);('aytar tobias pfaff david budden tom', 1);('paine ziyu wang nando', 1);('playing hard exploration', 1);('watching youtube proceedings', 1);('neural information processingsystems montral canada nips18 curran associates inc', 1);('hook nyusa', 1);('babes vukosi n marivate kaushik subramanian michael', 1);('littman2011 apprenticeship', 1);('multiple intentions', 1);('icml', 1);('bajcsy dylan p losey marcia k omalley anca dragan', 1);('robot objectives physical', 1);('interaction proceedings', 1);('ofthe 1st', 1);('annual', 1);('robot learning proceedings machine learningresearch vol', 1);('sergey levine vincent vanhoucke ken goldberg edspmlr', 1);('balntas edgar riba daniel ponsa krystian mikolajczyk', 1);('descriptors triplets shallow convolutional neuralnetworks', 1);('biyik dorsa sadigh', 1);('batch', 1);('learning ofreward functions conference robot learning', 1);('bobu chris paxton wei yang balakumar sundaralingam yuweichao maya cakmak dieter fox', 1);('learning perceptual concepts', 1);('queries ieee robotics autom lett', 1);('inducingstructure reward learning learning features', 1);('international journal', 1);('httpsdoiorg10117702783649221078031 arxivhttpsdoiorg1011770278364922107803111', 1);('featureexpansive reward learning rethinking', 1);('input proceedings', 1);('humanrobot interaction boulder cousa hri', 1);('computing machinery', 1);('york ny usa216224', 1);('ralph allan bradley milton e terry', 1);('incomplete blockdesigns method', 1);('biometrika', 1);('daniel brown russell coleman ravi srinivasan scott niekum', 1);('safeimitation learning', 1);('fast bayesian reward inference preferences inproceedings', 1);('machine learning proceedingsof machine learning', 1);('hal daum iii aarti singh edspmlr', 1);('daniel brown wonjoon goo prabhat nagarajan scott niekum', 1);('extrapolating', 1);('suboptimal demonstrations', 1);('inverse reinforcement learningfrom observations', 1);('kalesha bullard sonia chernova andrea lockerd thomaz', 1);('humandriven feature selection robotic agent learning classification tasks', 1);('robotics automationicra', 1);('brisbane australia may', 1);('maya cakmak andrea lockerd thomaz', 1);('designing', 1);('robot learnersthat', 1);('good questions', 1);('humanrobot interactionhri12 boston usa march', 1);('holly yanco aaron steinfeldvanessa evers odest chadwicke jenkins eds acm', 1);('annie chen suraj nair chelsea finn', 1);('roboticreward functions inthewild', 1);('videos corr', 1);('abs2103168172021 arxiv210316817 httpsarxivorgabs21031681718', 1);('ricky q chen xuechen li roger', 1);('grosse david k duvenaud', 1);('sources disentanglement variational autoencoders advancesin neural information processing systems bengio h wallach h larochellek grauman n cesabianchi r garnett eds vol', 1);('curran associatesinc19 ting chen simon kornblith mohammad norouzi geoffrey hinton', 1);('asimple', 1);('framework contrastive learning visual representations', 1);('international conference machine learning', 1);('xi chen yan duan rein houthooft john schulman ilya sutskever pieterabbeel', 1);('infogan', 1);('representation learning informationmaximizing generative adversarial nets proceedings', 1);('internationalconference neural information processing systems barcelona spain nips16 curran associates inc', 1);('hook ny usa', 1);('jaedeug choi keeeung kim', 1);('nonparametric bayesian', 1);('inverse reinforcement learning', 1);('multiple reward functions', 1);('paul', 1);('christiano jan leike tom brown miljan martic shane legg darioamodei', 1);('deep reinforcement learning', 1);('preferences inadvances neural information processing systems guyon u v luxburgs bengio h wallach r fergus vishwanathan r garnett edsvol', 1);('curran associates inc', 1);('adam coates ng', 1);('learning feature representations kmeansinneural networks tricks', 1);('erwin coumans yunfei bai', 1);('pybullet python', 1);('module physicssimulation games robotics machine learning httppybulletorg25', 1);('cagatay demiralp michael bernstein jeffrey heer', 1);('learning perceptualkernels visualization', 1);('ieee transactions visualization computergraphics', 1);('christos dimitrakakis constantin rothkopf', 1);('bayesian', 1);('multitaskinverse reinforcement learning', 1);('european workshop reinforcement learning', 1);('springer', 1);('carl doersch abhinav kumar gupta alexei efros', 1);('unsupervisedvisual representation learning context prediction', 1);('ieee internationalconference computer vision iccv', 1);('dragan k muelling j andrew bagnell srinivasa', 1);('movementprimitives', 1);('icra', 1);('chelsea finn pieter abbeel sergey levine', 1);('modelagnostic metalearning fast adaptation deep networks proceedings', 1);('machine learning', 1);('sydney nsw australiaicml17 jmlrorg', 1);('adam gleave oliver habryka', 1);('multitask', 1);('maximum entropy inversereinforcement learning arxiv preprint arxiv180508882', 1);('bradley hayes brian scassellati', 1);('discovering', 1);('task constraints throughobservation', 1);('active learning', 1);('ieeersj', 1);('international conference onintelligent', 1);('robots systems chicago il usa september', 1);('ieee44424449', 1);('irina higgins loc matthey arka pal christopher p burgess xavier glorotmatthew botvinick shakir mohamed alexander lerchner', 1);('visual concepts constrained variational frameworkiniclr', 1);('chao huang wenhao luo rui liu', 1);('meta preference learning', 1);('user adaptation humansupervisory multirobot deployments', 1);('intelligent robots systems iros ieee5851585634 hong jun jeon smitha milli anca dragan', 1);('rewardrational', 1);('formalism reward learning', 1);('maurice george kendall', 1);('correlation methods', 1);('diederik p kingma max welling', 1);('autoencoding variational bayes in2nd', 1);('learning representations iclr', 1);('banff abcanada april', 1);('conference track', 1);('proceedings yoshua bengio yannlecun eds', 1);('johannes kulick marc toussaint tobias lang manuel lopes', 1);('activelearning', 1);('robot grounded relational symbols ijcai', 1);('2013proceedings 23rd', 1);('joint', 1);('artificial intelligencebeijing china august', 1);('francesca rossi ed ijcaiaaai', 1);('chengi lai', 1);('contrastive predictive coding based feature automaticspeaker verification', 1);('arxiv preprint arxiv190401575', 1);('michael laskin aravind srinivas pieter abbeel', 1);('curl contrastiveunsupervised representations reinforcement learning proceedings', 1);('ofthe 37th', 1);('machine learning proceedings machinelearning', 1);('hal daum iii aarti singh eds pmlr', 1);('sergey levine aviral kumar george tucker justin fu', 1);('offline', 1);('reinforcement learning', 1);('tutorial', 1);('review perspectives', 1);('problems arxivpreprint arxiv200501643', 1);('kejun li maegan tucker erdem byk ellen novoseller joel', 1);('burdick yanansui dorsa sadigh yisong yue aaron ames', 1);('roial region', 1);('interestactive learning', 1);('exoskeleton gait preference landscapes', 1);('in2021 ieee', 1);('robotics automation icra ieee3212321842 hoai luuduc jun miura', 1);('incremental feature set refinement', 1);('demonstration scenario', 1);('conferenceon advanced robotics mechatronics icarm', 1);('toyonaka japan july', 1);('zhao mandi pieter abbeel stephen james', 1);('effectiveness', 1);('versus metareinforcement learning', 1);('arxiv preprint arxiv2206032712022hri', 1);('stockholm sweden tbd44 brian mcfee gert lanckriet tony jebara', 1);('learning multimodalsimilarity', 1);('journal machine learning research', 1);('jonathan mumm bilge mutlu', 1);('physical andpsychological', 1);('humanrobot interaction', 1);('proceedings', 1);('6thinternational conference', 1);('kentaro nishi masamichi shimosaka', 1);('finegrained', 1);('contextaware multitask inverse reinforcement learning 2020ieee', 1);('robotics automation icra ieee', 1);('takayuki osa joni pajarinen gerhard neumann j andrew bagnell pieter abbeeljan peters', 1);('algorithmic perspective imitation learning foundations trends robotics', 1);('deepak pathak parsa mahmoudieh guanghao luo pulkit agrawal dian chenfred shentu evan shelhamer jitendra malik alexei efros trevor darrell2018 zeroshot visual imitation', 1);('ieeecvf', 1);('computervision pattern recognition', 1);('cvprw', 1);('httpsdoiorg101109cvprw20180027849 c', 1);('j reed x yue nrusimha ebrahimi v vijaykumar r mao', 1);('li szhang guillory metzger k keutzer darrell', 1);('selfsupervisedpretraining improves selfsupervised pretraining', 1);('ieeecvf winter', 1);('applications computer vision wacv ieee computer', 1);('losalamitos ca usa', 1);('dorsa sadigh anca dragan shankar sastry sanjit seshia', 1);('activepreferencebased', 1);('learning reward functions', 1);('science systems', 1);('seyed kamyar seyed ghasemipour shixiang shane gu richard zemel', 1);('scalable meta inverse reinforcement learning contextconditionalpolicies', 1);('advances neural information processing systems', 1);('arjun sripathy andreea bobu zhongyu li koushil sreenath daniel brownand anca dragan', 1);('robots span', 1);('functionalexpressive motion', 1);('neil stewart gordon da brown nick chater', 1);('absolute', 1);('relative judgment', 1);('psychological', 1);('omer tamuz ce liu serge belongie ohad shamir adam tauman kalai2011 adaptively', 1);('learning crowd kernel arxiv preprint arxiv11051033201155', 1);('wirth riad akrour gerhard neumann johannes frnkranz', 1);('reinforcement learning methods journal ofmachine', 1);('intent', 1);('metainverse reinforcement learning proceedings', 1);('machine learning proceedings', 1);('kamalika chaudhuri ruslan salakhutdinov eds pmlr', 1);('metainverse reinforcement learning', 1);('lantao yu tianhe yu chelsea finn stefano ermon', 1);('metainversereinforcement', 1);('learning probabilistic context variables', 1);('advances neuralinformation processing systems', 1);('stockholm swedena appendixa1 trajectory', 1);('trajectory space', 1);('continuous need construct', 1);('infinitedimensionaltrajectory space', 1);('startgoal pairs andcompute shortest path robots configuration space foreach trajectory horizon length andconsists ofdimensional states', 1);('random torque deformationsto trajectory', 1);('trajectoryand deform', 1);('different randomtorque deform trajectory direction follow1 5wherer11defines norm', 1);('hilbert', 1);('space oftrajectories dictates deformation shape 280scalesthe magnitude deformation', 1);('r1isat', 1);('chosendeformation state index deformation', 1);('generatedand index state deformation', 1);('smooth deformations', 1);('accelerationbut norm choices', 1);('dragan', 1);('28for details', 1);('inspiration deformation strategyfrom', 1);('bajcsy', 1);('present architecture optimization details', 1);('training setupa21', 1);('feature', 1);('networks embeddings networksize', 1);('used1024 units', 1);('input space environmentswe', 1);('linear layerwe', 1);('standard variational reconstruction loss', 1);('kldivergencebased', 1);('regularization term', 1);('latent space', 1);('regular regularizationpart loss weight', 1);('epochs withan', 1);('decay rate 099999and batchsize 32singlepref', 1);('standard preference loss', 1);('christiano', 1);('ensuredthat rewards', 1);('preference network', 1);('reward preference loss weightof', 1);('methods optimizethis', 1);('final loss way', 1);('for5000 epochs learning rate', 1);('result inmore', 1);('pretrain abovevae loss', 1);('similarity objective', 1);('gridrobot jacorobot', 1);('decay rate 099999and batchsize 64we note', 1);('current architectures', 1);('fixedlength trajectories', 1);('lstmbased', 1);('architecture trajectories', 1);('preference', 1);('top embeddings', 1);('respective methods weevaluate', 1);('layers 128units', 1);('forboth', 1);('2regularization loss', 1);('4as weight', 1);('jacorobot forgridrobot', 1);('final loss function', 1);('for500 epochs learning rate', 1);('forjacorobot', 1);('number epochs', 1);('ablationsfigure', 1);('illustrates results frozen', 1);('unfrozen baselineswithout', 1);('configurations wefound method section show', 1);('complete ablation', 1);('methods benefit frozen orunfrozen embeddings', 1);('showcases theresult ablation', 1);('gridrobot jacorobot overallwe', 1);('representation isfrozen methods', 1);('baselines performbetter', 1);('gridrobothri', 1);('stockholm sweden tbd10', 1);('preference queries050607080910tp agridrobot test preference accuracy tp n1000sirl', 1);('preference queries050607080910tp ajacorobot test preference accuracy tp n1000sirl', 1);('preference queries050607080910tp agridrobot test preference accuracy tp n1000singlepref', 1);('preference queries050607080910tp ajacorobot test preference accuracy tp n1000singlepref', 1);('preference queries050607080910tp agridrobot test preference accuracy tp n1000random', 1);('frozenrandom unfrozenvae frozenvae unfrozen10', 1);('preference queries050607080910tp ajacorobot test preference accuracy tp n1000random', 1);('frozenrandom unfrozenvae frozenvae unfrozenfigure', 1);('ablation', 1);('overall sirl', 1);('representation frozenwhile method', 1);('vaepretraining sirl', 1);