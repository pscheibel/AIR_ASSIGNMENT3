('rrt', 11);('rlrrt', 10);('polamp', 9);('ieee', 9);('dynamic obstacles', 6);('fig', 6);('ddpg', 6);('international conference', 6);('ppo', 5);('rrtx', 4);('dynamic environments', 3);('acmp', 3);('rrtdwa', 3);('federal research center', 2);('computerscience', 2);('robotics automation', 2);('cyan', 2);('contrary', 2);('actorcritic', 2);('global', 2);('rls teer', 2);('rlpid9', 2);('alg', 2);('nearest', 2);('open', 2);('closed', 2);('map1', 2);('learnable local planner', 2);('map2', 2);('rrtes', 2);('success rate', 2);('sr', 2);('polampa', 2);('polamprrt polampa acmp', 2);('motion planning', 2);('international journal ofrobotics research vol', 2);('reinforcement learning', 2);('roboticsand automation icra', 2);('online', 2);('policy optimization learn adaptive motion primitives path planningwith dynamic obstaclesbrian angulo1 aleksandr panov2and konstantin yakovlev3abstract', 1);('paper addresses kinodynamic', 1);('nonholonomic robots', 1);('dynamic environmentswith static', 1);('challenging problemthat lacks', 1);('universal solution', 1);('problem thesmaller subproblems', 1);('local solutions intothe', 1);('crux planning method nonholonomic robots generation motion primitives thatgenerates solutions', 1);('subproblems workwe introduce novel', 1);('function policy whichtakes', 1);('kinodynamic constraints robot andboth static', 1);('dynamic obstacles policy', 1);('policy optimization', 1);('empirically', 1);('show thatour', 1);('function generalizes', 1);('unseen problems', 1);('wethen', 1);('polampalgorithm policy optimization learns adaptive motionprimitives', 1);('challenging setups', 1);('acarlike robot', 1);('obstaclerich parkinglot environments', 1);('able plan collisionfree kinodynamic trajectories success rates', 1);('obstacles populate theenvironment', 1);('performance stateoftheart competitorsthe code', 1);('available httpsgithubcombriananguloyauripolamp', 1);('ntroductionautonomous', 1);('robotic systems', 1);('popular research topics', 1);('recent years', 1);('due itshuge potential', 1);('social benets', 1);('time requiresefcient motion planning', 1);('account kinodynamicconstraints nonholonomic autonomous vehicle', 1);('oftenthe', 1);('planners address rst aspect problem iedynamic environment', 1);('account kinodynamic constraints theother hand kinodynamic planners', 1);('future changes environments evenif changes forseen eg', 1);('controlsystem robot work', 1);('enrich thekinodynamic planning methods ability', 1);('thedynamics environment', 1);('angulo moscow', 1);('physics', 1);('technology andjsc', 1);('integrant', 1);('panov', 1);('ras airi', 1);('yakovlev', 1);('ras moscow', 1);('physics technologyairiyakovlevksgmailcomaccepted', 1);('regular paper', 1);('december', 1);('illustration polamp', 1);('red arrow representthe', 1);('blue arrow goal', 1);('black', 1);('andblue rectanglesrepresent static', 1);('apolamptwo', 1);('common approaches kinodynamic planningare', 1);('latticebased', 1);('methods utilize', 1);('motion primitives', 1);('regular lattice', 1);('eachmotion', 1);('small segment', 1);('feasible trajectory robot precomputedbefore planning planning stage searchbasedalgorithms eg', 1);('nd theresultant trajectory', 1);('sequence motionprimitives', 1);('planners eg', 1);('search tree', 1);('states therobots conguration space invoke local planner toconnect', 1);('kinematic constraintsof robot', 1);('motion primitives constructedonline ie planningone', 1);('prominent approaches alleviate complexity local planners respect kinematic constraintsof robot use methods', 1);('policy optimizationalgorithm learn adaptive motion primitives polampto', 1);('future changes environmentat planning stage', 1);('plans satisfythe kinodynamic constraints robot', 1);('utilizesa reinforcement learning approach nd policy thatgenerates', 1);('local segments trajectory embeddedin', 1);('generate aglobal motion plan', 1);('learnable local planner utilizes localobservation', 1);('dynamic obstacles andas', 1);('respect kinodynamic constraints robot', 1);('asa', 1);('able generate', 1);('feasible solutions withhigh success rate', 1);('environments toarxiv221214307v1 csro', 1);('dec', 1);('r elated workthe', 1);('problem kinodynamic planning', 1);('various approaches', 1);('basedoptimization reinforcement learning combination ofthem', 1);('kinodynamic planning presence', 1);('widespread approach kinodynamic planning inrobotics', 1);('popular wayto account robots dynamics sample therobots state space attempt', 1);('differentlocal planners', 1);('relieson local planner', 1);('assumes information obstaclesare', 1);('available eg predictedfrom sensors observations', 1);('informationinto account', 1);('planner carlike robots', 1);('planners carlike robots describedin', 1);('algorithms suggestedmethod construct lattice highdimensionalspace search', 1);('feasible plan uses', 1);('local learnableplanner', 1);('methods rst generate', 1);('rough pathoften', 1);('kinodynamic constraintsinto account generate', 1);('systems dynamics', 1);('obstaclesthe variants methods', 1);('19unlike method', 1);('work builds afeasible trajectory', 1);('avoiding', 1);('knowledge theirfuture', 1);('similar methods', 1);('inthis article', 1);('prmrl', 1);('methodalso uses learning local planner', 1);('local plannerconsiders presence', 1);('dynamic obstaclesiii', 1);('p roblem statementwe', 1);('feasible kinodynamictrajectory nonholomonic robot', 1);('carlike robots', 1);('20xvcos\x12yvsin\x12 1\x12vltan wherexyare coordinates robots referencepoint middle', 1);('rear axle \x12is orientation', 1);('listhe', 1);('wheelbase vis linear velocity', 1);('polampangle', 1);('state vectorxt xy\x12', 1);('variables form controlvector ut v', 1);('rewritten usingthe acceleration aand rotation rate', 1);('vv0a\x01t 0\x01tthe robot', 1);('2d workspace', 1);('dynamic obstacles shapes rectangularas', 1);('obsfobs', 1);('obsitmaps', 1);('timemoments positions obstacles reference pointin workspace static obstacles', 1);('obsi0', 1);('thefunctionsobsitto knowndenote byxfreetall congurations robotwhich collision obstacles timemomenttwrt robots obstacles shapes', 1);('theproblem', 1);('functions timethat', 1);('conguration sstart thegoal onesgoal st kinodynamic constraints', 1);('aremet resultant trajectory', 1);('xfreetiv ethodwe', 1);('global localplanners', 1);('decompose problem theset subproblems', 1);('local planneris', 1);('problem subproblem essence twoboundary value problem withadditional constraints', 1);('robot collide withboth static', 1);('crux approach cast problem asthe', 1);('markov', 1);('decision process', 1);('pomdpand', 1);('pomdp', 1);('thereinforcement learning', 1);('proximal policy optimization', 1);('algorithm thepolicy', 1);('global planneras', 1);('global planner use adaptation renownedalgorithms', 1);('nal solver namethis type solvers', 1);('polamp policy optimization', 1);('adaptive motion primitivesa', 1);('plannera background formally pomdp', 1);('representedas tuple', 1);('sapr', 1);('wheresis state space', 1);('aisthe', 1);('action spacepis statetransition model', 1);('ris', 1);('learning environment', 1);('green rectangle', 1);('redarrow', 1);('current state robot', 1);('green rectangle thecyan orientation goal', 1);('blue rectangles thestatic obstacles', 1);('blue rectangle', 1);('pink', 1);('orange', 1);('lines laser beamsreward function observation space learningat time step agent', 1);('observation ot2takes action at2', 1);('reward rt2', 1);('policy ie', 1);('theobservations distributions actions \x19', 1);('pathe', 1);('policy maximize', 1);('state stj\x19', 1);('erisi\x18pai\x18\x19txit', 1);('concise denition mostessential information agent order', 1);('anoptimal decisionq\x19stat', 1);('erisi\x18pa\x18\x19rtjstatitin', 1);('variance lessprone convergence local minimum actor updatesthe policy approximator', 1);('e\x19w\x02rwlog', 1);('\x19wsaq\x19wsa\x03where \x19wis arbitrary', 1);('differentiable policy', 1);('critic', 1);('evaluatesthe approximation', 1);('q\x19wsavalue', 1);('current policy\x19w', 1);('parameters acritic updates parameters', 1);('theqfunction actorupdates parameters wof policy', 1);('criticassumptionsin work use', 1);('proximal policy optimizationmethod', 1);('evaluationactor part', 1);('loss functionlsawkw min\x19wajs\x19wkajsa\x19woldsaclip\x19wajs\x19woldajs1\x00\x0f1 \x0fa\x19woldsawherea\x19wis estimation advantage functionasa', 1);('qsa\x00vsgiven', 1);('critic part', 1);('clippingis', 1);('incentives policy', 1);('hyperparameter \x0fcorresponds faraway', 1);('new policy', 1);('protingfrom objective', 1);('algorithm intoour method', 1);('state stas function fromobservation st\x19fot wherefis', 1);('layers neuralnetwork approximator actor critic', 1);('observations', 1);('actions rewards paperwe', 1);('actions a2r2that composedof setting linear acceleration a2\x0055ms2androtation rate 2\x00\x1912\x1912rads', 1);('ones canbe', 1);('robots controls', 1);('eq', 1);('range linear velocity inv204ms', 1);('angle 2\x00\x196\x196radthe observation otis vector', 1);('measurements lidar coverthe', 1);('robot length ofbeammax 20m', 1);('features\x01x\x01y\x01\x12\x01v\x01 \x12v', 1);('\x01sistands forthe difference', 1);('respective parameter siof goalstate', 1);('current state aare', 1);('current controlswe', 1);('ideal environment simulation andactuation model errorsthe reward function', 1);('byrwtrrgoalrcolreldrtrbackwardrvmaxr maxwherewris vector weights rgoalis', 1);('goal state \x0f\x1a\x0f\x12tolerance 0otherwisercolis\x001if agent collides obstaclesand', 1);('reld\x1acurr\x00\x1alast where\x1alast kst\x001\x00sgoalkand\x1acurr kst\x00sgoalkwe penalize theagent', 1);('goal rt\x001is theconstant penalty time step rbackward is\x001the theagent', 1);('rear gear', 1);('maximum speed limit r maxis\x001for', 1);('angle thresholdwe', 1);('weights wr', 1);('values result efcient learningc', 1);('curriculum', 1);('policy learning accelerate training end', 1);('threestage curriculum learning seefig', 1);('rst stage train agent emptyenvironment stage', 1);('kinodynamicconstraints vehicle agent', 1);('acceptable success rate', 1);('stage retrainthe policy', 1);('new environment', 1);('withstatic obstacles agent learns', 1);('adversarial dynamicobstacle static environment agent learns tocircumnavigate', 1);('essential skill planning withdynamic obstaclesfig', 1);('maps', 1);('red rectangles arrows', 1);('coordinates orientations', 1);('velocity v 0and', 1);('rectangles arrows', 1);('goal coordinates orientationsalgorithm', 1);('polamp rrt', 1);('rlpidnnbsrexensurep motion plan1sstartt', 1);('initialize treesstart3whilenmax', 1);('random sample5neighbors nearest tsrandnnbs6', 1);('forsi2neighbors do7sj', 1);('extend', 1);('ifsjtris empty then10t', 1);('appendsj11', 1);('ifsgoaltris empty then12', 1);('appendsgoal13', 1);('otion plant14', 1);('else15 break16returnpb', 1);('learnable local planner generate trajectory', 1);('nearby states', 1);('longterm plans', 1);('aglobal planner', 1);('explore differentregions workspace', 1);('global observationand nd ways', 1);('inthis', 1);('work utilize', 1);('classical algorithms', 1);('rrt aas', 1);('global planners', 1);('explanation thesealgorithms', 1);('original papers andnow', 1);('overviewthe pseudocodes algorithms', 1);('main difference', 1);('aalgorithms', 1);('usesrandomsample state space', 1);('state tree', 1);('extendto', 1);('maximum distance states', 1);('uses deterministic priority queue ofstates', 1);('theopen', 1);('fvalues wherefs gs \x0f\x01hsconsists', 1);('terms gsandhsgsis', 1);('shortest path', 1);('state tothe', 1);('hsis heuristic estimate thecost fromsto goal', 1);('upon', 1);('promising statea', 1);('successors', 1);('motionprimitives robot', 1);('statesthe major difference', 1);('classical algorithmsand', 1);('polamp polamp', 1);('reasons timemoments', 1);('rlsteerfunction', 1);('function solves', 1);('states siandsj distance betweensiandsgoal', 1);('dthen', 1);('target state policy', 1);('rlpiis', 1);('access information thedynamic obstacles', 1);('obst ifrlpi', 1);('toconnect states returns', 1);('trajectory sjtrand time target state', 1);('ie sjtthus states search tree', 1);('information ontheir', 1);('planningin work use', 1);('ateach iteration', 1);('nnbswith', 1);('rextand', 1);('tries generate trajectories tothem', 1);('original algorithma search ends goal state expandedin work search ends', 1);('trajectory tothe nal state', 1);('generate successors usethe technique online motion primitives', 1);('htodetermine', 1);('congurations useour', 1);('policy construct collisionfree trajectories tothese congurationsv', 1);('e xperimental evaluationwe', 1);('types environments static obstaclesand static', 1);('dynamic obstaclesa', 1);('policy', 1);('learningto train policy', 1);('different tasksstart goal states', 1);('types environments emptystatic', 1);('environments size40m\x0240m task', 1);('way thatthe distance', 1);('goal locations theinterval 1530m moreover difference orientationsalgorithm', 1);('rlpinmaxensurep motion plan1closed open', 1);('2sstartt 0gsstart 0fsstart hsstart3open', 1);('insert', 1);('nmax', 1);('openpop closed insert', 1);('successors getnextstates', 1);('\x18t7 forsj2successors do8sj', 1);('ifsjtris empty then10 continue11 ifsgoaltris empty then12', 1);('sgoal13 returnp', 1);('otion planclosed14csisj coststntr15', 1);('open insert', 1);('sj17returnpdid exceeded\x194 task', 1);('goal state', 1);('euclidean', 1);('error \x0f\x1a\x1403m orientation error \x0f\x12\x14\x1918rad collisionsto generate tasks static environments', 1);('12fragments size 40m\x0240mfrom map', 1);('size 100m\x0260mfor training', 1);('dynamic obstacle', 1);('ie', 1);('thestart state', 1);('dynamic obstacles trajectory', 1);('way highchance intersect path agent forcethe', 1);('detourwait illustration', 1);('train dataset', 1);('separate set ofvalidation tasks', 1);('measure progress', 1);('policy validation tasks thesuccess', 1);('case stoppedlearningthe effect curriculum learning', 1);('assessthe effect', 1);('curriculum learning', 1);('twopolicies rst baseline', 1);('thedynamic environment \x19stand', 1);('threestage curriculum', 1);('thecorresponding', 1);('learning curves', 1);('evidentlythe', 1);('curriculum policy \x19currstarts converge approx300m time step', 1);('reward time thestandard policy', 1);('conrm suggestedcurriculum', 1);('convergence especiallyuseful resources eg training time', 1);('learnable baseline', 1);('learnable baselinewhich', 1);('combination globalfig', 1);('comparison learning curves curriculum andstandart learning policy dash lines', 1);('dynamic orientation sr', 1);('\x19stwo\x00\x12no 99\x19stw\x00\x12no yes 32\x19dynwo\x00\x12yes 28\x19dynw\x00\x12yes yes 22table results', 1);('different setupsplanner', 1);('theddpg policy', 1);('fair comparison', 1);('thispolicy dataset scratch', 1);('training success rate validation taskswas', 1);('reasons performance', 1);('additional training', 1);('variants thispolicy', 1);('simple setups characteristics thosesetups resultant success rates', 1);('inotably', 1);('orientation constraintsand', 1);('dynamic obstacles setting', 1);('rlrrt\x19statwo\x00\x12', 1);('good performance', 1);('original paper onrlrrt authors', 1);('howeverwhen', 1);('complex performance ofthe policy drops', 1);('example policy whichignores', 1);('dynamic obstacle \x19statw\x00\x12', 1);('srand', 1);('ignores goal orientation \x19dynwo\x00\x12 28thus conduct type policy acceptableperformance', 1);('basic setupsthe', 1);('poor performance', 1);('case morecomplex environmental conditions', 1);('large numberof', 1);('due instability ofthe learning process', 1);('algorithm stochasticenvironment', 1);('class offpolicymethods', 1);('different episodes thereplay buffer', 1);('collisions andgenerates deterministic policy', 1);('relative value functionin', 1);('method onlythe', 1);('relevant trajectories', 1);('stages training', 1);('unlikely tocontain collision situations number', 1);('planner sr ttr samples time1polamprrt', 1);('ii', 1);('experiments static maps24 onpolicy algorithms', 1);('signicant advantageover offpolicy stochastic environment', 1);('abilityto generate stochastic policy advantage', 1);('overddpg task', 1);('replay buffer prevents', 1);('new conditions', 1);('stage trainingb', 1);('evaluation static environmentswe', 1);('different maps', 1);('parkinglots evaluation', 1);('map sizeof100m\x0260mand', 1);('please', 1);('map1were', 1);('policy training', 1);('training map', 1);('instances ie startgoallocation pairs', 1);('discardedthe instances straightline distance betweenstart goal', 1);('50m order', 1);('startgoal', 1);('multiplicative 90\x0e test repeated30 times test', 1);('failure robot', 1);('tolerance \x0f\x1a\x1405mand\x0f\x12\x14\x1918we', 1);('rrtthat', 1);('exponential stabilization', 1);('kinodynamic motion planner', 1);('sst', 1);('stateoftheart planning method', 1);('learnable localplanner details learning planner providedabovefor', 1);('part algorithms', 1);('radius ofthe', 1);('rext', 1);('maximum distancewhich', 1);('nnbs', 1);('maximum number ofiterations', 1);('rrt nmax', 1);('polamprrtand rlrrt nmax', 1);('rrtes sstfor polampa', 1);('rrtadditionally', 1);('min max linear velocity v 2and time horizon', 1);('generate lattice themotion primitives values', 1);('preliminary evaluation', 1);('suitableparameters valuesthe metrics', 1);('oftenthe planner', 1);('goal time toreach goal', 1);('ttr', 1);('total number samples theruntime algorithmthe results', 1);('ii notably polamphas', 1);('map showsthat', 1);('tothe unseen consitions', 1);('observable trend', 1);('polamprequires', 1);('generate themotion plan example', 1);('map2 polamp', 1);('requires14x 12x', 1);('rrtes sstrespectively polamp', 1);('performs collisionavoidance local', 1);('rrtes sst', 1);('donot comparison', 1);('rlrrt polamp', 1);('requiresless samplesalso', 1);('maps meaning thatunlike policy policy', 1);('map2and map3', 1);('learnable component planner ieddpg', 1);('themap1 words', 1);('able learnwell dataset', 1);('map1 meanwhileppo', 1);('amount data training', 1);('wasable generalize', 1);('queries theunseen training', 1);('map2 map3', 1);('infer thatppo sample efcient policy', 1);('similar setupsc', 1);('evaluation dynamic environmentsfor', 1);('series experiments', 1);('andmap3 ie maps', 1);('thesemaps', 1);('number dynamicobstacles 0to70', 1);('dynamic obstacle rectangular shape carlike robot trajectory', 1);('random control input aevery', 1);('different trajectories', 1);('different startgoal pairs', 1);('eachmap test', 1);('times samplingbasedplannersas', 1);('polamp rlrrt', 1);('algorithm usedthe parameters', 1);('baselinewas combination', 1);('dynamicwindow approach dwa', 1);('local planner', 1);('notaccount nal orientation', 1);('dwa', 1);('toobey orientation constraints', 1);('dubins', 1);('planexecutereplan type algorithm reuses search tree robot', 1);('results maps', 1);('dynamic obstacles success rate time', 1);('number samples legend allalgorithms', 1);('robot case ofrrtx', 1);('additionallybecause rrtx', 1);('show metric', 1);('rrtxthe', 1);('clear trend isthat', 1);('casesmaintain high success rate', 1);('number ofdynamic obstacles', 1);('polampaand polamprrt', 1);('nd trajectory', 1);('acmprequires', 1);('groups primitives', 1);('group primitivesallows accelerate', 1);('constant speed anothergroup tries decelerate', 1);('collision dynamicobstacles', 1);('group ofprimitives policy', 1);('able decelerate avoidcollision', 1);('dynamic obstacles necessarywe', 1);('note tradeoff', 1);('acmpand polampa', 1);('terms success rate', 1);('thanks', 1);('rrt polamprrt', 1);('able explore moreand', 1);('performsa systematic nonexplorative search hand', 1);('duration comparison restalgorithms', 1);('acmpuses', 1);('maximum acceleration generatethe neighbors ie algorithm', 1);('abrupt changes', 1);('tries changethe speed', 1);('due presence obstacles', 1);('ouralgorithm', 1);('poor performance \x19dynw\x00\x12the', 1);('algorithm show good results', 1);('workswell number obstacles smallpolamprrt', 1);('tries replan path online sometimeswhen', 1);('current path', 1);('dynamic obstacles therobot', 1);('place ndsanother solution situations robot', 1);('intoa deadlock', 1);('obstacles problem dueto', 1);('future trajectories ofdynamic obstacles', 1);('ttr rrtxis', 1);('double algorithms becauserrtx', 1);('abrupt path changes path', 1);('bythe appearance', 1);('dynamic obstaclesoverall', 1);('new environments', 1);('dynamic obstacles recall', 1);('obstacle combination thatpolicy', 1);('global plannerworks', 1);('challenging environments dozens', 1);('experimental videosare', 1);('multimedia materialsvi', 1);('onclusionin', 1);('nonholonomic robot environments withdynamic obstacles', 1);('classical planningmethods', 1);('account kinodynamic constraints bothstatic', 1);('reward functionand', 1);('specic curriculum learning steeringbehaviors resultant algorithm', 1);('dynamic environments andwas', 1);('outperform stateoftheart baselines', 1);('bothlearnable nonlearnablereferences1', 1);('otte e frazzoli rrtx asymptotically', 1);('international journal', 1);('robotics', 1);('research vol', 1);('phillips likhachev sipp safe', 1);('interval path planning fordynamic environments', 1);('international conference onrobotics', 1);('automation', 1);('likhachev ferguson', 1);('feasiblemaneuvers autonomous vehicles', 1);('p e hart n j nilsson', 1);('raphael', 1);('formal basis theheuristic determination minimum cost paths', 1);('transactions onsystems science', 1);('cybernetics', 1);('lavalle rapidlyexploring', 1);('random trees', 1);('new tool', 1);('karaman walter perez e frazzoli teller anytimemotion', 1);('rrt proceedings', 1);('ieeeinternational', 1);('chiang j hsu fiser', 1);('tapia faust rlrrt kinodynamic', 1);('reachability estimatorsfrom rl policies', 1);('ieee robotics automation', 1);('letters vol', 1);('faust k oslund ramirez francis', 1);('tapia fiserand j davidson prmrl longrange', 1);('robotic navigation tasks', 1);('robotics automationicra', 1);('gonz', 1);('j p', 1);('v milan', 1);('es f', 1);('nashashibi', 1);('review ofmotion planning techniques', 1);('ieee transactions intelligent', 1);('systems', 1);('jeon karaman e frazzoli anytime', 1);('computationof timeoptimal offroad vehicle maneuvers', 1);('rrt 201150th', 1);('decision', 1);('controlconference', 1);('j webb j van den berg kinodynamic rrt asymptotically', 1);('optimal motion planning robots linear dynamics inproceedings', 1);('xie', 1);('j van den patil p abbeel toward', 1);('asymptoticallyoptimal motion planning kinodynamic systems', 1);('twopointboundary value problem solver', 1);('proceedings ieeeinternational', 1);('robotics automation icra', 1);('g vailland v gouranton babel cubic bezier', 1);('pathplanner nonholonomic feasible', 1);('path generation ieee', 1);('may', 1);('z h chen li horizonbased', 1);('lazy optimal rrt fastefcient', 1);('dynamic environment', 1);('auton robot', 1);('vol 43p', 1);('j lin zhou zhu j liu qh meng searchbased', 1);('online trajectory planning carlike robots', 1);('rui r siegwart', 1);('deformable inputstatelattice graphs', 1);('roboticsand automation', 1);('j ziegler', 1);('stiller spatiotemporal', 1);('state lattices', 1);('fast trajectory planning', 1);('dynamic onroad', 1);('scenarios 2009ieeersj', 1);('intelligent robots systems', 1);('erezdarpino c', 1);('liu p goebel r mart', 1);('nmart n ands', 1);('savarese robot', 1);('conferenceon robotics automation icra ieee', 1);('g p kontoudis k g vamvoudakis kinodynamic', 1);('online modelfree andsafe navigation framework', 1);('transactions neural networksand learning systems vol', 1);('paden cap z yong yershov e frazzoli asurvey', 1);('motion planning control techniques selfdrivingurban vehicles', 1);('ieee transactions intelligent vehicles', 1);('vol 1no', 1);('j schulman', 1);('wolski p dhariwal radford', 1);('klimov proximal', 1);('policy optimization algorithms', 1);('corr', 1);('vol abs170706347', 1);('available httparxivorgabs17070634722', 1);('haarnoja zhou p abbeel levine soft actorcriticoffpolicy maximum entropy deep reinforcement learning', 1);('actor proceedings', 1);('conferenceon machine learning pmlr', 1);('abdolmaleki j springenberg tassa r munos n heessand riedmiller maximum', 1);('posteriori policy optimisation in6th', 1);('learning representations', 1);('hessel danihelka', 1);('viola guez schmitt', 1);('sifret weber silver h', 1);('hasselt muesli combiningimprovements policy optimization proceedings', 1);('38thinternational conference', 1);('machine learning pmlr', 1);('available httparxivorgabs210406159httpproceedingsmlrpressv139hessel21ahtml25', 1);('mr hsieh', 1);('lin', 1);('h hsu dronebased', 1);('object countingby', 1);('regional proposal network pp', 1);('astol exponential', 1);('mobile robot viadiscontinuous control journal', 1);('dynamic systems measurementand', 1);('control vol', 1);('li z littleeld k e bekris asymptotically', 1);('kinodynamic planning', 1);('fox', 1);('burgard thrun', 1);('dynamic window approachto collision avoidance', 1);('ieee robotics automation magazine', 1);('vol 4no', 1);('e dubins', 1);('curves minimal length constraint onaverage curvature', 1);('initial terminal positionsand tangents', 1);('american journal mathematics vol', 1);