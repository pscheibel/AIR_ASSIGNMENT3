('trinet', 33);('data2vec', 19);('ssl', 13);('fig', 10);('ema', 8);('lregul', 7);('asr', 6);('teacher network', 6);('slow collapse', 6);('frozen teacher', 6);('byol', 5);('high-level space', 5);('librispeech', 5);('ls-100h', 5);('icml', 5);('icassp', 5);('interspeech', 5);('ssl-sg', 4);('hubert', 4);('conformer', 4);('a. baevski', 4);('m. auli', 4);('wu', 4);('werr', 3);('automatic speech recognition', 3);('pre-training', 3);('neurips', 3);('q. xu', 3);('lecun', 3);('iclr', 3);('c. chiu', 3);('zhang', 3);('speech recognition', 3);('sota', 2);('sg', 2);('student network', 2);('simsiam', 2);('student networks weights', 2);('wav2vec2', 2);('network architecture', 2);('re- sults', 2);('vicreg', 2);('random-projection quantizer', 2);('k-means', 2);('right teacher networks', 2);('pseudo-class space', 2);('l2', 2);('dx b\x02t\x02d', 2);('suitable measure', 2);('ls-960h', 2);('data2vec base', 2);('blue blocks', 2);('wer', 2);('werrs', 2);('ls-1h', 2);('different natures', 2);('ablation study', 2);('t. chen', 2);('s. kornblith', 2);('m. norouzi', 2);('g. hinton', 2);('a. mohamed', 2);('contrastive learning', 2);('l. jing', 2);('i. misra', 2);('self-supervised', 2);('j. qin', 2);('b. li', 2);('r. pang', 2);('j. kahn', 2);('a. hannun', 2);('self-training', 2);('t. likhomanenko', 2);('d. s.', 2);('w. han', 2);('q. v', 2);('stabilizing self-supervised learning from complete or slow collapse lixin cao1yjun wang1yben yang1', 1);('su1dong yu3', 1);('ai lab', 1);('china2peking university3tencent ai lab', 1);('usa abstract self-supervised', 1);('abrupt informational collapse', 1);('slow dimensional collapse', 1);('novel triple-branch architecture', 1);('pre- training', 1);('experimental results', 1);('relative word error rate reduction', 1);('code athttps', 1);('index terms self-supervised', 1);('pseudo label', 1);('introduction self-supervised', 1);('models leverage', 1);('signicant advances [', 1);('per- formances', 1);('speech processing [', 1);('state-of-the-art contrastive learning methods [', 1);('positive pairs', 1);('dis- tance', 1);('negative pairs', 1);('different samples', 1);('yield good performance', 1);('large amounts', 1);('contrastive pairs [', 1);('alternative methods', 1);('boot-', 1);('approaches [', 1);('negative examples', 1);('repre- sentation', 1);('complete informational collapse', 1);('rep- resentations', 1);('vector regardless', 1);('] sim-', 1);('update yequal contribution', 1);('tencentthe', 1);('prediction task', 1);('original data', 1);('top layers', 1);('top layer', 1);('collapse issue', 1);('speech tasks', 1);('computer vision', 1);('natural lan- guage processing tasks', 1);('adjacent targets', 1);('speech modality', 1);('dif- ferent natures [', 1);('complete collapse', 1);('slow col- lapse', 1);('architectural tricks', 1);('siasiam', 1);('novel network structure', 1);('trivet', 1);('main challenge', 1);('above joint', 1);('contribu- tions', 1);('regularization objectives', 1);('experimental', 1);('related work aside', 1);('contrastive methods', 1);('informational collapse', 1);('main trends', 1);('regularization methods', 1);('information content', 1);('pre- vent collapse', 1);('recently', 1);('various regularization approaches', 1);('redundant information', 1);('w-mse', 1);('barlow-twinss', 1);('] at- tempt', 1);('corinfomax', 1);('covariance matrix degeneracy', 1);('regular- izer loss function', 1);('recent investigations', 1);('thatarxiv:2301.00656v1 [ eess.as ]', 1);('dec', 1);('2022these regularization terms', 1);('structural settings [', 1);('strong data aug- mentation [', 1);('regularization methods [', 1);('ssl-no-sg', 1);('regularization terms', 1);('covari- ance regularization terms', 1);('structural tricks akin', 1);('novel regularization methods', 1);('prac- tical', 1);('different re- search area', 1);('best-rq', 1);('generate discrete pseudo la- bels', 1);('] uses', 1);('discrete pseudo labels', 1);('methods simplify', 1);('self- training [', 1);('model [', 1);('teacher model [', 1);('initial teacher model', 1);('student model', 1);('different', 1);('frame-level alignment', 1);('discrete target units', 1);('level space', 1);('target vectors', 1);('target units', 1);('negative samples', 1);('statistical assumption', 1);('regularization approaches', 1);('decorrelation [', 1);('log determinant [', 1);('stale target vectors', 1);('construct regulariza- tion loss', 1);('space degeneracy', 1);('regularization method stabilizes', 1);('illustration', 1);('different modes', 1);('original input', 1);('middle network', 1);('student mode', 1);('bottom', 1);('t-sne visualization', 1);('signicant perfor- mance improvements', 1);('data augmentation', 1);('model capacities', 1);('rst work', 1);('of- fers', 1);('successful regularization', 1);('speech pro-', 1);('above advances', 1);('frozen teacher model', 1);('method', 1);('middle leg', 1);('stu- dent network', 1);('predicts tar-', 1);('left teacher tracks', 1);('student parameters', 1);('regression tar-', 1);('right teacher', 1);('generate high-level target vectors', 1);('whole training', 1);('different nature', 1);('right teachers', 1);('frozen teacher model.we mask spans', 1);('input sequence xto generate', 1);('sequence x0and feed', 1);('en- coder [', 1);('target zstruc', 1);('intact input xwith', 1);('left leg', 1);('teachers top-k layer outputs [', 1);('leg adopts', 1);('straightforward comparison', 1);('applica- ble', 1);('intact input xand generates pseudo target yregul', 1);('original input data', 1);('design prevents', 1);('rest joint', 1);('output vectors pro-', 1);('low-dimension subspace', 1);('student encoder', 1);('differ- ent levels', 1);('structural natures', 1);('teacher relies', 1);('structural tricks', 1);('model weights', 1);('top-k layer outputs', 1);('prediction targets', 1);('mid- level contextual representations', 1);('experi- ment section 4.2.1', 1);('important role', 1);('teacher degenerate', 1);('high-level targets', 1);('t-sne [', 1);('point denotes', 1);('random batch', 1);('color denotes', 1);('arranges intra-class samples', 1);('higher-level space', 1);('inter-class samples scatter', 1);('regularization', 1);('mid-level em-', 1);('targets zstruc', 1);('norm loss', 1);('lstruc', 1);('z0 n\x00', 1);('b\x02t\x02delements', 1);('dare', 1);('dimension sizes', 1);('prediction y0in', 1);('pseudo-class targets yregul', 1);('lregre', 1);('y0 n\x00', 1);('cross-entropy loss', 1);('dcrossentropy', 1);('softmax', 1);('ablation study shows', 1);('effective thanlregre', 1);('pre- dictions', 1);('pseudo-phoneme classication', 1);('differ- ence', 1);('various-level spaces', 1);('complementary reg- ularization', 1);('l=lstruc', 1);('training objective', 1);('experiments', 1);('fine-tuning', 1);('pre-train models', 1);('ne-tune pre-', 1);('clean 100h', 1);('dev- clean/other', 1);('reference model', 1);('fairseq', 1);('fair comparison', 1);('khz waveform', 1);('80-dim lter bank', 1);('memory footprint', 1);('raw waveform', 1);('feature extrac- tor', 1);('convolution-2d', 1);('kernel widths', 1);('output sequence', 1);('original length', 1);('layer norm', 1);('en- coder', 1);('time steps', 1);('typical training sequence', 1);('learning rate schedulers', 1);('16at- tention heads', 1);('additional data', 1);('model capacity', 1);('model structural tricks', 1);('arbitrary teacher', 1);('heterogeneous architecture', 1);('future work', 1);('rst 11conformer blocks', 1);('mid-level latent', 1);('sec.3', 1);('blank blocks', 1);('learnable model size', 1);('v100', 1);('training stability', 1);('convergence', 1);('word error rate', 1);('greedy search', 1);('blue curve', 1);('absolute loss values', 1);('sta- ble', 1);('stale regularization', 1);('sub- space', 1);('overall epochs', 1);('downstream asr performance pre-trained', 1);('linear projection', 1);('modeldev', 1);('wav2vec', 1);('data2vec32.61', 1);('ctc', 1);('] loss', 1);('relative word error rate reductions', 1);('% =0=6:80 % =8:35 %', 1);('% =27:92 % =28:14 % =25:23 %', 1);('libri-light', 1);('data setup2', 1);('low-resource setting', 1);('ablations', 1);('layer specic', 1);('high-level target', 1);('orange curve', 1);('learning process', 1);('regulariza- tion terms', 1);('mse', 1);('complementary nature', 1);('different levels', 1);('triple legs', 1);('conclusion', 1);('novel architecture', 1);('sota data2vec', 1);('base', 1);('fairseq6', 1);('references', 1);('k. swersky', 1);('strong semi-', 1);('w. hsu', 1);('a. babu', 1);('j. gu', 1);('general framework', 1);('zhou', 1);('speech representations', 1);('w.n', 1);('hsu', 1);('tsai', 1);('b. bolte', 1);('r. salakhutdinov', 1);('bad teacher benet asr', 1);('g. e. hinton', 1);('simple framework', 1);('visual representations.', 1);('d. jiang', 1);('w. li', 1);('m. cao', 1);('w. zou', 1);('x. li', 1);('speech', 1);('combining', 1);('reconstruction ob- jective', 1);('speech representation learn- ing.', 1);('j.b', 1);('grill', 1);('f. strub', 1);('f. altche', 1);('c. tallec', 1);('p.h', 1);('richemond', 1);('e. buchatskaya', 1);('c. doersch', 1);('b.a', 1);('pires', 1);('z.d', 1);('guo', 1);('m.g', 1);('azar', 1);('b.piot', 1);('k.kavukcuoglu', 1);('r.munos', 1);('m. valko', 1);('bootstrap', 1);('own latent', 1);('new approach', 1);('x. chen', 1);('k.', 1);('exploring', 1);('simple siamese repre- sentation learning', 1);('cvpr', 1);('p. vincent', 1);('tian', 1);('understand-', 1);('dimensional collapse', 1);('a. bardes', 1);('j. ponce', 1);('variance-', 1);('invariance-covariance regularization', 1);('a. ermolov', 1);('a. siarohin', 1);('e. sangineto', 1);('n. sebe', 1);('whitening', 1);('representation learning', 1);('j. zbontar', 1);('s. deny', 1);('barlow', 1);('redundancy reduction', 1);('s. ozsoy', 1);('s. hamdan', 1);('s. o. arik', 1);('d. yuret', 1);('a. t. erdogan', 1);('information maximization criterion', 1);('t. lepage', 1);('r. dehak', 1);('label-efcient', 1);('speaker verication', 1);('information max- imization', 1);('j. yu', 1);('self-', 1);('t. n. sainath', 1);('z. wu', 1);('semi-', 1);('end-to-end models', 1);('weak dis- tillation', 1);('a. lee', 1);('end- to-end speech recognition', 1);('g. syn-', 1);('r. collobert', 1);('iterative', 1);('jia', 1);('improved', 1);('noisy student training', 1);('pushing', 1);('sas', 1);('p. tomasello', 1);('a. conneau', 1);('g. synnaeve r. collobert', 1);('c. wang', 1);('wang', 1);('s. chen', 1);('j. li', 1);('s. liu', 1);('f. wei', 1);('supervision-guided', 1);('pre- diction', 1);('m. ott', 1);('s. edunov', 1);('fan', 1);('s. gross', 1);('n. ng', 1);('d. grangier', 1);('extensible toolkit', 1);('naacl', 1);('m. caron', 1);('h. touvron', 1);('h. jegou', 1);('j. mairal', 1);('p. bojanowski', 1);('a. joulin', 1);('emerging', 1);('vision transformers', 1);('iccv', 1);('l.', 1);('v. d.', 1);('maaten', 1);('visualizing', 1);('machine learning', 1);('panayotov', 1);('g. chen', 1);('d. povey', 1);('s. khudanpur', 1);('asr corpus', 1);('public domain au- dio books', 1);('a. graves', 1);('s. fern', 1);('f. gomez', 1);('connection-', 1);('ist temporal classication', 1);('labelling', 1);('se- quence data', 1);('recurrent neural networks', 1);