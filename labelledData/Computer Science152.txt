('internal model', 41);('human learning', 30);('figure', 16);('physical dynamics', 15);('equation', 13);('march', 12);('stockholm', 12);('robot actions', 11);('dynamics', 10);('h|', 10);('international conference', 10);('human-robot interaction', 9);('internal models', 8);('ran tian', 8);('masayoshi tomizuka', 8);('bayesian', 8);('robot strategy', 8);('ieee', 8);('robotics', 8);('influencing', 7);('hu- mans', 7);('anca d. dragan', 7);('andrea bajcsy', 7);('robot', 7);('human learning dynamics', 7);('user study', 7);('internal model parameter', 7);('dare', 7);('humans learning dynamics', 6);('robot arm', 6);('modeling', 6);('human policy', 6);('h.', 6);('gaussian', 5);('learning hri', 5);('robot arm teleoperation', 5);('anca d dragan', 5);('dorsa sadigh', 5);('exhibit human learning', 4);('dynamics model', 4);('learning', 4);('hri', 4);('proceedings', 4);('robot dynamics', 4);('human actions', 4);('mdp', 4);('mle', 4);('time step', 4);('lq', 4);('intuitively', 4);('lunar lander', 4);('preference influence', 4);('task cost', 4);('optimal control', 4);('acm/ieee', 4);('automation', 4);('sergey levine', 4);('advances', 4);('neural information processing systems', 4);('interestingly', 3);('nonlinear dynamical system', 3);('new observations', 3);('robots actions', 3);('key challenge', 3);('leveraging', 3);('motion preferences', 3);('irl', 3);('humans policy', 3);('human internal model', 3);('current estimate', 3);('reward function', 3);('current state', 3);('humans action', 3);('true dynamics', 3);('human learns', 3);('inference problem', 3);('humans reward', 3);('hh', 3);('robots reward', 3);('human learners', 3);('ground-truth dynamics', 3);('internal model error', 3);('oracle', 3);('passive learn', 3);('hypotheses', 3);('goal influence', 3);('learning assist', 3);('static assist', 3);('mental model', 3);('humans input', 3);('inferring', 3);('henny admoni', 3);('springer', 3);('siddhartha', 3);('srinivasa', 3);('international journal', 3);('probabilistic', 3);('brian d ziebart', 3);('icra', 3);('dylan p losey', 3);('vol', 3);('output size', 3);('towards modeling', 2);('robots inertia', 2);('key idea', 2);('humans learning', 2);('open problem', 2);('acm', 2);('human teleoperates', 2);('usa', 2);('robots dynamics', 2);('functional form', 2);('dynamical system', 2);('specific model', 2);('leverage demonstrations', 2);('approximate dynamics model', 2);('markov decision process', 2);('robot teaching', 2);('robot physics', 2);('7dof robot arm', 2);('overall', 2);('preferences [', 2);('tractable approximation', 2);('cognitive', 2);('human cognition', 2);('probabilistic models', 2);('inspired', 2);('human behavior', 2);('latent representations', 2);('letrbe', 2);('physical properties', 2);('regardless', 2);('internal parameter', 2);('human action', 2);('human plans', 2);('hfrom', 2);('human acts', 2);('humans learning process', 2);('hbe', 2);('initial internal model', 2);('action history', 2);('h=', 2);('h,0', 2);('approximate model', 2);('maximum likelihood estimation', 2);('formalizing', 2);('optimization problem', 2);('r0', 2);('henters', 2);('hencodes', 2);('reward weights', 2);('h-value', 2);('continuous state', 2);('lq-approximation', 2);('physical state', 2);('inference objective', 2);('neural network parameters', 2);('recent work [', 2);('jacobians', 2);('human- robot interaction', 2);('robot planning', 2);('influence problem', 2);('re- ward', 2);('simulated', 2);('human threshold', 2);('robot effort', 2);('human objective', 2);('baselines', 2);('learning dynamics', 2);('active teach', 2);('h1', 2);('optimal action', 2);('physical robot dynamics', 2);('robotic arm', 2);('gesture interface', 2);('participants experience', 2);('likert', 2);('h6', 2);('quantitative', 2);('dynamics bias', 2);('h7', 2);('active teaching condition', 2);('technical backgrounds', 2);('anova', 2);('marginal interaction effect', 2);('autonomous cars', 2);('joshua', 2);('tenenbaum', 2);('cognition', 2);('model', 2);('siddhartha srinivasa', 2);('anca dragan', 2);('stefanos nikolaidis', 2);('acm transactions', 2);('thri', 2);('joshua b. tenenbaum', 2);('thomas', 2);('griffiths', 2);('soheil habibian', 2);('behavioral', 2);('david fridovich-keil', 2);('benjamin', 2);('newman', 2);('systems', 2);('j andrew bagnell', 2);('theorem', 2);('=exp\x10 \x11', 2);('humans policy gradient', 2);('recall', 2);('human input action', 2);('human input actions', 2);('learning ran tian uc berkeleymasayoshi tomizuka uc berkeleyanca d. dragan uc berkeleyandrea bajcsy uc berkeley abstract humans', 1);('physical capabili- ties', 1);('peoples inter- nal models', 1);('human models', 1);('real- ity', 1);('novel optimization problem', 1);('humans learn-', 1);('robot planning problem', 1);('concrete problem statements', 1);('full generality', 1);('approxima- tion', 1);('nonlinear dynamics', 1);('in-person user study', 1);('7dof robotic arm', 1);('real human-robot interaction', 1);('ccs concepts computing', 1);('artificial', 1);('keywords', 1);('acm reference format', 1);('onr yip', 1);('nsf nri', 1);('weride corp', 1);('author', 1);('{ rantian', 1);('abajcsy } @ berkeley.edu', 1);('project', 1);('permission', 1);('hard copies', 1);('classroom use', 1);('commercial advantage', 1);('full citation', 1);('copyrights', 1);('abstracting', 1);('specific permission and/or', 1);('request', 1);('permissions @ acm.org', 1);('computing machinery', 1);('acm isbn', 1);('h+1dynamics', 1);('h h figure', 1);('new robot', 1);('robot influences', 1);('robot interaction', 1);('york', 1);('ny', 1);('introduction imagine', 1);('initial', 1);('worlds dynamics', 1);('lemon juice', 1);('own preferences', 1);('internal models evolve', 1);('experi- ences', 1);('different things', 1);('collaborative tasks', 1);('assis- tance [', 1);('model changes', 1);('enhance assistance', 1);('humans understanding', 1);('proper model', 1);('humansarxiv:2301.00901v1 [ cs.ro ]', 1);('jan', 1);('world changes', 1);('robot doesnt experience', 1);('internal robot physics model', 1);('personal preferences', 1);('model human learning', 1);('dy- namical system', 1);('computational cognitive science work', 1);('predominant lens', 1);('probabilistic models [', 1);('cognitive biases', 1);('gradient information [', 1);('en- tire observation', 1);('sensory overload [', 1);('exhibit systematic bias', 1);('under- estimation [', 1);('general dynamics learning problem', 1);('robotics [', 1);('human learning model', 1);('cur- rent', 1);('different possible learning updates', 1);('general model learning problem', 1);('tractable approxima- tion', 1);('neural network representations', 1);('formalize robot', 1);('transition function', 1);('solution yields robot actions', 1);('humans observations', 1);('run experiments', 1);('humans understand-', 1);('kinova jaco', 1);('tractable learning', 1);('related work inferring', 1);('human preferences', 1);('large body', 1);('human reward functions', 1);('inverse rein- forcement learning', 1);('[ 19,22,31 ]', 1);('hu- man', 1);('exoskeleton gaits [', 1);('goals [', 1);('motion preferences [', 1);('human under-', 1);('physics [', 1);('key assumption', 1);('works isthat people', 1);('in-', 1);('dynamic model', 1);('robot decision-making', 1);('robotics model human learning', 1);('preferences [ 8,14,16 ]', 1);('trust [', 1);('learning rewards [', 1);('bandit algorithm', 1);('world state', 1);('internal physics model', 1);('model learning problem', 1);('human infer- ence', 1);('computational cognitive science [', 1);('psychology [', 1);('algorith- mic', 1);('] posits', 1);('human reasoning', 1);('point estimate', 1);('potential source', 1);('rational process models [', 1);('models prediction error', 1);('loss functions', 1);('human experiments leverage', 1);('approximate probabilistic inference models', 1);('such models', 1);('nonverbal cues', 1);('ap- pearance', 1);('curriculum design [ 1,37,39,41,46 ]', 1);('physical action [', 1);('common ap- proach', 1);('models human-robot interaction', 1);('game [ 15,23,32,40,44,48 ]', 1);('internal learning prob- lem', 1);('alternatively', 1);('model-free methods', 1);('latent representation', 1);('lever- age', 1);('latent dynamics', 1);('human [', 1);('latent rep- resentation', 1);('internal model evolves', 1);('interaction episodes', 1);('explicit parameterization', 1);('high-dimensional parameterization', 1);('evolve continu-', 1);('interaction episode', 1);('robot behaviors', 1);('correct internal model', 1);('modeling how humans learn', 1);('act', 1);('internal model evolution.towards', 1);('notation', 1);('robot end-effector position', 1);('hrandrrrespectively', 1);('deterministic world dynamics', 1);('in- ternal parameter vector', 1);('latent aspect', 1);('going', 1);('hcan', 1);('hcould', 1);('current preferences', 1);('new goal', 1);('hrepresents', 1);('human observes', 1);('h. following', 1);('works [', 1);('noisily-optimal actor', 1);('optimal state-action value', 1);('hthe', 1);('current parameter estimate', 1);('rthe', 1);('human reacts', 1);('models scenarios', 1);('policy generation', 1);('new observa- tions', 1);('observe reward signal', 1);('human update', 1);('physical aspects', 1);('core idea', 1);('timestep and+1', 1);('initial parameter estimate', 1);('state data', 1);('human evolves', 1);('whatmodels', 1);('robotics perspective', 1);('instance ofa dynamics learning problem', 1);('human data', 1);('aim tolearn', 1);('inferring the dynamics of human learning', 1);('human learn-', 1);('initial trials', 1);('demon- strations', 1);('action histories', 1);('ground-truth human', 1);('internal model data', 1);('nonlinear model', 1);('inference problem letd', 1);('= {', 1);('} =0be', 1);('human action trajectories', 1);('length time steps', 1);('initial human parameter estimate', 1);('for- mulate', 1);('max ,0', 1);('human action likelihood', 1);('equa-', 1);('constraint ensures', 1);('internal param- eter evolves', 1);('humans learning dynamics model', 1);('solving', 1);('main reasons', 1);('hof', 1);('new action policy', 1);('newreinforcement learning problem', 1);('action policy', 1);('inference horizon', 1);('high-dimensional parameter', 1);('compute per-timestep', 1);('secondly', 1);('high-dimensional space', 1);('gradient-based', 1);('natural choice', 1);('hwith', 1);('h.hri', 1);('approxima- tions', 1);('tractable solution', 1);('human reward', 1);('closed-form expression', 1);('differentiable inference objective', 1);('linear-quadratic', 1);('infinite-horizon linear-quadratic', 1);('control [', 1);('rare', 1);('humans mind', 1);('control effort', 1);('quadratic function', 1);('randrtradeoff', 1);('state reward', 1);('action reward', 1);('different ways', 1);('humans preferences', 1);('parameter encodes', 1);('humans goal state', 1);('humans reward function regulates', 1);('ifhencodes aspects', 1);('thephysical dynamics', 1);('closed-form h.recall', 1);('policy us-', 1);('hchanges', 1);('new policy', 1);('dy- namic', 1);('h-spaces', 1);('instantaneous reward', 1);('infinite-horizon optimal value wherehis', 1);('well-known positive-definite', 1);('discrete-time algebraic', 1);('riccati', 1);('obtaininghalso', 1);('optimal human action', 1);('hwhereh=', 1);('closed-form', 1);('continuous action spaces', 1);('integral [', 1);('closed-form human policy', 1);('full derivation', 1);('appendix a.1', 1);('h/2h', 1);('representing', 1);('computational cognitive science', 1);('inference [', 1);('specific functional form', 1);('broad range', 1);('recently', 1);('transformer models [', 1);('high- dimensional sequential tasks [', 1);('transformer encoder whereare', 1);('neural network', 1);('extract embeddings', 1);('transformer encoder', 1);('training', 1);('appendix a.3', 1);('deriving', 1);('humans pol-', 1);('icy gradient', 1);('relevant closed-form', 1);('appendix a.2', 1);('influencing human learning with robot actions inferring', 1);('human teleoperator', 1);('understanding improves', 1);('robot in- fluence problem', 1);('tuple <', 1);('state =', 1);('rr', 1);('stochastic state transition function', 1);('hp', 1);('impor-', 1);('deterministic function', 1);('hvia', 1);('robot optimizes', 1);('reward func- tionr', 1);('true internal model parameters', 1);('true physical dynamics', 1);('se because=', 1);('optimal policy', 1);('rwhich', 1);('humans action sequence', 1);('r=arg', 1);('maxreuhh =0r', 1);('i s.t', 1);('state transition function', 1);('computing solutions', 1);('humans nonlinear learning dynamics', 1);('transition function results', 1);('nonconvex optimization problem', 1);('optimal robot policy', 1);('receding-horizon con- trol', 1);('computational effi- ciency', 1);('long-horizon reasoning', 1);('efficient runtime performance', 1);('dyna-style', 1);('algorithm [', 1);('model-free learning', 1);('proximal policy optimization', 1);('simulated human experiments', 1);('ground-truth human learning dynamics', 1);('simulation experiments', 1);('autonomy contexts', 1);('influences human objectives', 1);('autonomy [ 9,17,26,29 ]', 1);('] trades', 1);('generate human demon- strations', 1);('experimental environment', 1);('model learning', 1);('initial state', 1);('interaction.1 gradient human threshold human', 1);('inference problem lets', 1);('1we randomize', 1);('rto', 1);('internal model changes.6.1', 1);('autonomy settings', 1);('task objective', 1);('true robot dynamics', 1);('physical robot dynam- ics', 1);('task performance', 1);('motivated', 1);('computational cognitive science models [', 1);('threshold learners', 1);('hu- mans update', 1);('structure oftakes', 1);('various forms', 1);('new state-action pair', 1);('learner updates', 1);('hac-', 1);('gradient-ascent update rule', 1);('h+h', 1);('step size', 1);('to- tal', 1);('randh', 1);('latent parameter', 1);('methods [', 1);('threshold learner', 1);('learning rule', 1);('internal parameters', 1);('h+', 1);('| > \x02', 1);('\x03where 1is', 1);('learning update andis', 1);('threshold parameter', 1);('various aspects', 1);('envi- ronments', 1);('action spaces', 1);('lunar landers', 1);('lander upright dur-', 1);('tilt angle', 1);('tilt angular velocity =', 1);('engine force', 1);('= [ 1,0.2', 1);('= [', 1);('humans in- ternal model', 1);('control matrix', 1);('humans inertia estimate', 1);('hand gestures', 1);('robot motors', 1);('robot end-effector position=', 1);('linear velocity', 1);('ro- bots end-effector dynamics', 1);('goal-dependent system2', 1);('\x03 whereis', 1);('defective robot motor', 1);('hadamard', 1);('prod- uct', 1);('and= [ 0.15,0,0 ]', 1);('system responsiveness', 1);('bias estimates', 1);('humans goal', 1);('linear system +1=+', 1);('system state', 1);('andrea bajcsy robot arm teleoperation lunar lander gradient', 1);('human passive', 1);('random oracle', 1);('herror', 1);('humanlunar lander', 1);('visualization', 1);('simulation environments', 1);('mean', 1);('standard deviation', 1);('human action optimality', 1);('forlunar lander', 1);('lander upright', 1);('robot objective', 1);('align humans in- ternal model', 1);('true robot dynamics model', 1);('mathematically', 1);('robots reward function', 1);('=||h||2 2||h||2', 1);('envi- ronment', 1);('true learning dynamics', 1);('robot intervention', 1);('hwell', 1);('objective .h2', 1);('internal model .h3', 1);('forh1', 1);('h. figure', 1);('lander', 1);('gradient andthreshold human learners', 1);('log likelihood', 1);('humans actions increases', 1);('hprediction', 1);('error decreases', 1);('robots ef- fort', 1);('method performs', 1);('robots dy- namics', 1);('h2', 1);('h3', 1);('environ- ment', 1);('threshold human', 1);('human doesnt', 1);('implicitly influencing', 1);('objectives', 1);('accurate un-', 1);('specifically', 1);('assistive robots', 1);('human motion preferences', 1);('importantly', 1);('setting influenc-', 1);('robots objective', 1);('minimal assistance', 1);('emergent behavior', 1);('simulate3thegradient hu- man learner', 1);('new human', 1);('cognition [', 1);('full posterior', 1);('model parameters', 1);('state-action observation', 1);('bayes', 1);('humans objectives', 1);('reward parameter', 1);('goal state', 1);('robotarm', 1);('human doesnt notice', 1);('robots sensors detect', 1);('nonetheless', 1);('4bayesian humans', 1);('goal location', 1);('initial preference', 1);('robots end-effector', 1);('straight line', 1);('qua- dratic cost function', 1);('ingoal influence', 1);('robot end-effector', 1);('current preference matricies', 1);('assistive robot', 1);('reward weights lead', 1);('goal', 1);('||h||2 2and', 1);('preference', 1);('robots reward parameter', 1);('=||h||2 2whereis', 1);('robot action', 1);('robot as- sists', 1);('robot as- sistance', 1);('h4', 1);('assistance', 1);('human-robot team', 1);('ro- bot effort', 1);('long-term assistance', 1);('h4andh5', 1);('humans control', 1);('teaching to teleoperate', 1);('human learning dynamics model', 1);('robots ability', 1);('real human learning', 1);('enable robots', 1);('real users', 1);('gradient human', 1);('random oracle learning', 1);('h0figure', 1);('environments', 1);('human objec- tives', 1);('internal', 1);('model error', 1);('robotic wheelchair', 1);('new robot dynamics', 1);('irb-', 1);('teleoperation performance', 1);('experimental setup', 1);('teleoperation task', 1);('jaco', 1);('participant uses', 1);('index finger', 1);('counter-clockwise pattern', 1);('diamond pattern', 1);('familiarzation task', 1);('default robot dynamics', 1);('new robots', 1);('different physical properties', 1);('variables', 1);('end-effector dynamics bias', 1);('x-direction andbias', 1);('dependent measures', 1);('humans ground-truth', 1);('measure human action optimality distance', 1);('internal understanding', 1);('true physics', 1);('subjective measures', 1);('scale survey', 1);('active teaching condition be-', 1);('optimal teleoperators', 1);('optimality distanceparticipant trajectorie', 1);('qualitative', 1);('late suboptimality w/', 1);('avg', 1);('human action optimality distance', 1);('% confidence interval', 1);('dashed', 1);('participant', 1);('active teaching robot', 1);('campus community', 1);('procedure', 1);('within-subjects design', 1);('robot strategy andphysical dynamics conditions', 1);('random order', 1);('participant experiences', 1);('robot strategies', 1);('familiar- ization round', 1);('experimental condition', 1);('human action opti- mality distance varies', 1);('ran- dom effect', 1);('significant main effect', 1);('interaction stage', 1);('post-hoc analysis', 1);('early- stage changes', 1);('robot behavior', 1);('humans later-stage action optimality', 1);('quantitative results', 1);('significant improvement', 1);('humans action optimality', 1);('subjective', 1);('active- teaching', 1);('representative examples', 1);('color gra- dient', 1);('optimal path', 1);('initial portion', 1);('trajectory exhibits', 1);('human trajectory', 1);('passive learning trajectory', 1);('comparable timesteps', 1);('appendix a.4', 1);('survey questions', 1);('survey', 1);('performance improvements', 1);('robot understanding', 1);('physical properties.', 1);('across', 1);('significant effect', 1);('teaching condition', 1);('longer-term interactions', 1);('conclusion', 1);('tractable method', 1);('approxi- mate human learning dynamics', 1);('hu- man learning', 1);('approximate dynamics', 1);('experimental results', 1);('limitations', 1);('general function approximator', 1);('as- sumptions', 1);('useful model', 1);('real data', 1);('neural networks', 1);('abundant human data', 1);('low-data settings', 1);('specific bias', 1);('data-driven models', 1);('cognitive science models', 1);('user study relies', 1);('average dynamics model', 1);('human learning trainedtowards', 1);('participants data', 1);('unique ways', 1);('exciting future direction', 1);('new humans', 1);('tractable inference', 1);('non-lq settings', 1);('references', 1);('brian scassellati', 1);('social eye gaze', 1);('chris', 1);('baker', 1);('rebecca saxe', 1);('action understand-', 1);('inverse planning', 1);('dimitri p bertsekas', 1);('dynamic', 1);('volume ii', 1);('belmont', 1);('ma', 1);('athena scientific', 1);('maya cakmak', 1);('manuel lopes', 1);('algorithmic', 1);('human teaching', 1);('sequential decision tasks', 1);('artificial intelligence', 1);('eduardo', 1);('camacho', 1);('carlos bordons alba', 1);('predictive control', 1);('business media', 1);('lawrence chan', 1);('dylan hadfield-menell', 1);('min chen', 1);('harold soh', 1);('david hsu', 1);('siddhartha srini-', 1);('trust-aware', 1);('decision making', 1);('human-robot collaboration', 1);('kenton ct lee', 1);('legibility', 1);('robot motion', 1);('sebastian', 1);('marco gallieri', 1);('jonathan masci', 1);('jan koutnk', 1);('mark cannon', 1);('infinite-horizon', 1);('differentiable model predictive control', 1);('learning representations', 1);('hugging face', 1);('transformers', 1);('//huggingface.co/docs/transformers/ index [', 1);('noah d goodman', 1);('probmods contributors', 1);('accessed', 1);('nick chater', 1);('charles kemp', 1);('amy perfors', 1);('exploring', 1);('inductive biases', 1);('trends', 1);('cognitive sciences', 1);('dylan p. losey', 1);('interaction', 1);('robot teams', 1);('legible', 1);('subtask allocations', 1);('ieee robotics', 1);('haimin hu', 1);('jaime', 1);('fisac', 1);('active uncertainty reduction', 1);('implicit dual', 1);('approach', 1);('wafr', 1);('sandy h huang', 1);('david', 1);('pieter abbeel', 1);('enabling', 1);('autonomous robots', 1);('siddarth jain', 1);('brenna argall', 1);('human intent recognition', 1);('assistive robotics', 1);('michael janner', 1);('qiyang li', 1);('offline', 1);('reinforcement learning', 1);('big sequence', 1);('julian jara-ettinger', 1);('theory', 1);('inverse reinforcement learning', 1);('current opinion', 1);('rudolf emil kalman', 1);('bol', 1);('wisama khalil', 1);('etienne dombre', 1);('crc', 1);('kris', 1);('kitani', 1);('james andrew bagnell', 1);('martial hebert', 1);('activity', 1);('european conference', 1);('computer vision', 1);('forrest laine', 1);('chih-yuan chiu', 1);('claire tomlin', 1);('multi-hypothesis interactions', 1);('game-theoretic motion', 1);('vladlen koltun', 1);('continuous', 1);('inverse optimal control', 1);('optimal examples', 1);('arxiv preprint arxiv:1206.4617', 1);('kejun li', 1);('maegan tucker', 1);('erdem byk', 1);('ellen novoseller', 1);('joel', 1);('burdick', 1);('yanan sui', 1);('yisong yue', 1);('aaron d ames', 1);('roial', 1);('region', 1);('active learning', 1);('exoskeleton gait preference landscapes', 1);('hong jun jeon', 1);('mengxi li', 1);('krishnan srinivasan', 1);('ajay mandlekar', 1);('animesh garg', 1);('jeannette bohg', 1);('latent actionsto control assistive robots', 1);('autonomous', 1);('david marr', 1);('vision', 1);('computational investigation', 1);('human represen- tation', 1);('visual information', 1);('w.h', 1);('freeman', 1);('max mulder', 1);('cybernetics', 1);('tunnel-in-the-sky displays', 1);('reuben', 1);('aronson', 1);('kris kitani', 1);('harmonic', 1);('multimodal dataset', 1);('assistive human robot collaboration', 1);('abhijat biswas', 1);('sarthak ahuja', 1);('siddharth girdhar', 1);('kris k kitani', 1);('examining', 1);('anticipatory robot assistance', 1);('human decision making', 1);('andrew y ng', 1);('stuart j russell', 1);('algorithms', 1);('inverse reinforcement learning ..', 1);('icml', 1);('swaprava nath', 1);('ariel d procaccia', 1);('game-theoretic', 1);('human adaptation', 1);('human-robot collabora- tion', 1);('sagar parekh', 1);('rili', 1);('robustly influenc-', 1);('latent intent', 1);('arxiv preprint arxiv:2203.12705', 1);('lasse peters', 1);('vicen rubies-royo', 1);('claire j tomlin', 1);('cyrill stachniss', 1);('continuous dynamic games', 1);('partial state observations', 1);('arxiv preprint arxiv:2106.03611', 1);('mark pfeiffer', 1);('ulrich schwesinger', 1);('hannes sommer', 1);('enric galceran', 1);('roland siegwart', 1);('predicting', 1);('cooperative', 1);('partial mo- tion planning', 1);('maximum entropy models', 1);('ieee/rsj', 1);('intelligent robots', 1);('iros', 1);('david premack', 1);('guy woodruff', 1);('does', 1);('brain sciences', 1);('irene rae', 1);('leila takayama', 1);('bilge mutlu', 1);('sid reddy', 1);('siddharth reddy', 1);('assisted', 1);('communicate state', 1);('arxiv preprint arxiv:2008.02840', 1);('shankar sastry', 1);('sanjit', 1);('seshia', 1);('human actions ..', 1);('shane saunderson', 1);('goldie nejat', 1);('sur- vey', 1);('nonverbal communication', 1);('social humanrobot interaction', 1);('sydney y schaefer', 1);('iris', 1);('shelly', 1);('kurt', 1);('thoroughman', 1);('beside', 1);('motor adaptation', 1);('error correction', 1);('task-irrelevant conditions', 1);('neurophysiology', 1);('john schulman', 1);('filip wolski', 1);('prafulla dhariwal', 1);('alec radford', 1);('oleg klimov', 1);('proximal', 1);('policy optimization algorithms', 1);('arxiv preprint arxiv:1707.06347', 1);('wilko schwarting', 1);('alyssa pierson', 1);('javier alonso-mora', 1);('sertac karaman', 1);('daniela rus', 1);('social behavior', 1);('autonomous vehicles', 1);('national academy', 1);('lei shi', 1);('naomi h feldman', 1);('performing bayesian', 1);('exemplar models', 1);('annual meeting', 1);('science society', 1);('megha srivastava', 1);('erdem biyik', 1);('suvir mirchandani', 1);('noah goodman', 1);('assistive', 1);('motor control', 1);('tasks', 1);('humans', 1);('arxiv preprint arxiv:2211.14003', 1);('richard', 1);('sutton', 1);('andrew g barto', 1);('reinforcement', 1);('intro- duction', 1);('mit', 1);('liting sun', 1);('anca d dra-', 1);('safety', 1);('confidence-aware game-theoretic human models', 1);('luke tierney', 1);('joseph', 1);('kadane', 1);('accurate', 1);('posterior moments', 1);('marginal densities', 1);('statistical association', 1);('tomer d. ullman', 1);('con-', 1);('ceptual development', 1);('building models', 1);('annual review', 1);('developmental psychology', 1);('533558. https', 1);('1146/annurev-devpsych-121318-084833 arxiv', 1);('//doi.org/10.1146/annurev- devpsych-121318-084833 [', 1);('ashish vaswani', 1);('noam shazeer', 1);('niki parmar', 1);('jakob uszkoreit', 1);('llion jones', 1);('aidan n gomez', 1);('kaiser', 1);('illia polosukhin', 1);('attention', 1);('kevin waugh', 1);('inverse correlated equilibrium', 1);('matrix', 1);('yair weiss', 1);('eero p simoncelli', 1);('edward h adelson', 1);('motion', 1);('optimal percepts', 1);('nature', 1);('annie xie', 1);('ryan tolsma', 1);('chelsea finn', 1);('multi-agent interaction', 1);('robot learning', 1);('andrew', 1);('maas', 1);('anind k dey', 1);('maximum', 1);('entropy inverse reinforcement learning ..', 1);('aaai', 1);('chicago', 1);('il', 1);('appendix a.1 derivation', 1);('closed-form solution', 1);('gaussian integral', 1);('positive-definite matrix', 1);('r.', 1);('2+\x11 =', 1);('exp\x101 21\x11', 1);('infinite-horizon linear-quadratic regulator', 1);('discrete-time dynamics', 1);('cost quadratic', 1);('infinite-horizon optimal cost-to-go', 1);('infinite- horizon', 1);('ricatti', 1);('feedback matrix =', 1);('derivation', 1);('assume', 1);('hfunction', 1);('plugging', 1);('= exp\x10', 1);('=exp\x10 \x02', 1);('1\x03 \x11', 1);('exponent contains', 1);('substituting', 1);('\x11 =exp\x10 \x11', 1);('=minh ++', 1);('\x11 =exp\x10', 1);('h+1h+2h+3', 1);('h+1', 1);('h+2', 1);('transformer', 1);('encoder encoder encoder', 1);('architecture', 1);('a.2 details', 1);('human learning dy- namics', 1);('l =l', 1);('transformers in- ternal model predictions', 1);('neural network weights', 1);('how-', 1);('component l', 1);('h=\x02logp', 1);('h\x03', 1);('hthrough', 1);('h-', 1);('implicit function', 1);('precise form', 1);('robot executes exaggerations', 1);('example participant trajectory', 1);('blue vectors', 1);('solid line', 1);('timesteps sam-', 1);('proposition', 1);('compute l', 1);('a.3 training', 1);('ar- chitecture', 1);('optimization details', 1);('multilayer perceptron', 1);('lay- ers', 1);('hugging faces', 1);('implementation [', 1);('transformer encoder [', 1);('adam', 1);('neural network.in', 1);('transformer architecture', 1);('output layer size', 1);('diagonal elements', 1);('human belief', 1);('tray goals', 1);('prob- ability', 1);('diago- nal terms', 1);('r33', 1);('diagonal elements ofand', 1);('h. a.4 user', 1);('alignment', 1);('user study data', 1);('robot executes actions', 1);('hwhere=0.5for', 1);('ris', 1);('influence-aware planning method', 1);('fig', 1);('sample participant trajectory', 1);('solid blue vector', 1);('blue vector', 1);('s time intervals', 1);('qualitatively', 1);('robots physics model', 1);('robot plans', 1);