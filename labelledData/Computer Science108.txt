('visual encoder', 33);('ppgeo', 20);('imagenet', 13);('posenet', 13);('zhang', 12);('cvpr', 12);('rst stage', 10);('fig', 10);('aco', 10);('selfd', 9);('depthnet', 8);('mim', 7);('godard', 7);('carla', 7);('dynamic', 7);('learning', 7);('youtube', 6);('chen', 6);('kaiming', 6);('moco', 6);('icml', 6);('relu fc', 6);('odometry estimation tasks', 5);('contrastive learning', 5);('visual input', 5);('wu', 5);('town01', 5);('corl', 5);('iccv', 5);('masked', 5);('consecutive frames', 4);('policy learning', 4);('shah', 4);('caesar', 4);('end-to-end autonomous', 4);('target image', 4);('it0', 4);('reinforcement learning', 4);('town02', 4);('cilrs', 4);('infraction score', 4);('training samples', 4);('collision rate', 4);('sec', 4);('neurips', 4);('pieter abbeel', 4);('decision making', 3);('future ego-motion', 3);('challenging scenarios', 3);('learning process', 3);('parisi', 3);('radosavovic', 3);('xie', 3);('radford', 3);('irrelevant information', 3);('video data', 3);('depth estimation', 3);('dynamic model', 3);('task data', 3);('imitation learning', 3);('navigation task', 3);('codevilla', 3);('leaderboard town05-long', 3);('town05', 3);('completion', 3);('salient objects', 3);('figure', 3);('rl', 3);('eigen-cam', 3);('ego vehicle', 3);('pre-training', 3);('nachum', 3);('li dong', 3);('furu wei', 3);('beit', 3);('iniclr', 3);('vladlen koltun', 3);('ross girshick', 3);('felipe codevilla', 3);('sergey levine', 3);('unsupervised', 3);('deep reinforcement learning', 3);('trevor darrell', 3);('end-to-end', 3);('eccv', 3);('urban driving', 3);('image', 3);('model details', 3);('detailed', 3);('relu conv', 3);('large-scale data', 2);('computer vision', 2);('policy pre-training', 2);('geometric', 2);('photometric error', 2);('side product', 2);('wide span', 2);('visuomotor', 2);('mnih', 2);('toromanoff', 2);('large corpus', 2);('visuomotor policy learning', 2);('kumar', 2);('xiao', 2);('deng', 2);('background buildings', 2);('optimal choice', 2);('yamada', 2);('robotic control tasks', 2);('data augmentations', 2);('view invariance', 2);('geographical locations', 2);('camera intrinsics', 2);('frame input', 2);('visual en- coder', 2);('depth', 2);('a.2 stage', 2);('-we need', 2);('stop', 2);('policy \x19', 2);('source images', 2);('3d geometry', 2);('photometric reconstruction error', 2);('current frame', 2);('specically', 2);('direct supervision', 2);('visuomotor autonomous', 2);('initial weights', 2);('wang', 2);('hz', 2);('adam', 2);('learning rate', 2);('resnet-34', 2);('torchvision', 2);('classication task', 2);('simmim', 2);('moco-v2', 2);('dosovitskiy', 2);('corl2017', 2);('training data', 2);('generalization ability', 2);('trafc lights', 2);('prakash', 2);('tcp', 2);('driving score', 2);('rate', 2);('random trials', 2);('vehicle collisions', 2);('red light infractions', 2);('conduct experiments', 2);('ground truth trajectory', 2);('episode return', 2);('competitive performance', 2);('agent needs', 2);('input contains', 2);('sequence', 2);('muhammad', 2);('yeasin', 2);('nlp', 2);('representation learning', 2);('peng', 2);('resnet', 2);('rl-based', 2);('vision models', 2);('yang', 2);('d4rl', 2);('fu', 2);('shao', 2);('bert', 2);('prafulla dhariwal', 2);('girish sastry', 2);('amanda askell', 2);('chelsea finn', 2);('simple framework', 2);('xinlei chen', 2);('haoqi fan', 2);('antonio', 2);('alexey dosovitskiy', 2);('li fei-fei', 2);('andreas geiger', 2);('clement godard', 2);('oisin mac aodha', 2);('gabriel j. brostow', 2);('xiangyu zhang', 2);('shaoqing ren', 2);('jian sun', 2);('saining xie', 2);('georg ostrovski', 2);('david silver', 2);('iclr', 2);('kimin lee', 2);('hangbo bao', 2);('alec radford', 2);('ilija radosavovic', 2);('tete xiao', 2);('stephen james', 2);('jitendra malik', 2);('mlp', 2);('dims', 2);('examples', 2);('failure cases', 2);('policy pre-training for autonomous driving viaself-supervised geometric modeling penghao wu1', 1);('chen1hongyang li1', 1);('jia1', 1);('yan1', 1);('qiao1', 1);('shanghai ai laboratory2uc san diego3shanghai jiao tong', 1);('abstract witnessing', 1);('impressive achievements', 1);('natural language processing', 1);('won- der', 1);('grab-and-go spirit', 1);('sample inefciency problem', 1);('variant nature', 1);('translation invariance', 1);('visual input contains', 1);('massive irrelevant in- formation', 1);('general vision', 1);('policy pre- training', 1);('policy representations', 1);('powerful abstraction', 1);('3d geometric scenes', 1);('framework generates pose', 1);('depth predictions simulta-', 1);('visual encoder learns', 1);('policy representation', 1);('current visual observation', 1);('rich driving pol-', 1);('multiple visuomotor driv-', 1);('extensive', 1);('improvements range', 1);('code', 1);('ntroduction policy', 1);('learning refers', 1);('autonomous agent', 1);('decision-making policy', 1);('certain task', 1);('particular environment', 1);('levine', 1);('hessel', 1);('laskin', 1);('raw sensor observations', 1);('control modules', 1);('end-to-end fashion', 1);('visuomotor policy models', 1);('learning tabula rasa', 1);('en- vironment interactions', 1);('satisfactory performance', 1);('espeholt', 1);('wijmans', 1);('yarats', 1);('sample efciency caveat', 1);('visual per- ception network', 1);('promising solution', 1);('recent', 1);('popular visual', 1);('classica- tion', 1);('guar- antee', 1);('superior representation', 1);('robotic policy learning tasks', 1);('dexterous manipulation', 1);('motor \x03work', 1);('shanghai ai laboratory', 1);('email', 1);('lihongyang @ pjlab.org.cn 1arxiv:2301.01006v1 [ cs.cv ]', 1);('jan', 1);('uniqueness', 1);('red points', 1);('static obstacles', 1);('yellow rectangles', 1);('trafc signal', 1);('green box', 1);('control outputs', 1);('different light', 1);('weather conditions', 1);('photo', 1);('control skills', 1);('visual navigation', 1);('challenging visuomotor task', 1);('end-to-end autonomous driving1', 1);('aforementioned predominant', 1);('general computer vision tasks', 1);('failin case', 1);('general vision tasks', 1);('wide range', 1);('input sequence', 1);('small resolution', 1);('environment setting', 1);('geometric relationships', 1);('complex scenarios', 1);('input data', 1);('nearby static obstacles', 1);('decision making task', 1);('good driving policy', 1);('desirable model', 1);('particular parts/patterns', 1);('deterministic relation', 1);('trafc signals', 1);('necessary demand', 1);('massive amount', 1);('various driving environments', 1);('pivotal question', 1);('introduce driving-decision awareness', 1);('crucial visual cues', 1);('frame sensor input', 1);('previous', 1);('literature tackles', 1);('supervision problem', 1);('open dataset', 1);('target domain data', 1);('approaches suffer', 1);('noisy predictions', 1);('exists distinct domain gap', 1);('trafc complexities', 1);('rizve', 1);('3d geometric scene', 1);('learning ego-motion', 1);('intrinsics training', 1);('intrinsics learning', 1);('conventional depth estimation frameworks', 1);('go-', 1);('camera intrinsics network', 1);('current input', 1);('1we use end-to-end autonomous', 1);('andvisuomotor autonomous', 1);('posenet depthnet visual encoder', 1);('focus', 1);('self-supervised visuomotor policy pre', 1);('downstream tasks intrinsic kego motion t photometric', 1);('ego motion tphotometric', 1);('reconstruction a.1 stage', 1);('two-single', 1);('frame input -since', 1);('stop-consecutive', 1);('frames input -since frames', 1);('encoder', 1);('fine-tuned', 1);('policy learningvisual input figure', 1);('overview', 1);('effective visual encoder', 1);('a.1 stage', 1);('temporal inputs', 1);('illustrative example', 1);('ego-vehicle needs', 1);('pose networks', 1);('new initial weights', 1);('additional performance gain', 1);('key contributions', 1);('various visuomotor', 1);('rst attempt', 1);('full extent', 1);('different types', 1);('difculty levels', 1);('various metrics', 1);('challenging cases', 1);('ethodology', 1);('verview', 1);('control actions', 1);('visual observation x', 1);('raw image input', 1);('compact repre- sentation', 1);('important information', 1);('method pre-trains', 1);('t wo-stage self-supervised training', 1);('self-supervised geometric modeling', 1);('it0in', 1);('6-dof ego-motion', 1);('articial labels', 1);('3the source images', 1);('formally', 1);('pixel-wise correspondence', 1);('itandit0is', 1);('k\x001pt', 1);('homogeneous coordinates', 1);('itandit0respectively', 1);('kis', 1);('camera intrinsic matrix', 1);('dt', 1);('depth value', 1);('pixel piin', 1);('following godard', 1);('images adjacent', 1);('t+ 1g', 1);('common encoder-decoder structure', 1);('depth map', 1);('input image', 1);('ego- motion', 1);('mlp-based', 1);('camera intrinsics estimation', 1);('optical center', 1);('focal lengths fx', 1);('gordon', 1);('chanduri', 1);('visuomotor policy pre-training', 1);('ego-motion estimation', 1);('policy learning tasks', 1);('frame image', 1);('predicts ego-motion', 1);('subsequent frame', 1);('visual encoder estimates', 1);('tt', 1);('onitalone andtt', 1);('inverse operation', 1);('intrinsics estimation', 1);('actual driving policy', 1);('current timestamp', 1);('pseudo motion labels', 1);('geometric projection approach', 1);('inaccurate depth estimation', 1);('accurate ones', 1);('global optimization', 1);('undesirable prediction inaccuracy', 1);('noise results', 1);('online videos', 1);('ossfunction following godard', 1);('loss function', 1);('photometric loss', 1);('smoothness loss', 1);('` 1term', 1);('ssim', 1);('structural similarity index measure', 1);('` pe=', 1);('smooth loss', 1);('` s=j @ xd\x03 tje\x00j @ xitj+j @ yd\x03 tje\x00j @ yitj', 1);('whered\x03 tis', 1);('inverse depth map', 1);('minimum reprojection loss', 1);('e xperiments', 1);('different driving conditions', 1);('kingma', 1);('ba', 1);('10\x004which drops', 1);('adamw', 1);('loshchilov', 1);('hutter', 1);('cyclic learning rate scheduler', 1);('batch size', 1);('colorjitter', 1);('ramdomgrayscale', 1);('gaussianblur', 1);('d escription on compared baselines', 1);('convolution layers', 1);('constant initialization', 1);('model weight', 1);('marcel', 1);('rodriguez', 1);('reconstruct images', 1);('random masked-out patches', 1);('convolutional networks', 1);('randomresizedcrop', 1);('randomhorizontalflip', 1);('following zhang', 1);('generate pseudo', 1);('whole policy model', 1);('methods aforemen-', 1);('visual model', 1);('close relationship', 1);('d escription on downstream autonomous driving tasks', 1);('open-loop planning task', 1);('real-world autonomous', 1);('dataset nuscenes', 1);('briey describe', 1);('bench- mark', 1);('unseen weather', 1);('trafc participants', 1);('different sizes', 1);('vi- sual encoders', 1);('closed-loop evaluation', 1);('evaluation metric', 1);('success rate', 1);('classic image', 1);('dynamic task', 1);('dosovit-', 1);('setting differentiates', 1);('dynamic objects', 1);('realistic benchmark corresponds', 1);('leader-', 1);('board benchmark', 1);('40k training data', 1);('eval- uate', 1);('au- tonomous', 1);('major metrics', 1);('closed-loop navigation task', 1);('pre-train methodnavigation', 1);('\x060.0 9.6\x065.2 15.3\x064.5 73.3\x062.3', 1);('\x062.0 42.0\x062.0 69.3\x066.4 87.3\x064.6', 1);('\x061.2 8.0\x060.0 31.3\x062.3 57.3\x063.1', 1);('\x062.1 39.3\x069.2 48.7\x064.2 69.3\x061.2', 1);('\x062.0 44.0\x061.2 71.3\x061.2 92.0\x063.5', 1);('\x060.0 32.0\x060.0 50.7\x062.3 62.7\x061.2', 1);('42.0\x062.0 73.3\x066.1 91.3\x061.2 96.7\x061.2', 1);('pedstrain collisions', 1);('main metric', 1);('proximal policy optimization', 1);('ppo', 1);('schulman', 1);('carla town01', 1);('roach', 1);('task involves trajectory planning', 1);('real-world dataset nuscenes', 1);('current visual input', 1);('model plans', 1);('3-second trajectory', 1);('ground truth log', 1);('future vehicles', 1);('l2', 1);('metrics', 1);('different time lengths', 1);('planning model', 1);('gru-based', 1);('ofcial train-val', 1);('n umeric comparison on downstream tasks', 1);('evaluation results', 1);('table 1-', 1);('environment steps', 1);('reinforcement learning experiments', 1);('open-loop nuscenes', 1);('different number', 1);('full 40k', 1);('small size', 1);('improvement gap', 1);('new environment', 1);('real-world style', 1);('performance degrades', 1);('complex cases', 1);('d epth and odometry estimation', 1);('large-scale training', 1);('odometry estimation models', 1);('rst-stage training', 1);('specif-', 1);('mon-', 1);('kitti', 1);('geiger', 1);('performance improve-', 1);('closed-loop navigation', 1);('pre-train methodnavigation dynamic', 1);('\x060.0 2.0\x060.0 10.0\x060.0 32.0\x068.0', 1);('\x061.2 28.7\x065.0 64.7\x062.3 72.7\x061.2', 1);('\x061.2 10.3\x062.5 14.7\x063.1 58.7\x061.2', 1);('\x061.2 12.0\x064.0 28.0\x065.3 66.7\x062.3', 1);('\x061.2 12.0\x060.0 22.0\x062.0 47.3\x065.0', 1);('\x060.0 29.3\x061.2 38.0\x061.6 59.3\x066.4', 1);('23.3\x061.2 34.0\x065.3 71.3\x061.2 84.0\x065.3', 1);('closed-loop leaderboard town05-long', 1);('task results', 1);('main metrics', 1);('infraction details', 1);('evaluation', 1);('pre-train methoddriving scoreinfraction scoreroute completioncollisions', 1);('pedestriancollisions vehiclecollisions layoutoff-road violationsagent', 1);('light violations', 1);('random 33.50\x061.67 0.65\x060.02 60.49\x062.93 0.09\x060.07 1.16\x060.40 0.00\x060.00 0.44\x060.13 0.97\x060.09 0.53\x060.12', 1);('41.29\x063.20 0.77\x060.03 57.52\x064.87 0.00\x060.00 0.71\x060.20 0.11\x060.15 0.15\x060.01 1.01\x060.16 0.29\x060.10', 1);('36.39\x060.21 0.72\x060.04 61.75\x062.26 0.14\x060.11 0.91\x060.12 0.04\x060.07 0.18\x060.17 0.87\x060.03 0.14\x060.11', 1);('32.10\x062.04 0.65\x060.02 64.09\x064.01 0.13\x060.11 0.79\x060.16 0.00\x060.00 0.49\x060.07 0.81\x060.15 0.45\x060.13', 1);('33.05\x063.05 0.67\x060.06 59.52\x063.21 0.00\x060.00 0.69\x060.28 0.05\x060.07 0.54\x060.05 0.94\x060.08 0.73\x060.10', 1);('38.76\x063.02 0.65\x060.03 68.72\x067.36 0.17\x060.07 0.84\x060.18 0.00\x060.00 0.32\x060.03 0.75\x060.15 0.12\x060.08', 1);('47.44\x065.63 0.79\x060.08 65.05\x065.11 0.04\x060.05 0.54\x060.29 0.00\x060.00 0.16\x060.11 0.76\x060.10 0.04\x060.05 /uni00000014/uni00000013/uni00000013 /uni00000015/uni00000013/uni00000013 /uni00000016/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013 /uni00000018/uni00000013/uni00000013 /uni00000019/uni00000013/uni00000013 /uni0000001a/uni00000013/uni00000013 /uni0000001b/uni00000013/uni00000013 /uni00000036/uni00000057/uni00000048/uni00000053/uni00000056/uni00000003/uni0000000b/uni0000002e/uni0000000c/uni00000014/uni00000013/uni00000013 /uni00000013/uni00000014/uni00000013/uni00000013/uni00000015/uni00000013/uni00000013/uni00000016/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013/uni00000018/uni00000013/uni00000013/uni00000028/uni00000053/uni0000004c/uni00000056/uni00000052/uni00000047/uni00000048/uni00000003/uni00000035/uni00000048/uni00000057/uni00000058/uni00000055/uni00000051/uni00000039/uni0000004c/uni00000056/uni00000058/uni00000044/uni0000004f/uni00000003/uni00000028/uni00000051/uni00000046/uni00000052/uni00000047/uni00000048/uni00000055/uni00000003/uni00000029/uni0000004c/uni00000051/uni00000048/uni00000010/uni00000057/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a /uni0000002c/uni00000050/uni00000044/uni0000004a/uni00000048/uni00000031/uni00000048/uni00000057 /uni00000030/uni00000052/uni00000026/uni00000052 /uni00000024/uni00000026/uni00000032 /uni00000033/uni00000033/uni0000002a/uni00000048/uni00000052 /uni00000014/uni00000013/uni00000013 /uni00000015/uni00000013/uni00000013 /uni00000016/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013 /uni00000018/uni00000013/uni00000013 /uni00000019/uni00000013/uni00000013 /uni0000001a/uni00000013/uni00000013 /uni0000001b/uni00000013/uni00000013 /uni00000036/uni00000057/uni00000048/uni00000053/uni00000056/uni00000003/uni0000000b/uni0000002e/uni0000000c/uni00000014/uni00000013/uni00000013/uni00000015/uni00000013/uni00000013/uni00000016/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013/uni00000028/uni00000053/uni0000004c/uni00000056/uni00000052/uni00000047/uni00000048/uni00000003/uni00000035/uni00000048/uni00000057/uni00000058/uni00000055/uni00000051/uni00000039/uni0000004c/uni00000056/uni00000058/uni00000044/uni0000004f/uni00000003/uni00000028/uni00000051/uni00000046/uni00000052/uni00000047/uni00000048/uni00000055/uni00000003/uni00000029/uni00000055/uni00000052/uni0000005d/uni00000048/uni00000051 /uni0000002c/uni00000050/uni00000044/uni0000004a/uni00000048/uni00000031/uni00000048/uni00000057 /uni00000030/uni00000052/uni00000026/uni00000052 /uni00000024/uni00000026/uni00000032 /uni00000033/uni00000033/uni0000002a/uni00000048/uni00000052', 1);('left', 1);('right', 1);('standard deviation', 1);('different random seeds', 1);('open-loop', 1);('` 2distance', 1);('model predic- tions', 1);('pre-train methodl2', 1);('# 1s 2s 3s 1s 2s 3s', 1);('imagnet', 1);('improvement', 1);('pre-train methoddepth estimation odometry estimation', 1);('absrel # sqrel # rmse # rmse log # a1', 1);('\x060.010 0.015\x060.010', 1);('\x060.009 0.013\x060.009', 1);('ours aco imagenet moco origin figure', 1);('activation maps', 1);('ablative', 1);('key designs', 1);('experimentnavigation', 1);('single', 1);('\x062.0 53.3\x061.2 79.3\x064.2 92.7\x062.3', 1);('\x061.2 58.0\x062.0 86.0\x062.1 92.0\x062.0', 1);('\x062.0 52.0\x062.0 76.7\x061.2 90.0\x060.0 4ppgeo 42.0\x062.0 73.3\x066.1 91.3\x061.2 96.7\x061.2 ment', 1);('additional harvest', 1);('v isualization results', 1);('feature representations', 1);('specic cues', 1);('brake action', 1);('front vehicles', 1);('classication tends', 1);('wrong objects', 1);('blative study', 1);('conduct ablative study', 1);('different designs', 1);('train-', 1);('inferior performance', 1);('correct ego- motion', 1);('depth estimation quality', 1);('per- formance', 1);('8pseudo label supervision', 1);('inferior results', 1);('inaccurate pseudo label impairs', 1);('great extent', 1);('r elated work pre-training', 1);('vision', 1);('essential key', 1);('articial intelligence', 1);('language processing', 1);('powerful capability', 1);('transformer', 1);('vaswani', 1);('large- scale datasets', 1);('large models', 1);('dominant paradigm', 1);('kenton', 1);('toutanova', 1);('brown', 1);('training specic', 1);('recently', 1);('im- age', 1);('bao', 1);('impressive improvement', 1);('various vision benchmarks', 1);('recent vision-language', 1);('extraordinary potential', 1);('multi-modal learning', 1);('generic representation learning methods', 1);('various data augmentation techniques', 1);('dynamic environment', 1);('visuomotor applications', 1);('control policy', 1);('raw visual input', 1);('model needs', 1);('visual pixels', 1);('dynamic behaviors', 1);('training visuomotor models', 1);('environment interactions', 1);('dexterous manipulation tasks', 1);('extensive experiments', 1);('diverse control domains', 1);('train control policies', 1);('clip', 1);('ai', 1);('robot navigation problems', 1);('visuomotor tasks', 1);('paradigm learning policy representations', 1);('raw data', 1);('seo', 1);('gupta', 1);('control tasks', 1);('representation learning objectives', 1);('visual inputs', 1);('such control tasks', 1);('rst training', 1);('crucial driv-', 1);('vast amounts', 1);('target domain', 1);('dense geometric reconstruction', 1);('possible adverse effect', 1);('autonomous driving', 1);('sensor inputs', 1);('end-to-end manner', 1);('liang', 1);('inherent difculty', 1);('urban-style autonomous', 1);('such meth- ods data-hungry', 1);('interfuser', 1);('current top-1 method', 1);('carla leader-', 1);('data samples', 1);('marln', 1);('environ- ment steps', 1);('sample efciency problem', 1);('real-world application', 1);('such approaches', 1);('satisfying performance', 1);('onclusion and discussion', 1);('direct approach', 1);('method out- performs', 1);('time step', 1);('future direction', 1);('multi-step motion prediction', 1);('references hangbo bao', 1);('songhao piao', 1);('image transformers', 1);('tom brown', 1);('benjamin mann', 1);('nick ryder', 1);('melanie subbiah', 1);('jared d kaplan', 1);('arvind neelakantan', 1);('pranav shyam', 1);('language', 1);('few-shot learners', 1);('holger caesar', 1);('varun bankiti', 1);('alex h lang', 1);('sourabh v', 1);('venice erin liong', 1);('qiang xu', 1);('anush krishnan', 1);('yu pan', 1);('giancarlo baldan', 1);('oscar beijbom', 1);('multimodal dataset', 1);('sai shyam chanduri', 1);('zeeshan khan suri', 1);('igor v', 1);('christian m uller', 1);('camlessmonodepth', 1);('monocular', 1);('unknown camera parameters', 1);('arxiv preprint arxiv:2110.14347', 1);('annie', 1);('suraj nair', 1);('generalizable robotic reward functions', 1);('in-the-wild human videos', 1);('rss', 1);('dian chen', 1);('brady zhou', 1);('philipp kr', 1);('ahenb uhl', 1);('ting chen', 1);('simon kornblith', 1);('mohammad norouzi', 1);('geoffrey hinton', 1);('visual representations', 1);('improved', 1);('momentum contrastive learning', 1);('arxiv preprint arxiv:2003.04297', 1);('matthias', 1);('m uller', 1);('l opez', 1);('end-', 1);('conditional imitation learning', 1);('icra', 1);('eder santana', 1);('m l opez', 1);('adrien gaidon', 1);('exploring', 1);('jia deng', 1);('wei dong', 1);('richard socher', 1);('li-jia li', 1);('kai li', 1);('large-scale hierarchical image database', 1);('ros', 1);('antonio lopez', 1);('open urban driving simulator', 1);('lasse espeholt', 1);('hubert soyer', 1);('remi munos', 1);('karen simonyan', 1);('vlad mnih', 1);('tom ward', 1);('yotam doron', 1);('vlad firoiu', 1);('tim harley', 1);('iain dunning', 1);('impala', 1);('actor-learner architectures', 1);('aviral kumar', 1);('george tucker', 1);('datasets', 1);('deep data-driven reinforcement learning', 1);('arxiv preprint arxiv:2004.07219', 1);('philip lenz', 1);('raquel urtasun', 1);('kitti vision benchmark suite', 1);('monocular depth esti- mation', 1);('left-right consistency', 1);('michael firman', 1);('digging', 1);('monocular depth prediction', 1);('ariel gordon', 1);('hanhan li', 1);('rico jonschkowski', 1);('anelia angelova', 1);('monocular depth learning', 1);('unknown cameras', 1);('agrim gupta', 1);('stephen tian', 1);('yunzhi zhang', 1);('jiajun wu', 1);('roberto mart', 1);('n-mart n', 1);('maskvit', 1);('video prediction', 1);('arxiv preprint arxiv:2206.11894', 1);('delving', 1);('surpassing', 1);('human-level performance', 1);('imagenet classication', 1);('deep', 1);('residual learning', 1);('image recog- nition', 1);('yuxin wu', 1);('momentum', 1);('visual representation learning', 1);('yanghao li', 1);('piotr doll', 1);('scalable vision learners', 1);('matteo hessel', 1);('joseph modayil', 1);('hado van hasselt', 1);('tom schaul', 1);('will dabney', 1);('dan horgan', 1);('bilal piot', 1);('mohammad azar', 1);('rainbow', 1);('combining', 1);('aaai', 1);('jacob devlin ming-wei chang kenton', 1);('lee kristina toutanova', 1);('deep bidirectional transformers', 1);('language understanding', 1);('naacl-hlt', 1);('diederik p. kingma', 1);('jimmy ba', 1);('stochastic optimization', 1);('misha laskin', 1);('adam stooke', 1);('lerrel pinto', 1);('aravind srinivas', 1);('rein-', 1);('forcement learning', 1);('deep visuo- motor policies', 1);('jmlr', 1);('xiaodan liang', 1);('tairui wang', 1);('luona yang', 1);('eric xing', 1);('cirl', 1);('controllable imitative reinforcement learning', 1);('ilya loshchilov', 1);('frank hutter', 1);('decoupled', 1);('weight decay regularization', 1);('arxiv preprint arxiv:1711.05101', 1);('sebastien marcel', 1);('yann rodriguez', 1);('machine-vision package', 1);('acmmm', 1);('koray kavukcuoglu', 1);('andrei', 1);('rusu', 1);('joel veness', 1);('marc g belle-', 1);('alex graves', 1);('martin riedmiller', 1);('andreas k fidjeland', 1);('human-level', 1);('nature', 1);('mohammed bany muhammad', 1);('mohammed yeasin', 1);('class activation map', 1);('principal components', 1);('ijcnn', 1);('simone parisi', 1);('aravind rajeswaran', 1);('senthil purushwalkam', 1);('abhinav kumar gupta', 1);('qixiang ye', 1);('visual tokenizers', 1);('arxiv preprint arxiv:2208.06366', 1);('aditya prakash', 1);('kashyap chitta', 1);('multi-modal', 1);('fusion transformer', 1);('end-to- end autonomous', 1);('jong wook kim', 1);('chris hallacy', 1);('aditya ramesh', 1);('gabriel goh', 1);('sandhini agarwal', 1);('pamela mishkin', 1);('jack clark', 1);('transferable visual models', 1);('natural language supervision', 1);('real', 1);('world robot learning', 1);('mamshad nayeem rizve', 1);('kevin duarte', 1);('yogesh', 1);('rawat', 1);('mubarak shah', 1);('uncertainty-aware pseudo-label selection framework', 1);('john schulman', 1);('filip wolski', 1);('oleg klimov', 1);('proximal', 1);('policy optimization algorithms', 1);('arxiv preprint arxiv:1707.06347', 1);('younggyo seo', 1);('danijar hafner', 1);('hao liu', 1);('fangchen liu', 1);('world models', 1);('visual control', 1);('arxiv preprint arxiv:2206.14244', 1);('dhruv shah', 1);('blazej osinski', 1);('brian ichter', 1);('lm-nav', 1);('robotic', 1);('rutav shah', 1);('vikash kumar', 1);('rrl', 1);('hao shao', 1);('letian wang', 1);('ruobing chen', 1);('hongsheng li', 1);('yu liu', 1);('safety-enhanced', 1);('interpretable sensor fusion transformer', 1);('marin toromanoff', 1);('emilie wirbel', 1);('fabien moutarde', 1);('model-free reinforcement learning', 1);('implicit affordances', 1);('ashish vaswani', 1);('noam shazeer', 1);('niki parmar', 1);('jakob uszkoreit', 1);('llion jones', 1);('aidan n gomez', 1);('kaiser', 1);('illia polosukhin', 1);('attention', 1);('wenhui wang', 1);('johan bjorck', 1);('zhiliang peng', 1);('qiang liu', 1);('kriti aggarwal', 1);('owais khan mohammed', 1);('saksham singhal', 1);('subhojit som', 1);('foreign lan- guage', 1);('vision-language tasks', 1);('arxiv preprint arxiv:2208.10442', 1);('zhou wang', 1);('alan', 1);('bovik', 1);('hamid r sheikh', 1);('eero p simoncelli', 1);('quality assessment', 1);('error visibility', 1);('structural similarity', 1);('tip', 1);('erik wijmans', 1);('abhishek kadian', 1);('ari morcos', 1);('stefan lee', 1);('irfan essa', 1);('devi parikh', 1);('manolis savva', 1);('dhruv batra', 1);('dd-ppo', 1);('near-perfect pointgoal navigators', 1);('arxiv preprint arxiv:1911.00357', 1);('penghao wu', 1);('xiaosong jia', 1);('li chen', 1);('junchi yan', 1);('hongyang li', 1);('yu qiao', 1);('trajectory-guided', 1);('control prediction', 1);('strong baseline', 1);('zhirong wu', 1);('yuanjun xiong', 1);('stella x yu', 1);('dahua lin', 1);('non- parametric instance discrimination', 1);('motor control', 1);('arxiv preprint arxiv:2203.06173', 1);('zhenda xie', 1);('zheng zhang', 1);('yue cao', 1);('yutong lin', 1);('jianmin bao', 1);('zhuliang yao', 1);('qi dai', 1);('han hu', 1);('jun yamada', 1);('karl pertsch', 1);('anisha gunjal', 1);('joseph j lim', 1);('task-induced', 1);('representation', 1);('sequential decision making', 1);('denis yarats', 1);('ilya kostrikov', 1);('rob fergus', 1);('regularizing', 1);('jimuyang zhang', 1);('ruizhao zhu', 1);('eshed ohn-bar', 1);('self-learning', 1);('qihang zhang', 1);('zhenghao peng', 1);('bolei zhou', 1);('youtube videos', 1);('action-conditioned', 1);('contrastive policy', 1);('richard zhang', 1);('phillip isola', 1);('alexei', 1);('efros', 1);('colorful image colorization', 1);('zhejun zhang', 1);('alexander liniger', 1);('dengxin dai', 1);('fisher yu', 1);('luc van gool', 1);('reinforcement learning coach', 1);('pre-training for autonomous driving viaself-supervised geometric modeling supplementary materials', 1);('supplementary', 1);('network structures', 1);('de- scription', 1);('visual illustrations', 1);('common failure cases', 1);('c.', 1);('n etwork details', 1);('model structure', 1);('focal length head', 1);('optical center head', 1);('intrinsic matrix', 1);('please', 1);('codev-', 1);('trajectory planning model structure', 1);('layer type channels stride kernel size activation function image encoder resnet-34 measurement encoder conv', 1);('relu average pooling', 1);('layer type dims', 1);('activation function image encoder resnet-34', 1);('speed encoder fc', 1);('speed pred head fc', 1);('relu', 1);('pred head fc', 1);('sigmoid', 1);('trajectory planning model', 1);('image encoder resnet-34 bottleneck layer type dims', 1);('activation function fc', 1);('decoder layer type hidden', 1);('input dim output dim gru', 1);('d ownstream tasks details fornavigation', 1);('sparse waypoints', 1);('start point', 1);('end point', 1);('dynamic vehicles', 1);('theleaderboard-town05-long', 1);('different chal-', 1);('maps', 1);('navigation navigation', 1);('dynamic figure', 1);('front view image', 1);('15c l', 1);('imitations', 1);('visual encoder need', 1);('future motion', 1);('front-view image', 1);('navigation information', 1);('such cases', 1);('correct prediction', 1);('decision/future motion', 1);('itandit+1', 1);('ego vehicle stops', 1);('clear clue', 1);('itindicating', 1);('turning direction', 1);('italone', 1);