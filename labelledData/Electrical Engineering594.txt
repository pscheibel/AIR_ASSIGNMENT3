('mongolian', 21);('mntts2', 19);('tts', 14);('mongolian tts', 10);('fastspeech2', 8);('mntts', 8);('hifigan', 7);('multispeaker mongolian texttospeech synthesis dataset', 6);('f1', 6);('ieee', 6);('kailin liang bin liu yifan hu', 5);('experimental results', 4);('f2', 4);('robust multispeaker', 3);('fig', 3);('nmos', 3);('opinion score', 3);('ssmos', 3);('interspeech', 3);('international conference', 3);('baseline system', 2);('encoderdecoder', 2);('english mandarin', 2);('mainstream languages', 2);('latin sequences', 2);('f1 f2 f3', 2);('melspectrogram', 2);('conv', 2);('fastspeech2hifigan', 2);('tacotron2', 2);('truth', 2);('processing', 2);('van den', 2);('isca', 2);('mntts2 opensource multispeakermongolian texttospeech synthesis datasetkailin liangy bin liuy yifan huy rui liu feilong bao guanglai gaoinner mongolia', 1);('hohhot chinaliangkailin98foxmailcomiframeliu163comhyfwalker163comliuruiimu163com', 1);('csfeilong csgglimueducnabstract', 1);('texttospeech tts', 1);('synthesis lowresource languagesis', 1);('attractive research issue academia industry nowadaysmongolian ocial language', 1);('inner mongolia autonomousregion', 1);('representative lowresource language', 1);('10million people worldwide', 1);('relative lack opensourcedatasets', 1);('public opensourcemultispeaker', 1);('researchers work', 1);('transcription fromvarious topics invite', 1);('announcers toform threespeaker', 1);('dataset announcer records 10hours speeches', 1);('hours total', 1);('furthermorewe', 1);('fastspeech2model hifigan', 1);('dataset sucient', 1);('models realworld applications', 1);('keywords mongolian texttospeech tts opensource datasetmultispeaker1 introductiontexttospeech tts', 1);('input text humanlike speech 1it', 1);('standard technology humancomputer interaction cell phonevoice assistants car navigation smart speakers etc eld speech synthesishas', 1);('recent years', 1);('dierent', 1);('traditional methodswhich use concatenation', 1);('methods synthesizespeech neural endtoend', 1);('remarkable performance thehelp', 1);('typical models', 1);('tacotron', 1);('transformer tts', 1);('deep voice', 1);('etc accelerate', 1);('corresponding author rui liuy', 1);('research fundedby', 1);('highlevel talents introduction project inner mongolia', 1);('no1000022311201002', 1);('scientists', 1);('sciencefoundation china', 1);('62206136arxiv230100657v1 eessas', 1);('dec', 1);('althe inference speed nonautoregressive', 1);('fastspeech', 1);('mainstream methods', 1);('ttsnote', 1);('neural network', 1);('wavenet10 wavernn', 1);('melgan', 1);('model cansynthesize speech sounds', 1);('comparable human soundswe note', 1);('important factor', 1);('rapid development neural', 1);('ttsmentioned', 1);('large scale corpus resources', 1);('true forlanguages', 1);('worldwidehowever lowresource language', 1);('diculties corpus collection', 1);('thereforebuilding', 1);('largescale highquality', 1);('inaddition', 1);('singlespeaker dataset calledmntts', 1);('young female native', 1);('attention academia industry', 1);('thisalso', 1);('shows necessity', 1);('speechsynthesis datasets', 1);('baseline models source', 1);('mongolian ttsdataset', 1);('increases number speakers threeand increases data size', 1);('hours average', 1);('hoursper speaker textual content', 1);('inthe domain', 1);('mntts mntts2', 1);('available toacademics industry practitionersto', 1);('naturalness mean opinion score nmos', 1);('similaritymean opinion score ssmos', 1);('results terms naturalness speakersimilarity', 1);('system canachieve', 1);('satisfactory performance', 1);('themntts2 corpus', 1);('main contributions', 1);('dataset termd', 1);('speakers totalaudio duration', 1);('various domainssuchassportsandcultureetc2weusedthestateoftheartnonautoregressivefastspeech2modeltobuildthebaselinemodelandvalidateourmntts23themntts2 dataset source code', 1);('availableto academics industry practitionersthe rest paper', 1);('revisits relatedworks', 1);('corpus section', 1);('introduce details ofmntts2', 1);('corpus structure', 1);('statistical information section 4explains discusses', 1);('experimental setup', 1);('section5', 1);('speech synthesis futureresearch directions section', 1);('concludes paper summarizes work andresearch papermntts2', 1);('related workfor', 1);('datasets example', 1);('ljspeech', 1);('singlespeaker dataset', 1);('english', 1);('rich speaker diversity multispeakertts dataset', 1);('libritts', 1);('english aishell', 1);('forchinesefor lowresource language', 1);('available resourcesare', 1);('note attempts', 1);('eect oftts synthesis', 1);('low resource data', 1);('methods etc', 1);('dueto lack largescale training data', 1);('methods dicultto', 1);('practical scenariosin order', 1);('works builttheir', 1);('various models achievegood results example', 1);('huang', 1);('emotional embeddingsby', 1);('learning emotional', 1);('rui liu', 1);('new method segment', 1);('phrase predictionsystem', 1);('immediately rui liu', 1);('dnnbased mongolianspeech', 1);('synthesis system performs', 1);('hmm', 1);('bidirectional', 1);('term memory bilstm', 1);('model toimprove phrase', 1);('prediction step', 1);('traditional speech synthesissystem making', 1);('available public', 1);('m2asrmongo', 1);('speech recognitionare', 1);('speech recognition corpus', 1);('due environment noise', 1);('improper speaking style issuesetcwe', 1);('calledmntts total duration', 1);('hours recordedin studio', 1);('professional female native', 1);('theduration speaker diversity', 1);('necessary construct highquality multispeaker', 1);('research focus paperwe introduce details', 1);('mntts2 datasetin', 1);('section rst revisit', 1);('dataset briey introduceour', 1);('mnttsin', 1);('preliminary work', 1);('highquality singlespeaker', 1);('mongoliantts', 1);('transcription dataset collectedfrom', 1);('wide range topics policy sports culture etc', 1);('mongolianscript', 1);('possible professional female native', 1);('torecord audio', 1);('check realignthe alignment errors audio', 1);('ambient noise mispronunciationwas', 1);('ensure overall qualitymntts', 1);('attention researchers industryupon release', 1);('furthermore', 1);('mongolian texttospeech challenge lowresource scenario ncmmsc20221 theorganizers', 1);('hours data participants train modelsthis competition', 1);('promotes development', 1);('minority languages', 1);('china32 mntts2the', 1);('construction pipeline', 1);('text', 1);('collection narrationtext', 1);('audio', 1);('audiotext alignment willintroduce order report corpus structure statisticstext collection narration', 1);('rst step', 1);('large amount transcription', 1);('thenatural', 1);('text materials crawl text information fromwebsites', 1);('electronic books text topics', 1);('following', 1);('rich content', 1);('wide range topics eg politics cultureeconomy sports etc meet requirements', 1);('unsuitable content whichmay', 1);('sensitive political issues', 1);('religious issues pornographic contentthese contents', 1);('hope dataset', 1);('positivecontribution development', 1);('language originalintention worktext', 1);('compared', 1);('mandarinand english', 1);('performs agglutinative characteristic 27this', 1);('letters express dierent styles dierent contextsand', 1);('serious harmonic phenomenon', 1);('latin alphabet', 1);('mongolianrepresentation tts', 1);('entire pipeline', 1);('mongoliantexts', 1);('latinconversion', 1);('text regularization', 1);('ourprevious work', 1);('5mntts2f1spkid1uttidtxt spkid1uttidwavf2', 1);('f3spkid1uttidtxt', 1);('spkid1uttidwav spkid1uttidtxt spkid1uttidwavfig1 folder structure', 1);('dierentwiththemntts15weinvitedthreenativemongolianspeakingannouncerstorecordtheaudioeachannouncer', 1);('consent form', 1);('data collection use protocol', 1);('f1 f3', 1);('grade recordings', 1);('inner mongolia', 1);('adobe audition2as', 1);('end audio segment', 1);('constant distance betweenthe lips microphone performs', 1);('slight pause comma position andperforms', 1);('appropriate pitch boost question mark positionto ensure quality', 1);('specically', 1);('volunteersto check text', 1);('corresponding natural audio volunteersare', 1);('sentences aligningthe', 1);('sentences text', 1);('latinsequence', 1);('latin word sequence', 1);('word letterthatmakesupthewordiscalledacharactercharactersalsoincludepunctuationmarkssuchascommasperiodsquestionmarkexclamationmark etc', 1);('30h speech data', 1);('accuracy 16bitcorpus structure statistics', 1);('corresponding textcollection', 1);('speaker audios', 1);('statistics results', 1);('unitspeaker idf1 f2 f3charactertotal', 1);('f3a f1', 1);('f3fig2', 1);('word number distributions b c sentence duration distributionsd e f speakers', 1);('mntts2wav', 1);('format les', 1);('bits text savedin', 1);('txt', 1);('utf8', 1);('name audio', 1);('name name', 1);('id', 1);('idthe', 1);('statistical results', 1);('entire corpus total', 1);('characters andthe average number characters', 1);('shortest sentencemntts2', 1);('characters longest sentence', 1);('characters wordsare', 1);('statistical unit total number words dataset', 1);('value words sentence', 1);('minimum value 3and', 1);('maximum value', 1);('example word numbersof sentences', 1);('45seconds comparison', 1);('word numbers sentences', 1);('f3', 1);('onthe hand', 1);('obvious concentration', 1);('thestatistics', 1);('speakers line', 1);('normal distribution4', 1);('speech synthesis experimentstoverifythevalidityofourmntts2weconductedmongolianttsexperimentsbased fastspeech2', 1);('mean opinion score mos', 1);('metric terms ofnaturalness speaker similarity41', 1);('experimental setupweusethetensorflowttstoolkit3tobuildanendtoendttsmodelbasedonthe fastspeech2', 1);('model converts input', 1);('mongoliantextintomelspectrogramfeaturesandthenthehifiganvocoderreconstructsthe', 1);('speech synthesis model extracts duration pitchand energy', 1);('speech waveform uses', 1);('inputconditions training model', 1);('errors repetitionand word', 1);('fast training speed', 1);('fastspeech2introduces', 1);('variance information alleviate onetomany mappingproblem', 1);('pitch prediction', 1);('wavelet transform ofall', 1);('fast robust', 1);('controllable speechsynthesis', 1);('main reason', 1);('fig3basedonfastspeech2weimplementthemultispeakerfastspeech2byaddingthe', 1);('speaker encoder module speaker encoder', 1);('speaker embeddingdense softplus layers3 network architecture setting number ofspeakers', 1);('dimension speaker', 1);('side ofthe text encoder', 1);('layersize decoder', 1);('layers variance predictors', 1);('dropout rate', 1);('theinitial', 1);('learning rate', 1);('dropout rate 02the', 1);('vocoder builds network generative adversarialnetwork converts', 1);('highquality audio generatorof', 1);('convolution multireceptive', 1);('fusion module3httpsgithubcomtensorspeechtensorflowtts8', 1);('embeddingmongolian scriptstext encoderspeaker encoderspeakeridpositional encodingvariance adaptorpositional encodingmelspectrogram decoderhifiganmrfa fastspeech2', 1);('hifiganspeaker embeddingfig3', 1);('speaker encoder modulewhich', 1);('agenerative adversarial network', 1);('kinds discriminators', 1);('list discriminators cycle scaleis', 1);('periodic discriminator', 1);('poolingtype output', 1);('melgan discriminator', 1);('averagepooling1dthe', 1);('kernel size', 1);('activation function', 1);('leakyrelu hifigan', 1);('speaker generator onlystft loss rst', 1);('100k steps generator discriminatorare', 1);('100k steps', 1);('corresponding vocoder ofthe', 1);('speakersnote teacher', 1);('speaker usedto extract duration attention contrast', 1);('fastspeech2model', 1);('training speaker', 1);('100k stepsof', 1);('extract duration multispeakerfastspeech2modelwastrainedwith200kstepstodothenalspeechgeneration100k steps', 1);('hifigans', 1);('generator 100k steps jointlymntts2', 1);('generator discriminator models', 1);('tesla v100 gpus42 naturalness evaluationfor', 1);('full comparison naturalness', 1);('baseline systemfastspeech2hifigan theground', 1);('speech addition toverify performance', 1);('fastspeech2grinlimbaseline', 1);('model comparison', 1);('grinlim', 1);('algorithm directlyobtain phase information audio reconstruct waveform withoutadditional training', 1);('naturalness mean opinion score ssmos29', 1);('assess naturalness speaker', 1);('sentencesas evaluation', 1);('training modelgeneratedaudio ground truth audio', 1);('distributedto listeners evaluation process', 1);('audio speeches quietenvironmentthe', 1);('ground', 1);('truth speech', 1);('thebest performance', 1);('outperforms thefastspeech2grinlim', 1);('performance groundtruth speakers', 1);('combination offastspeech2', 1);('hifiganspecically f1 f2 f3 nmos fastspeech2hifiganachieved', 1);('encouraging provesthat highquality', 1);('model nutshell results', 1);('dataset canbe', 1);('system highquality speech', 1);('naturalness', 1);('results systems with95condence intervalssystemspeaker', 1);('idf1 f2 f3fastspeech2grinlim', 1);('\x0600943 speaker', 1);('similarity evaluationwe', 1);('speaker similarityperformance', 1);('fastspeechhifigan', 1);('speakersimilarity mean opinion score ssmos', 1);('table 3we', 1);('audios speaker', 1);('fastspeech2hifiganbaseline', 1);('ten', 1);('mongolianspeaking', 1);('thespeaker person', 1);('ground truth audiothe', 1);('encouraging results', 1);('thefastspeech2hifigan system performs', 1);('good performance terms speakersimilarity', 1);('f1 auditioningthe', 1);('audio nd', 1);('f1s', 1);('timbre signicant characteristics', 1);('speakers voice information', 1);('nutshellthisexperimentshowsthatthemntts2datasetcanbeusedforspeechsynthesiswork multispeaker', 1);('similarity mean', 1);('results forfastspeech2hifigan system', 1);('condence', 1);('idf1 f2 f3fastspeech2hifigan', 1);('challenges', 1);('workwith', 1);('empathy ai', 1);('research emotional', 1);('speech', 1);('synthesis conversationalscenarios emotional speech synthesis', 1);('hot research topics nowadays 31furthermore control emotion category emotional', 1);('speech generation', 1);('interesting direction', 1);('mntts2does', 1);('emotion category emotion intensityin', 1);('future work', 1);('comprehensive indepth expansionof data', 1);('development emotional', 1);('mongolian tts6 conclusionwepresentedalargescaleopensourcemongoliantexttospeechcorpusmntts2which', 1);('durations topics speakers', 1);('releasingour', 1);('knowledge attribution', 1);('license', 1);('academic commercial use describe detailthe process building corpus validate usability thecorpus', 1);('hifiganvocoder', 1);('future work introduce emotional', 1);('dataset enrich ourcorpus', 1);('eects dierent', 1);('architectures andmodel hyperparameters results conduct', 1);('subsequent analysesmntts2', 1);('jonathan shen ruoming pang ron j weiss mike schuster navdeep jaitlyzongheng yang zhifeng chen yu zhang yuxuan wang rj skerrvryan', 1);('alnatural tts synthesis', 1);('wavenet mel spectrogram predictionsin2018', 1);('international conference acoustics speech signal processingicassp pages', 1);('fcharpentierandmstella diphonesynthesisusinganoverlapaddtechniqueforspeech', 1);('waveforms concatenation', 1);('icassp', 1);('conferenceon acoustics speech', 1);('lawrencerrabiner atutorialonhiddenmarkovmodelsandselectedapplicationsin', 1);('speech recognition', 1);('proceedings ieee', 1);('kyunghyun cho bart', 1);('merrinboer dzmitry bahdanau yoshua bengioon', 1);('properties neural machine translation', 1);('semantics structure statistical translation', 1);('yuxuan wang rj skerryryan daisy stanton yonghui wu ron j weissnavdeep jaitly zongheng yang ying xiao zhifeng chen samy bengio', 1);('towards', 1);('endtoend speech synthesis', 1);('proc interspeech', 1);('naihan li shujie liu yanqing liu sheng zhao ming liu neural', 1);('speechsynthesis transformer network', 1);('proceedings thirtythird aaaiconference articial intelligence thirtyfirst innovative applications', 1);('intelligence', 1);('aaai symposium educationaladvances articial intelligence', 1);('sercan', 1);('arik mike chrzanowski adam coates gregory frederick diamosandrew gibiansky yongguo kang xian li john miller andrew ng jonathanraiman', 1);('deep', 1);('realtime', 1);('neural texttospeech', 1);('icml', 1);('yi ren yangjun ruan xu tan tao qin sheng zhao zhou zhao tieyan liu fastspeech fast', 1);('controllable text speech', 1);('hanna mwallach hugo larochelle alina beygelzimer florence', 1);('emily', 1);('foxand', 1);('garnett', 1);('advances neural information processing systems32 annual', 1);('neural information processing systems', 1);('neurips2019 december', 1);('vancouver bc canada', 1);('yi ren chenxu hu xu tan tao qin sheng zhao zhou zhao tieyan liufastspeech', 1);('fast', 1);('highquality endtoend text speech', 1);('internationalconference learning representations iclr', 1);('virtual event austria may37', 1);('openreviewnet', 1);('aron', 1);('oord sander dieleman heiga zen karen simonyan oriol vinyalsalex graves nal kalchbrenner andrew', 1);('senior koray kavukcuogluwavenet', 1);('generative model', 1);('raw audio', 1);('isca speech synthesisworkshop sunnyvale ca usa', 1);('september', 1);('nal kalchbrenner erich elsen karen simonyan seb noury norman casagrandeedward lockhart florian stimberg aron', 1);('oord sander dieleman', 1);('kavukcuoglu ecient', 1);('neural audio synthesis', 1);('jennifer g dy', 1);('krause', 1);('proceedings', 1);('international conference onmachine', 1);('learning icml', 1);('stockholmsmssan stockholm sweden july', 1);('proceedings machine learning', 1);('research pages', 1);('pmlr', 1);('kundan kumar rithesh kumar thibault', 1);('boissiere lucas gestin wei zhenteohjosesoteloalexandredebrbissonyoshuabengioandaaronccourville12 kailin liang bin liu yifan hu', 1);('generative', 1);('adversarial networks conditional waveform synthesiscorr abs191006711', 1);('jungil kong jaehyeon kim jaekyoung bae higan generative', 1);('adversarialnetworks ecient high delity speech synthesis', 1);('advances neuralinformation processing systems', 1);('uradyn e bulag mongolian', 1);('ethnicity linguistic anxiety china', 1);('americananthropologist', 1);('yifan hu pengkai yin rui liu feilong bao guanglai gao mntts anopensource', 1);('mongolian texttospeech synthesis dataset', 1);('baselinecorr abs220910848', 1);('keith ito linda johnson', 1);('lj speech dataset', 1);('heiga zen viet dang rob clark yu zhang ron j weiss ye jia zhifeng chenand yonghui wu libritts', 1);('librispeech texttospeechproc', 1);('yao shi hui bu xin xu shaoji zhang ming li aishell3', 1);('multispeakermandarin tts corpus baselines', 1);('horace', 1);('barlow unsupervised', 1);('neural', 1);('xiaojin jerry zhu semisupervised', 1);('learning literature survey', 1);('karl weiss taghi khoshgoftaar dingding wang', 1);('big data', 1);('aihong huang feilong bao guanglai gao yu rui liu mongolianemotional', 1);('speech synthesis', 1);('learning emotional embeddingin2021', 1);('language processing ialp', 1);('ruiliufeilongbaoguanglaigaoandweihuawang mongolianprosodicphraseprediction', 1);('sux segmentation', 1);('asianlanguage processing ialp', 1);('rui liu feilong bao guanglai gao yonghe wang mongolian', 1);('deep neural network', 1);('national conference manmachinespeech communication pages', 1);('springer', 1);('rui liu feilong bao guanglai gao hui zhang yonghe wang improvingmongolianphrasebreakpredictionbyusingsyllableandmorphologicalembeddingswith', 1);('bilstm model', 1);('tiankai zhi ying shi wenqiang', 1);('guanyu li dong wang m2asrmongo', 1);('free mongolian speech database', 1);('in24th', 1);('cocosda', 1);('international committee', 1);('coordination standardisation speech databases assessment techniquesococosda', 1);('singapore november', 1);('feilong bao guanglai gao xueliang yan weihua wang segmentationbasedmongolian', 1);('lvcsr approach', 1);('acousticsspeech', 1);('jiatao gu james bradbury caiming xiong victor k li richard sochernonautoregressive', 1);('neural machine translation', 1);('corr', 1);('robert', 1);('streijl stefan winkler david hands mean', 1);('methods applications limitations alternatives', 1);('multimediasystems', 1);('rui liu berrak sisman guanglai gao haizhou li decoding', 1);('knowledgetransfer neural texttospeech training', 1);('ieeeacm transactions audiospeech language processing', 1);('rui liu berrak sisman haizhou li reinforcement', 1);('learning emotionaltexttospeech synthesis', 1);('emotion discriminability', 1);('hynekhermansky honza cernock luks burget lori lamel odette scharenborgand petr motlcek', 1);('annual', 1);('conference theinternational', 1);('speech communication', 1);('brno czechia', 1);('august', 1);('rui liu haolin zuo de hu guanglai gao haizhou li explicit', 1);