('preprint', 19);('elsevier page', 19);('construction representations oil wells', 18);('transformer', 9);('cosdist', 7);('romanenkova', 6);('informer', 6);('lstm', 6);('performer', 5);('siamese', 4);('petroleum', 4);('neural networks', 3);('roc auc', 3);('rnns', 3);('triplet', 3);('ffn', 3);('attention', 3);('transformerbased', 3);('top1', 3);('int', 3);('attention mechanism', 2);('informer performer', 2);('recurrent', 2);('transformers', 2);('petlab', 2);('normalize', 2);('figure', 2);('attention weights', 2);('models quality', 2);('fc', 2);('siamese transformer', 2);('top2', 2);('upper part table contains reference results', 2);('romanenkovaet', 2);('eucldist', 2);('xgboost', 2);('siamese informer', 2);('technology conference', 2);('august', 2);('problem statement', 2);('interval int', 2);('intervals', 2);('architecture use', 2);('fully', 2);('fcpart', 2);('distribution embeddings', 2);('dashed', 2);('horizontal lines', 2);('unsupervised', 1);('construction representations oil wells viatransformersalina', 1);('rogulinaa nikita baramiiaa valerii kornilova sergey petrakovaand alexey zaytsevaaskolkovo', 1);('institute science technology', 1);('moscow russiaarticle infokeywords', 1);('machine learningdeep learningtransformersimilarity', 1);('reservoir formation properties', 1);('asignicant challenge', 1);('variations properties evaluation wellinterval similaritymanymethodologiesforsimilaritylearningexist fromrulebasedapproachestodeepneuralnetworks', 1);('recently', 1);('eg recurrent neural networks', 1);('similarity modelas', 1);('sequential data approach suers shortterm memory paysmore attention end sequenceneuralnetworkwithtransformerarchitectureinsteadcasttheirattentionoverallsequencesto', 1);('ecient terms computational time introducea', 1);('architecturesweconductexperimentsonopendatasetswithmorethan20wellsmakingourexperimentsreliable suitable industrial usage', 1);('adaptation oftheinformervariantoftransformerwithrocauc', 1);('itoutperformsclassicalapproacheswith roc auc', 1);('0934and straightforwardusage', 1);('transformers roc auc', 1);('0961credit authorship contribution statementalina', 1);('rogulina conceptualization methodology software validation', 1);('data curation writing original draft writing reviewediting visualization nikita baramiia methodology software validationinvestigation writing original draft writing reviewediting visualization valerii kornilov methodologysoftware validation', 1);('writing original draft writing reviewediting visualization sergey petrakovmethodology software validation', 1);('writing original draft writing reviewediting visualization alexey zaytsev conceptualization writing original draft writing reviewediting supervisionproject', 1);('introductiondrilling', 1);('process plays', 1);('essential role basin characteristic exploration', 1);('accidents avoidance', 1);('well', 1);('wellinterval similarity', 1);('properties oil', 1);('wells withknown properties', 1);('estimate lithological', 1);('physical properties', 1);('appropriate wellintervalrepresentation', 1);('process changes depth sequentiallywelllogs', 1);('multivariate sequences', 1);('sequential', 1);('data forces', 1);('architecture utilizes itsstructure', 1);('maximum benet', 1);('wide usage', 1);('asignicant breakthrough area', 1);('natural language processing', 1);('nlp', 1);('transformerorcids0000000213926908 rogulinaarogulina', 1);('13arxiv221214246v1 cslg', 1);('dec', 1);('transformersarchitecture', 1);('common problems', 1);('rnns vaswani', 1);('evident benet', 1);('rnns ismail', 1);('2019however processing largelength sequences', 1);('signicant computation time memoryresources', 1);('recentapproacheseg informerzhouetal2021andperformerchoromanskietal2020addressthisproblembyproposingseveralimprovementstothebasictransformerarchitectureallowingtofocusattentiononlyonthe', 1);('relevant part sequencegivenallthiswepresentourapproachtosimilaritylearningforwellloggingdatabasedontransformerarchitecture', 1);('main contributions work follows1', 1);('problem similarity estimation intervals', 1);('transformerarchitecture', 1);('appropriate hyperparameters2', 1);('improvement terms eciency', 1);('ideas wellinterval similarity task3 models', 1);('architecture processing', 1);('data oilgas', 1);('similarity estimation overallbetter representations', 1);('expert labels', 1);('required4 analyze attention matrices nd', 1);('vital part', 1);('tool sensitivity analysis', 1);('related', 1);('way estimate wells correlation', 1);('approach authors', 1);('startzman kuo1987', 1);('expert knowledge', 1);('logical rules welltowell correlation', 1);('zoraster', 1);('ali', 1);('usage geometrical distances', 1);('jaccard overlap', 1);('similarities wells similarity calculation', 1);('properties areunproductive terms', 1);('wellinterval representationstheexperimentspresentedinrogulinaetal2022showbetterperformanceofclassicalmachinelearningmodelsthan', 1);('approaches model', 1);('rocauc', 1);('althoughpredictionscanbeusedfordistancecalculationandclusteringrepresentationsreectingwellsgeographical', 1);('physical features', 1);('possible case', 1);('models architecturefor wellinterval', 1);('sense authors', 1);('wellinterval representations', 1);('lstm kumar', 1);('transformersa transformerbased', 1);('approach welllogs processing approach', 1);('egorovet', 1);('data representations', 1);('applicable dierentoilgaseldproblems', 1);('theauthorsofmarusovetal2022adaptnoncontrastiveapproacheslikebyolandbarlowtwinsforloggingdatarepresentations theauthorsofabdrakhmanovetal2021traintransformerstopredictwellsproductivity itisclaimedthatthisapproachallowstransferringthemodeltoanotherwelltogetanaccuracyincreaseof', 1);('bottomhole pressure ow rate evolutions', 1);('wellinterval similarity problems', 1);('methods31 data overview preprocessingdata', 1);('open access dataset', 1);('zealand petroleum minerals onlineexploration database', 1);('taranaki', 1);('zealand', 1);('urenui', 1);('due presence expert labels classes andlayersinour work weconsider similaritynot betweenwhole wellsbut betweenintervals fromwells oflength 100measurement 33ft selection', 1);('appropriate aggregation local properties', 1);('eliminate', 1);('negative zeroequal resistivity cavernous intervals dierence betweencalliper bit size', 1);('fill', 1);('backward ll3', 1);('convert', 1);('electrical resistivity data lognormal scale4', 1);('gammaray neutron log data', 1);('standard scaler5', 1);('unit variancein experiments', 1);('density log', 1);('drho', 1);('density logdens gammaray', 1);('gr', 1);('sonic log', 1);('dtc32 methodology321 similarity', 1);('problem statementourgoalistoprovideamodelthatcanreportasimilaritybetweentwointervalsintermsoftheirphysicalpropertiesasexactlabellingonsimilaritiesisrarelyavailableweusebothdirectandindirectapproachestoestimatethequalityof', 1);('transformersto', 1);('train model focus', 1);('pair ofintervals', 1);('target value pair', 1);('pair intervals isnegative', 1);('dierent wells target value case', 1);('labelsfor training', 1);('problem statement model training', 1);('note scope ofapplicationofmodelstrainedviathisapproachiswiderthandistinguishingintervalsfromdierentwellsaswehopeto', 1);('useful problems well322', 1);('neural', 1);('network architectureswe use', 1);('types loss functions experiments', 1);('siamese triplet', 1);('figures', 1);('pair intervals label', 1);('input triplet intervals anchor interval', 1);('positive intervalfrom', 1);('negative interval', 1);('anchoras encoders architectures', 1);('dierent variants', 1);('transformerinformer performer details', 1);('belowtransformer isaneuralnetworkarchitectureintroducedfornlptasksvaswanietal2017', 1);('itoutperformsalternative', 1);('recurrent convolution architectures', 1);('recursion avoidance', 1);('speedupcalculationsviaparallelcomputationduringbothtrainingandinferencestepstheauthorsintroduceaselfattentionmechanism encoder part utilize backbone classication tasklets', 1);('consider', 1);('sequence length l', 1);('encoder composition', 1);('nlayers inputtotherstlayeristheoriginsequences inputtothenextlayeristheoutputofapreviouslayer theinputsequencelengthis lforalllayers eachlayerconsistsofattentionmechanismsandpositionwisefeedforwardlayersor', 1);('feedforward network', 1);('important layer normalization', 1);('connectionthe attention mechanism', 1);('querykeyvalue qkv', 1);('aqkv', 1);('connections points sequenceaqkv softmaxhqkdivwhereqrld query matrix', 1);('krld', 1);('key matrix', 1);('vrld', 1);('value matrix encoder', 1);('qxwqkxwkvxwvwherexis', 1);('previous layer rst layer', 1);('original inputsequence', 1);('matrix', 1);('qandkwith', 1);('attention matrix', 1);('weightsforvmatrix elements', 1);('information dierent words log data dierent', 1);('thusthe', 1);('elements attention matrix', 1);('information correspondence dierent parts sequencearogulina', 1);('transformershigherattentionresultsfromclosercorrespondencebetweenakeyandaqueryandthusreectclosercorrespondencebetween', 1);('dierent elements sequence division dis', 1);('gradient problemhoweveritisbenecialtoutilizenotoneattentionfunctionbutseveralwithdierentprojectionmatricestoretrievedierent information data', 1);('multihead', 1);('attention introducedma', 1);('qkv', 1);('concathead1headhwowhere headiaqwqikwkivwviprocessingthesequenceasawholeavoidsproblemsofforgettingpastinformationwhichistypicalforalternativearchitecture', 1);('rnnsthe', 1);('element sequence', 1);('andidenticallyffnx max0xw1b1w2b2informer', 1);('zhouetal2021inheritstheattentionmechanismfromtransformerbutimplementsitmoreecientlyintermsofmemoryconsumptionandcomputationalcost themainnoveltyoftheproposedencoderpartisthesparseselfattentionmechanism probsparse consideragain qkandvarequerykeyandvaluematricescorrespondinglyqikiviith', 1);('rows matrices', 1);('classic selfattention ith query isaqikv jkqikjlkqiklvjjpkjqivjwherekqikj exp\x18qikjd\x19 idea use queries pkjqiis', 1);('usingquerysparsitymeasurementwecangettopqueriesanddeneselfattentionsimilartotheoriginalwork probsparseselfattention', 1);('asaqkv softmaxhqkdivwhereqis', 1);('top queries', 1);('sparse attention matrix havemultipleheadsweavoidseverelossofinformationvia', 1);('probsparse', 1);('generallycomputationcomplexityforeachquerykeylookupdecreasefromquadratic ol2tologarithmic olnlthelayermemoryusagefrom ol2toollnl', 1);('transformersperformer', 1);('choromanski', 1);('fast attention via', 1);('orthogonal', 1);('random featuresfavor mechanism', 1);('linear time space complexity', 1);('lof', 1);('fullrank attention matrixestimation precision', 1);('assumptions matrix structurefavor mechanism', 1);('amay', 1);('kernelmatrixwithakernel kxy expxy', 1);('thisallowsittobeestimatedbyunbiasedapproximationwithrandomfeaturemap', 1);('d1qktvd', 1);('diagqkt1lqkrlrhave rows qtitand', 1);('positive regularisedand orthogonal', 1);('stable estimation attention matrix', 1);('low variance approach results', 1);('small bounds', 1);('large deviation probabilities', 1);('old2logdcomplexitythere', 1);('architecture class', 1);('scope example random featuremethods', 1);('broader family kernels', 1);('peng', 1);('munkhoeva', 1);('2018attention analysis', 1);('sensitivity inputs identify', 1);('important inputs', 1);('theattention matrix', 1);('hypothesis models', 1);('wayscalculate correlation attention weights models gradients', 1);('according serrano smith2019 jain wallace', 1);('jain wallace', 1);('resistant random elimination parts ofan interval', 1);('thesepapers authors', 1);('simple architectures interpretability', 1);('open questioneliminate parts intervals', 1);('somecriterion measurements reduces quality', 1);('random drop', 1);('important features', 1);('criteria ith element asequence2lucidrainsperformerpytorch3xl402performer4nawnoesperformerarogulina', 1);('transformers1', 1);('attention weights qiki', 1);('select top kqueries', 1);('diagonal elements theattention matrix nd index ksmallest elements drop row index theoriginal wellinterval2', 1);('gradient respect inputs', 1);('select top kmodels gradients nd indicesand delete elements', 1);('original wellinterval', 1);('indiceswe use zeros values', 1);('normal distribution masks', 1);('part gaps withrandom values', 1);('complete drop them33', 1);('technical', 1);('modelwevarythemostessentialhyperparametersforallmodels thenumberofheadsformultiheadattention numheads andthedropoutprobability dropoutprob', 1);('forvanillatransformerwealsovarythedimensionofthefullyconnectedlayers', 1);('dimfeedforward number layers encoder numlayers size model encoders', 1);('dimension dmodel probsparse attentionfactor factordimensionoffullyconnectednetworkpart dffandthenumberofencoderlayers elayers', 1);('weusehiddensize', 1);('number random featuresnumrandfeatures', 1);('weuse', 1);('hiddensize 16infcpartofsiamesearchitecture', 1);('thebesthyperparametersandhyperparameters', 1);('search space model', 1);('table 1all models', 1);('group 5fold crossvalidation', 1);('ingeneralsiamesebasedarchitecturesqualityisbetterthan', 1);('experiments resultsour', 1);('experimental evaluation', 1);('questionsdoes usage models', 1);('transformer architectures', 1);('transformer informer performer', 1);('animprovement similarity models', 1);('previouslydoes attention', 1);('interpretability model', 1);('new way conduct sensitivity analysiscan embeddings', 1);('problems similarity estimationthe empirical evidence', 1);('answers questions', 1);('separate subsections5optunaarogulina', 1);('transformerstable 1the', 1);('hyperparameters hyperparameters search spaces', 1);('siamese triplet transformer informer', 1);('andperformer use', 1);('python', 1);('value stepandminimummaximum notation oat', 1);('optunaparameters siamese triplet search spacetransformernumheads', 1);('models qualitywe', 1);('crossvalidation splits', 1);('train model 25000pairsinthecaseofsiamesebasedapproachortripletsinthecaseoftripletbasedapproach', 1);('fortestingweuse', 1);('5000pairs wellsresults', 1);('5dierent metricsto measure quality models', 1);('according informer', 1);('work withcontinuous', 1);('fullyconnected', 1);('layers encoder similarity evaluation', 1);('simpler cosine distance betweenembeddings', 1);('cosdistafter', 1);('models training embeddings', 1);('agglomerative', 1);('judging ari informer', 1);('ari', 1);('scores models', 1);('table342 attention', 1);('attention analysis point sensitivity analysis', 1);('weconducted', 1);('estimation', 1);('attentions models gradients2', 1);('repeatedly', 1);('replacement part interval', 1);('transformerstable 2comparison quality models', 1);('problem quality metric', 1);('encoder accuracy precision recall roc auc pr aucresults romanenkova', 1);('regression0725', 1);('fclstm0842', 1);('0046our resultssiamese', 1);('fctransformer0909', 1);('fcinformer0927', 1);('fcperformer0872', 1);('0046table 3comparison embeddings', 1);('encoder ariresults romanenkova', 1);('0173siameselstm0569 0162triplet', 1);('0122our resultssiamesetransformer0426 0144triplet', 1);('0062siameseinformer0933 0076triplet', 1);('006siameseperformer0721 009triplet', 1);('0058withzerosornumbersfromrandomnormaldistributionandcalculatetheinitialandobtainedmodelsaccuracywe generate', 1);('pairs intervals', 1);('gradients attention weights calculate correlation betweenthem', 1);('accuracy percentage', 1);('elements inan intervalthecorrelationvalueis corr', 1);('thusthereisnodirectionbetweengradientsandattentionvaluesalthough', 1);('correlation high', 1);('small attention weights', 1);('inuences models quality', 1);('mask elements normalarogulina', 1);('transformersdistribution43 embeddings', 1);('qualityin part', 1);('multiclass classication problem', 1);('embeddingsthe procedure training encoder embeddings', 1);('generate data intervals fromthe', 1);('original data series wells', 1);('data intervalsare', 1);('embeddings', 1);('siamese rnn', 1);('triplet rnn siamesetransformer', 1);('siamese performerthe', 1);('common machine learning models1', 1);('classier default hyperparameters2', 1);('logistic', 1);('linear layer neural network3', 1);('neuralnetworkof3linearlayersconnectedbyreluactivationfunction thedimensionoftherstlayerequalsthe', 1);('layer equals', 1);('layerequals 128sinceourtasksareconnectedwithclassicationweusethefollowingmetrics', 1);('accuracyprecisionrecallrocauc pr auc', 1);('present comparison', 1);('table quality models ishigh fact', 1);('classication part', 1);('metrics conclude thattransformers embeddings', 1);('classes total bestvalues', 1);('conclusionwedemonstrateapplicationsoftransformerbasedarchitecturestooilgasloggingdatacollectedduringdrillingwe', 1);('similarity problemthe results', 1);('signicant model quality increase', 1);('score equals', 1);('previous modelsgive', 1);('note improvement place', 1);('informerother', 1);('lstmbased', 1);('similarity estimation', 1);('informerbased', 1);('siamese tripletvariationsprovidemeaningfulwellintervalrepresentationsintermsofgeologicalscope ourbestariscoreisequalto0933', 1);('value 0569and', 1);('ourconclusionsaboutstrongembeddingsarealsoexposedbytheexperimentwithembeddingsclassicationonwellsbya', 1);('simple machine learning algorithmarogulina', 1);('transformerstable 4mean values quality metrics multiclass classication task', 1);('top2best', 1);('embedding classier accuracy precision recall roc auc pr aucxgboostxgboost0344', 1);('0846xgboostone linear layer0142', 1);('0572xgboostfullyconnectedneural network0197', 1);('0795furthermore show attention maps models', 1);('attention valuescorrespondtoamoreimportantpartofaninterval', 1);('parts intervals', 1);('low attention score6', 1);('acknowledgementsthis', 1);('grant', 1);('transformerscode', 1);('availability sectionname codelibrary transoilgascontact', 1);('alinarogulinaskoltechruprogram', 1);('language pythonsoftware', 1);('python 3x', 1);('jupyter notebookthesourcecodesareavailablefordownloadingatthelink', 1);('ir kanin ea boronin sa burnaev ev osiptsov aa', 1);('models longterm prediction transient production oil wells', 1);('spe', 1);('onepetroali jiang r h pan h abbas k ashraf u ullah j', 1);('machine', 1);('learninga novel approach', 1);('logs similarity', 1);('onsynchronization measures', 1);('shear sonic logs journal', 1);('engineering', 1);('k likhosherstov v dohan', 1);('x gane sarlos hawkins p davis j mohiuddin kaiser', 1);('attention performers arxiv preprint arxiv200914794', 1);('egorov gevorgyan n zaytsev', 1);('selfsupervised', 1);('similarity models', 1);('data arxiv preprint arxiv220912444', 1);('ismailaagunadympessoalcorradabravohfeizis2019inputcellattentionreducesvanishingsaliencyofrecurrentneuralnetworksadvances neural information processing systems', 1);('wallace bc', 1);('explanation arxiv preprint arxiv190210186', 1);('kumar', 1);('deep learning models', 1);('log processing quality control', 1);('global dependence thecomplex sequences', 1);('abu dhabi', 1);('exhibition conference', 1);('onepetromarusovabaianovvzaytseva2022 noncontrastiveapproachestosimilaritylearning', 1);('munkhoevamkapushevyburnaeveoseledetsi2018quadraturebasedfeaturesforkernelapproximationadvancesinneuralinformationprocessing', 1);('systems 31peng', 1);('h pappas n yogatama schwartz r smith na kong', 1);('thenewzealandpetroleummineralsonlineexplorationdatabase url', 1);('similaritylearningforwelllogspredictionusingmachinelearning', 1);('onepetroromanenkova e rogulina shakirov stulov n zaytsev ismailova', 1);('kovalev katterbauer k alshehri', 1);('similaritylearning', 1);('data arxiv preprint arxiv220205583 science', 1);('zealands', 1);('national rock mineral geoanalytical database', 1);('url', 1);('httppetgnscrinz doi httpsdoiorg10214209djhrp34', 1);('smith na', 1);('interpretable arxiv preprint arxiv190603731', 1);('startzman ra kuo tb', 1);('log correlation', 1);('spe formation evaluation', 1);('shazeer n parmar n uszkoreit j jones', 1);('gomez kaiser polosukhin', 1);('advancesin', 1);('neural information processing systems 30arogulina', 1);('transformerszhou h zhang peng j zhang li j xiong h zhang', 1);('ecient transformer', 1);('long sequence', 1);('proceedings aaai', 1);('articial intelligence', 1);('pp 1110611115zorastersparuchurirdarbys2004', 1);('curvealignmentforwelltowelllogcorrelationin speannualtechnicalconferenceandexhibitiononepetroarogulina', 1);('transformerslist figures1 welllinking', 1);('similar predictionfor pair', 1);('dierent wells theirsimilarity prediction', 1);('equal zero', 1);('targetsimilarity pair intervals', 1);('rocaucscoresoftransformerinformerandperformerinsiameseandtripletarchitecturesduringhyperparameter', 1);('xgboost lstmsresults romanenkova', 1);('accuracydropduringtheincreaseofthepercentageofeliminatedintervalspartsfordierentcriteriafor', 1);('important measurements 19arogulina', 1);('transformersdepthfeatureswell', 1);('1depthfeatureswell 2int', 1);('3int 2figure', 1);('welllinking', 1);('similar prediction thispair', 1);('dierent wells similarity prediction shouldequal zeroarogulina', 1);('transformersinterval', 1);('1interval 2encoderencoderconcatenationfcpart yfigure', 1);('target similarity fora pair intervalsarogulina', 1);('transformersencoder encoderanchor positive negativeencoderembeddingsembeddingspacepositiveembeddinganchorembeddingnegativeembeddingfigure', 1);('transformerssiamese triplet0506070809roc aucinformerperformertransformerxgboostsiamese lstm', 1);('fctriplet lstm eucldisttriplet lstm cosdistfigure', 1);('transformer informer performer siamese triplet', 1);('architectures hyperparameter optimization', 1);('xgboost lstms', 1);('transformers10', 1);('elements interval0506070809accuracy scoreinitial scorerandom 0random randomgradient 0gradient randomattention 0attention randomfigure', 1);('accuracy', 1);('drop increase percentage', 1);('intervals parts dierent criteria forelimination', 1);('important measurementsarogulina', 1);